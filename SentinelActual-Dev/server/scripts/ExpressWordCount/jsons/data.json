{
  "numpages": 818,
  "numrender": 818,
  "info": {
    "PDFFormatVersion": "1.7",
    "IsAcroFormPresent": false,
    "IsXFAPresent": false,
    "Title": "Linux® Command Line and Shell Scripting Bible",
    "Author": "Richard Blum",
    "Creator": "Adobe InDesign CS5 (7.0)",
    "Producer": "Acrobat Distiller 9.0.0 (Windows)",
    "CreationDate": "D:20141222161159+05'30'",
    "ModDate": "D:20141223173912+05'30'"
  },
  "metadata": {
    "_metadata": {
      "xap:modifydate": "2014-12-23T17:39:12+05:30",
      "xap:createdate": "2014-12-22T16:11:59+05:30",
      "xap:metadatadate": "2014-12-23T17:39:12+05:30",
      "xap:creatortool": "Adobe InDesign CS5 (7.0)",
      "dc:format": "application/pdf",
      "dc:title": "Linux® Command Line and Shell Scripting Bible",
      "dc:creator": "Richard Blum",
      "xapmm:documentid": "uuid:622183be-c5a6-45f8-a610-fd8cd5f1c4ea",
      "xapmm:instanceid": "uuid:25dbcf35-ea81-4011-9bb3-8a52965b0987",
      "pdf:producer": "Acrobat Distiller 9.0.0 (Windows)"
    }
  },
  "text": "\n\n\n\n\n\nffi  rs.indd  12/17/2014  Page  i\nLinux\n®\n \nCommand Line \nand Shell Scripting \nBible\nThird Edtion\n\n\n\nffi  rs.indd  12/17/2014  Page  iii\nLinux\n®\n \nCommand Line \nand Shell Scripting \nBIBLE\nThird Edition\nRichard Blum\nChristine Bresnahan\n\nffi  rs.indd  12/17/2014  Page  iv\nLinux® Command Line and Shell Scripting Bible, Third Edition\nPublished by\nJohn Wiley & Sons, Inc.\n10475 Crosspoint Boulevard\nIndianapolis, IN 46256\nwww.wiley.com\nCopyright © 2015 by John Wiley & Sons, Inc., Indianapolis, Indiana\nPublished simultaneously in Canada\nISBN: 978-1-118-98384-3\nISBN: 978-1-118-98385-0 (ebk)\nISBN: 978-1-118-98419-2 (ebk)\nManufactured in the United States of America\n10 9 8 7 6 5 4 3 2 1\nNo part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any \nmeans, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under \nSections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the \nPublisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, \n222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-8600. Requests to the Publisher for \npermission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, \nHoboken, NJ  07030, (201) 748-6011, fax (201) 748-6008, or online at \nhttp://www.wiley.com/go/permissions.\nLimit of Liability/Disclaimer of Warranty: The publisher and the author make no representations or warranties \nwith respect to the accuracy or completeness of the contents of this work and specifically disclaim all \nwarranties, including without limitation warranties of fitness for a particular purpose. No warranty may be \ncreated or extended by sales or promotional materials. The advice and strategies contained herein may not be \nsuitable for every situation. This work is sold with the understanding that the publisher is not engaged in \nrendering legal, accounting, or other professional services. If professional assistance is required, the services of \na competent professional person should be sought. Neither the publisher nor the author shall be liable for \ndamages arising herefrom. The fact that an organization or Web site is referred to in this work as a citation and/\nor a potential source of further information does not mean that the author or the publisher endorses the \ninformation the organization or website may provide or recommendations it may make. Further, readers should \nbe aware that Internet websites listed in this work may have changed or disappeared between when this work \nwas written and when it is read.\nFor general information on our other products and services please contact our Customer Care Department within \nthe United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002.\nWiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included \nwith standard print versions of this book may not be included in e-books or in print-on-demand. If this book \nrefers to media such as a CD or DVD that is not included in the version you purchased, you may download this \nmaterial at \nhttp://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com.\nLibrary of Congress Control Number: 2014954688\nTrademarks: Wiley and the Wiley logo are trademarks or registered trademarks of John Wiley & Sons, Inc. and/or \nits affiliates, in the United States and other countries, and may not be used without written permission. Linux is \na registered trademark of Linus Torvalds. All other trademarks are the property of their respective owners. John \nWiley & Sons, Inc. is not associated with any product or vendor mentioned in this book.\n\nv\nffi  rs.indd  12/17/2014  Page  v\nAbout the Authors\nRichard Blum has worked in the IT industry for more than 20 years as both a systems \nand network administrator and has published numerous Linux and open-source books. He \nhas administered UNIX, Linux, Novell, and Microsoft servers, as well as helped design and \nmaintain a 3,500-user network utilizing Cisco switches and routers. He has used Linux \nservers and shell scripts to perform automated network monitoring and has written shell \nscripts in most of the common Linux shell environments. Rich is an online instructor for \nan Introduction to Linux course that is used by colleges and universities across the United \nStates. When he isn’t being a computer nerd, Rich plays electric bass in a couple of dif-\nferent church worship bands, and enjoys spending time with his wife, Barbara, and two \ndaughters, Katie Jane and Jessica.\nChristine Bresnahan starting working with computers more than 25 years ago in the IT \nindustry as a system administrator. Christine is currently an Adjunct Professor at Ivy Tech \nCommunity College in Indianapolis, Indiana. She teaches classes on Linux system adminis-\ntration, Linux security, and Windows security.\n\n\n\nvii\nffi  rs.indd  12/17/2014  Page  vii\nAbout the Technical Editor\nKevin E. Ryan holds a bachelor’s degree in electrical engineering technology from Purdue \nUniversity and has served as system administrator for a number of computing platforms \nincluding HP-UX, Solaris, and Red Hat Linux. He’s also been involved with system plan-\nning, database management and application programming. When not pursuing his techni-\ncal endeavors, Kevin enjoys reading, baseball, and camping with his wife and their fearless \nPapillon.\n\n\n\nffi  rs.indd  12/17/2014  Page  ix\nix\nCredits\nAssociate Publisher\nJim Minatel\nProject Editor\nMartin V. Minner\nTechnical Editor\nKevin E. Ryan \nProduction Manager\nKathleen Wisor\nCopy Editor\nGwenette Gaddis \nManager of Content Development and \nAssembly\nMary Beth Wakefield \nMarketing Director\nDavid Mayhew\nMarketing Manager\nCarrie Sherrill\nProfessional Technology and Strategy \nDirector\nBarry Pruett\nBusiness Manager\nAmy Knies\nProject Coordinator, Cover\nPatrick Redmond\nProofreader\nNancy Carrasco\nIndexer\nRobert Swanson\nCover Designer\nWiley\nCover Image\niStockphoto.com / Aleksandar Velasevic\n\n\n\nxi\nffi  rs.indd  12/17/2014  Page  xi\nF\nirst, all glory and praise go to God, Who through His Son, Jesus Christ, makes all \nthings possible, and gives us the gift of eternal life.\nMany thanks go to the fantastic team of people at John Wiley & Sons for their out-\nstanding work on this project. Thanks to Mary James, the former acquisitions editor, for \noffering us the opportunity to work on this book. Also thanks to Marty Minner, the project \neditor, for keeping things on track and making this book more presentable. Thanks, Marty, \nfor all your hard work and diligence. The technical editor, Kevin E. Ryan, did a wonderful \njob of double-checking all the work in the book, plus making suggestions to improve the \ncontent. Thanks to Gwenette Gaddis, the copy editor, for her endless patience and diligence \nto make our work readable. We would also like to thank Carole McClendon at Waterside \nProductions, Inc., for arranging this opportunity for us, and for helping us out in our \nwriting careers. In addition, we would like to give a special thank you to H.L. Craft, who \nproduced several diagrams for our chapters.\nChristine would like to thank her husband, Timothy, for his encouragement, patience, and \nwillingness to listen, even when he has no idea what she is talking about.\nAcknowledgments\n\n\n\nxiii\nffi  rs.indd  12/17/2014  Page  xiii\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxi\nPart I  The Linux Command Line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nChapter 1 Starting with Linux Shells .............................................................................. 3\nChapter 2 Getting to the Shell ...................................................................................... 23\nChapter 3 Basic bash Shell Commands ............................................................................47\nChapter 4 More bash Shell Commands ............................................................................ 85\nChapter 5 Understanding the Shell ...............................................................................113\nChapter 6 Using Linux Environment Variables ...............................................................135\nChapter 7 Understanding Linux File Permissions ........................................................... 161\nChapter 8 Managing Filesystems ..................................................................................187\nChapter 9 Installing Software ...................................................................................... 211\nChapter 10 Working with Editors ..................................................................................233\nPart II  Shell Scripting Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267\nChapter 11 Basic Script Building ..................................................................................269\nChapter 12 Using Structured Commands .......................................................................297\nChapter 13 More Structured Commands ........................................................................331\nChapter 14 Handling User Input ...................................................................................365\nChapter 15 Presenting Data .........................................................................................395\nChapter 16 Script Control ............................................................................................ 419\nPart III  Advanced Shell Scripting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447\nChapter 17 Creating Functions .....................................................................................449\nChapter 18 Writing Scripts for Graphical Desktops .........................................................477\nChapter 19 Introducing sed and gawk ...........................................................................505\nChapter 20 Regular Expressions ...................................................................................535\nChapter 21 Advanced sed ............................................................................................ 561\nChapter 22 Advanced gawk ..........................................................................................591\nChapter 23 Working with Alternative Shells ..................................................................623\nContents at a Glance\n\nxiv\nffi  rs.indd  12/17/2014  Page  xiv\nPart IV  Creating Practical Scripts  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 643\nChapter 24 Writing Simple Script Utilities ....................................................................645\nChapter 25 Producing Scripts for Database, Web, and E-Mail ...........................................681\nChapter 26 Creating Fun Little Shell Scripts ..................................................................709\nAppendix A: Quick Guide to bash Commands ................................................................739\nAppendix B: Quick Guide to sed and gawk .................................................................... 751\nIndex ........................................................................................................................ 763\nContents at a Glance\n\nxv\nftoc.indd  12/08/2014  Page  xv\nContents\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxi\nPart I:  The Linux Command Line 1\nChapter 1: Starting with Linux Shells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nWhat Is Linux? ..................................................................................................... 3\nLooking into the Linux kernel ....................................................................... 4\nSystem memory management ................................................................ 5\nSoftware program management ............................................................. 6\nHardware management ......................................................................... 7\nFilesystem management ....................................................................... 8\nThe GNU utilities .......................................................................................... 9\nThe core GNU utilities .......................................................................... 9\nThe shell ........................................................................................... 10\nThe Linux desktop environment ...................................................................11\nThe X Window system ..........................................................................11\nThe KDE desktop ................................................................................ 12\nThe GNOME desktop ............................................................................ 13\nThe Unity desktop ............................................................................. 13\nOther desktops .................................................................................. 14\nLinux Distributions ............................................................................................. 17\nCore Linux distributions ............................................................................. 17\nSpecialized Linux distributions ................................................................... 18\nThe Linux LiveCD ....................................................................................... 19\nSummary ............................................................................................................ 21\nChapter 2: Getting to the Shell  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nReaching the Command Line ................................................................................ 23\nConsole terminals ....................................................................................... 24\nGraphical terminals .................................................................................... 24\nAccessing CLI via a Linux Console Terminal ........................................................... 25\nAccessing CLI via Graphical Terminal Emulation .................................................... 28\n\nxvi\nContents\nftoc.indd  12/08/2014  Page  xvi\nUsing the GNOME Terminal Emulator ..................................................................... 29\nAccessing the GNOME terminal .................................................................... 30\nThe menu bar ............................................................................................. 31\nUsing the Konsole Terminal Emulator ................................................................... 35\nAccessing the Konsole terminal ................................................................... 35\nThe menu bar ............................................................................................. 37\nUsing the xterm Terminal Emulator .......................................................................41\nAccessing xterm ......................................................................................... 42\nCommand line parameters ........................................................................... 43\nSummary ............................................................................................................ 44\nChapter 3: Basic bash Shell Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\nStarting the Shell ................................................................................................47\nUsing the Shell Prompt ........................................................................................ 48\nInteracting with the bash Manual ........................................................................ 49\nNavigating the Filesystem ................................................................................... 52\nLooking at the Linux fi lesystem .................................................................. 52\nTraversing directories ................................................................................. 55\nUsing absolute directory references ..................................................... 56\nUsing relative directory references ...................................................... 57\nListing Files and Directories ................................................................................ 59\nDisplaying a basic listing ............................................................................ 59\nDisplaying a long listing ..............................................................................61\nFiltering listing output ............................................................................... 62\nHandling Files .................................................................................................... 64\nCreating fi les .............................................................................................. 64\nCopying fi les .............................................................................................. 65\nUsing tab auto-complete ............................................................................. 68\nLinking fi les .............................................................................................. 68\nRenaming fi les ........................................................................................... 70\nDeleting fi les.............................................................................................. 72\nManaging Directories .......................................................................................... 73\nCreating directories .................................................................................... 73\nDeleting directories .....................................................................................74\nViewing File Contents .......................................................................................... 77\nViewing the fi le type .................................................................................. 77\nViewing the whole fi le ................................................................................ 78\nUsing the cat command ...................................................................... 78\nUsing the more command ................................................................... 79\nUsing the less command ..................................................................... 80\nViewing parts of a fi le ................................................................................. 81\nUsing the tail command ..................................................................... 81\nUsing the head command .................................................................... 82\nSummary ............................................................................................................ 83\n\nxvii\nContents\nftoc.indd  12/08/2014  Page  xvii\nChapter 4: More bash Shell Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nMonitoring Programs ........................................................................................... 85\nPeeking at the processes ............................................................................. 85\nUnix-style parameters ........................................................................ 86\nBSD-style parameters ......................................................................... 89\nThe GNU long parameters .................................................................... 91\nReal-time process monitoring ...................................................................... 92\nStopping processes ..................................................................................... 95\nThe kill command .............................................................................. 95\nThe killall command ........................................................................... 96\nMonitoring Disk Space ......................................................................................... 96\nMounting media ......................................................................................... 97\nThe mount command .......................................................................... 97\nThe unmount command ...................................................................... 99\nUsing the df command ...............................................................................100\nUsing the du command ..............................................................................101\nWorking with Data Files ......................................................................................102\nSorting data ..............................................................................................102\nSearching for data .....................................................................................107\nCompressing data ......................................................................................108\nArchiving data .......................................................................................... 110\nSummary ........................................................................................................... 111\nChapter 5: Understanding the Shell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\nExploring Shell Types .........................................................................................113\nExploring Parent and Child Shell Relationships .................................................... 115\nLooking at process lists ............................................................................. 119\nCreatively using subshells ..........................................................................121\nInvestigating background mode .........................................................121\nPutting process lists into the background ...........................................123\nLooking at co-processing ...................................................................124\nUnderstanding Shell Built-In Commands ..............................................................125\nLooking at external commands ...................................................................125\nLooking at built-in commands ....................................................................127\nUsing the history command ...............................................................128\nUsing command aliases .....................................................................131\nSummary ...........................................................................................................132\nChapter 6: Using Linux Environment Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\nExploring Environment Variables.........................................................................135\nLooking at global environment variables .....................................................136\nLooking at local environment variables .......................................................138\nSetting User-Defi ned Variables ............................................................................138\nSetting local user-defi ned variables ............................................................139\n\nxviii\nContents\nftoc.indd  12/08/2014  Page  xviii\nSetting global environment variables ..........................................................140\nRemoving Environment Variables ........................................................................142\nUncovering Default Shell Environment Variables ..................................................143\nSetting the PATH Environment Variable ...............................................................148\nLocating System Environment Variables ...............................................................150\nUnderstanding the login shell process .........................................................150\nViewing the /etc/profi le fi le .............................................................. 151\nViewing the $HOME startup fi les ........................................................154\nUnderstanding the interactive shell process ................................................156\nUnderstanding the non-interactive shell process ..........................................156\nMaking environment variables persistent ....................................................157\nLearning about Variable Arrays ...........................................................................158\nSummary ...........................................................................................................159\nChapter 7: Understanding Linux File Permissions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\nLinux Security ................................................................................................... 161\nThe /etc/passwd fi le ..................................................................................162\nThe /etc/shadow fi le ..................................................................................164\nAdding a new user .....................................................................................164\nRemoving a user ........................................................................................168\nModifying a user .......................................................................................168\nusermod ...........................................................................................169\npasswd and chpasswd ........................................................................169\nchsh, chfn, and chage ....................................................................... 170\nUsing Linux Groups ............................................................................................ 172\nThe /etc/group fi le ....................................................................................173\nCreating new groups .................................................................................. 174\nModifying groups ...................................................................................... 175\nDecoding File Permissions................................................................................... 175\nUsing fi le permission symbols .................................................................... 176\nDefault fi le permissions .............................................................................177\nChanging Security Settings ................................................................................ 179\nChanging permissions ................................................................................ 179\nChanging ownership ..................................................................................181\nSharing Files .....................................................................................................182\nSummary ...........................................................................................................184\nChapter 8: Managing Filesystems  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\nExploring Linux Filesystems ...............................................................................187\nUnderstanding the basic Linux fi lesystems ..................................................188\nLooking at the ext fi lesystem ............................................................188\nLooking at the ext2 fi lesystem ...........................................................188\nUnderstanding journaling fi lesystems .........................................................189\nLooking at the ext3 fi lesystem ...........................................................190\nLooking at the ext4 fi lesystem ...........................................................190\nLooking at the Reiser fi lesystem ........................................................190\n\nxix\nContents\nftoc.indd  12/08/2014  Page  xix\nLooking at the journaled fi lesystem ...................................................191\nLooking at the XFS fi lesystem ............................................................191\nUnderstanding the copy-on-write fi lesystems ..............................................192\nLooking at the ZFS fi lesystem ............................................................192\nLooking at the Btrfs fi lesystem ..........................................................192\nWorking with Filesystems ...................................................................................192\nCreating partitions ....................................................................................193\nCreating a fi lesystem .................................................................................196\nChecking and repairing a fi lesystem ...........................................................198\nManaging Logical Volumes ..................................................................................200\nExploring logical volume management layout ..............................................200\nUsing the LVM in Linux ..............................................................................201\nTaking a snapshot .............................................................................202\nStriping ...........................................................................................202\nMirroring .........................................................................................202\nUsing the Linux LVM ..................................................................................203\nDefi ning physical volumes .................................................................203\nCreating volume groups .....................................................................205\nCreating logical volumes ....................................................................206\nCreating the fi lesystem .....................................................................208\nModifying the LVM ............................................................................209\nSummary ...........................................................................................................210\nChapter 9: Installing Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\nPackage Management Primer ............................................................................... 211\nThe Debian-Based Systems ..................................................................................212\nManaging packages with aptitude ...............................................................212\nInstalling software packages with aptitude .................................................215\nUpdating software with aptitude ................................................................ 217\nUninstalling software with aptitude ...........................................................218\nThe aptitude repositories ...........................................................................219\nThe Red Hat–Based Systems ................................................................................221\nListing installed packages ..........................................................................221\nInstalling software with yum .....................................................................223\nUpdating software with yum ......................................................................224\nUninstalling software with yum .................................................................225\nDealing with broken dependencies ..............................................................225\nYum repositories .......................................................................................227\nInstalling from Source Code ................................................................................228\nSummary ...........................................................................................................232\nChapter 10: Working with Editors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\nVisiting the vim Editor .......................................................................................233\nChecking your vim package ........................................................................234\nExploring vim basics ..................................................................................235\nEditing data ..............................................................................................238\n\nxx\nContents\nftoc.indd  12/08/2014  Page  xx\nCopying and pasting ..................................................................................238\nSearching and substituting ........................................................................239\nNavigating the nano Editor .................................................................................240\nExploring the emacs Editor .................................................................................242\nChecking your emacs package ....................................................................243\nUsing emacs on the console ........................................................................245\nExploring the basics of emacs ............................................................245\nEditing data ..................................................................................... 247\nCopying and pasting ......................................................................... 247\nSearching and replacing ....................................................................248\nUsing buffers in emacs ......................................................................248\nUsing windows in console mode emacs ...............................................249\nUsing emacs in a GUI .................................................................................250\nExploring the KDE Family of Editors .................................................................... 251\nLooking at the KWrite editor ...................................................................... 251\nLooking at the Kate editor .........................................................................256\nExploring the GNOME Editor ................................................................................260\nStarting gedit ...........................................................................................260\nUnderstanding basic gedit features .............................................................262\nSetting preferences ...................................................................................262\nSetting view preferences ...................................................................262\nSetting editor preferences .................................................................263\nSetting font & color preferences .........................................................264\nManaging plug-ins ............................................................................264\nSummary ...........................................................................................................265\nPart II:  Shell Scripting Basics 267\nChapter 11: Basic Script Building  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\nUsing Multiple Commands ...................................................................................269\nCreating a Script File ..........................................................................................270\nDisplaying Messages ...........................................................................................272\nUsing Variables .................................................................................................. 274\nEnvironment variables ............................................................................... 274\nUser variables ...........................................................................................275\nCommand substitution ...............................................................................277\nRedirecting Input and Output .............................................................................279\nOutput redirection .....................................................................................279\nInput redirection .......................................................................................280\nPipes .................................................................................................................281\nPerforming Math ................................................................................................285\nThe expr command ....................................................................................285\nUsing brackets ..........................................................................................287\nA fl oating-point solution ...........................................................................288\n\nxxi\nContents\nftoc.indd  12/08/2014  Page  xxi\nThe basics of bc ................................................................................288\nUsing bc in scripts ............................................................................289\nExiting the Script ..............................................................................................292\nChecking the exit status ............................................................................292\nThe exit command .....................................................................................293\nSummary ...........................................................................................................295\nChapter 12: Using Structured Commands  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297\nWorking with the if-then Statement ....................................................................297\nExploring the if-then-else Statement ...................................................................300\nNesting ifs .........................................................................................................301\nTrying the test Command ...................................................................................304\nUsing numeric comparisons ........................................................................307\nUsing string comparisons ...........................................................................308\nLooking at string equality .................................................................309\nLooking at string order ..................................................................... 310\nLooking at string size .......................................................................312\nUsing fi le comparisons ...............................................................................313\nChecking directories ......................................................................... 314\nChecking whether an object exists .....................................................315\nChecking for a fi le ............................................................................. 316\nChecking for read access .................................................................... 317\nChecking for empty fi les ....................................................................318\nChecking whether you can write to a fi le ............................................319\nChecking whether you can run a fi le ..................................................321\nChecking ownership ..........................................................................321\nChecking default group membership ...................................................322\nChecking fi le date .............................................................................322\nConsidering Compound Testing............................................................................324\nWorking with Advanced if-then Features .............................................................325\nUsing double parentheses ...........................................................................325\nUsing double brackets ................................................................................326\nConsidering the case Command ...........................................................................327\nSummary ...........................................................................................................329\nChapter 13: More Structured Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\nThe for Command ...............................................................................................331\nReading values in a list ..............................................................................332\nReading complex values in a list .................................................................333\nReading a list from a variable .....................................................................335\nReading values from a command .................................................................336\nChanging the fi eld separator ......................................................................337\nReading a directory using wildcards ...........................................................339\nThe C-Style for Command ....................................................................................341\nThe C language for command ......................................................................341\n\nxxii\nContents\nftoc.indd  12/08/2014  Page  xxii\nUsing multiple variables.............................................................................342\nThe while Command ...........................................................................................343\nBasic while format .....................................................................................343\nUsing multiple test commands ....................................................................344\nThe until Command ............................................................................................346\nNesting Loops ....................................................................................................347\nLooping on File Data ..........................................................................................350\nControlling the Loop .......................................................................................... 351\nThe break command ...................................................................................352\nBreaking out of a single loop .............................................................352\nBreaking out of an inner loop ............................................................353\nBreaking out of an outer loop ............................................................354\nThe continue command ..............................................................................355\nProcessing the Output of a Loop ..........................................................................358\nPractical Examples .............................................................................................359\nFinding executable fi les .............................................................................359\nCreating multiple user accounts .................................................................. 361\nSummary ...........................................................................................................362\nChapter 14: Handling User Input  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365\nPassing Parameters ............................................................................................365\nReading parameters ...................................................................................366\nReading the script name ............................................................................368\nTesting parameters .................................................................................... 370\nUsing Special Parameter Variables ....................................................................... 371\nCounting parameters ................................................................................. 371\nGrabbing all the data .................................................................................373\nBeing Shifty ...................................................................................................... 375\nWorking with Options ......................................................................................... 376\nFinding your options ................................................................................. 376\nProcessing simple options ..................................................................377\nSeparating options from parameters ...................................................378\nProcessing options with values ..........................................................379\nUsing the getopt command .........................................................................380\nLooking at the command format ........................................................381\nUsing getopt in your scripts...............................................................382\nAdvancing to getopts .................................................................................384\nStandardizing Options ........................................................................................387\nGetting User Input .............................................................................................388\nReading basics ..........................................................................................388\nTiming out ................................................................................................389\nReading with no display ............................................................................391\nReading from a fi le ....................................................................................391\nSummary ...........................................................................................................392\n\nxxiii\nContents\nftoc.indd  12/08/2014  Page  xxiii\nChapter 15: Presenting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395\nUnderstanding Input and Output ........................................................................395\nStandard fi le descriptors ............................................................................395\nSTDIN ...............................................................................................396\nSTDOUT ............................................................................................397\nSTDERR ............................................................................................398\nRedirecting errors .....................................................................................398\nRedirecting errors only ......................................................................398\nRedirecting errors and data ...............................................................399\nRedirecting Output in Scripts .............................................................................400\nTemporary redirections ..............................................................................400\nPermanent redirections ..............................................................................401\nRedirecting Input in Scripts ...............................................................................402\nCreating Your Own Redirection ............................................................................403\nCreating output fi le descriptors ..................................................................403\nRedirecting fi le descriptors ........................................................................404\nCreating input fi le descriptors ....................................................................405\nCreating a read/write fi le descriptor ...........................................................406\nClosing fi le descriptors ...............................................................................407\nListing Open File Descriptors ..............................................................................408\nSuppressing Command Output ............................................................................. 410\nUsing Temporary Files ........................................................................................ 411\nCreating a local temporary fi le .................................................................... 411\nCreating a temporary fi le in /tmp ............................................................... 413\nCreating a temporary directory ................................................................... 413\nLogging Messages ............................................................................................... 414\nPractical Example .............................................................................................. 416\nSummary ........................................................................................................... 418\nChapter 16: Script Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\nHandling Signals................................................................................................ 419\nSignaling the bash shell ............................................................................. 419\nGenerating signals .....................................................................................420\nInterrupting a process .......................................................................420\nPausing a process ..............................................................................421\nTrapping signals ........................................................................................422\nTrapping a script exit ................................................................................423\nModifying or removing a trap .....................................................................424\nRunning Scripts in Background Mode ..................................................................427\nRunning in the background ........................................................................427\nRunning multiple background jobs ..............................................................429\nRunning Scripts without a Hang-Up ....................................................................430\nControlling the Job ............................................................................................432\nViewing jobs .............................................................................................432\nRestarting stopped jobs .............................................................................434\n\nxxiv\nContents\nftoc.indd  12/08/2014  Page  xxiv\nBeing Nice .........................................................................................................436\nUsing the nice command ............................................................................436\nUsing the renice command .........................................................................437\nRunning Like Clockwork .....................................................................................438\nScheduling a job using the at command ......................................................438\nUnderstanding the at command format ...............................................438\nRetrieving job output ........................................................................439\nListing pending jobs .........................................................................440\nRemoving jobs ..................................................................................441\nScheduling regular scripts ..........................................................................441\nLooking at the cron table ..................................................................441\nBuilding the cron table .....................................................................442\nViewing cron directories ....................................................................443\nLooking at the anacron program ........................................................443\nStarting scripts with a new shell ................................................................445\nSummary ...........................................................................................................446\nPart III:  Advanced Shell Scripting 447\nChapter 17: Creating Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449\nBasic Script Functions ........................................................................................449\nCreating a function ...................................................................................450\nUsing functions .........................................................................................450\nReturning a Value ..............................................................................................453\nThe default exit status ...............................................................................453\nUsing the return command .........................................................................454\nUsing function output ...............................................................................455\nUsing Variables in Functions ...............................................................................456\nPassing parameters to a function ................................................................456\nHandling variables in a function ................................................................459\nGlobal variables ................................................................................459\nLocal variables ..................................................................................460\nArray Variables and Functions ............................................................................461\nPassing arrays to functions ........................................................................461\nReturning arrays from functions ................................................................463\nFunction Recursion ............................................................................................464\nCreating a Library ..............................................................................................465\nUsing Functions on the Command Line ................................................................467\nCreating functions on the command line .....................................................468\nDefi ning functions in the .bashrc fi le ..........................................................468\nDirectly defi ning functions ................................................................469\nSourcing function fi les ......................................................................469\nFollowing a Practical Example ............................................................................. 470\nDownloading and installing ....................................................................... 471\nBuilding the library ................................................................................... 471\n\nxxv\nContents\nftoc.indd  12/08/2014  Page  xxv\nThe shtool library functions ....................................................................... 472\nUsing the library .......................................................................................473\nSummary ........................................................................................................... 474\nChapter 18: Writing Scripts for Graphical Desktops . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477\nCreating Text Menus ...........................................................................................477\nCreate the menu layout .............................................................................. 478\nCreate the menu functions ......................................................................... 479\nAdd the menu logic ...................................................................................480\nPutting it all together................................................................................481\nUsing the select command ..........................................................................482\nDoing Windows ..................................................................................................484\nThe dialog package ....................................................................................484\nThe msgbox widget ...........................................................................486\nThe yesno widget ..............................................................................487\nThe inputbox widget .........................................................................487\nThe textbox widget ...........................................................................488\nThe menu widget ..............................................................................489\nThe fselect widget .............................................................................490\nThe dialog options .....................................................................................491\nUsing the dialog command in a script .........................................................493\nGetting Graphic .................................................................................................496\nThe KDE environment ................................................................................496\nkdialog widgets ................................................................................496\nUsing kdialog ...................................................................................498\nThe GNOME environment ............................................................................500\nzenity widgets ..................................................................................500\nUsing zenity in scripts ......................................................................501\nSummary ...........................................................................................................504\nChapter 19: Introducing sed and gawk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505\nManipulating Text ..............................................................................................505\nGetting to know the sed editor ...................................................................505\nDefi ning an editor command in the command line ...............................506\nUsing multiple editor commands in the command line .........................507\nReading editor commands from a fi le ..................................................508\nGetting to know the gawk program .............................................................509\nVisiting the gawk command format .................................................... 510\nReading the program script from the command line ............................ 510\nUsing data fi eld variables .................................................................. 511\nUsing multiple commands in the program script .................................. 512\nReading the program from a fi le ......................................................... 513\nRunning scripts before processing data .............................................. 514\nRunning scripts after processing data ................................................ 514\nCommanding at the sed Editor Basics................................................................... 516\nIntroducing more substitution options ........................................................ 516\n\nxxvi\nContents\nftoc.indd  12/08/2014  Page  xxvi\nSubstituting fl ags ............................................................................. 516\nReplacing characters ......................................................................... 518\nUsing addresses ......................................................................................... 518\nAddressing the numeric line .............................................................. 519\nUsing text pattern fi lters ...................................................................520\nGrouping commands ..........................................................................520\nDeleting lines............................................................................................521\nInserting and appending text .....................................................................523\nChanging lines ..........................................................................................525\nTransforming characters ............................................................................527\nPrinting revisited ......................................................................................527\nPrinting lines ...................................................................................528\nPrinting line numbers .......................................................................529\nListing lines .....................................................................................529\nUsing fi les with sed ...................................................................................530\nWriting to a fi le ................................................................................530\nReading data from a fi le ....................................................................531\nSummary ...........................................................................................................533\nChapter 20: Regular Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535\nWhat Are Regular Expressions? ...........................................................................535\nA defi nition ..............................................................................................535\nTypes of regular expressions .......................................................................536\nDefi ning BRE Patterns ........................................................................................537\nPlain text .................................................................................................537\nSpecial characters .....................................................................................539\nAnchor characters .....................................................................................540\nStarting at the beginning ..................................................................540\nLooking for the ending ......................................................................541\nCombining anchors ...........................................................................542\nThe dot character ......................................................................................542\nCharacter classes .......................................................................................543\nNegating character classes .........................................................................546\nUsing ranges .............................................................................................546\nSpecial character classes ............................................................................547\nThe asterisk ..............................................................................................548\nExtended Regular Expressions .............................................................................549\nThe question mark .....................................................................................550\nThe plus sign............................................................................................. 551\nUsing braces .............................................................................................. 551\nThe pipe symbol ........................................................................................553\nGrouping expressions .................................................................................553\nRegular Expressions in Action .............................................................................554\nCounting directory fi les .............................................................................554\nValidating a phone number ........................................................................556\n\nxxvii\nContents\nftoc.indd  12/08/2014  Page  xxvii\nParsing an e-mail address...........................................................................558\nSummary ...........................................................................................................560\nChapter 21: Advanced sed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561\nLooking at Multiline Commands .......................................................................... 561\nNavigating the next command ....................................................................562\nUsing the single-line next command ..................................................562\nCombining lines of text .....................................................................563\nNavigating the multiline delete command ...................................................566\nNavigating the multiline print command .....................................................567\nHolding Space ....................................................................................................567\nNegating a Command ..........................................................................................569\nChanging the Flow ............................................................................................. 572\nBranching ................................................................................................. 572\nTesting ..................................................................................................... 574\nReplacing via a Pattern ...................................................................................... 575\nUsing the ampersand ................................................................................. 576\nReplacing individual words ........................................................................ 576\nPlacing sed Commands in Scripts.........................................................................577\nUsing wrappers ..........................................................................................578\nRedirecting sed output ..............................................................................578\nCreating sed Utilities..........................................................................................579\nSpacing with double lines ..........................................................................579\nSpacing fi les that may have blanks .............................................................580\nNumbering lines in a fi le ............................................................................581\nPrinting last lines .....................................................................................582\nDeleting lines............................................................................................584\nDeleting consecutive blank lines ........................................................584\nDeleting leading blank lines ..............................................................585\nDeleting trailing blank lines ..............................................................586\nRemoving HTML tags .................................................................................586\nSummary ...........................................................................................................588\nChapter 22: Advanced gawk  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591\nUsing Variables ..................................................................................................591\nBuilt-in variables .......................................................................................592\nThe fi eld and record separator variables ..............................................592\nData variables ...................................................................................595\nUser-defi ned variables ...............................................................................598\nAssigning variables in scripts ............................................................598\nAssigning variables on the command line ...........................................599\nWorking with Arrays ..........................................................................................600\nDefi ning array variables .............................................................................600\nIterating through array variables................................................................601\nDeleting array variables .............................................................................602\n\nxxviii\nContents\nftoc.indd  12/08/2014  Page  xxviii\nUsing Patterns ...................................................................................................602\nRegular expressions ...................................................................................603\nThe matching operator ...............................................................................603\nMathematical expressions ..........................................................................604\nStructured Commands ........................................................................................605\nThe if statement ........................................................................................605\nThe while statement ..................................................................................607\nThe do-while statement .............................................................................608\nThe for statement ......................................................................................609\nFormatted Printing ............................................................................................ 610\nBuilt-In Functions .............................................................................................. 613\nMathematical functions ............................................................................. 613\nString functions ........................................................................................ 615\nTime functions .......................................................................................... 616\nUser-Defi ned Functions ....................................................................................... 617\nDefi ning a function ................................................................................... 617\nUsing your functions ................................................................................. 618\nCreating a function library ......................................................................... 619\nWorking through a Practical Example ..................................................................620\nSummary ...........................................................................................................621\nChapter 23: Working with Alternative Shells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623\nWhat Is the dash Shell? ......................................................................................623\nThe dash Shell Features ......................................................................................624\nThe dash command line parameters ............................................................624\nThe dash environment variables .................................................................625\nDefault environment variables ...........................................................626\nPositional parameters ........................................................................627\nUser-defi ned environment variables ...................................................627\nThe dash built-in commands .......................................................................628\nScripting in dash ...............................................................................................629\nCreating dash scripts .................................................................................629\nThings that don’t work ...............................................................................629\nUsing arithmetic ...............................................................................629\nThe test command .............................................................................630\nThe function Command .....................................................................631\nThe zsh Shell .....................................................................................................632\nParts of the zsh Shell .........................................................................................632\nShell options .............................................................................................632\nBuilt-in commands ....................................................................................633\nCore built-in commands .....................................................................634\nAdd-in modules .................................................................................636\nViewing, adding, and removing modules .............................................637\nScripting with zsh .............................................................................................638\nMathematical operations ............................................................................639\n\nxxix\nContents\nftoc.indd  12/08/2014  Page  xxix\nPerforming calculations .....................................................................639\nMathematical functions .....................................................................640\nStructured commands ................................................................................640\nFunctions .................................................................................................641\nSummary ...........................................................................................................642\nPart IV:  Creating Practical Scripts 643\nChapter 24 Writing Simple Script Utilities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\nPerforming Archives ...........................................................................................645\nArchiving data fi les ...................................................................................645\nObtaining the required functions .......................................................646\nCreating a daily archive location ........................................................648\nCreating a daily archive script ...........................................................649\nRunning the daily archive script ........................................................ 651\nCreating an hourly archive script .......................................................652\nRunning the hourly archive script ......................................................655\nManaging User Accounts .....................................................................................656\nObtaining the required functions ...............................................................657\nGetting the correct account name ......................................................657\nCreating a function to get the correct account name ...........................658\nVerifying the entered account name ...................................................660\nDetermining whether the account exists ............................................ 661\nRemoving any account processes ........................................................662\nFinding account fi les .........................................................................664\nRemoving the account .......................................................................665\nCreating the script ....................................................................................665\nRunning the script ....................................................................................671\nMonitoring Disk Space ........................................................................................673\nObtaining the required functions ...............................................................673\nCreating the script .................................................................................... 676\nRunning the script ....................................................................................677\nSummary ...........................................................................................................678\nChapter 25: Producing Scripts for Database, Web, and E-Mail . . . . . . . . . . . . . . . . . . . . 681\nUsing a MySQL Database .....................................................................................681\nUsing MySQL .............................................................................................682\nConnecting to the server ...................................................................682\nThe mysql commands ........................................................................683\nCreating a database ...........................................................................685\nCreating a user account .....................................................................687\nCreating a table ................................................................................688\nInserting and deleting data ...............................................................690\n\nxxx\nContents\nftoc.indd  12/08/2014  Page  xxx\nQuerying data...................................................................................691\nUsing the database in your scripts ..............................................................692\nLogging into the server .....................................................................692\nSending commands to the server ........................................................693\nFormatting data ................................................................................696\nUsing the Web....................................................................................................697\nInstalling Lynx .........................................................................................698\nThe lynx command line ..............................................................................699\nThe Lynx confi guration fi le ........................................................................700\nCapturing data from Lynx ..........................................................................701\nUsing E-Mail ......................................................................................................704\nSummary ...........................................................................................................708\nChapter 26: Creating Fun Little Shell Scripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 709\nSending a Message .............................................................................................709\nUnderstanding the required functions ........................................................709\nDetermining who is on the system ..................................................... 710\nAllowing messages ............................................................................ 710\nSending a message to another user ..................................................... 711\nCreating the script ....................................................................................712\nChecking if user is logged on .............................................................713\nChecking if user accepts messages ...................................................... 714\nChecking if message was included ......................................................715\nTransmitting a simple message ..........................................................715\nTransmitting a long message ............................................................. 716\nObtaining a Quote ..............................................................................................720\nUnderstanding the required functions ........................................................720\nLearning about the wget utility .........................................................720\nTesting a web address ........................................................................723\nCreating the script ....................................................................................724\nChecking the passed URL ...................................................................724\nObtaining web page information ........................................................726\nParsing out the desired information ...................................................727\nGenerating an Excuse .........................................................................................731\nUnderstanding the required functions ........................................................732\nLearning about curl ..........................................................................732\nChoosing to use e-mail ......................................................................734\nCreating the script ....................................................................................735\nSummary ...........................................................................................................737\nAppendix A: Quick Guide to bash Commands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 739\nAppendix B: Quick Guide to sed and gawk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 751\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 763\n\nxxxi\nfl  ast.indd  12/09/2014  Page  xxxi\nIntroduction\nW\nelcome to the third edition of Linux Command Line and Shell Scripting Bible. Like all books in \nthe Bible series, you can expect to fi nd both hands-on tutorials and real-world information, \nas well as reference and background information that provide a context for what you are \nlearning. This book is a fairly comprehensive resource on the Linux command line and shell com-\nmands. By the time you have completed Linux Command Line and Shell Scripting Bible, you will be \nwell prepared to write your own shell scripts that can automate practically any task on your Linux \nsystem.\nWho Should Read This Book\nIf you’re a system administrator in a Linux environment, you’ll benefi t greatly by knowing how to \nwrite shell scripts. The book doesn’t walk you through the process of setting up a Linux system, \nbut after you have it running, you’ll want to start automating some of the routine administrative \ntasks. That’s where shell scripting comes in, and that’s where this book helps you out. This book \ndemonstrates how to automate any administrative task using shell scripts, from monitoring system \nstatistics and data fi les to generating reports for your boss.\nIf you’re a home Linux enthusiast, you’ll also benefi t from Linux Command Line and Shell Scripting \nBible. Nowadays, it’s easy to get lost in the graphical world of pre-built widgets. Most desktop Linux \ndistributions try their best to hide the Linux system from the typical user. However, sometimes \nyou must know what’s going on under the hood. This book shows you how to access the Linux com-\nmand line prompt and what to do when you get there. Often, performing simple tasks, such as fi le \nmanagement, can be done more quickly from the command line than from a fancy graphical inter-\nface. You can use a wealth of commands from the command line, and this book shows you how to \nuse them.\nHow This Book Is Organized\nThis book leads you through the basics of the Linux command line and into more complicated top-\nics, such as creating your own shell scripts. The book is divided into four parts, each one building \non the previous parts.\nPart I assumes that you either have a Linux system running or are looking into getting a Linux \nsystem. Chapter 1, “Starting with Linux Shells,” describes the parts of a total Linux system and \n\nxxxii\nIntroduction\nfl  ast.indd  12/09/2014  Page  xxxii\nshows how the shell fi ts in. After describing the basics of the Linux system, this part con-\ntinues with the following:\n ■\nUsing a terminal emulation package to access the shell (Chapter 2)\n ■\nIntroducing the basic shell commands (Chapter 3)\n ■\nUsing more advanced shell commands to peek at system information (Chapter 4)\n ■\nUnderstanding what the shell is used for (Chapter 5)\n ■\nWorking with shell variables to manipulate data (Chapter 6)\n ■\nUnderstanding the Linux fi lesystem and security (Chapter 7)\n ■\nWorking with Linux fi lesystems from the command line (Chapter 8)\n ■\nInstalling and updating software from the command line (Chapter 9)\n ■\nUsing the Linux editors to start writing shell scripts (Chapter 10)\nIn Part II, you begin writing shell scripts. As you go through the chapters, you’ll do the \nfollowing:\n ■\nLearn how to create and run shell scripts (Chapter 11)\n ■\nAlter the program fl ow in a shell script (Chapter 12)\n ■\nIterate through code sections (Chapter 13)\n ■\nHandle data from the user in your scripts (Chapter 14)\n ■\nSee different methods for storing and displaying data from your Script (Chapter 15)\n ■\nControl how and when your shell scripts run on the system (Chapter 16)\nPart III dives into more advanced areas of shell script programming, including these things:\n ■\nCreating your own functions to use in all your scripts (Chapter 17)\n ■\nUtilizing the Linux graphical desktop for interacting with your script users \n(Chapter 18)\n ■\nUsing advanced Linux commands to fi lter and parse data fi les (Chapter 19)\n ■\nUsing regular expressions to defi ne data (Chapter 20)\n ■\nLearning advanced methods of manipulating data in your scripts (Chapter 21)\n ■\nGenerating reports from raw data (Chapter 22)\n ■\nModifying your shell scripts to run in other Linux shells (Chapter 23)\nThe last section of the book, Part IV, demonstrates how to use shell scripts in real-world \nenvironments. In this part, you will learn these things:\n ■\nHow to put all the scripting features together to write your own scripts (Chapter \n24)\n\nxxxiii\nIntroduction\nfl  ast.indd  12/09/2014  Page  xxxiii\n ■\nHow to store and retrieve data using databases, access data on the Internet, and \nsend e-mail messages (Chapter 25)\n ■\nWrite more advanced shell scripts to interact on your Linux system (Chapter 26)\nCautions, Tips, and Notes\nYou will fi nd many different organizational and typographical features throughout this \nbook designed to help you get the most of the information.\nThis information is important and is set off in a separate paragraph with a special icon. Cautions provide information \nabout things to watch out for, whether simply inconvenient or potentially hazardous to your data or systems.\nTips provide helpful advice to make your work easier and more effective. Tips may suggest a solution to a problem or \na better way to accomplish a task.\nNotes provide additional, ancillary information that is helpful, but somewhat outside of the current presentation of \ninformation.\nDownloadable code\nYou can obtain the book’s code fi les at www.wiley.com/go/linuxcommandline.\nMinimum Requirements\nLinux Command Line and Shell Scripting Bible doesn’t focus on any specifi c Linux distribu-\ntion, so you can follow along in the book using any Linux system you have available. \nThe bulk of the book references the bash shell, which is the default shell for most Linux \nsystems.\n\nxxxiv\nIntroduction\nfl  ast.indd  12/09/2014  Page  xxxiv\nWhere to Go from Here\nAfter you’ve fi nished reading Linux Command Line and Shell Scripting Bible, you’re well on \nyour way to incorporating Linux commands in your daily Linux work. In the ever-changing \nworld of Linux, it’s always a good idea to stay in touch with new developments. Often, \nLinux distributions change, adding new features and removing older ones. To keep your \nknowledge of Linux fresh, always stay well-informed. Find a good Linux forum site and \nmonitor what’s happening in the Linux world. Many popular Linux news sites, such as \nSlashdot and Distrowatch, provide up-to-the-minute information about new advances in \nLinux.\n\nc01.indd  12/16/2014  Page  1\nPart I\nThe Linux Command Line\nIN THIS PART\nChapter 1\nStarting with Linux Shells\nChapter 2\nGetting to the Shell\nChapter 3\nBasic bash Shell Commands\nChapter 4\nMore bash Shell Commands\nChapter 5\nUnderstanding the Shell\nChapter 6\nUsing Linux Environment Variables\nChapter 7\nUnderstanding Linux File Permissions\nChapter 8\nManaging Filesystems\nChapter 9\nInstalling Software\nChapter 10\nWorking with Editors\n\n\n\n3\nc01.indd  12/16/2014  Page  3\nCHAPTER \n1\nStarting with Linux Shells\nIN THIS CHAPTER\nWhat is Linux?\nParts of the Linux kernel\nExploring the Linux desktop\nVisiting Linux distributions\nB\nefore you can dive into working with the Linux command line and shells, you should fi rst \nunderstand what Linux is, where it came from, and how it works. This chapter walks you \nthrough what Linux is and explains where the shell and command line fi t in the overall \nLinux picture.\nWhat Is Linux?\nIf you’ve never worked with Linux before, you may be confused about why so many different \nversions are available. I’m sure you have been confused by various terms such as distribution, \nLiveCD, and GNU when looking at Linux packages. Wading through the world of Linux for the fi rst \ntime can be a tricky experience. This chapter takes some of the mystery out of the Linux system \nbefore you start working on commands and scripts.\nFirst, four main parts make up a Linux system:\n ■\nThe Linux kernel\n ■\nThe GNU utilities\n ■\nA graphical desktop environment\n ■\nApplication software\nEach of these parts has a specifi c job in the Linux system. No part is very useful by itself. \nFigure 1-1 shows a basic diagram of how the parts fi t together to create the overall Linux system.\n\n4\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  4\nFIGURE 1-1\nThe Linux system\nApplication Software\nWindow\nManagement\nSoftware\nGNU\nSystem\nUtilities\nLinux Kernel\nComputer Hardware\nThis section describes these four main parts in detail and gives you an overview of how \nthey work together to create a complete Linux system.\nLooking into the Linux Kernel\nThe core of the Linux system is the kernel. The kernel controls all the hardware and soft-\nware on the computer system, allocating hardware when necessary and executing software \nwhen required.\nIf you’ve been following the Linux world at all, no doubt you’ve heard the name Linus \nTorvalds. Linus is the person responsible for creating the fi rst Linux kernel software when \nhe was a student at the University of Helsinki. He intended it to be a copy of the Unix \nsystem, at the time a popular operating system used at many universities.\nAfter developing the Linux kernel, Linus released it to the Internet community and solic-\nited suggestions for improving it. This simple process started a revolution in the world of \ncomputer operating systems. Soon Linus was receiving suggestions from students as well as \nprofessional programmers from around the world.\nAllowing anyone to change programming code in the kernel would result in complete chaos. \nTo simplify things, Linus acted as a central point for all improvement suggestions. It was \nultimately Linus’s decision whether or not to incorporate suggested code in the kernel. \n\n5\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  5\n1\n1\nThis same concept is still in place with the Linux kernel code, except that instead of just \nLinus controlling the kernel code, a team of developers has taken on the task.\nThe kernel is primarily responsible for four main functions:\n ■\nSystem memory management\n ■\nSoftware program management\n ■\nHardware management\n ■\nFilesystem management\nThe following sections explore each of these functions in more detail.\nSystem Memory Management\nOne of the primary functions of the operating system kernel is memory management. Not \nonly does the kernel manage the physical memory available on the server, but it can also \ncreate and manage virtual memory, or memory that does not actually exist.\nIt does this by using space on the hard disk, called the swap space. The kernel swaps the \ncontents of virtual memory locations back and forth from the swap space to the actual \nphysical memory. This allows the system to think there is more memory available than \nwhat physically exists, as shown in Figure 1-2.\nFIGURE 1-2\nThe Linux system memory map\nVirtual Memory\nThe Kernel\nPhysical Memory\nSwap Space\n\n6\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  6\nThe memory locations are grouped into blocks called pages. The kernel locates each page \nof memory either in the physical memory or the swap space. The kernel then maintains a \ntable of the memory pages that indicates which pages are in physical memory and which \npages are swapped out to disk.\nThe kernel keeps track of which memory pages are in use and automatically copies memory \npages that have not been accessed for a period of time to the swap space area (called \nswapping out), even if there’s other memory available. When a program wants to access a \nmemory page that has been swapped out, the kernel must make room for it in physical \nmemory by swapping out a different memory page and swapping in the required page from \nthe swap space. Obviously, this process takes time and can slow down a running process. \nThe process of swapping out memory pages for running applications continues for as long \nas the Linux system is running.\nSoftware Program Management\nThe Linux operating system calls a running program a process. A process can run in the \nforeground, displaying output on a display, or it can run in the background, behind the \nscenes. The kernel controls how the Linux system manages all the processes running on the \nsystem.\nThe kernel creates the fi rst process, called the init process, to start all other processes on the \nsystem. When the kernel starts, it loads the init process into virtual memory. As the kernel \nstarts each additional process, it gives it a unique area in virtual memory to store the data \nand code that the process uses.\nSome Linux implementations contain a table of processes to start automatically on bootup. \nOn Linux systems, this table is usually located in the special fi le \n/etc/inittabs.\nOther systems (such as the popular Ubuntu Linux distribution) utilize the \n/etc/init.d \nfolder, which contains scripts for starting and stopping individual applications at boot \ntime. The scripts are started via entries under the \n/etc/rcX.d folders, where X is a \nrun level.\nThe Linux operating system uses an init system that utilizes run levels. A run level can be \nused to direct the init process to run only certain types of processes, as defi ned in the \n/\netc/inittabs\n fi le or the /etc/rcX.d folders. There are fi ve init run levels in the Linux \noperating system.\nAt run level 1, only the basic system processes are started, along with one console terminal \nprocess. This is called single-user mode. Single-user mode is most often used for emergency \nfi lesystem maintenance when something is broken. Obviously, in this mode, only one per-\nson (usually the administrator) can log in to the system to manipulate data.\nThe standard init run level is 3. At this run level, most application software, such as net-\nwork support software, is started. Another popular run level in Linux is run level 5. This is \n\n7\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  7\n1\nthe run level where the system starts the graphical X Window software and allows you to \nlog in using a graphical desktop window.\nThe Linux system can control the overall system functionality by controlling the init run \nlevel. By changing the run level from 3 to 5, the system can change from a console-based \nsystem to an advanced, graphical X Window system.\nIn Chapter 4, you’ll see how to use the \nps command to view the processes currently run-\nning on the Linux system. \nHardware Management\nStill another responsibility for the kernel is hardware management. Any device that the \nLinux system must communicate with needs driver code inserted inside the kernel code. \nThe driver code allows the kernel to pass data back and forth to the device, acting as a \nmiddle man between applications and the hardware. Two methods are used for inserting \ndevice driver code in the Linux kernel:\n ■\nDrivers compiled in the kernel\n ■\nDriver modules added to the kernel\nPreviously, the only way to insert device driver code was to recompile the kernel. Each time \nyou added a new device to the system, you had to recompile the kernel code. This process \nbecame even more ineffi cient as Linux kernels supported more hardware. Fortunately, \nLinux developers devised a better method to insert driver code into the running kernel.\nProgrammers developed the concept of kernel modules to allow you to insert driver code \ninto a running kernel without having to recompile the kernel. Also, a kernel module could \nbe removed from the kernel when the device was fi nished being used. This greatly simpli-\nfi ed and expanded using hardware with Linux.\nThe Linux system identifi es hardware devices as special fi les, called device files. There are \nthree classifi cations of device fi les:\n ■\nCharacter\n ■\nBlock\n ■\nNetwork\nCharacter device fi les are for devices that can only handle data one character at a time. \nMost types of modems and terminals are created as character fi les. Block fi les are for \ndevices that can handle data in large blocks at a time, such as disk drives.\nThe network fi le types are used for devices that use packets to send and receive data. This \nincludes network cards and a special loopback device that allows the Linux system to com-\nmunicate with itself using common network programming protocols.\n\n8\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  8\nLinux creates special fi les, called nodes, for each device on the system. All communication \nwith the device is performed through the device node. Each node has a unique number pair \nthat identifi es it to the Linux kernel. The number pair includes a major and a minor device \nnumber. Similar devices are grouped into the same major device number. The minor device \nnumber is used to identify a specifi c device within the major device group.\nFilesystem Management\nUnlike some other operating systems, the Linux kernel can support different types of \nfi lesystems to read and write data to and from hard drives. Besides having over a dozen \nfi lesystems of its own, Linux can read and write to and from fi lesystems used by other \noperating systems, such as Microsoft Windows. The kernel must be compiled with support \nfor all types of fi lesystems that the system will use. Table 1-1 lists the standard fi lesystems \nthat a Linux system can use to read and write data.\nTABLE 1-1    Linux  Filesystems\nFilesystemDescription\nextLinux Extended fi lesystem — the original Linux fi lesystem\next2Second extended fi lesystem, provided advanced features over ext\next3Third extended fi lesystem, supports journaling\next4Fourth extended fi lesystem, supports advanced journaling\nhpfsOS/2 high-performance fi lesystem\njfsIBM’s journaling fi lesystem\niso9660ISO 9660 fi lesystem (CD-ROMs)\nminixMINIX fi lesystem\nmsdosMicrosoft FAT16\nncpNetware fi lesystem\nnfsNetwork File System\nntfsSupport for Microsoft NT fi lesystem\nprocAccess to system information\nReiserFSAdvanced Linux fi lesystem for better performance and disk recovery\nsmbSamba SMB fi lesystem for network access\nsysvOlder Unix fi lesystem\nufsBSD fi lesystem\numsdosUnix-like fi lesystem that resides on top of msdos\nvfatWindows 95 fi lesystem (FAT32)\nXFSHigh-performance 64-bit journaling fi lesystem\n\n9\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  9\n1\nAny hard drive that a Linux server accesses must be formatted using one of the fi lesystem \ntypes listed in Table 1-1.\nThe Linux kernel interfaces with each fi lesystem using the Virtual File System (VFS). This \nprovides a standard interface for the kernel to communicate with any type of fi lesystem. \nVFS caches information in memory as each fi lesystem is mounted and used.\nThe GNU Utilities\nBesides having a kernel to control hardware devices, a computer operating system needs \nutilities to perform standard functions, such as controlling fi les and programs. While Linus \ncreated the Linux system kernel, he had no system utilities to run on it. Fortunately for \nhim, at the same time he was working, a group of people were working together on the \nInternet trying to develop a standard set of computer system utilities that mimicked the \npopular Unix operating system.\nThe GNU organization (GNU stands for GNU’s Not Unix) developed a complete set of Unix \nutilities, but had no kernel system to run them on. These utilities were developed under a \nsoftware philosophy called open source software (OSS).\nThe concept of OSS allows programmers to develop software and then release it to the world \nwith no licensing fees attached. Anyone can use the software, modify it, or incorporate it \ninto his or her own system without having to pay a license fee. Uniting Linus’s Linux ker-\nnel with the GNU operating system utilities created a complete, functional, free operating \nsystem.\nWhile the bundling of the Linux kernel and GNU utilities is often just called Linux, you will \nsee some Linux purists on the Internet refer to it as the GNU/Linux system to give credit to \nthe GNU organization for its contributions to the cause.\nThe Core GNU Utilities\nThe GNU project was mainly designed for Unix system administrators to have a Unix-like \nenvironment available. This focus resulted in the project porting many common Unix \nsystem command line utilities. The core bundle of utilities supplied for Linux systems is \ncalled the coreutils package.\nThe GNU coreutils package consists of three parts:\n ■\nUtilities for handling fi les\n ■\nUtilities for manipulating text\n ■\nUtilities for managing processes\nEach of these three main groups of utilities contains several utility programs that are \ninvaluable to the Linux system administrator and programmer. This book covers each of the \nutilities contained in the GNU coreutils package in detail.\n\n10\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  10\nThe Shell\nThe GNU/Linux shell is a special interactive utility. It provides a way for users to start pro-\ngrams, manage fi les on the fi lesystem, and manage processes running on the Linux system. \nThe core of the shell is the command prompt. The command prompt is the interactive part \nof the shell. It allows you to enter text commands, and then it interprets the commands \nand executes them in the kernel.\nThe shell contains a set of internal commands that you use to control things such as copy-\ning fi les, moving fi les, renaming fi les, displaying the programs currently running on the \nsystem, and stopping programs running on the system. Besides the internal commands, \nthe shell also allows you to enter the name of a program at the command prompt. The shell \npasses the program name off to the kernel to start it.\nYou can also group shell commands into fi les to execute as a program. Those fi les are called \nshell scripts. Any command that you can execute from the command line can be placed in \na shell script and run as a group of commands. This provides great fl exibility in creating \nutilities for commonly run commands, or processes that require several commands grouped \ntogether.\nThere are quite a few Linux shells available to use on a Linux system. Different shells have \ndifferent characteristics, some being more useful for creating scripts and some being more \nuseful for managing processes. The default shell used in all Linux distributions is the bash \nshell. The bash shell was developed by the GNU project as a replacement for the standard \nUnix shell, called the Bourne shell (after its creator). The bash shell name is a play on this \nwording, referred to as the “Bourne again shell.”\nIn addition to the bash shell, we will cover several other popular shells in this book. \nTable 1-2 lists the different shells we will examine.\nTABLE 1-2    Linux  Shells\nShellDescription\nashA simple, lightweight shell that runs in low-memory environments but has full compat-\nibility with the bash shell\nkornA programming shell compatible with the Bourne shell but supporting advanced pro-\ngramming features like associative arrays and fl oating-point arithmetic\ntcshA shell that incorporates elements from the C programming language into shell scripts\nzshAn advanced shell that incorporates features from bash, tcsh, and korn, providing \nadvanced programming features, shared history fi les, and themed prompts\n\n11\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  11\n1\nMost Linux distributions include more than one shell, although usually they pick one of \nthem to be the default. If your Linux distribution includes multiple shells, feel free to \nexperiment with different shells and see which one fi ts your needs.\nThe Linux Desktop Environment\nIn the early days of Linux (the early 1990s) all that was available was a simple text inter-\nface to the Linux operating system. This text interface allowed administrators to start pro-\ngrams, control program operations, and move fi les around on the system.\nWith the popularity of Microsoft Windows, computer users expected more than the old text \ninterface to work with. This spurred more development in the OSS community, and the \nLinux graphical desktops emerged.\nLinux is famous for being able to do things in more than one way, and no place is this more \nrelevant than in graphical desktops. There are a plethora of graphical desktops you can \nchoose from in Linux. The following sections describe a few of the more popular ones.\nThe X Window System\nTwo basic elements control your video environment: the video card in your PC and your \nmonitor. To display fancy graphics on your computer, the Linux software needs to know \nhow to talk to both of them. The X Window software is the core element in presenting \ngraphics.\nThe X Window software is a low-level program that works directly with the video card and \nmonitor in the PC, and it controls how Linux applications can present fancy windows and \ngraphics on your computer.\nLinux isn’t the only operating system that uses X Window; versions are written for many \ndifferent operating systems. In the Linux world, several different software packages can \nimplement it.\nThe most popular package is X.org. It provides an open source software implementation of \nthe X Window system and supports many of the newer video cards used today.\nTwo other X Window packages are gaining in popularity. The Fedora Linux distribution is \nexperimenting with the Wayland software, and the Ubuntu Linux distribution has devel-\noped the Mir display server for use with its desktop environment.\nWhen you fi rst install a Linux distribution, it attempts to detect your video card and moni-\ntor, and then it creates an X Window confi guration fi le that contains the required informa-\ntion. During installation, you may notice a time when the installation program scans your \nmonitor for supported video modes. Sometimes, this causes your monitor to go blank for a \n\n12\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  12\nfew seconds. Because there are lots of different types of video cards and monitors, this pro-\ncess can take a while to complete.\nThe core X Window software produces a graphical display environment, but nothing else. \nAlthough this is fi ne for running individual applications, it is not useful for day-to-day \ncomputer use. No desktop environment allows users to manipulate fi les or launch programs. \nTo do that, you need a desktop environment on top of the X Window system software.\nThe KDE Desktop\nThe K Desktop Environment (KDE) was fi rst released in 1996 as an open source project to \nproduce a graphical desktop similar to the Microsoft Windows environment. The KDE desk-\ntop incorporates all the features you are probably familiar with if you are a Windows user. \nFigure 1-3 shows a sample KDE 4 desktop running in the openSUSE Linux distribution.\nFIGURE 1-3\nThe KDE 4 desktop on an openSUSE Linux system\n\n13\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  13\n1\nThe KDE desktop allows you to place both application and fi le icons in a special area on the \ndesktop. If you click an application icon, the Linux system starts the application. If you \nclick a fi le icon, the KDE desktop attempts to determine what application to start to \nhandle the fi le.\nThe bar at the bottom of the desktop is called the Panel. The Panel consists of four parts:\n ■\nThe K menu: Much like the Windows Start menu, the K menu contains links to \nstart installed applications.\n ■\nProgram shortcuts: These are quick links to start applications directly from the \nPanel.\n ■\nThe taskbar: The taskbar shows icons for applications currently running on the \ndesktop.\n ■\nApplets: These are small applications that have an icon in the Panel that often can \nchange depending on information from the application.\nThe Panel features are similar to what you would fi nd in Windows. In addition to the desk-\ntop features, the KDE project has produced a wide assortment of applications that run in \nthe KDE environment.\nThe GNOME Desktop\nThe GNU Network Object Model Environment (GNOME) is another popular Linux desktop \nenvironment. First released in 1999, GNOME has become the default desktop environment \nfor many Linux distributions. (However, the most popular is Red Hat Linux.)\nAlthough GNOME chose to depart from the standard Microsoft Windows look-and-feel, it \nincorporates many features that most Windows users are comfortable with:\n ■\nA desktop area for icons\n ■\nA panel area for showing running applications\n ■\nDrag-and-drop capabilities\nFigure 1-4 shows the standard GNOME desktop used in the CentOS Linux distribution.\nNot to be outdone by KDE, the GNOME developers have also produced a host of graphical \napplications that integrate with the GNOME desktop. \nThe Unity Desktop\nIf you’re using the Ubuntu Linux distribution, you’ll notice that it’s somewhat different \nfrom both the KDE and GNOME desktop environments. Canonical, the company responsible \n\n14\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  14\nfor developing Ubuntu, has decided to embark on its own Linux desktop environment, \ncalled Unity.\nFIGURE 1-4\nA GNOME desktop on a CentOS Linux system\nThe Unity desktop gets its name from the goal of the project — to provide a single desktop \nexperience for workstations, tablet devices, and mobile devices. The Unity desktop works \nthe same whether you’re running Ubuntu on a workstation or a mobile phone! Figure 1-5 \nshows an example of the Unity desktop in Ubuntu 14.04 LTS.\nOther Desktops\nThe downside to a graphical desktop environment is that it requires a fair amount of \nsystem resources to operate properly. In the early days of Linux, a hallmark and selling \n\n15\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  15\n1\nfeature of Linux was its ability to operate on older, less powerful PCs that the newer \nMicrosoft desktop products couldn’t run on. However, with the popularity of KDE and \nGNOME desktops, this has changed, because it takes just as much memory to run a KDE or \nGNOME desktop as the latest Microsoft desktop environment.\nFIGURE 1-5\nThe Unity desktop on the Ubuntu Linux distribution\nIf you have an older PC, don’t be discouraged. The Linux developers have banded together \nto take Linux back to its roots. They’ve created several low-memory–oriented graphical \ndesktop applications that provide basic features that run perfectly fi ne on older PCs.\nAlthough these graphical desktops don’t have a plethora of applications designed around \nthem, they still run many basic graphical applications that support features such as word \nprocessing, spreadsheets, databases, drawing, and, of course, multimedia support.\nTable 1-3 shows some of the smaller Linux graphical desktop environments that can be used \non lower-powered PCs and laptops.\n\n16\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  16\nTABLE 1-3    Other Linux Graphical Desktops\nDesktopDescription\nFluxboxA bare-bones desktop that doesn’t include a Panel, only a pop-up menu to \nlaunch applications\nXfceA desktop that’s similar to the KDE desktop, but with fewer graphics for low-\nmemory environments\nJWMJoe’s Window Manager, a very lightweight desktop ideal for low-memory and \nlow-disk space environments\nFvwmSupports some advanced desktop features such as virtual desktops and Panels, \nbut runs in low-memory environments\nfvwm95Derived from fvwm, but made to look like a Windows 95 desktop\nThese graphical desktop environments are not as fancy as the KDE and GNOME desktops, but \nthey provide basic graphical functionality just fi ne. Figure 1-6 shows what the JWM desk-\ntop used in the Puppy Linux antiX distribution looks like.\nFIGURE 1-6\nThe JWM desktop as seen in the Puppy Linux distribution\n\n17\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  17\n1\nIf you are using an older PC, try a Linux distribution that uses one of these desktops and \nsee what happens. You may be pleasantly surprised.\nLinux Distributions\nNow that you have seen the four main components required for a complete Linux system, \nyou may be wondering how you are going to get them all put together to make a Linux sys-\ntem. Fortunately, other people have already done that for you.\nA complete Linux system package is called a distribution. Many different Linux distributions \nare available to meet just about any computing requirement you could have. Most distribu-\ntions are customized for a specifi c user group, such as business users, multimedia enthu-\nsiasts, software developers, or average home users. Each customized distribution includes \nthe software packages required to support specialized functions, such as audio- and video-\nediting software for multimedia enthusiasts, or compilers and integrated development envi-\nronments (IDEs) for software developers.\nThe different Linux distributions are often divided into three categories:\n ■\nFull core Linux distributions\n ■\nSpecialized distributions\n ■\nLiveCD test distributions\nThe following sections describe these different types of Linux distributions and show some \nexamples of Linux distributions in each category.\nCore Linux Distributions\nA core Linux distribution contains a kernel, one or more graphical desktop environments, \nand just about every Linux application that is available, precompiled for the kernel. It \nprovides one-stop shopping for a complete Linux installation. Table 1-4 shows some of the \nmore popular core Linux distributions.\nTABLE 1- 4    Core  Linux  Distributions\nDistributionDescription\nSlackwareOne of the original Linux distribution sets, popular with Linux geeks\nRed HatA commercial business distribution used mainly for Internet servers\nFedoraA spin-off from Red Hat but designed for home use\nContinues\n\n18\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  18\nDistributionDescription\nGentooA distribution designed for advanced Linux users, containing only Linux source \ncode\nopenSUSEDifferent distributions for business and home use\nDebianPopular with Linux experts and commercial Linux products\nIn the early days of Linux, a distribution was released as a set of fl oppy disks. You had to \ndownload groups of fi les and then copy them onto disks. It would usually take 20 or more \ndisks to make an entire distribution! Needless to say, this was a painful experience.\nNowadays, with home computers commonly having CD and DVD players built in, Linux \ndistributions are released as either a CD set or a single DVD. This makes installing Linux \nmuch easier.\nHowever, beginners still often run into problems when they install one of the core Linux \ndistributions. To cover just about any situation in which someone might want to use Linux, \na single distribution must include lots of application software. They include everything \nfrom high-end Internet database servers to common games. Because of the quantity of \napplications available for Linux, a complete distribution often takes four or more CDs.\nAlthough having lots of options available in a distribution is great for Linux geeks, it can \nbecome a nightmare for beginning Linux users. Most distributions ask a series of questions \nduring the installation process to determine which applications to load by default, what \nhardware is connected to the PC, and how to confi gure the hardware. Beginners often fi nd \nthese questions confusing. As a result, they often either load way too many programs on \ntheir computer or don’t load enough and later discover that their computer won’t do what \nthey want it to.\nFortunately for beginners, there’s a much simpler way to install Linux.\nSpecialized Linux Distributions\nA new subgroup of Linux distributions has started to appear. These are typically based on \none of the main distributions but contain only a subset of applications that would make \nsense for a specifi c area of use.\nIn addition to providing specialized software (such as only offi ce products for business \nusers), customized Linux distributions also attempt to help beginning Linux users by \nTABLE 1- 4   (continued)\n\n19\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  19\n1\nautodetecting and autoconfi guring common hardware devices. This makes installing Linux \na much more enjoyable process.\nTable 1-5 shows some of the specialized Linux distributions available and what they \nspecialize in.\nTABLE 1-5    Specialized  Linux  Distributions\nDistributionDescription\nCentOSA free distribution built from the Red Hat Enterprise Linux source code\nUbuntuA free distribution for school and home use\nPCLinuxOSA free distribution for home and offi ce use\nMintA free distribution for home entertainment use\ndyne:bolicA free distribution designed for audio and MIDI applications\nPuppy LinuxA free small distribution that runs well on older PCs\nThat’s just a small sampling of specialized Linux distributions. There are literally hundreds \nof specialized Linux distributions, and more are popping up all the time on the Internet. No \nmatter what your specialty, you’ll probably fi nd a Linux distribution made for you.\nMany of the specialized Linux distributions are based on the Debian Linux distribution. \nThey use the same installation fi les as Debian but package only a small fraction of a full-\nblown Debian system.\nThe Linux LiveCD\nA relatively new phenomenon in the Linux world is the bootable Linux CD distribution. \nThis lets you see what a Linux system is like without actually installing it. Most modern \nPCs can boot from a CD instead of the standard hard drive. To take advantage of this, some \nLinux distributions create a bootable CD that contains a sample Linux system (called a Linux \nLiveCD). Because of the limitations of the single CD size, the sample can’t contain a complete \nLinux system, but you’d be surprised at all the software they can cram in there. The result \nis that you can boot your PC from the CD and run a Linux distribution without having to \ninstall anything on your hard drive!\n\n20\nPart I: The Linux Command Line\nc01.indd  12/16/2014  Page  20\nThis is an excellent way to test various Linux distributions without having to mess with \nyour PC. Just pop in a CD and boot! All the Linux software will run directly from the CD. \nYou can download lots of Linux LiveCDs from the Internet and burn onto a CD to test drive.\nTable 1-6 shows some popular Linux LiveCDs that are available.\nTABLE 1- 6    Linux  LiveCD  Distributions\nDistributionDescription\nKnoppixA German Linux, the fi rst Linux LiveCD developed\nPCLinuxOSFull-blown Linux distribution on a LiveCD\nUbuntuA worldwide Linux project, designed for many languages\nSlaxA live Linux CD based on Slackware Linux\nPuppy LinuxA full-featured Linux designed for older PCs\nYou may notice a familiarity in this table. Many specialized Linux distributions also have \na Linux LiveCD version. Some Linux LiveCD distributions, such as Ubuntu, allow you to \ninstall the Linux distribution directly from the LiveCD. This enables you to boot with the \nCD, test drive the Linux distribution, and then if you like it, install it on your hard drive. \nThis feature is extremely handy and user-friendly.\nAs with all good things, Linux LiveCDs have a few drawbacks. Because you access every-\nthing from the CD, applications run more slowly, especially if you’re using older, slower \ncomputers and CD drives. Also, because you can’t write to the CD, any changes you make to \nthe Linux system will be gone the next time you reboot.\nBut advances are being made in the Linux LiveCD world that will help to solve some of \nthese problems. These advances include the ability to:\n ■\nCopy Linux system fi les from the CD to memory\n ■\nCopy system fi les to a fi le on the hard drive\n ■\nStore system settings on a USB memory stick\n ■\nStore user settings on a USB memory stick\nSome Linux LiveCDs, such as Puppy Linux, are designed with a minimum number of Linux \nsystem fi les. The LiveCD boot scripts copy them directly into memory when the CD boots. \nThis allows you to remove the CD from the computer as soon as Linux boots. Not only does \nthis make your applications run much faster (because applications run faster from mem-\nory), but it also gives you a free CD tray to use for ripping audio CDs or playing video DVDs \nfrom the software included in Puppy Linux.\nOther Linux LiveCDs use an alternative method that allows you to remove the CD from the \ntray after booting. It involves copying the core Linux fi les onto the Windows hard drive as \n\n21\nChapter 1: Starting with Linux Shells\nc01.indd  12/16/2014  Page  21\n1\na single fi le. After the CD boots, it looks for that fi le and reads the system fi les from it. The \ndyne:bolic Linux LiveCD uses this technique, which is called docking. Of course, you must \ncopy the system fi le to your hard drive before you can boot from the CD.\nA very popular technique for storing data from a live Linux CD session is to use a com-\nmon USB memory stick (also called a fl ash drive or a thumb drive). Just about every Linux \nLiveCD can recognize a plugged-in USB memory stick (even if the stick is formatted for \nWindows) and read and write fi les to and from it. This allows you to boot a Linux LiveCD, \nuse the Linux applications to create fi les, store those fi les on your memory stick, and then \naccess them from your Windows applications later (or from a different computer). How \ncool is that?\nSummary\n This chapter discussed the Linux system and the basics of how it works. The Linux kernel \nis the core of the system, controlling how memory, programs, and hardware all interact \nwith one another. The GNU utilities are also an important piece in the Linux system. The \nLinux shell, which is the main focus of this book, is part of the GNU core utilities. The \nchapter also discussed the fi nal piece of a Linux system, the Linux desktop environment. \nThings have changed over the years, and Linux now supports several graphical desktop \nenvironments.\nThe chapter also discussed the various Linux distributions. A Linux distribution bundles \nthe various parts of a Linux system into a simple package that you can easily install on \nyour PC. The Linux distribution world consists of full-blown Linux distributions that \ninclude just about every application imaginable, as well as specialized Linux distributions \nthat include applications focused only on a special function. The Linux LiveCD craze has \ncreated another group of Linux distributions that allow you to easily test-drive Linux with-\nout even having to install it on your hard drive.\nIn the next chapter, you look at what you need to start your command line and shell script-\ning experience. You’ll see what you need to do to get to the Linux shell utility from your \nfancy graphical desktop environment. These days, that’s not always an easy thing. \n\n\n\n23\nCHAPTER \n2\nGetting to the Shell\nIN THIS CHAPTER\nAccessing the command line\nReaching CLI via a Linux console terminal\nReaching CLI via a graphical terminal emulator\nUsing the GNOME terminal emulator\nUsing the Konsole terminal emulator\nUsing the xterm terminal emulator\nI\nn the old days of Linux, all you had to work with was the shell. System administrators, pro-\ngrammers, and system users all sat at something called a Linux console terminal entering shell \ncommands and viewing text output. These days, with graphical desktop environments, it’s get-\nting harder to fi nd a shell prompt on the system in order to enter shell commands. This chapter \ndiscusses what is required to reach a command line environment. It walks you through the terminal \nemulation packages that you may run into in the various Linux distributions.\nReaching the Command Line\nBefore the days of graphical desktops, the only way to interact with a Unix system was through a \ntext command line interface (CLI) provided by the shell. The CLI allowed text input only and could \ndisplay only text and rudimentary graphics output.\nBecause of these restrictions, output devices were not very fancy. Often, you needed only a simple \ndumb terminal to interact with the Unix system. A dumb terminal was usually nothing more than \na monitor and keyboard connected to the Unix system via a communication cable (usually a multi-\nwire serial cable). This simple combination provided an easy way to enter text data into the Unix \nsystem and view text results.\nAs you well know, things are signifi cantly different in today’s Linux environment. Just about every \nLinux distribution uses some type of graphical desktop environment. However, to enter shell com-\nmands, you still need a text display to access the shell’s CLI. The problem now is getting to one. \nSometimes fi nding a way to get a CLI in a Linux distribution is not an easy task.\nc02    December 3, 2014 4:28 PM    V1\n\n24\nPart I: The Linux Command Line\nConsole Terminals\nOne way to get to a CLI is to take the Linux system out of graphical desktop mode and \nplace it in text mode. This provides nothing more than a simple shell CLI on the monitor, \njust like the days before graphical desktops. This mode is called the Linux console because \nit emulates the old days of a hard-wired console terminal and is a direct interface to the \nLinux system.\nWhen the Linux system starts, it automatically creates several virtual consoles. A virtual \nconsole is a terminal session that runs in Linux system memory. Instead of having several \ndumb terminals connected to the computer, most Linux distributions start fi ve or six (or \nsometimes even more) virtual consoles that you can access from a single computer keyboard \nand monitor.\nGraphical Terminals\nThe alternative to using a virtual console terminal is to use a terminal emulation \npackage from within the Linux graphical desktop environment. A terminal emulation \npackage simulates working on a console terminal, but within a desktop graphical window. \nFigure 2-1 shows an example of a terminal emulator running in a Linux graphical desktop \nenvironment.\nFIGURE 2-1\nA simple terminal emulator running on a Linux desktop\nc02    December 3, 2014 4:28 PM    V1\n\n25\nChapter 2: Getting to the Shell\n2\nGraphical terminal emulation is responsible only for a portion of the Linux graphical experi-\nence. As a whole, the experience is accomplished via several components, including graphi-\ncal terminal emulation software (called a client). Table 2-1 shows the different components \nin the Linux graphical desktop environment.\nTABLE 2-1    Graphical  Interface  Elements\nNameExamplesDescription\nClientGraphical terminal emulator, desktop \nenvironment, network browser\nAn application that requests \ngraphical services\nDisplay ServerMir, Wayland Compositor, XserverElement that manages the \ndisplay (screen) and the input \ndevices (keyboard, mouse, \ntouch screen)\nWindow \nManager\nCompiz, Metacity, KwinElement that adds borders to \nwindows and provides features \nto move and manage windows\nWidgets \nLibrary\nAthena(Xaw), X IntrinsicsElement that adds menus and \nappearance items for desktop \nenvironment clients\nFor dealing with the command line from the desktop, the focus is on the graphical terminal \nemulator. You can think of graphical terminal emulators as CLI terminals “in the GUI” and \nvirtual console terminals as CLI terminals “outside the GUI.” Understanding the various \nterminals and their features can enhance your command line experience.\nAccessing CLI via a Linux Console Terminal\nIn the early days of Linux, when you booted up your system you would see a login prompt \non your monitor, and that’s all. As mentioned earlier, this is called the Linux console. It \nwas the only place you could enter commands for the system.\nEven though several virtual consoles are created at boot time, many Linux distributions \nswitch to a graphical environment after the boot sequence completes. This provides the \nuser with a graphical login and desktop experience. Therefore, in this case, accessing a vir-\ntual console is done manually.\nIn most Linux distributions, you can access one of the Linux virtual consoles using a sim-\nple keystroke combination. Usually, you must hold down the Ctrl+Alt key combination and \nthen press a function key (F1 through F7) for the virtual console you want to use. Function \nkey F2 produces virtual console 2, key F3 produces virtual console 3, key F4 produces \nvirtual console 4, and so on.\nc02    December 3, 2014 4:28 PM    V1\n\n26\nPart I: The Linux Command Line\nLinux distributions typically use the Ctrl+Alt key combination with either F1 or F7 to reach the graphical interface. \nUbuntu uses F7, while RHEL uses F1. It is best to test and see where your distribution puts the graphical interface.\nText mode virtual consoles use the whole screen and start with the text login screen dis-\nplayed. An example of a text login screen from a virtual console is shown in Figure 2-2.\nFIGURE 2-2\nLinux virtual console login screen\nNotice in Figure 2-2 the words tty2 at the end of the fi rst text line. The 2 in tty2 indi-\ncates that it is virtual console 2 and was reached by pressing the Ctrl+Alt+F2 key sequence. \ntty stands for teletypewriter. Teletypewriter is an old term, indicating a machine used for \nsending messages.\nNot all Linux distributions show the virtual console’s tty number at the login screen.\nYou log into a console terminal by entering your user ID after the login: prompt and \ntyping your password after the \nPassword: prompt. If you have never logged in this way \nbefore, be aware that typing your password is a different experience than in a graphical \nenvironment. In a graphical environment, you may see dots or asterisks indicating the \npassword characters as you type. However, at the virtual console, nothing is displayed when \nyou type your password.\nAfter logging into a virtual console, you are taken to the Linux CLI. Keep in mind that, \nwithin the Linux virtual console, you do not have the ability to run any graphical \nprograms.\nc02    December 3, 2014 4:28 PM    V1\n\n27\nChapter 2: Getting to the Shell\n2\nAfter you have logged in to a virtual console, you can keep it active and switch to another \nvirtual console without losing your active session. You can switch between all the virtual \nconsoles, with multiple active sessions running. This feature provides a great deal of fl ex-\nibility while you work at the CLI.\nAdditional fl exibility deals with the virtual console’s appearance. Even though it is a text \nmode console terminal, you can modify the text and background colors.\nFor example, it may be easier on your eyes to set the background of the terminal to white \nand the text to black. After you have logged in, you can accomplish this modifi cation in a \ncouple of ways. One way is to type in the command setterm  -inversescreen  on and press \nthe Enter key, as shown in Figure 2-3. Notice in the fi gure that the \ninversescreen fea-\nture is being turned on using the option \non. You can also turn it off using the off option.\nFIGURE 2-3\nLinux virtual console with inversescreen being turned on\nAnother way is to type two commands, one after the other. Type setterm  -background  \nwhite and press Enter, and then type setterm -foreground  black and press Enter. Be care-\nful because, when you change your terminal background fi rst, it may be hard to see the \ncommands you are typing.\nWith the commands in the preceding paragraph, you are not turning features on and off, as \nwith \ninversescreen. Instead, you have a choice of eight colors. The choices are black, \nred, green, yellow, blue, magenta, cyan, and white (which looks gray on some \nc02    December 3, 2014 4:28 PM    V1\n\n28\nPart I: The Linux Command Line\ndistributions). You can get rather creative with your plain text mode console terminals. \nTable 2-2 shows some options you can use with the \nsetterm command to help improve \nyour console terminal’s readability or appearance.\nTABLE 2-2    setterm Options for Foreground and Background \nAppearance\nOptionParameter ChoicesDescription\n-background      black, red, green, yellow, \nblue, magenta, cyan, \nor white\nChanges the terminal’s back-\nground color to the one \nspecifi ed\n-foreground      black, red, green, yellow, \nblue, magenta, cyan, \nor white\nChanges the terminal’s fore-\nground color, specifi cally text, \nto the one specifi ed\n-inversescreen\non\n or offSwitches the background color \nto the foreground color and the \nforeground color to the back-\nground color\n-reset\nNoneChanges the terminal appear-\nance back to its default setting \nand clears the screen\n-store\nNoneSets the current terminal’s fore-\nground and background colors \nas the values to be used for \n-reset \nVirtual console terminals are great for accessing the CLI outside the GUI. However, some-\ntimes, you need to access the CLI and run graphical programs. Using a terminal emulation \npackage solves this problem and is a popular way to access the shell CLI from within the \nGUI. The following sections describe common software packages that provide graphical ter-\nminal emulation.\nAccessing CLI via Graphical Terminal Emulation\nThe graphical desktop environment offers a great deal more variety for CLI access than the \nvirtual console terminal does. Many graphical terminal emulator packages are available \nfor the graphical environment. Each package provides its own unique set of features and \noptions. Some popular graphical terminal emulator packages are shown in Table 2-3 along \nwith their websites.\nc02    December 3, 2014 4:28 PM    V1\n\n29\nChapter 2: Getting to the Shell\n2\nTABLE 2-3    Popular Graphical Terminal Emulator Packages\nNameWebsite\nEterm\nhttp://www.eterm.org\nFinal Term\nhttp://finalterm.org\nGNOME Terminal\nhttps://help.gnome.org/users/gnome-terminal/stable\nGuake\nhttps://github.com/Guake/guake\nKonsole Terminal\nhttp://konsole.kde.org\nLillyTerm\nhttp://lilyterm.luna.com.tw/index.html\nLXTerminal\nhttp://wiki.lxde.org/en/LXTerminal\nmrxvt\nhttps://code.google.com/p/mrxvt\nROXTerm\nhttp://roxterm.sourceforge.net\nrxvt\nhttp://sourceforge.net/projects/rxvt\nrxvt-unicode\nhttp://software.schmorp.de/pkg/rxvt-unicode\nSakura\nhttps://launchpad.net/sakura\nst\nhttp://st.suckless.org\nTe r mina to r\nhttps://launchpad.net/terminator\nTe r min olo g y\nhttp://www.enlightenment.org/p.php?p=about/terminology\ntilda\nhttp://tilda.sourceforge.net/tildaabout.php\nUXterm\nhttp://manpages.ubuntu.com/manpages/gutsy/man1/\nuxterm.1.html\nWterm\nhttp://sourceforge.net/projects/wterm\nxterm\nhttp://invisible-island.net/xterm\nXfce4 Terminal\nhttp://docs.xfce.org/apps/terminal/start\nYakuake\nhttp://extragear.kde.org/apps/yakuake\nAlthough many graphical terminal emulator packages are available, the focus in this chap-\nter is on three commonly used ones. Often installed in Linux distributions by default, they \nare GNOME Terminal, Konsole Terminal, and xterm.\nUsing the GNOME Terminal Emulator\nGNOME Terminal is the GNOME desktop environment’s default terminal emulator. Many \ndistributions, such as RHEL, Fedora, and CentOS, use the GNOME desktop environment by \nc02    December 3, 2014 4:28 PM    V1\n\n30\nPart I: The Linux Command Line\ndefault, and therefore use GNOME Terminal by default. However, other desktop environ-\nments, such as Ubuntu Unity, also use the GNOME terminal as their default terminal emula-\ntor package. It is fairly easy to use and a good terminal emulator for individuals who are \nnew to Linux. This chapter section walks you through the various parts of accessing, con-\nfi guring and using the GNOME terminal emulator.\nAccessing the GNOME Terminal\nEach graphical desktop environment has different methods for accessing the GNOME termi-\nnal emulator. This section looks at accessing the GNOME Terminal in the GNOME, Unity, and \nKDE desktop environments.\nIf you are using a different desktop environment than the ones listed in Table 2.3, you must look through the vari-\nous menus offered in your environment to fi nd the GNOME terminal emulator. In the menus, it is typically named \nTerminal.\nIn the GNOME desktop environment, accessing the GNOME Terminal is fairly straightfor-\nward. From the menu system in the upper-left corner of the window, click \nApplications, \nthen select \nSystem Tools from the drop-down menu, and fi nally click Terminal. Written \nin shorthand, the directions look like the following: \nApplications ➪ System Tools ➪  \nTerminal.\nRefer to Figure 2-1 to see a picture of the GNOME Terminal. It was accessed in a GNOME \ndesktop environment on a CentOS distribution.\nIn the Unity desktop environment, accessing the GNOME terminal takes a little more effort. \nThe simplest access method is \nDash ➪ Search and type Terminal. The GNOME terminal \nshows up in the Dash home area as an application named \nTerminal. Click that icon to \nopen the GNOME terminal emulator.\nIn some Linux distribution desktop environments, such as Ubuntu’s Unity, you can quickly access the GNOME terminal using the \nshortcut key combination Ctrl+Alt+T.\nIn the KDE desktop environment, the Konsole terminal emulator is the default emulator. \nTherefore, you must dig down through the menus to access GNOME Terminal. Start with the \nicon labeled \nKickoff Application Launcher in the lower-left corner of the screen and \nthen click \nApplications ➪ Utilities ➪ Terminal.\nc02    December 3, 2014 4:28 PM    V1\n\n31\nChapter 2: Getting to the Shell\n2\nIn most desktop environments, you can create a launcher for accessing GNOME Terminal. \nA launcher is an icon you create on your desktop that allows you to start a chosen applica-\ntion. This is a great feature that allows you to quickly access a terminal emulator in the \ngraphical desktop. It is especially helpful if you do not want to use shortcut keys or the \nshortcut key feature is not available in your desktop environment of choice.\nFor example, in the GNOME desktop environment, to create a launcher, right-click your \nmouse in the middle of the desktop area; a drop-down menu appears. \nSelect Create \nLauncher...\n from the menu; the Create Launcher application window opens. In the Type \nfi eld, select \nApplication. Type a name for your icon in the Name fi eld. In the Command \nfi eld, type gnome-terminal. Click \nOk to save your new launcher. An icon with the name \nyou gave the launcher now appears on your desktop. Double-click it to open the GNOME \nterminal emulator.\nWhen you type gnome-terminal in the Command fi eld, you are typing the shell command for starting the GNOME ter-\nminal emulator. You learn in Chapter 3 how to add special options to commands, such as gnome-terminal, to provide \nspecial confi guration options, and how to view all the options available to you.\nSeveral confi guration options are provided by menus and short-cut keys in the application, \nwhich you can apply after you get the GNOME terminal emulation started. Understanding \nthese options can enhance your GNOME Terminal CLI experience.\nThe Menu Bar\nThe GNOME Terminal menu bar contains the confi guration and customization options you \nneed to make your GNOME Terminal just the way you want it. The following tables briefl y \ndescribe the different confi guration options in the menu bar and shortcut keys associated \nwith the options.\nAs you read through these GNOME Terminal menu options, keep in mind that your Linux distribution’s GNOME \nTerminal may have slightly different menu options available. This is because several Linux distributions use older ver-\nsions of GNOME Terminal.\nTable 2-4 shows the confi guration options available within the GNOME Terminal File menu \nsystem. The \nFile menu item contains items to create and manage your overall CLI terminal \nsessions.\nc02    December 3, 2014 4:28 PM    V1\n\n32\nPart I: The Linux Command Line\nTABLE 2- 4    The  File  Menu\nNameShortcut KeyDescription\nOpen Terminal\nShift+Ctrl+NStarts a new shell session in a new GNOME Terminal \nwindow\nOpen Tab\nShift+Ctrl+TStarts a new shell session in a new tab in the existing \nGNOME Terminal window\nNew Profile\nNoneCustomizes a session and saves as a profi le, which can be \nrecalled for later use\nSave Contents\nNoneSaves the scrollback buffer contents to a text fi le \nClose Tab\nShift+Ctrl+WCloses the current tab session\nClose Window\nShift+Ctrl+QCloses the current GNOME Terminal session\nNotice that, as in a network browser, you can open new tabs within the GNOME Terminal \nsession to start a whole new CLI session. Each tab session is considered to be an indepen-\ndent CLI session.\nYou do not have to click through the menu to reach options in the File menu. Most of the items are also available by right-\nclicking in the session area. \nThe Edit menu contains items, shown in Table 2-5, for handling text within the tabs. You \ncan use your mouse to copy and paste text anywhere within the session window.\nTABLE 2-5    The  Edit  Menu\nNameShortcut KeyDescription\nCopy\nShift+Ctrl+CCopies selected text to the GNOME clipboard\nPaste\nShift+Ctrl+VPastes text from the GNOME clipboard into a \nsession\nPaste Filenames\nProperly pastes copied fi lenames and their paths\nSelect All\nNoneSelects output in the entire scrollback buffer\nProfiles\nNoneAdds, deletes, or modifi es GNOME Terminal profi les\nKeyboard \nShortcuts\nNoneCreates key combinations to quickly access GNOME \nTe r minal fea t u re s\nProfile \nPreferences\nNoneEdits the current session profi le\nc02    December 3, 2014 4:28 PM    V1\n\n33\nChapter 2: Getting to the Shell\n2\nThe Paste Filenames menu option is available only in later versions of GNOME Terminal. \nTherefore, you may not see that menu option on your system.\nThe \nView menu, shown in Table 2-6, contains items for controlling how the CLI session \nwindows appear. These options can be helpful for individuals with visual impairment.\nTABLE 2- 6    The  View  Menu\nNameShortcut KeyDescription\nShow Menubar\nNoneToggles on/off the menu bar display\nFull Screen\nF11Toggles on/off the terminal window fi lling the entire \ndesktop\nZoom In\nCtrl++Enlarges the font size in the window incrementally\nZoom Out\nCtrl+-Reduces the font size in the window incrementally\nNormal Size\nCtrl+0Returns the font size to default\nBe aware that if you toggle off the menu bar display, the session’s menu bar disappears. \nHowever, you can easily get the menu bar to display again by right-clicking in any terminal \nsession window and toggling on the \nShow Menubar option.\nThe \nSearch menu, shown in Table 2-7, contains items for conducting simple searches \nwithin the terminal session. These searches are similar to ones you may have conducted in \na network browser or word processor.\nTABLE 2-7    The  Search  Menu\nNameShortcut KeyDescription\nFind\nShift+Ctrl+FOpens Find window to provide designated text search \noptions\nFind Next\nShift+Ctrl+HSearches forward from current terminal session location for \ndesignated text\nFind \nPrevious\nShift+Ctrl+GSearches backward from current terminal session location \nfor designated text\nThe Terminal menu, shown in Table 2-8, contains options for controlling the terminal \nemulation session features. There are no shortcut keys to access these items.\nc02    December 3, 2014 4:28 PM    V1\n\n34\nPart I: The Linux Command Line\nTABLE 2-8    The Terminal Menu\nNameDescription\nChange Profile\nSwitches to a new profi le confi guration\nSet Title\nModifi es session tab title bar setting\nSet Character Encoding\nSelects character set used to send and display characters\nReset\nSends reset terminal session control code\nReset and Clear\nSends reset terminal session control code and clears terminal \nsession screen\nWindow Size List\nLists window sizes for adjusting the current terminal window \nsize\nThe Reset option is extremely useful. One day, you may accidently cause your terminal \nsession to display random characters and symbols. When this occurs, the text is unread-\nable. It is typically caused by displaying a non-text fi le to the screen. You can quickly get \nthe terminal session back to normal by selecting \nReset or Reset and Clear.\nThe \nTabs menu, shown in Table 2-9, provides items for controlling the location of the tabs \nand selecting which tab is active. This menu displays only when you have more than one \ntab session open.\nTABLE 2-9    The  Tabs  Menu\nNameShortcut KeyDescription\nNext Tab\nCtrl+Page DownMakes the next tab in the list active\nPrevious \nTab\nCtrl+Page UpMakes the previous tab in the list active\nMove Tab \nLeft\nShift+Ctrl+Page \nUp\nShuffl es the current tab in front of the previous tab\nMove Tab \nRight\nShift+Ctrl+Page \nDown\nShuffl es the current tab in front of the next tab\nDetach Tab\nNoneRemoves the tab and starts a new GNOME Terminal win-\ndow using this tab session\nTab List\nNoneLists the currently running tabs (Select a tab to jump to \nthat session.)\nTerminal \nList\nNoneLists the currently running terminals (Select a terminal to \njump to that session. This is displayed only if multiple \nwindow sessions are open.)\nc02    December 3, 2014 4:28 PM    V1\n\n35\nChapter 2: Getting to the Shell\n2\nFinally, the Help menu contains two menu options. Contents provides a full GNOME \nTerminal manual so you can research individual GNOME Terminal items and features. The \nAbout option shows you the current GNOME Terminal version that’s running.\nBesides the GNOME terminal emulator package, another commonly used package is Konsole \nTerminal. In many ways, Konsole Terminal is similar to GNOME Terminal. However, enough \ndifferences exist to warrant its own section.\nUsing the Konsole Terminal Emulator\nThe KDE Desktop Project created its own terminal emulation package called Konsole \nTerminal. The Konsole package incorporates basic terminal emulation features, along with \nmore advanced ones expected from a graphical application. This section describes Konsole \nTerminal features and shows you how to use them.\nAccessing the Konsole Terminal\nThe Konsole Terminal is the default terminal emulator for the KDE desktop environment. \nYou can easily access it via the KDE environment’s menu system. In other desktop environ-\nments, accessing the Konsole Terminal can be a little more diffi cult.\nIn the KDE desktop environment, you can access the Konsole Terminal by clicking the icon \nlabeled \nKickoff Application Launcher in the lower-left corner of the screen. Then \nclick \nApplications ➪ System ➪ Terminal (Konsole).\nYou may see two terminal menu options within the KDE menu environment. If you do, the Terminal menu option with \nthe words Konsole beneath it is the Konsole terminal.\nIn the GNOME desktop environment, the Konsole terminal is typically not installed \nby default. If Konsole Terminal has been installed, you can access it via the GNOME \nmenu system. In the upper-left corner of the window, click \nApplications ➪ \nSystem Tools ➪ Konsole. \nYou may not have the Konsole terminal emulation package installed on your system. If you would like to install it, \nread through Chapter 9 to learn how to install software via the command line.\nc02    December 3, 2014 4:28 PM    V1\n\n36\nPart I: The Linux Command Line\nIn the Unity desktop environment, if Konsole has been installed, you can access it via \nDash ➪ Search and type Konsole. The Konsole Terminal shows up in the Dash home area \nas an application named \nKonsole. Click that icon to open the Konsole terminal emulator.\nFigure 2-4 shows the Konsole Terminal. It was accessed on a KDE desktop environment in a \nCentOS Linux distribution.\nFIGURE 2-4\nThe Konsole Terminal\nRemember that, in most desktop environments, you can create a launcher to access appli-\ncations such as the Konsole Terminal. The command you need to type for the launcher \nto start up the Konsole terminal emulator is konsole. Also, if the Konsole Terminal is \ninstalled, you can start it from another terminal emulator by typing konsole and pressing \nEnter.\nThe Konsole Terminal, similar to GNOME Terminal, has several confi guration options pro-\nvided by menus and shortcut keys. The following section describes these various options.\nc02    December 3, 2014 4:28 PM    V1\n\n37\nChapter 2: Getting to the Shell\n2\nThe Menu Bar\nThe Konsole Terminal menu bar contains the confi guration and customization options you \nneed to easily view and change features in your terminal emulation session. The following \ntables briefl y describe the menu options and associated shortcut keys.\nThe Konsole Terminal provides a simple menu when you right-click in the active session area. Several menu items are \navailable in this easy-to-access menu.\nThe File menu, shown in Table 2-10, provides options for starting a new tab in the current \nwindow or in a new window.\nTABLE 2-10    The  File  Menu\nNameShortcut KeyDescription\nNew Tab\nCtrl+Shift+NStarts a new shell session in a new tab in the existing \nKonsole Terminal window\nNew Window\nCtrl+Shift+MStarts a new shell session in a new Konsole Terminal \nwindow\nShell\nNoneOpens the default profi le, Shell\nOpen Browser \nHere\nNoneOpens the default fi le browser application\nClose Tab\nCtrl+Shift+WCloses the current tab session\nQuit\nCtrl+Shift+QQuits the Konsole Terminal emulation application\nWhen you fi rst start the Konsole Terminal, the only profi le listed in the menu is Shell. As \nmore profi les are created and saved, their names appear in the menu list.\nAs you read through these Konsole Terminal menu options, keep in mind that your Linux distribution’s Konsole \nTerminal may have very different menu options available. This is because some Linux distributions have kept older \nversions of the Konsole Terminal emulation package.\nc02    December 3, 2014 4:28 PM    V1\n\n38\nPart I: The Linux Command Line\nThe Edit menu, shown in Table 2-11, provides options for handling text in the session. \nAlso, managing tab names is in this options list.\nTABLE 2-11    The  Edit  Menu\nNameShortcut KeyDescription\nCopy\nCtrl+Shift+CCopies selected text to the Konsole clipboard\nPaste\nCtrl+Shift+VPastes text from the Konsole clipboard into a session\nRename Tab\nCtrl+Alt+SModifi es session tab title bar setting\nCopy Input To\nNoneStarts/stops session input copies to chosen additional \nsessions\nClear Display\nNoneClears the terminal session screen\nClear & Reset\nNoneClears the terminal session screen and sends the reset \nterminal session control code\nKonsole provides an excellent method for tracking what function is taking place in each \ntab session. Using the Rename Tab menu option, you can name a tab to match its current \ntask. This helps in tracking which open tab session is performing what function.\nThe \nView menu, shown in Table 2-12, contains items for controlling individual session \nviews in the Konsole Terminal window. In addition, options are available that aid in moni-\ntoring terminal session activity.\nTABLE 2-12    The  View  Menu\nNameShortcut KeyDescription\nSplit View\nNoneControls the multiple tab session display within \nthe current Konsole Terminal window\nDetach View\nCtrl+Shift+HRemoves a tab session and starts a new Konsole \nTerminal window using this tab session\nShow Menu Bar\nNoneToggles on/off Menu bar display\nFull Screen Mode\nCtrl+Shift+F11Toggles on/off the terminal window fi lling the  \nentire monitor display area\nMonitor for \nSilence\nCtrl+Shift+IToggles on/off a special message for tab silence \nMonitor for \nActivity\nCtrl+Shift+AToggles on/off a special message for tab \nactivity\nCharacter Encoding\nNoneSelects the character set used to send and dis-\nplay characters\nc02    December 3, 2014 4:28 PM    V1\n\n39\nChapter 2: Getting to the Shell\n2\nIncrease Text Size\nCtrl++Enlarges the font size in the window \nincrementally\nDecrease Text Size\nCtrl+- Reduces the font size in the window \nincrementally\nThe Monitor for Silence menu option is used for indicating tab silence. Tab silence \noccurs when no new text appears in the current tab session for 10 seconds. This allows you \nto switch to another tab while waiting for application output to stop.\nTab activity, toggled by the \nMonitor for Activity option, issues a special message \nwhen new text appears in the tab session. This option allows you to be notifi ed when out-\nput from an application occurs.\nKonsole retains a history, formally called a scrollback buffer, for each tab. The history \ncontains output text that has scrolled out of the terminal viewing area. By default, the \nlast 1,000 lines in the scrollback buffer are retained. The \nScrollback menu, shown in \nTable 2-13, contains options for viewing this buffer.\nTABLE 2-13    The  Scrollback  Menu\nNameShortcut KeyDescription\nSearch Output\nCtrl+Shift+FOpens the Find window at the bottom of the \nKonsole Terminal window to provide scrollback text \nsearch options\nFind Next\nF3Finds the next text match in more recent scrollback \nbuffer history\nFind Previous\nShift+F3Finds the next text match in older scrollback buffer \nhistory\nSave Output\nNoneSaves scrollback buffer contents to a text or \nHTML fi le\nScrollback \nOptions\nNoneOpens the Scrollback Options window to confi gure \nscrollback buffer options\nClear Scrollback\nNoneRemoves scrollback buffer contents\nClear Scrollback \n& Reset\nCtrl+Shift+XRemoves scrollback buffer contents and resets the \nterminal window\nYou can scroll back through the scrollback buffer by simply using the scrollbar in the view-\ning area. Also, you can scroll back line by line by pressing the Shift+Up Arrow or scroll back \na page (24 lines) at a time by pressing Shift+Page Up. \nc02    December 3, 2014 4:28 PM    V1\n\n40\nPart I: The Linux Command Line\nThe Bookmarks menu options, shown in Table 2-14, provide a way to manage bookmarks \nset in the Konsole Terminal window. A bookmark enables you to save your active session’s \ndirectory location and then easily return there in either the same session or a new session.\nTABLE 2-14    The  Bookmarks  Menu\nNameShortcut KeyDescription\nAdd Bookmark\nCtrl+Shift+BCreates a new bookmark at the current direc-\ntory location\nBookmark Tabs as \nFolder\nNoneCreates a new bookmark for all current terminal \ntab sessions\nNew Bookmark Folder\nNoneCreates a new bookmark storage folder\nEdit Bookmarks\nNoneEdits existing bookmarks\nThe Settings menu, shown in Table 2-15, allows you to customize and manage your pro-\nfi les. Also, you can add a little more functionality to your current tab session. There are no \nshortcut keys to access these items.\nTABLE 2-15    The  Settings  Menu\nNameDescription\nChange Profile\nApplies to the current tab a selected profi le\nEdit Current Profile\nOpens the Edit Profi le window to provide profi le confi guration \noptions\nManage Profiles\nOpens the Manage Profi le window to provide profi le manage-\nment options\nConfigure Shortcuts\nCreates Konsole Terminal command keyboard shortcuts\nConfigure \nNotifications\nCreates custom Konsole Terminal schemas and sessions\nConfigure Notifications allows you to associate specifi c events that can occur within \na session with different actions. When one of the events occurs, the defi ned action (or \nactions) is taken.\nThe \nHelp menu, shown in Table 2-16, provides the full Konsole handbook (if KDE handbooks \nwere installed in your Linux distribution) and the standard About Konsole dialog box.\nc02    December 3, 2014 4:28 PM    V1\n\n41\nChapter 2: Getting to the Shell\n2\nTABLE 2-16    The  Help  Menu\nNameShortcut KeyDescription\nKonsole Handbook\nNoneContains the full Konsole Handbook\nWhat’s This?\nShift+F1Contains help messages for terminal \nwidgets\nReport Bug\nNoneOpens the Submit Bug Report form\nSwitch Application \nLanguage\nNoneOpens the Switch Application’s Language \nform\nAbout Konsole\nNoneDisplays the current Konsole Terminal \nversion\nAbout KDE\nDisplays the current KDE desktop envi-\nronment version\nRather extensive documentation is provided to help you use the Konsole terminal emulator \npackage. In addition to help items, you are provided with a Bug Report form to submit to \nthe Konsole Terminal developers when you encounter a program bug.\nThe Konsole terminal emulator package is young compared to another popular package, \nxterm. In the next section, we explore the “old-timer” xterm.\nUsing the xterm Terminal Emulator\nThe oldest and most basic of terminal emulation packages is xterm. The xterm package has \nbeen around since before the original days of X Window, a popular display server, and it’s \noften included by default in distributions.\nAlthough xterm is a full terminal emulation package, it doesn’t require many resources \n(such as memory) to operate. Because of this, the xterm package is still popular in Linux \ndistributions designed to run on older hardware. Some graphical desktop environments use \nit as the default terminal emulation package.\nAlthough it doesn’t offer many fancy features, the xterm package does one thing extremely \nwell: It emulates older terminals, such as the Digital Equipment Corporation (DEC) VT102, \nVT220, and Tektronix 4014 terminals. For the VT102 and VT220 terminals, xterm can even \nemulate the VT series of color control codes, allowing you to use color in your scripts.\nc02    December 3, 2014 4:28 PM    V1\n\n42\nPart I: The Linux Command Line\nThe DEC VT102 and VT220 were dumb text terminals popular for connecting to Unix systems in the 1980s and early \n1990s. A VT102/VT220 could display text and display rudimentary graphics using block mode graphics. This style of \nterminal access is still used in many business environments today, thus keeping VT102/VT220 emulation popular.\nFigure 2-5 shows what the basic xterm display looks like running on a graphical Linux \ndesktop. You can see it is very basic.\nFIGURE 2-5\nThe xterm Terminal\nThe xterm terminal emulator can be tricky to fi nd these days. Often, it is not included in a \ndesktop environment graphical menu arrangement.\nAccessing xterm\nIn Ubuntu’s Unity desktop, xterm is installed by default. You can access it via \nDash ➪ Search and type xterm. xterm shows up in the Dash home area as an application \nnamed \nXTerm. Click that icon to open the xterm terminal emulator.\nYou may see another terminal called UXTerm when you search for xterm on Ubuntu. This is simply the xterm emulator \npackage with Unicode support.\nc02    December 3, 2014 4:28 PM    V1\n\n43\nChapter 2: Getting to the Shell\n2\nIn the GNOME and KDE desktop environment, xterm is not installed by default. You must \ninstall it fi rst (see Chapter 9 for help on installing software packages). After it’s installed, \nyou must start xterm from another terminal emulator. Open a terminal emulator for CLI \naccess, type xterm, and press Enter. Also, remember that you can create your own desktop \nlauncher to startup xterm.\nThe xterm package allows you to set individual features using command line parameters. \nThe following sections discuss these features and how to change them.\nCommand Line Parameters\nThe list of xterm command line parameters is extensive. You can control lots of features \nto customize the terminal emulation features, such as enabling or disabling individual VT \nemulations.\nxterm has a huge number of confi guration options — so many that they cannot all be covered here. Extensive docu-\nmentation is available via the bash manual. Accessing the bash manual is covered in Chapter 3. In addition, the xterm \ndevelopment team provides some excellent help on its website: \nhttp://invisible-island.net/xterm/.\nYou can invoke certain confi guration options by adding a parameter to the xterm com-\nmand. For example, to have the xterm emulate a DEC VT100 terminal, type the command \nxterm  -ti  vt100 and press Enter. Table 2-17 shows some parameters you can include when \ninvoking the xterm terminal emulator software.\nTABLE 2-17    xterm Command Line Parameters\nParameterDescription\n-bg color\nSpecifi es the color to use for the terminal background\n-fb font\nSpecifi es the font to use for bold text\n-fg color\nSpecifi es the color to use for the foreground text\n-fn font\nSpecifi es the font to use for text\n-fw font\nSpecifi es the font to use for wide text\n-lf filename\nSpecifi es the fi lename to use for screen logging\n-ms color\nSpecifi es the color used for the text cursor\n-name name\nSpecifi es the name of the application that appears in the title bar\n-ti terminal\nSpecifi es the terminal type to emulate\nc02    December 3, 2014 4:28 PM    V1\n\n44\nPart I: The Linux Command Line\nSome xterm command line parameters use a plus sign (+) or minus sign (-) to signify how a \nfeature is set. A plus sign may turn a feature on, while a minus sign turns it off. However, \nthe opposite can be true as well. A plus sign may disable a feature, while a minus sign \nenables it, such as when using the \nbc parameter. Table 2-18 lists some of the more common \nfeatures you can set using the +/- command line parameters.\nTABLE 2-18    xterm +/- Command Line Parameters\nParameterDescription\nah\nEnables/disables highlighted text cursor\naw\nEnables/disables auto-line-wrap\nbc\nEnables/disables text cursor blinking\ncm\nEnables/disables recognition of ANSI color change control codes\nfullscreen\nEnables/disables full screen mode\nj\nEnables/disables jump scrolling\nl\nEnables/disables logging screen data to a log fi le\nmb\nEnables/disables margin bell\nrv\nEnables/disables reverse video colors\nt\nEnables/disables Tektronix mode\nIt is important to note that not all implementations of xterm support all these command \nline parameters. You can determine which parameters your xterm implements by using the \n-help parameter when you start xterm on your system.\nNow that you have been introduced to three terminal emulator packages, the big question \nis which is the best terminal emulator to use? There is no defi nite answer to that question. \nWhich terminal emulator package you use depends upon your individual needs and desires. \nBut it is great to have so many choices.\nSummary\n To start learning Linux command line commands, you need access to a CLI. In the world of \ngraphical interfaces, this can sometimes be challenging. This chapter discussed different \ninterfaces you should consider to get to the Linux command line. \nFirst, this chapter discussed the difference between accessing the CLI via a virtual console \nterminal (a terminal outside the GUI) and a graphical terminal emulation package \nc02    December 3, 2014 4:28 PM    V1\n\n45\nChapter 2: Getting to the Shell\n2\n(a terminal inside the GUI). We took a brief look at the basic differences between these two \naccess methods.\nNext, we explored in detail accessing the CLI via a virtual console terminal, including spe-\ncifi cs on how to change console terminal confi guration options such as background color.\nAfter looking at virtual console terminals, the chapter traveled through accessing the CLI \nvia a graphical terminal emulator. Primarily, we covered three different types of terminal \nemulators: GNOME Terminal, Konsole Terminal, and xterm.\nThis chapter also covered the GNOME desktop project’s GNOME terminal emulation package. \nGNOME Terminal is typically installed by default on the GNOME desktop environment. It pro-\nvides convenient ways to set many terminal features via menu options and shortcut keys.\nWe also covered the KDE desktop project’s Konsole terminal emulation package. The Konsole \nTerminal is typically installed by default on the KDE desktop environment. It provides sev-\neral nice features, such as the ability to monitor a terminal for silence.\nFinally, we covered the xterm terminal emulator package. xterm was the fi rst terminal \nemulator available for Linux. It can emulate older terminal hardware such as the VT and \nTektronix terminals.\nIn the next chapter, you start looking at the Linux command line commands. It walks you \nthrough the commands necessary to navigate around the Linux fi lesystem, and to create, \ndelete, and manipulate fi les. \n \nc02    December 3, 2014 4:28 PM    V1\n\n\n\n47\nc03.indd  12/03/2014  Page  47\nCHAPTER \n3\n Basic bash Shell Commands\nIN THIS CHAPTER\nInteracting with the shell\nUsing the bash manual\nTraversing the fi lesystem\nListing fi les and directories\nManaging fi les and directories\nViewing fi le contents\nT\nhe default shell used in many Linux distributions is the GNU bash shell. This chapter describes \nthe basic features available in the bash shell, such as the bash manual, tab auto-completion \nand how to display a fi le’s contents. You will walk through how to work with Linux fi les and \ndirectories using the basic commands provided by the bash shell. If you’re already comfortable with \nthe basics in the Linux environment, feel free to skip this chapter and continue with Chapter 4 to \nsee more advanced commands.\nStarting the Shell\nThe GNU bash shell is a program that provides interactive access to the Linux system. It runs as a \nregular program and is normally started whenever a user logs in to a terminal. The shell that the \nsystem starts depends on your user ID confi guration.\nThe \n/etc/passwd fi le contains a list of all the system user accounts, along with some basic con-\nfi guration information about each user. Here’s a sample entry from a \n/etc/passwd fi le:\nchristine:x:501:501:Christine Bresnahan:/home/christine:/bin/bash\nEach entry has seven data fi elds, with fi elds separated by colons. The system uses the data in these \nfi elds to assign specifi c features for the user. Most of these entries are discussed in more detail in \nChapter 7. For now, just pay attention to the last fi eld, which specifi es the user’s shell program.\n\n48\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  48\nThough the focus is on the GNU bash shell, additional shells are reviewed in this book. Chapter 23 covers working \nwith alternative shells, such as dash and tcsh.\nIn the earlier /etc/passwd sample entry, the user christine has /bin/bash set as her \ndefault shell program. This means when \nchristine logs into the Linux system, the bash \nshell program is automatically started.\nAlthough the bash shell program is automatically started at login, whether a shell com-\nmand line interface (CLI) is presented depends on which login method is used. If a virtual \nconsole terminal is used to log in, the CLI prompt is automatically presented, and you can \nbegin to type shell commands. However, if you log into the Linux system via a graphical \ndesktop environment, you need to start a graphical terminal emulator to access the shell \nCLI prompt.\nUsing the Shell Prompt\nAfter you start a terminal emulation package or log in to a Linux virtual console, you get \naccess to the shell CLI prompt. The prompt is your gateway to the shell. This is the place \nwhere you enter shell commands.\nThe default prompt symbol for the bash shell is the dollar sign (\n$). This symbol indicates \nthat the shell is waiting for you to enter text. Different Linux distributions use different \nformats for the prompt. On this Ubuntu Linux system, the shell prompt looks like this:\nchristine@server01:~$\nOn the CentOS Linux system, it looks like this:\n[christine@server01 ~]$\nBesides acting as your access point to the shell, the prompt can provide additional help-\nful information. In the two preceding examples, the current user ID name, \nchristine, is \nshown in the prompt. Also, the name of the system is shown, \nserver01. You learn later in \nthis chapter about additional items shown in the prompt.\nIf you are new to the CLI, keep in mind that, after you type in a shell command at the prompt, you need to press the \nEnter key for the shell to act upon your command.\nThe shell prompt is not static. It can be changed to suit your needs. Chapter 6, “Using \nLinux Environment Variables,” covers modifying your shell CLI prompt confi guration.\n\n49\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  49\n3\n3\nThink of the shell CLI prompt as a helpmate, assisting you with your Linux system, \ngiving you helpful insights, and letting you know when the shell is ready for new \ncommands. Another helpful item in the shell is the bash Manual.\nInteracting with the bash Manual\nMost Linux distributions include an online manual for looking up information on shell \ncommands, as well as lots of other GNU utilities included in the distribution. You should \nbecome familiar with the manual, because it’s invaluable for working with commands, espe-\ncially when you’re trying to fi gure out various command line parameters.\nThe \nman command provides access to the manual pages stored on the Linux system. \nEntering the \nman command followed by a specifi c command name provides that utility’s \nmanual entry. Figure 3-1 shows an example of looking up the \nxterm command’s manual \npages. This page was reached by typing the command man  xterm.\nFIGURE 3-1\nManual pages for the xterm command\nNotice the xterm command DESCRIPTION paragraphs in Figure 3-1. They are rather sparse \nand full of technical jargon. The bash manual is not a step-by-step guide, but instead a \nquick reference.\n\n50\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  50\nIf you are new to the bash shell, you may fi nd that the man pages are not very helpful at fi rst. However, get into \nthe habit of using them, especially to read the fi rst paragraph or two of a command’s \nDESCRIPTION section. \nEventually, you will learn the technical lingo, and the man pages will become more helpful to you.\nWhen you use the man command to view a command’s manual pages, they are displayed \nwith something called a pager. A pager is a utility that allows you to page through \ndisplayed text. Thus, you can page through the man pages by pressing the spacebar, or you \ncan go line by line using the Enter key. In addition, you can use the arrow keys to scroll \nforward and backward through the man page text (assuming that your terminal emulation \npackage supports the arrow key functions).\nWhen you are fi nished with the man pages, press the q key to quit. When you quit the man \npages, you receive a shell CLI prompt, indicating the shell is waiting for your next command.\nThe bash manual even has reference information on itself. Type man man to see manual pages concerning the \nman pages.\nThe manual page divides information about a command into separate sections. Each section \nhas a conventional naming standard as shown in Table 3-1.\nTABLE 3-1    The Linux man Page Conventional Section Names\nSectionDescription\nNameDisplays command name and a short description\nSynopsisShows command syntax\nConfi gurationProvides confi guration information\nDescriptionDescribes command generally\nOptionsDescribes command option(s)\nExit StatusDefi nes command exit status indicator(s)\nReturn ValueDescribes command return value(s)\nErrorsProvides command error messages\nEnvironmentDescribes environment variable(s) used\nFilesDefi nes fi les used by command\nVersionsDescribes command version information\n\n51\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  51\n3\nConforming ToProvides standards followed\nNotesDescribes additional helpful command material\nBugsProvides the location to report found bugs\nExampleShows command use examples\nAuthorsProvides information on command developers\nCopyrightDefi nes command code copyright status\nSee AlsoRefers similar available commands\nNot every command’s man page has all the section names described in Table 3-1. Also, some \ncommands have section names that are not listed in the conventional standard.\nWhat if you can’t remember the command name? You can search the man pages using keywords. The syntax is \nman  -k  keyword. For example, to fi nd commands dealing with the terminals, you type man  -k  terminal.\nIn addition to the conventionally named sections for a man page, there are man page sec-\ntion areas. Each section area has an assigned number, starting at 1 and going to 9; they are \nlisted in Table 3-2.\nTABLE 3-2    The Linux man Page Section Areas\nSection NumberArea Contents\n1Executable programs or shell commands\n2System calls \n3Library calls\n4Special fi les\n5File formats and conventions\n6Games\n7Overviews, conventions, and miscellaneous\n8Super user and system administration commands\n9Kernel routines\n\n52\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  52\nTypically, the man utility provides the lowest numbered content area for the command. For \nexample, looking back to Figure 3-1 where the command man  xterm was entered, notice \nthat in the upper-left and upper-right display corners, the word \nXTERM is followed by a \nnumber in parentheses, \n(1). This means the man pages displayed are coming from content \narea 1 (executable programs or shell commands).\nOccasionally, a command has man pages in multiple section content areas. For example, \nthere is a command called \nhostname. The man pages contain information on the command \nas well as an overview section on system hostnames. To see the pages desired, you type \nman \nsection# topic. For the command’s man pages in section 1, type man  1  \nhostname. For the overview man pages in section 7, type man  7  hostname.\nYou can also step through an introduction to the various section content areas by typing \nman  1  intro to read about section 1, man  2  intro to read about section 2, man  3  intro \nto read about section 3, and so on.\nThe man pages are not the only reference. There are also the information pages called info \npages. You can learn about the info pages by typing info  info.\nIn addition, most commands accept the \n-help or --help option. For example, you can \ntype hostname  -help to see a help screen. For more information on using help, type \nhelp  help. (See a pattern here?)\nObviously, several helpful resources are available for reference. However, many basic shell \nconcepts still need detailed explanation. In the next section, we cover navigating through \nthe Linux fi lesystem.\nNavigating the Filesystem\nWhen you log into the system and reach the shell command prompt, you are usually placed \nin your home directory. Often, you want to explore other areas in the Linux system besides \njust your home directory. This section describes how to do that using shell commands. To \nstart, you need to take a tour of just what the Linux fi lesystem looks like so you know \nwhere you are going.\nLooking at the Linux fi lesystem\nIf you’re new to the Linux system, you may be confused by how it references fi les and \ndirectories, especially if you’re used to the way the Microsoft Windows operating system \ndoes that. Before exploring the Linux system, it helps to have an understanding of how it’s \nlaid out.\nThe fi rst difference you’ll notice is that Linux does not use drive letters in pathnames. In \nthe Windows world, the physical drives installed on the computer determine the pathname \n\n53\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  53\n3\nof the fi le. Windows assigns a letter to each physical disk drive, and each drive contains its \nown directory structure for accessing fi les stored on it.\nFor example, in Windows you may be used to seeing the fi le paths such as:\nc:\\Users\\Rich\\Documents\\test.doc\nThe Windows fi le path tells you exactly which physical disk partition contains the fi le \nnamed \ntest.doc. For example, if you saved test.doc on a fl ash drive, designated by the \nJ drive, the fi le path would be \nJ:\\test.doc. This path indicates that the fi le is located at \nthe root of the drive assigned the letter \nJ.\nThis is not the method used by Linux. Linux stores fi les within a single directory struc-\nture, called a virtual directory. The virtual directory contains fi le paths from all the storage \ndevices installed on the computer, merged into a single directory structure.\nThe Linux virtual directory structure contains a single base directory, called the root. \nDirectories and fi les beneath the root directory are listed based on the directory path used \nto get to them, similar to the way Windows does it.\nYou’ll notice that Linux uses a forward slash (/) instead of a backward slash (\\) to denote directories in fi le paths. \nThe backslash character in Linux denotes an escape character and causes all sorts of problems when you use it in a \nfi le path. This may take some getting used to if you’re coming from a Windows environment.\nIn Linux, you will see fi le paths similar to the following:\n/home/Rich/Documents/test.doc\nThis indicates the fi le test.doc is in the directory Documents, under the directory rich, \nwhich is contained in the directory \nhome. Notice that the path doesn’t provide any infor-\nmation as to which physical disk the fi le is stored on.\nThe tricky part about the Linux virtual directory is how it incorporates each storage device. \nThe fi rst hard drive installed in a Linux system is called the root drive. The root drive con-\ntains the virtual directory core. Everything else builds from there.\nOn the root drive, Linux can use special directories as mount points. Mount points are \ndirectories in the virtual directory where you can assign additional storage devices. Linux \ncauses fi les and directories to appear within these mount point directories, even though \nthey are physically stored on a different drive.\nOften system fi les are physically stored on the root drive. User fi les are typically stored on a \nseparate drive or drives, as shown in Figure 3-2.\n\n54\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  54\nFIGURE 3-2\nThe Linux fi le structure\nbin\nbarbara\njessica\nkatie\nrich\netc\nhome\nusr\nvar\nDisk 1\nDisk 2\nFigure 3-2 shows two hard drives on the computer. One hard drive is associated with the \nroot of the virtual directory (indicated by a single forward slash). Other hard drives can \nbe mounted anywhere in the virtual directory structure. In this example, the second hard \ndrive is mounted at the location \n/home, which is where the user directories are located.\nThe Linux fi lesystem structure originally evolved from the Unix fi le structure. In a Linux \nfi lesystem, common directory names are used for common functions. Table 3-3 lists some of \nthe more common Linux virtual top-level directory names and their contents.\nTABLE 3-3    Common Linux Directory Names\nDirectoryUsage\n/root of the virtual directory, where normally, no fi les are placed\n/binbinary directory, where many GNU user-level utilities are stored\n/bootboot directory, where boot fi les are stored\n/devdevice directory, where Linux creates device nodes\n/etcsystem confi guration fi les directory\n/homehome directory, where Linux creates user directories\n/liblibrary directory, where system and application library fi les are stored\n/mediamedia directory, a common place for mount points used for removable media\n/mntmount directory, another common place for mount points used for \nremovable media\n/optoptional directory, often used to store third-party software packages \nand data fi les\n\n55\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  55\n3\n/procprocess directory, where current hardware and process information is stored\n/rootroot home directory\n/sbinsystem binary directory, where many GNU admin-level utilities are stored\n/runrun directory, where runtime data is held during system operation\n/srvservice directory, where local services store their fi les\n/syssystem directory, where system hardware information fi les are stored\n/tmptemporary directory, where temporary work fi les can be created and destroyed\n/usruser binary directory, where the bulk of GNU user-level utilities and data fi les are \nstored\n/varvariable directory, for fi les that change frequently, such as log fi les\nThe common Linux directory names are based upon the Filesystem Hierarchy Standard \n(FHS). Many Linux distributions maintain compliance with FHS. Therefore, you should be \nable to easily fi nd fi les on any FHS-compliant Linux systems.\nThe FHS is occasionally updated. You may fi nd that some Linux distributions are still using an older FHS standard, \nwhile other distributions only partially implement the current standard. To keep up to date on the FHS standard, visit \nits offi cial home at \nhttp://www.pathname.com/fhs/.\nWhen you log in to your system and reach a shell CLI prompt, your session starts in your \nhome directory. Your home directory is a unique directory assigned to your user account. \nWhen a user account is created, the system normally assigns a unique directory for the \naccount (see Chapter 7).\nYou can move around the virtual directory using a graphical interface. However, to move \naround the virtual directory from a CLI prompt, you need to learn to use the \ncd command.\nTraversing directories\nYou use the change directory command (cd) to move your shell session to another directory \nin the Linux fi lesystem. The \ncd command syntax is pretty simplistic: cd destination.\nThe \ncd command may take a single parameter, destination, which specifi es the directory \nname you want to go to. If you don’t specify a destination on the \ncd command, it takes you \nto your home directory.\n\n56\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  56\nThe destination parameter can be expressed using two different methods. One method is \nusing an absolute directory reference. The other method uses a relative directory reference.\nThe following sections describe each of these methods. The differences between these two \nmethods are important to understand as you traverse the fi lesystem.\nUsing absolute directory references\nYou can reference a directory name within the virtual directory system using an absolute \ndirectory reference. The absolute directory reference defi nes exactly where the directory is \nin the virtual directory structure, starting at the root. Think of the absolute directory ref-\nerence as the full name for a directory.\nAn absolute directory reference always begins with a forward slash (\n/), indicating the \nvirtual directory system’s root. Thus, to reference user binaries, contained within the \nbin \ndirectory stored within the \nusr directory, you would use an absolute directory reference as \nfollows:\n/usr/bin\nWith the absolute directory reference, there’s no doubt as to exactly where you want to go. \nTo move to a specifi c location in the fi lesystem using the absolute directory reference, you \njust specify the full pathname in the \ncd command:\nchristine@server01:~$ cd /usr/bin\nchristine@server01:/usr/bin$ \nNotice in the preceding example that the prompt originally had a tilde (~) in it. After the \nchange to a new directory occurred, the tilde was replaced by \n/usr/bin. This is where a \nCLI prompt can help you keep track of where you are in the virtual directory structure. The \ntilde indicates that your shell session is located in your home directory. After you move \nout of your home directory, the absolute directory reference is shown in the prompt, if the \nprompt has been confi gured to do so.\nIf your shell CLI prompt does not show your shell session’s current location, then it has not been confi gured to do so. \nChapter 6 shows you how to make confi guration changes, if you desire modifi cations to your CLI prompt.\nIf your prompt has not been confi gured to show the shell session’s current absolute direc-\ntory location, then you can display the location via a shell command. The \npwd command \ndisplays the shell session’s current directory location, which is called the present working \ndirectory. An example of using the \npwd command is shown here.\nchristine@server01:/usr/bin$ pwd\n/usr/bin\nchristine@server01:/usr/bin$\n\n57\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  57\n3\nIt is a good habit to use the pwd command whenever you change to a new present working directory. Because many \nshell commands operate on the present working directory, you always want to make sure you are in the correct direc-\ntory before issuing a command.\nYou can move to any level within the entire Linux virtual directory structure from any \nlevel using the absolute directory reference: \nchristine@server01:/usr/bin$ cd /var/log\nchristine@server01:/var/log$\nchristine@server01:/var/log$ pwd\n/var/log\nchristine@server01:/var/log$\nYou can also quickly jump to your home directory from any level within the Linux virtual \ndirectory structure:\nchristine@server01:/var/log$ cd\nchristine@server01:~$\nchristine@server01:~$ pwd\n/home/christine\nchristine@server01:~$\nHowever, if you’re just working within your own home directory structure, often using \nabsolute directory references can get tedious. For example, if you’re already in the directory \n/home/christine, it seems somewhat cumbersome to have to type the command:\ncd /home/christine/Documents\njust to get to your Documents directory. Fortunately, there’s a simpler solution.\nUsing relative directory references\nRelative directory references allow you to specify a destination directory reference relative to \nyour current location. A relative directory reference doesn’t start with a forward slash (\n/).\nInstead, a relative directory reference starts with either a directory name (if you’re travers-\ning to a directory under your current directory) or a special character. For example, if you \nare in your \nhome directory and want to move to your Documents subdirectory, you can use \nthe \ncd command along with a relative directory reference:\nchristine@server01:~$ pwd\n/home/christine\nchristine@server01:~$\nchristine@server01:~$ cd Documents\nchristine@server01:~/Documents$ pwd\n/home/christine/Documents\nchristine@server01:~/Documents$ \n\n58\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  58\nIn the preceding example, note that no forward slash (/) was used. Instead a relative direc-\ntory reference was used and the present work directory was changed from \n/home/\nchristine\n to /home/christine/Documents, with much less typing.\nAlso notice in the example that if the prompt is confi gured to display the present working \ndirectory, it keeps the tilde in the display. This shows that the present working directory is \nin a directory under the user’s \nhome directory.\nIf you are new to the command line and the Linux directory structure, it is recommended that you stick with absolute \ndirectory references for a while. After you become more familiar with the directory layout, switch to using relative \ndirectory references.\nYou can use a relative directory reference with the cd command in any directory contain-\ning subdirectories. You can also use a special character to indicate a relative directory \nlocation.\nThe two special characters used for relative directory references are:\n ■\nThe single dot (.) to represent the current directory\n ■\nThe double dot (..) to represent the parent directory\nYou can use the single dot, but it doesn’t make sense to use it with the \ncd command. Later \nin the chapter, you will see how another command uses the single dot for relative directory \nreferences effectively.\nThe double dot character is extremely handy when trying to traverse a directory hierarchy. \nFor example, if you are in the \nDocuments directory under your home directory and need to \ngo to your \nDownloads directory, also under your home directory, you can do this:\nchristine@server01:~/Documents$ pwd\n/home/christine/Documents\nchristine@server01:~/Documents$ cd ../Downloads\nchristine@server01:~/Downloads$ pwd\n/home/christine/Downloads\nchristine@server01:~/Downloads$ \nThe double dot character takes you back up one level to your home directory; then the /\nDownloads\n portion of the command takes you back down into the Downloads directory. \nYou can use as many double dot characters as necessary to move around. For example, if \nyou are in your home directory (\n/home/christine) and want to go to the /etc directory, \nyou could type the following:\nchristine@server01:~$ cd ../../etc\nchristine@server01:/etc$ pwd\n/etc\nchristine@server01:/etc$ \n\n59\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  59\n3\nOf course, in a case like this, you actually have to do more typing rather than just typing \nthe absolute directory reference, \n/etc. Thus, use a relative directory reference only if it \nmakes sense to do so.\nIt’s helpful to have a long informative shell CLI prompt, as used in this chapter section. However, for clarity purposes, \na simple \n$ prompt is used in the rest of the book’s examples.\nNow that you know how to traverse the directory system and confi rm your present work-\ning directory, you can start to explore what’s contained within the various directories. \nThe next section takes you through the process of looking at fi les within the directory \nstructure.\nListing Files and Directories\nTo see what fi les are available on the system, use the list command (ls). This section \ndescribes the \nls command and options available to format the information it can display.\nDisplaying a basic listing\nThe ls command at its most basic form displays the fi les and directories located in your \ncurrent directory:\n$ ls\nDesktop    Downloads         Music      Pictures  Templates  Videos\nDocuments  examples.desktop  my_script  Public    test_file\n$ \nNotice that the ls command produces the listing in alphabetical order (in columns rather \nthan rows). If you’re using a terminal emulator that supports color, the \nls command may \nalso show different types of entries in different colors. The \nLS_COLORS environment \nvariable controls this feature. (Environment variables are covered in Chapter 6). Different \nLinux distributions set this environment variable depending on the capabilities of the \nterminal emulator.\nIf you don’t have a color terminal emulator, you can use the \n-F parameter with the ls \ncommand to easily distinguish fi les from directories. Using the \n-F parameter produces the \nfollowing output:\n$ ls -F\nDesktop/   Downloads/       Music/     Pictures/ Templates/ Videos/\nDocuments/ examples.desktop my_script* Public/   test_file\n$\n\n60\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  60\nThe -F parameter fl ags the directories with a forward slash (/), to help identify them in \nthe listing. Similarly, it fl ags executable fi les (like the \nmy_script fi le in the preceding \ncode) with an asterisk (\n*), to help you more easily fi nd fi les that can be run on the system.\nThe basic \nls command can be somewhat misleading. It shows the fi les and directories \ncontained in the current directory, but not necessarily all of them. Linux often uses \nhidden fi les to store confi guration information. In Linux, hidden fi les are fi les with \nfi lenames starting with a period (\n.). These fi les don’t appear in the default ls listing. Thus, \nthey are called hidden fi les.\nTo display hidden fi les along with normal fi les and directories, use the \n-a parameter. Here \nis an example of using the \n-a parameter with the ls command.\n$ ls -a\n.              .compiz    examples.desktop  Music      test_file\n..             .config    .gconf            my_script  Videos\n.bash_history  Desktop    .gstreamer-0.10   Pictures   .Xauthority\n.bash_logout   .dmrc      .ICEauthority     .profile   .xsession-errors\n.bashrc        Documents  .local            Public     .xsession-errors.old\n.cache         Downloads  .mozilla          Templates\n$\nAll the fi les beginning with a period, hidden fi les, are now shown. Notice that three fi les \nbegin with \n.bash. These are hidden fi les that are used by the bash shell environment. \nThese features are covered in detail in Chapter 6.\nThe \n-R parameter is another option the ls command can use. Called the recursive option, \nit shows fi les that are contained within subdirectories in the current directory. If you have \nlots of subdirectories, this can be quite a long listing. Here’s a simple example of what the \n-R parameter produces. The -F option was tacked on to help you see the fi le types:\n$ ls -F -R\n.:\nDesktop/   Downloads/       Music/     Pictures/ Templates/ Videos/\nDocuments/ examples.desktop my_script* Public/   test_file\n./Desktop:\n./Documents:\n./Downloads:\n./Music:\nILoveLinux.mp3*\n./Pictures:\n./Public:\n\n61\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  61\n3\n./Templates:\n./Videos:\n$\nNotice that the -R parameter shows the contents of the current directory, which are the \nfi les from a user’s \nhome directory shown in earlier examples. It also shows each subdirec-\ntory in the user’s \nhome directory and their contents. The only subdirectory containing a \nfi le is the \nMusic subdirectory, and it contains the executable fi le, ILoveLinux.mp3.\nOption parameters don’t have to be entered separately as shown in the nearby example: ls  -F  -R. They can \noften be combined as follows:  \nls  -FR.\nIn the previous example, there were no subdirectories within subdirectories. If there \nhad been further subdirectories, the \n-R parameter would have continued to traverse \nthose as well. As you can see, for large directory structures, this can become quite a \nlarge output listing.\nDisplaying a long listing\nIn the basic listings, the ls command doesn’t produce much information about each fi le. \nFor listing additional information, another popular parameter is \n-l. The -l parameter \nproduces a long listing format, providing more information about each fi le in the directory:\n$ ls -l\ntotal 48\ndrwxr-xr-x 2 christine christine 4096 Apr 22 20:37 Desktop\ndrwxr-xr-x 2 christine christine 4096 Apr 22 20:37 Documents\ndrwxr-xr-x 2 christine christine 4096 Apr 22 20:37 Downloads\n-rw-r--r-- 1 christine christine 8980 Apr 22 13:36 examples.desktop\n-rw-rw-r-- 1 christine christine    0 May 21 13:44 fall\n-rw-rw-r-- 1 christine christine    0 May 21 13:44 fell\n-rw-rw-r-- 1 christine christine    0 May 21 13:44 fill\n-rw-rw-r-- 1 christine christine    0 May 21 13:44 full\ndrwxr-xr-x 2 christine christine 4096 May 21 11:39 Music\n-rw-rw-r-- 1 christine christine    0 May 21 13:25 my_file\n-rw-rw-r-- 1 christine christine    0 May 21 13:25 my_scrapt\n-rwxrw-r-- 1 christine christine   54 May 21 11:26 my_script\n-rw-rw-r-- 1 christine christine    0 May 21 13:42 new_file\ndrwxr-xr-x 2 christine christine 4096 Apr 22 20:37 Pictures\ndrwxr-xr-x 2 christine christine 4096 Apr 22 20:37 Public\ndrwxr-xr-x 2 christine christine 4096 Apr 22 20:37 Templates\n-rw-rw-r-- 1 christine christine    0 May 21 11:28 test_file\ndrwxr-xr-x 2 christine christine 4096 Apr 22 20:37 Videos\n$ \n\n62\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  62\nThe long listing format lists each fi le and subdirectory on a single line. In addition to \nthe fi lename, the listing shows additional useful information. The fi rst line in the output \nshows the total number of blocks contained within the directory. After that, each line con-\ntains the following information about each fi le (or directory):\n ■\nThe fi le type — such as directory (d), fi le (-), linked fi le (l), character device (c), \nor block device (\nb)\n ■\nThe fi le permissions (see Chapter 6)\n ■\nThe number of fi le hard links (See the section “Linking Files” in Chapter 7.)\n ■\nThe fi le owner username\n ■\nThe fi le primary group name\n ■\nThe fi le byte size\n ■\nThe last time the fi le was modifi ed\n ■\nThe fi lename or directory name\nThe \n-l parameter is a powerful tool to have. Armed with this parameter, you can see most \nof the information you need for any fi le or directory.\nThe \nls command has lots of parameters that can come in handy as you do fi le management. \nIf you type at the shell prompt \nman  ls, you see several pages of available parameters for \nyou to use to modify the \nls command output.\nDon’t forget that you can also combine many of the parameters. You can often fi nd a param-\neter combination that not only displays the desired output, but also is easy to remember, \nsuch as \nls  -alF.\nFiltering listing output\nAs you’ve seen in the examples, by default the ls command lists all the non-hidden direc-\ntory fi les. Sometimes, this can be overkill, especially when you’re just looking for informa-\ntion on a few fi les.\nFortunately, the \nls command also provides a way for you to defi ne a fi lter on the \ncommand line. It uses the fi lter to determine which fi les or directories it should display in \nthe output.\nThe fi lter works as a simple text-matching string. Include the fi lter after any command line \nparameters you want to use:\n$ ls -l my_script\n-rwxrw-r-- 1 christine christine 54 May 21 11:26 my_script\n$ \nWhen you specify the name of a specifi c fi le as the fi lter, the ls command only shows that \nfi le’s information. Sometimes, you might not know the exact fi lename you’re looking for. \n\n63\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  63\n3\nThe ls command also recognizes standard wildcard characters and uses them to match \npatterns within the fi lter:\n ■\nA question mark (?) to represent one character\n ■\nAn asterisk (*) to represent any number of characters\nThe question mark can be used to replace exactly one character anywhere in the fi lter \nstring. For example:\n$ ls -l my_scr?pt\n-rw-rw-r-- 1 christine christine  0 May 21 13:25 my_scrapt\n-rwxrw-r-- 1 christine christine 54 May 21 11:26 my_script\n$ \nThe fi lter my_scr?pt matched two fi les in the directory. Similarly, the asterisk can be used \nto match zero or more characters:\n$ ls -l my*\n-rw-rw-r-- 1 christine christine  0 May 21 13:25 my_file\n-rw-rw-r-- 1 christine christine  0 May 21 13:25 my_scrapt\n-rwxrw-r-- 1 christine christine 54 May 21 11:26 my_script\n$ \nUsing the asterisk fi nds three different fi les, starting with the name my. As with the \nquestion mark, you can place the asterisks anywhere in the fi lter:\n$ ls -l my_s*t\n-rw-rw-r-- 1 christine christine  0 May 21 13:25 my_scrapt\n-rwxrw-r-- 1 christine christine 54 May 21 11:26 my_script\n$ \nUsing the asterisk and question mark in the fi lter is called fi le globbing. File globbing is the \nprocessing of pattern matching using wildcards. The wildcards are offi cially called \nmetacharacter wildcards. You can use more metacharacter wildcards for fi le globbing than \njust the asterisk and question mark. You can also use brackets:\n$ ls -l my_scr[ai]pt\n-rw-rw-r-- 1 christine christine  0 May 21 13:25 my_scrapt\n-rwxrw-r-- 1 christine christine 54 May 21 11:26 my_script\n$ \nIn this example, we used the brackets along with two potential choices for a single character \nin that position, \na or i. The brackets represent a single character position and give you mul-\ntiple options for fi le globbing. You can list choices of characters, as shown in the preceding \nexample, and you can specify a range of characters, such as an alphabetic range \n[a - i]:\n$ ls -l f[a-i]ll\n-rw-rw-r-- 1 christine christine 0 May 21 13:44 fall\n-rw-rw-r-- 1 christine christine 0 May 21 13:44 fell\n-rw-rw-r-- 1 christine christine 0 May 21 13:44 fill\n$ \n\n64\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  64\nAlso, you can specify what should not be included in the pattern match by using the excla-\nmation point (\n!):\n$ ls -l f[!a]ll\n-rw-rw-r-- 1 christine christine 0 May 21 13:44 fell\n-rw-rw-r-- 1 christine christine 0 May 21 13:44 fill\n-rw-rw-r-- 1 christine christine 0 May 21 13:44 full\n$ \nFile globbing is a powerful feature when searching for fi les. It can also be used with other \nshell commands besides \nls. You fi nd out more about this later in the chapter.\nHandling Files\nThe shell provides many fi le manipulation commands on the Linux fi lesystem. This section \nwalks you through the basic shell commands you need to handle fi les.\nCreating fi les\nEvery once in a while you run into a situation where you need to create an empty fi le. For \nexample, sometimes applications expect a log fi le to be present before they can write to it. \nIn these situations, you can use the \ntouch command to easily create an empty fi le:\n$ touch test_one\n$ ls -l test_one\n-rw-rw-r-- 1 christine christine 0 May 21 14:17 test_one\n$\nThe touch command creates the new fi le you specify and assigns your username as the fi le \nowner. Notice in the preceding example that the fi le size is zero because the \ntouch com-\nmand just created an empty fi le.\nThe \ntouch command can also be used to change the modifi cation time. This is done with-\nout changing the fi le contents:\n$ ls -l test_one\n-rw-rw-r-- 1 christine christine 0 May 21 14:17 test_one\n$ touch test_one\n$ ls -l test_one\n-rw-rw-r-- 1 christine christine 0 May 21 14:35 test_one\n$\nThe modifi cation time of test_one is now updated to 14:35 from the original time, \n14:17. To change only the access time, use the -a parameter with the touch command:\n$ ls -l test_one\n-rw-rw-r-- 1 christine christine 0 May 21 14:35 test_one\n$ touch -a test_one\n\n65\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  65\n3\n$ ls -l test_one\n-rw-rw-r-- 1 christine christine 0 May 21 14:35 test_one\n$ ls -l --time=atime test_one\n-rw-rw-r-- 1 christine christine 0 May 21 14:55 test_one\n$\nIn the preceding example, notice that by using only the ls -l command, the access time \ndoes not display. This is because the modifi cation time is shown by default. To see a fi le’s \naccess time, you need to add an additional parameter, \n--time=atime. After we add that \nparameter in the preceding example, the fi le’s altered access time is displayed.\nCreating empty fi les and altering fi le timestamps is not something you will do on a Linux \nsystem daily. However, copying fi les is an action you will do often while using the shell.\nCopying fi les\nCopying fi les and directories from one location in the fi lesystem to another is a common \npractice for system administrators. The \ncp command provides this feature.\nIn its most basic form, the \ncp command uses two parameters — the source object and the \ndestination object: \ncp  source  destination.\nWhen both the \nsource and destination parameters are fi lenames, the cp command \ncopies the source fi le to a new destination fi le. The new fi le acts like a brand new fi le, with \nan updated modifi cation time:\n$ cp test_one  test_two\n$ ls -l test_*\n-rw-rw-r-- 1 christine christine 0 May 21 14:35 test_one\n-rw-rw-r-- 1 christine christine 0 May 21 15:15 test_two\n$\nThe new fi le test_two shows a different modifi cation time than the test_one fi le. If the \ndestination fi le already exists, the \ncp command may not prompt you to this fact. It is best \nto add the \n-i option to force the shell to ask whether you want to overwrite a fi le:\n$ ls -l test_*\n-rw-rw-r-- 1 christine christine 0 May 21 14:35 test_one\n-rw-rw-r-- 1 christine christine 0 May 21 15:15 test_two\n$\n$ cp -i test_one  test_two\ncp: overwrite 'test_two'? n\n$\nIf you don’t answer y, the fi le copy does not proceed. You can also copy a fi le into a \npre-existing directory:\n$ cp -i test_one  /home/christine/Documents/\n$\n\n66\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  66\n$ ls -l /home/christine/Documents\ntotal 0\n-rw-rw-r-- 1 christine christine 0 May 21 15:25 test_one\n$\nThe new fi le is now under the Documents subdirectory, using the same fi lename as the \noriginal. \nThe preceding example uses a trailing forward slash (/) on the destination directory name. Using the slash indicates \nDocuments is a directory and not a fi le. This is helpful for clarity purposes and is important when copying single \nfi les. If the forward slash is not used and the subdirectory \n/home/christine/Documents does not exist, prob-\nlems can occur. In this case, attempting to copy a single fi le to the \nDocuments subdirectory creates a fi le named \nDocuments instead, and no error messages display!\nThis last example used an absolute directory reference, but you can just as easily use a rela-\ntive directory reference:\n$ cp -i test_one  Documents/\ncp: overwrite 'Documents/test_one'? y\n$\n$ ls -l Documents\ntotal 0\n-rw-rw-r-- 1 christine christine 0 May 21 15:28 test_one\n$\nEarlier in this chapter, you read about the special symbols that can be used in relative \ndirectory references. One of them, the single dot (.), is great to use with the \ncp command. \nRemember that the single dot represents your present working directory. If you need to \ncopy a fi le with a long source object name to your present working directory, the single dot \ncan simplify the task:\n$ cp -i /etc/NetworkManager/NetworkManager.conf  .\n$\n$ ls -l NetworkManager.conf\n-rw-r--r-- 1 christine christine 76 May 21 15:55 NetworkManager.conf\n$\nIt’s hard to see that single dot! If you look closely, you’ll see it at the end of the fi rst exam-\nple code line. Using the single dot symbol is much easier than typing a full destination \nobject name, when you have long source object names.\nThere are many more cp command parameters than those described here. Remember that you can see all the differ-\nent available parameters available for the \ncp command, by typing man cp.\n\n67\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  67\n3\nThe -R parameter is a powerful cp command option. It allows you to recursively copy the \ncontents of an entire directory in one command:\n$ ls -Fd *Scripts\nScripts/\n$ ls -l Scripts/\ntotal 25\n-rwxrw-r-- 1 christine christine 929 Apr  2 08:23 file_mod.sh\n-rwxrw-r-- 1 christine christine 254 Jan  2 14:18 SGID_search.sh\n-rwxrw-r-- 1 christine christine 243 Jan  2 13:42 SUID_search.sh\n$\n$ cp -R Scripts/  Mod_Scripts\n$ ls -Fd *Scripts\nMod_Scripts/  Scripts/\n$ ls -l Mod_Scripts\ntotal 25\n-rwxrw-r-- 1 christine christine 929 May 21 16:16 file_mod.sh\n-rwxrw-r-- 1 christine christine 254 May 21 16:16 SGID_search.sh\n-rwxrw-r-- 1 christine christine 243 May 21 16:16 SUID_search.sh\n$\nThe directory Mod_Scripts did not exist prior to the cp -R command. It was created \nwith the \ncp  -R command, and the entire Scripts directory’s contents were copied into \nit. Notice that all the fi les in the new \nMod_Scripts directory have new dates associated \nwith them. Now \nMod_Scripts is a complete copy of the Scripts directory.\nIn the preceding example, the options -Fd were added to the ls command. You read about the -F option earlier \nin this chapter. However, the \n-d option may be new to you. The -d option lists a directory’s information but not its \ncontents.\nYou can also use wildcard metacharacters in your cp commands:\n$ cp *script  Mod_Scripts/\n$ ls -l Mod_Scripts\ntotal 26\n-rwxrw-r-- 1 christine christine 929 May 21 16:16 file_mod.sh\n-rwxrw-r-- 1 christine christine 54  May 21 16:27 my_script\n-rwxrw-r-- 1 christine christine 254 May 21 16:16 SGID_search.sh\n-rwxrw-r-- 1 christine christine 243 May 21 16:16 SUID_search.sh\n$\nThis command copied any fi les that ended with script to Mod_Scripts. In this case, \nonly one fi le needed to be copied: \nmy_script.\nWhen copying fi les, another shell feature can help you besides the single dot and wildcard \nmetacharacters. It is called tab auto-complete.\n\n68\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  68\nUsing tab auto-complete\nWhen working at the command line, you can easily mistype a command, directory name, or \nfi lename. In fact, the longer a directory reference or fi lename, the greater the chance that \nyou will mistype it.\nThis is where tab auto-complete can be a lifesaver. Tab auto-complete allows you to start \ntyping a fi lename or directory name and then press the tab key to have the shell complete \nit for you:\n$ ls really*\nreally_ridiculously_long_file_name\n$\n$ cp really_ridiculously_long_file_name  Mod_Scripts/\nls -l Mod_Scripts\ntotal 26\n-rwxrw-r-- 1 christine christine 929 May 21 16:16 file_mod.sh\n-rwxrw-r-- 1 christine christine 54  May 21 16:27 my_script\n-rw-rw-r-- 1 christine christine  0  May 21 17:08 \nreally_ridiculously_long_file_name\n-rwxrw-r-- 1 christine christine 254 May 21 16:16 SGID_search.sh\n-rwxrw-r-- 1 christine christine 243 May 21 16:16 SUID_search.sh\n$\nIn the preceding example, we typed the command cp  really and pressed the tab key, and \nthe shell auto-completed the rest of the fi lename! Of course, the destination directory had \nto be typed, but still tab auto-complete saved the command from several potential typo-\ngraphical errors.\nThe trick to using tab auto-complete is to give the shell enough fi lename characters so it \ncan distinguish the desired fi le from other fi les. For example, if another fi lename started \nwith \nreally, pressing the tab key would not auto-complete the fi lename. Instead, you \nwould hear a beep. If this happens, you can press the tab key again, and the shell shows \nyou all the fi lenames starting with \nreally. This feature allows you to see what needs to be \ntyped for tab auto-complete to work properly.\nLinking fi les\nLinking fi les is a great option available in the Linux fi lesystem. If you need to maintain \ntwo (or more) copies of the same fi le on the system, instead of having separate physical \ncopies, you can use one physical copy and multiple virtual copies, called links. A link is a \nplaceholder in a directory that points to the real location of the fi le. Two types of fi le links \nare available in Linux:\n ■\nA symbolic link\n ■\nA hard link\n\n69\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  69\n3\nA symbolic link is simply a physical fi le that points to another fi le somewhere in the \nvirtual directory structure. The two symbolically linked together fi les do not share the \nsame contents.\nTo create a symbolic link to a fi le, the original fi le must pre-exist. We can then use the \nln \ncommand with the \n-s option to create the symbolic link:\n$ ls -l data_file\n-rw-rw-r-- 1 christine christine 1092 May 21 17:27 data_file\n$\n$ ln -s data_file  sl_data_file\n$\n$ ls -l *data_file\n-rw-rw-r-- 1 christine christine 1092 May 21 17:27 data_file\nlrwxrwxrwx 1 christine christine    9 May 21 17:29 sl_data_file -> data_file\n$ \nIn the preceding example, notice that the name of the symbolic link, sl_data_file, is \nlisted second in the \nln command. The —> symbol displayed after the symbolic link fi le’s \nlong listing shows that it is symbolically linked to the fi le \ndata_file.\nAlso note the symbolic link’s fi le size versus the data fi le’s fi le size. The symbolic link, \nsl_data_file, is only 9 bytes, whereas the data_file is 1092 bytes. This is because \nsl_data_file is only pointing to data_file. They do not share contents and are two \nphysically separate fi les.\nAnother way to tell that these linked fi les are separate physical fi les is by viewing their \ninode number. The inode number of a fi le or directory is a unique identifi cation number \nthat the kernel assigns to each object in the fi lesystem. To view a fi le or directory’s inode \nnumber, add the \n-i parameter to the ls command:\n$ ls -i *data_file\n296890 data_file  296891 sl_data_file\n$ \nThe example shows that the data fi le’s inode number is 296890, while the sl_data_file \ninode number is different. It is \n296891. Thus, they are different fi les.\nA hard link creates a separate virtual fi le that contains information about the original fi le \nand where to locate it. However, they are physically the same fi le. When you reference the \nhard link fi le, it’s just as if you’re referencing the original fi le. To create a hard link, again \nthe original fi le must pre-exist, except that this time no parameter is needed on the \nln \ncommand:\n$ ls -l code_file\n-rw-rw-r-- 1 christine christine 189 May 21 17:56 code_file\n$\n$ ln code_file  hl_code_file\n\n70\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  70\n$\n$ ls -li *code_file\n296892 -rw-rw-r-- 2 christine christine 189 May 21 17:56 \ncode_file\n296892 -rw-rw-r-- 2 christine christine 189 May 21 17:56 \nhl_code_file\n$\nIn the preceding example, we used the ls -li command to show both the inode numbers \nand a long listing for the \n*code_files. Notice that both fi les, which are hard linked \ntogether, share the name inode number. This is because they are physically the same fi le. \nAlso notice that the link count (the third item in the listing) now shows that both fi les \nhave two links. In addition, their fi le size is exactly the same size as well.\nYou can only create a hard link between fi les on the same physical medium. To create a link between fi les under \nseparate physical mediums, you must use a symbolic link.\nBe careful when copying linked fi les. If you use the cp command to copy a fi le that’s linked \nto another source fi le, all you’re doing is making another copy of the source fi le. This can \nquickly get confusing. Instead of copying the linked fi le, you can create another link to the \noriginal fi le. You can have many links to the same fi le with no problems. However, you also \ndon’t want to create soft links to other soft-linked fi les. This creates a chain of links that \ncan be confusing — and easily broken — causing all sorts of problems.\nYou may fi nd symbolic and hard links diffi cult concepts. Fortunately, renaming fi les in the \nnext section is a great deal easier to understand.\nRenaming fi les\nIn the Linux world, renaming fi les is called moving files. The mv command is available to \nmove both fi les and directories to another location or a new name:\n$ ls -li f?ll\n296730 -rw-rw-r-- 1 christine christine 0 May 21 13:44 fall\n296717 -rw-rw-r-- 1 christine christine 0 May 21 13:44 fell\n294561 -rw-rw-r-- 1 christine christine 0 May 21 13:44 fill\n296742 -rw-rw-r-- 1 christine christine 0 May 21 13:44 full\n$\n$ mv fall  fzll\n$\n$ ls -li f?ll\n296717 -rw-rw-r-- 1 christine christine 0 May 21 13:44 fell\n294561 -rw-rw-r-- 1 christine christine 0 May 21 13:44 fill\n296742 -rw-rw-r-- 1 christine christine 0 May 21 13:44 full\n296730 -rw-rw-r-- 1 christine christine 0 May 21 13:44 fzll\n$\n\n71\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  71\n3\nNotice that moving the fi le changed the name from fall to fzll, but it kept the same \ninode number and timestamp value. This is because \nmv affects only a fi le’s name.\nYou can also use \nmv to change a fi le’s location:\n$ ls -li /home/christine/fzll\n296730 -rw-rw-r-- 1 christine christine 0 May 21 13:44 \n/home/christine/fzll\n$\n$ ls -li /home/christine/Pictures/\ntotal 0\n$ mv fzll  Pictures/\n$\n$ ls -li /home/christine/Pictures/\ntotal 0\n296730 -rw-rw-r-- 1 christine christine 0 May 21 13:44 fzll\n$\n$ ls -li /home/christine/fzll\nls: cannot access /home/christine/fzll: No such file or directory\n$\nIn the preceding example, we moved the fi le fzll from /home/christine to /home/\nchristine/Pictures\n using the mv command. Again, there were no changes to the fi le’s \ninode number or timestamp value.\nLike the cp command, you can use the -i option on the mv command. Thus, you are asked before the command \nattempts to overwrite any pre-existing fi les.\nThe only change was to the fi le’s location. The fzll fi le no longer exists in /home/\nchristine\n, because a copy of it was not left in its original location, as the cp command \nwould have done.\nYou can use the \nmv command to move a fi le’s location and rename it, all in one easy step:\n$ ls -li Pictures/fzll\n296730 -rw-rw-r-- 1 christine christine 0 May 21 13:44 \nPictures/fzll\n$\n$ mv /home/christine/Pictures/fzll  /home/christine/fall\n$\n$ ls -li /home/christine/fall\n296730 -rw-rw-r-- 1 christine christine 0 May 21 13:44 \n/home/christine/fall\n$\n$ ls -li /home/christine/Pictures/fzll\nls: cannot access /home/christine/Pictures/fzll: \nNo such file or directory\n\n72\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  72\nFor this example, we moved the fi le fzll from a subdirectory, Pictures, to the home \ndirectory, \n/home/christine, and renamed it to fall. Neither the timestamp value nor \nthe inode number changed. Only the location and name were altered.\nYou can also use the \nmv command to move entire directories and their contents:\n$ ls -li Mod_Scripts\ntotal 26\n296886 -rwxrw-r-- 1 christine christine 929 May 21 16:16 \nfile_mod.sh\n296887 -rwxrw-r-- 1 christine christine  54 May 21 16:27 \nmy_script\n296885 -rwxrw-r-- 1 christine christine 254 May 21 16:16 \nSGID_search.sh\n296884 -rwxrw-r-- 1 christine christine 243 May 21 16:16 \nSUID_search.sh\n$\n$ mv Mod_Scripts  Old_Scripts\n$\n$ ls -li Mod_Scripts\nls: cannot access Mod_Scripts: No such file or directory\n$\n$ ls -li Old_Scripts\ntotal 26\n296886 -rwxrw-r-- 1 christine christine 929 May 21 16:16 \nfile_mod.sh\n296887 -rwxrw-r-- 1 christine christine  54 May 21 16:27 \nmy_script\n296885 -rwxrw-r-- 1 christine christine 254 May 21 16:16 \nSGID_search.sh\n296884 -rwxrw-r-- 1 christine christine 243 May 21 16:16 \nSUID_search.sh\n$\nThe directory’s entire contents are unchanged. The only thing that changes is the name of \nthe directory. \nAfter you know how to rename...err...move fi les with the \nmv command, you realize how \nsimple it is to accomplish. Another easy, but potentially dangerous, task is deleting fi les.\nDeleting fi les\nMost likely at some point you’ll want to be able to delete existing fi les. Whether it’s to \nclean up a fi lesystem or to remove a software package, you always have opportunities to \ndelete fi les.\nIn the Linux world, deleting is called removing. The command to remove fi les in the bash \nshell is \nrm. The basic form of the rm command is simple:\n\n73\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  73\n3\n$ rm -i fall\nrm: remove regular empty file 'fall'? y\n$ \n$ ls -l fall\nls: cannot access fall: No such file or directory\n$\nNotice that the -i command parameter prompts you to make sure that you’re serious \nabout removing the fi le. The shell has no recycle bin or trashcan. After you remove a fi le, \nit’s gone forever. Therefore, a good habit is to always tack on the \n-i parameter to the rm \ncommand.\nYou can also use wildcard metacharacters to remove groups of fi les. However, again, use \nthat \n-i option to protect yourself:\n$ rm -i f?ll\nrm: remove regular empty file 'fell'? y\nrm: remove regular empty file 'fill'? y\nrm: remove regular empty file 'full'? y\n$\n$ ls -l f?ll\nls: cannot access f?ll: No such file or directory\n$\nOne other feature of the rm command, if you’re removing lots of fi les and don’t want to \nbe bothered with the prompt, is to use the \n-f parameter to force the removal. Just be \ncareful!\nManaging Directories\nLinux has a few commands that work for both fi les and directories (such as the cp com-\nmand), and some that work only for directories. To create a new directory, you need to use \na specifi c command, which is covered in this section. Removing directories can get inter-\nesting, so that is covered in this section as well.\nCreating directories\nCreating a new directory in Linux is easy — just use the mkdir command:\n$ mkdir New_Dir\n$ ls -ld New_Dir\ndrwxrwxr-x 2 christine christine 4096 May 22 09:48 New_Dir\n$\nThe system creates a new directory named New_Dir. Notice in the new directory’s long list-\ning that the directory’s record begins with a \nd. This indicates that New_Dir is not a fi le, \nbut a directory.\n\n74\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  74\nYou can create directories and subdirectories in “bulk” if needed. However, if you attempt \nthis with just the \nmkdir command, you get the following error message:\n$ mkdir New_Dir/Sub_Dir/Under_Dir\nmkdir: cannot create directory 'New_Dir/Sub_Dir/Under_Dir': \nNo such file or directory\n$\nTo create several directories and subdirectories at the same time, you need to add the -p \nparameter:\n$ mkdir -p New_Dir/Sub_Dir/Under_Dir\n$\n$ ls -R New_Dir\nNew_Dir:\nSub_Dir\nNew_Dir/Sub_Dir:\nUnder_Dir\nNew_Dir/Sub_Dir/Under_Dir:\n$\nThe -p option on the mkdir command makes any missing parent directories as needed. A \nparent directory is a directory that contains other directories at the next level down the \ndirectory tree. \nOf course, after you make something, you need to know how to delete it. This is especially \nuseful if you created a directory in the wrong location.\nDeleting directories\nRemoving directories can be tricky, and for good reason. There are lots of opportunities for \nbad things to happen when you start deleting directories. The shell tries to protect us from \naccidental catastrophes as much as possible. The basic command for removing a directory is \nrmdir:\n$ touch New_Dir/my_file\n$ ls -li New_Dir/\ntotal 0\n294561 -rw-rw-r-- 1 christine christine 0 May 22 09:52 my_file\n$\n$ rmdir New_Dir\nrmdir: failed to remove 'New_Dir': Directory not empty\n$\nBy default, the rmdir command works only for removing empty directories. Because we cre-\nated a fi le, \nmy_file, in the New_Dir directory, the rmdir command refuses to remove it.\n\n75\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  75\n3\nTo fi x this, we must remove the fi le fi rst. Then we can use the rmdir command on the now \nempty directory:\n$ rm -i New_Dir/my_file\nrm: remove regular empty file 'New_Dir/my_file'? y\n$\n$ rmdir New_Dir\n$\n$ ls -ld New_Dir\nls: cannot access New_Dir: No such file or directory\nThe rmdir has no -i option to ask if you want to remove the directory. This is one reason \nit is helpful that \nrmdir removes only empty directories.\nYou can also use the \nrm command on entire non-empty directories. Using the -r option \nallows the command to descend into the directory, remove the fi les, and then remove the \ndirectory itself:\n$ ls -l My_Dir\ntotal 0\n-rw-rw-r-- 1 christine christine 0 May 22 10:02 another_file\n$\n$ rm -ri My_Dir\nrm: descend into directory 'My_Dir'? y\nrm: remove regular empty file 'My_Dir/another_file'? y\nrm: remove directory 'My_Dir'? y\n$\n$ ls -l My_Dir\nls: cannot access My_Dir: No such file or directory\n$\nThis also works for descending into multiple subdirectories and is especially useful when \nyou have lots of directories and fi les to delete:\n$ ls -FR Small_Dir\nSmall_Dir:\na_file  b_file  c_file  Teeny_Dir/  Tiny_Dir/\nSmall_Dir/Teeny_Dir:\ne_file\nSmall_Dir/Tiny_Dir:\nd_file\n$\n$ rm -ir Small_Dir\nrm: descend into directory 'Small_Dir'? y\nrm: remove regular empty file 'Small_Dir/a_file'? y\nrm: descend into directory 'Small_Dir/Tiny_Dir'? y\nrm: remove regular empty file 'Small_Dir/Tiny_Dir/d_file'? y\n\n76\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  76\nrm: remove directory 'Small_Dir/Tiny_Dir'? y\nrm: descend into directory 'Small_Dir/Teeny_Dir'? y\nrm: remove regular empty file 'Small_Dir/Teeny_Dir/e_file'? y\nrm: remove directory 'Small_Dir/Teeny_Dir'? y\nrm: remove regular empty file 'Small_Dir/c_file'? y\nrm: remove regular empty file 'Small_Dir/b_file'? y\nrm: remove directory 'Small_Dir'? y\n$\n$ ls -FR Small_Dir\nls: cannot access Small_Dir: No such file or directory\n$\nAlthough this works, it’s somewhat awkward. Notice that you still must verify each and \nevery fi le that gets removed. For a directory with lots of fi les and subdirectories, this can \nbecome tedious.\nFor the rm command, the -r parameter and the -R parameter work exactly the same. When used with the rm com-\nmand, the \n-R parameter also recursively traverses through the directory removing fi les. It is unusual for a shell com-\nmand to have different cased parameters with the same function.\nThe ultimate solution for throwing caution to the wind and removing an entire directory, \ncontents and all, is the \nrm command with both the -r and -f parameters:\n$ tree Small_Dir\nSmall_Dir\n├── a_file\n├── b_file\n├── c_file\n├── Teeny_Dir\n│   └── e_file\n└── Tiny_Dir\n    └── d_file\n2 directories, 5 files\n$\n$ rm -rf Small_Dir\n$\n$ tree Small_Dir\nSmall_Dir [error opening dir]\n0 directories, 0 files\n$\nThe rm  -rf command gives no warnings and no fanfare. This, of course, is an extremely \ndangerous tool to have, especially if have superuser privileges. Use it sparingly, and only \nafter triple checking to make sure that you’re doing exactly what you want to do!\n\n77\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  77\n3\nNotice in the preceding example that we used the tree utility. It nicely displays directories, subdirectories, and their \nfi les. It’s a useful utility when you need to understand a directory structure, especially before removing it. This utility \nmay not be installed by default in your Linux distribution. See Chapter 9 for learning about installing software.\nIn the last few sections, you looked at managing both fi les and directories. So far we \ncovered everything you need to know about fi les, except for how to peek inside of them.\nViewing File Contents\nYou can use several commands for looking inside fi les without having to pull out a text \neditor utility (see Chapter 10). This section demonstrates a few of the commands you have \navailable to help you examine fi les.\nViewing the fi le type\nBefore you go charging off trying to display a fi le, try to get a handle on what type of fi le \nit is. If you try to display a binary fi le, you get lots of gibberish on your monitor and may \neven lock up your terminal emulator.\nThe \nfile command is a handy little utility to have around. It can peek inside of a fi le and \ndetermine just what kind of fi le it is:\n$ file my_file\nmy_file: ASCII text\n$\nThe fi le in the preceding example is a text fi le. The file command determined not only \nthat the fi le contains text but also the character code format of the text fi le, \nASCII. \nThis following example shows a fi le that is simply a directory. Thus, the \nfile command \ngives you another method to distinguish a directory:\n$ file New_Dir\nNew_Dir: directory\n$\nThis third file command example shows a fi le, which is a symbolic link. Note that the \nfile command even tells you to which fi le it is symbolically linked:\n$ file sl_data_file\nsl_data_file: symbolic link to 'data_file'\n$\n\n78\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  78\nThe following example shows what the file command returns for a script fi le. \nAlthough the fi le is \nASCII text, because it’s a script fi le, you can execute (run) it on \nthe system:\n$ file my_script\nmy_script: Bourne-Again shell script, ASCII text executable\n$\nThe fi nal example is a binary executable program. The file command determines the \nplatform that the program was compiled for and what types of libraries it requires. This \nis an especially handy feature if you have a binary executable program from an unknown \nsource:\n$ file /bin/ls\n/bin/ls: ELF 64-bit LSB  executable, x86-64, version 1 (SYSV), \ndynamically linked (uses shared libs), for GNU/Linux 2.6.24, \n[...]\n$\nNow that you know a quick method for viewing a fi le’s type, you can start displaying and \nviewing fi les.\nViewing the whole fi le\nIf you have a large text fi le on your hands, you may want to be able to see what’s inside of \nit. Linux has three different commands that can help you here.\nUsing the cat command\nThe cat command is a handy tool for displaying all the data inside a text fi le:\n$ cat test1\nhello\nThis is a test file.\nThat we'll use to       test the cat command.\n$\nNothing too exciting, just the contents of the text fi le. However, the cat command has a \nfew parameters that can help you out.\nThe \n-n parameter numbers all the lines for you:\n\n79\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  79\n3\n$ cat -n test1\n     1  hello\n     2\n     3  This is a test file.\n     4\n     5\n     6  That we'll use to       test the cat command.\n$\nThat feature will come in handy when you’re examining scripts. If you just want to number \nthe lines that have text in them, the \n-b parameter is for you:\n$ cat -b test1\n     1  hello\n     2  This is a test file.\n     3  That we'll use to       test the cat command.\n$\nFinally, if you don’t want tab characters to appear, use the -T parameter:\n$ cat -T test1\nhello\nThis is a test file.\nThat we'll use to^Itest the cat command.\n$\nThe -T parameter replaces any tabs in the text with the ^I character combination.\nFor large fi les, the \ncat command can be somewhat annoying. The text in the fi le just \nquickly scrolls off the display without stopping. Fortunately, we have a simple way to solve \nthis problem.\nUsing the more command\nThe main drawback of the cat command is that you can’t control what’s happening \nafter you start it. To solve that problem, developers created the \nmore command. The \nmore command displays a text fi le, but stops after it displays each page of data. We \ntyped the command more  /etc/bash.bashrc to produce the sample \nmore screen shown \nin Figure 3-3.\n\n80\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  80\nFIGURE 3-3\nUsing the more command to display a text fi le\nNotice at the bottom of the screen in Figure 3-3 that the more command displays a tag \nshowing that you’re still in the \nmore application and how far along (56%) in the text fi le \nyou are. This is the prompt for the \nmore command.\nThe \nmore command is a pager utility. Remember from earlier in this chapter a pager utility \ndisplays selected bash manual pages when you use the \nman command. Similarly to navigat-\ning through the man pages, you can use \nmore to navigate through a text fi le by pressing \nthe spacebar or you can go forward line by line using the Enter key. When you are fi nished \nnavigating through the fi le using \nmore, press the q key to quit.\nThe \nmore command allows some rudimentary movement through the text fi le. For more \nadvanced features, try the \nless command.\nUsing the less command\nFrom its name, it sounds like it shouldn’t be as advanced as the more command. However, \nthe \nless command name is actually a play on words and is an advanced version of the \nmore command (the less command name comes from the phrase “less is more”). It pro-\nvides several very handy features for scrolling both forward and backward through a text \nfi le, as well as some pretty advanced searching capabilities.\nThe \nless command can also display a fi le’s contents before it fi nishes reading the entire \nfi le. The \ncat and more commands cannot do this. \n\n81\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  81\n3\nThe less command operates much the same as the more command, displaying one screen \nof text from a fi le at a time. It supports the same command set as the \nmore command, plus \nmany more options.\nTo see all the options available for the less command, view its man pages by typing man  less. You can do the same \nfor the \nmore command to see the reference material concerning its various options as well.\nOne set of features is that the less command recognizes the up and down arrow keys as \nwell as the Page Up and Page Down keys (assuming that you’re using a properly defi ned ter-\nminal). This gives you full control when viewing a fi le.\nViewing parts of a fi le\nOften the data you want to view is located either right at the top or buried at the bottom \nof a text fi le. If the information is at the top of a large fi le, you still need to wait for the \ncat or more commands to load the entire fi le before you can view it. If the information \nis located at the bottom of a fi le (such as a log fi le), you need to wade through thousands \nof lines of text just to get to the last few entries. Fortunately, Linux has specialized com-\nmands to solve both of these problems.\nUsing the tail command\nThe tail command displays the last lines in a fi le (the fi le’s “tail”). By default, it shows \nthe last 10 lines in the fi le.\nFor these examples, we created a text fi le containing 20 text lines. It is displayed here in \nits entirety using the \ncat command:\n$ cat log_file\nline1\nline2\nline3\nline4\nline5\nHello World - line 6\nline7\nline8\nline9\nline10\nline11\nHello again - line 12\nline13\nline14\nline15\nSweet - line16\n\n82\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  82\nline17\nline18\nline19\nLast line - line20\n$\nNow that you have seen the entire text fi le, you can see the effect of using tail to view \nthe fi le’s last 10 lines:\n$ tail log_file\nline11\nHello again - line 12\nline13\nline14\nline15\nSweet - line16\nline17\nline18\nline19\nLast line - line20\n$\nYou can change the number of lines shown using tail by including the -n parameter. In \nthis example, only the last two lines of the fi le are displayed, by adding \n-n  2 to the tail \ncommand:\n$ tail -n 2 log_file\nline19\nLast line - line20\n$\nThe -f parameter is a pretty cool feature of the tail command. It allows you to peek \ninside a fi le as the fi le is being used by other processes. The \ntail command stays active \nand continues to display new lines as they appear in the text fi le. This is a great way to \nmonitor the system log fi les in real-time mode.\nUsing the head command\nThe head command does what you’d expect; it displays a fi le’s fi rst group of lines (the fi le’s \n“head”). By default, it displays the fi rst 10 lines of text:\n$ head log_file\nline1\nline2\nline3\nline4\nline5\nHello World - line 6\n\n83\nChapter 3:  Basic bash Shell Commands\nc03.indd  12/03/2014  Page  83\n3\nline7\nline8\nline9\nline10\n$\nSimilar to the tail command, the head command supports the -n parameter so you can \nalter what’s displayed. Both commands also allow you to simply type a dash along with the \nnumber of lines to display, as shown here:\n$ head -5 log_file\nline1\nline2\nline3\nline4\nline5\n$\nUsually the beginning of a fi le doesn’t change, so the head command doesn’t support the \n-f parameter feature as the tail command does. The head command is a handy way to \njust peek at the beginning of a fi le.\nSummary\nThis chapter covered the basics of working with the Linux fi lesystem from a shell prompt. \nWe began with a discussion of the bash shell and showed you how to interact with the \nshell. The command line interface (CLI) uses a prompt string to indicate when it’s ready for \nyou to enter commands. \nThe shell provides a wealth of utilities you can use to create and manipulate fi les. Before \nyou start playing with fi les, you should understand how Linux stores them. This chapter \ndiscussed the basics of the Linux virtual directory and showed you how Linux references \nstorage media devices. After describing the Linux fi lesystem, the chapter walked you \nthrough using the \ncd command to move around the virtual directory.\nAfter showing you how to get to a directory, the chapter demonstrated how to use the \nls \ncommand to list the fi les and subdirectories. Lots of parameters can customize the output \nof the \nls command. You can obtain information on fi les and directories by using the ls \ncommand.\nThe \ntouch command is useful for creating empty fi les and for changing the access or modi-\nfi cation times on an existing fi le. The chapter also discussed using the \ncp command to copy \nexisting fi les from one location to another. It walked you through the process of linking \nfi les instead of copying them, providing an easy way to have the same fi le in two locations \nwithout making a separate copy. The \nln command provides this linking ability.\n\n84\nPart I: The Linux Command Line\nc03.indd  12/03/2014  Page  84\nNext, you learned how to rename fi les (called moving) in Linux using the mv command \nand saw how to delete fi les (called removing) using the \nrm command. This chapter also \nshowed you how to perform the same tasks with directories, using the \nmkdir and rmdir \ncommands.\nFinally, the chapter closed with a discussion on viewing the contents of fi les. The \ncat, \nmore, and less commands provide easy methods for viewing the entire contents of a fi le, \nwhile the \ntail and head commands are great for peeking inside a fi le to just see a small \nportion of it.\nThe next chapter continues the discussion on bash shell commands. We’ll look at more \nadvanced administrator commands that come in handy as you administer your Linux \nsystem.\n\n85\nc04.indd  12/03/2014  Page  85\nCHAPTER \n4\nMore bash Shell Commands\nIN THIS CHAPTER\nManaging processes\nGetting disk statistics\nMounting new disks\nSorting data\nArchiving data\nC\nhapter 3 covered the basics of walking through the Linux fi lesystem and working with fi les \nand directories. File and directory management is a major feature of the Linux shell; how-\never, we should look at some other things before we start our script programming. This chap-\nter digs into the Linux system management commands, showing you how to peek inside your Linux \nsystem using command line commands. After that, we show you a few handy commands that you \ncan use to work with data fi les on the system.\nMonitoring Programs\nOne of the toughest jobs of being a Linux system administrator is keeping track of what’s running \non the system — especially now, when graphical desktops take a handful of programs just to \nproduce a single desktop. You always have lots of programs running on the system.\nFortunately, a few command line tools are available to help make life easier for you. This section \ncovers a few of the basic tools you need to know how to use to manage programs on your Linux \nsystem.\nPeeking at the processes\nWhen a program runs on the system, it’s referred to as a process. To examine these processes, you \nneed to become familiar with the \nps command, the Swiss Army knife of utilities. It can produce \nlots of information about all the programs running on your system.\n\n86\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  86\nUnfortunately, with this robustness comes complexity — in the form of numerous param-\neters — making the \nps command probably one of the most diffi cult commands to master. \nMost system administrators fi nd a subset of these parameters that provide the information \nthey want, and they stick with using only those.\nThat said, however, the basic \nps command doesn’t really provide all that much information:\n $ ps\n   PID TTY          TIME CMD\n  3081 pts/0    00:00:00 bash\n  3209 pts/0    00:00:00 ps\n $\nNot too exciting. By default, the ps command shows only the processes that belong to the \ncurrent user and that are running on the current terminal. In this case, we had only our \nbash shell running (remember, the shell is just another program running on the system) \nand, of course, the \nps command itself.\nThe basic output shows the process ID (PID) of the programs, the terminal (TTY) that they \nare running from, and the CPU time the process has used.\nThe tricky feature of the ps command (and the part that makes it so complicated) is that at one time there were two \nversions of it. Each version had its own set of command line parameters controlling what information it displayed \nand how. Recently, Linux developers have combined the two \nps command formats into a single ps program (and of \ncourse added their own touches).\nThe GNU ps command that’s used in Linux systems supports three different types of com-\nmand line parameters:\n ■\nUnix-style parameters, which are preceded by a dash\n ■\nBSD-style parameters, which are not preceded by a dash\n ■\nGNU long parameters, which are preceded by a double dash\nThe following sections examine the three different parameter types and show examples of \nhow they work.\nUnix-style parameters\nThe Unix-style parameters originated with the original ps command that ran on the AT&T \nUnix systems invented by Bell Labs. Table 4-1 shows these parameters.\n\n87\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  87\n4\nTABLE 4 -1    The ps Command Unix Parameters\nParameterDescription\n-A\nShows all processes\n-N\nShows the opposite of the specifi ed parameters\n-a\nShows all processes except session headers and processes without a \nterminal\n-d\nShows all processes except session headers\n-e\nShows all processes\n-C cmslist\nShows processes contained in the list cmdlist\n-G grplist\nShows processes with a group ID listed in grplist\n-U userlist\nShows processes owned by a userid listed in userlist\n-g\n grplistShows processes by session or by groupid contained in grplist\n-p\n pidlistShows processes with PIDs in the list pidlist\n-s\n sesslistShows processes with session ID in the list sesslist\n-t\n ttylistShows processes with terminal ID in the list ttylist\n-u\n userlistShows processes by effective userid in the list userlist\n-F\nUses extra full output\n-O formatDisplays specifi c columns in the list format, along with the default \ncolumns\n-M\nDisplays security information about the process\n-c\nShows additional scheduler information about the process\n-f\nDisplays a full format listing\n-j\nShows job information\n-l\nDisplays a long listing\n-o format\nDisplays only specifi c columns listed in format\n-y\nPrevents display of process fl ags\n-Z\nDisplays the security context information\n-H\nDisplays processes in a hierarchical format (showing parent processes)\n-n namelistDefi nes the values to display in the WCHAN column\n-w\nUses wide output format, for unlimited width displays\n-L\nShows process threads\n-V\nDisplays the version of ps\n\n88\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  88\nThat’s a lot of parameters, and there are still more! The key to using the ps command is not \nto memorize all the available parameters — only those you fi nd most useful. Most Linux \nsystem administrators have their own sets of parameters that they use for extracting per-\ntinent information. For example, if you need to see everything running on the system, use \nthe \n-ef parameter combination (the ps command lets you combine parameters like this):\n $ ps -ef\n UID        PID  PPID  C STIME TTY          TIME CMD\n root         1     0  0 11:29 ?        00:00:01 init [5]\n root         2     0  0 11:29 ?        00:00:00 [kthreadd]\n root         3     2  0 11:29 ?        00:00:00 [migration/0]\n root         4     2  0 11:29 ?        00:00:00 [ksoftirqd/0]\n root         5     2  0 11:29 ?        00:00:00 [watchdog/0]\n root         6     2  0 11:29 ?        00:00:00 [events/0]\n root         7     2  0 11:29 ?        00:00:00 [khelper]\n root        47     2  0 11:29 ?        00:00:00 [kblockd/0]\n root        48     2  0 11:29 ?        00:00:00 [kacpid]\n 68        2349     1  0 11:30 ?        00:00:00 hald\n root      3078  1981  0 12:00 ?        00:00:00 sshd: rich [priv]\n rich      3080  3078  0 12:00 ?        00:00:00 sshd: rich@pts/0\n rich      3081  3080  0 12:00 pts/0    00:00:00 -bash\n rich      4445  3081  3 13:48 pts/0    00:00:00 ps -ef\n $\nQuite a few lines have been cut from the output to save space, but you can see that lots of \nprocesses are running on a Linux system. This example uses two parameters: the \n-e param-\neter, which shows all the processes running on the system, and the \n-f parameter, which \nexpands the output to show a few useful columns of information:\n ■\nUID: The user responsible for launching the process\n ■\nPID: The process ID of the process\n ■\nPPID: The PID of the parent process (if a process is started by another process)\n ■\nC: Processor utilization over the lifetime of the process\n ■\nSTIME: The system time when the process started\n ■\nTTY: The terminal device from which the process was launched\n ■\nTIME: The cumulative CPU time required to run the process\n ■\nCMD: The name of the program that was started\nThis produces a reasonable amount of information, which is what many system administra-\ntors want to see. For even more information, you can use the \n-l parameter, which produces \nthe long format output:\n $ ps -l\n F S  UID PID  PPID  C PRI  NI ADDR SZ WCHAN  TTY      TIME   CMD\n\n89\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  89\n4\n 0 S  500 3081  3080  0  80   0 -  1173 wait pts/0   00:00:00 bash\n 0 R  500 4463  3081  1  80   0 -  1116 -    pts/0   00:00:00 ps\n $\nNotice the extra columns that appear when you use the -l parameter:\n ■\nF: System fl ags assigned to the process by the kernel\n ■\nS: The state of the process (O = running on processor; S = sleeping; R = runnable, \nwaiting to run; \nZ = zombie, process terminated but parent not available; \nT = process stopped)\n ■\nPRI: The priority of the process (higher numbers mean lower priority)\n ■\nNI: The nice value, which is used for determining priorities\n ■\nADDR: The memory address of the process\n ■\nSZ: Approximate amount of swap space required if the process was swapped out\n ■\nWCHAN: Address of the kernel function where the process is sleeping\nBSD-style parameters\nNow that you’ve seen the Unix parameters, let’s look at the BSD-style parameters. The \nBerkeley Software Distribution (BSD) was a version of Unix developed at (of course) the \nUniversity of California, Berkeley. It had many subtle differences from the AT&T Unix \nsystem, thus sparking many Unix wars over the years. Table 4-2 shows the BSD version of \nthe \nps command parameters.\nTABLE 4 -2    The ps Command BSD Parameters\nParameterDescription\nT\nShows all processes associated with this terminal\na\nShows all processes associated with any terminal\ng\nShows all processes including session headers\nr\nShows only running processes\nx\nShows all processes, even those without a terminal device assigned\nU userlistShows processes owned by a userid listed in userlist\np\n pidlistShows processes with a PID listed in pidlist\nt\n ttylistShows processes associated with a terminal listed in ttylist\nO\n formatLists specifi c columns in format to display along with the standard columns\nX\nDisplays data in the register format\nZ\nIncludes security information in the output\nj\nShows job information\nl\nUses the long format\nContinues\n\n90\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  90\nParameterDescription\no formatDisplays only columns specifi ed in format\ns\nUses the signal format\nu\nUses the user-oriented format\nv\nUses the virtual memory format\nN namelistDefi nes the values to use in the WCHAN column\nO order\nDefi nes the order in which to display the information columns\nS\nSums numerical information, such as CPU and memory usage, for child \nprocesses into the parent process\nc\nDisplays the true command name (the name of the program used to start the \nprocess)\ne\nDisplays any environment variables used by the command\nf\nDisplays processes in a hierarchical format, showing which processes started \nwhich processes\nh\nPrevents display of the header information\nk sort\nDefi nes the column(s) to use for sorting the output\nn\nUses numeric values for user and group IDs, along with WCHAN information\nw\nProduces wide output for wider terminals\nH\nDisplays threads as if they were processes\nm\nDisplays threads after their processes\nL\nLists all format specifi ers\nV\nDisplays the version of ps\nAs you can see, the Unix and BSD types of parameters have lots of overlap. Most of the \ninformation you can get from one you can also get from the other. Most of the time, you \nchoose a parameter type based on which format you’re more comfortable with (for example, \nif you were used to a BSD environment before using Linux).\nWhen you use the BSD-style parameters, the \nps command automatically changes the output \nto simulate the BSD format. Here’s an example using the \nl parameter:\n $ ps l\n F  UID  PID PPID PRI  NI  VSZ  RSS WCHAN  STAT TTY      TIME COMMAND\n 0  500 3081 3080  20   0 4692 1432 wait   Ss   pts/0    0:00 -bash\n 0  500 5104 3081  20   0 4468  844 -      R+   pts/0    0:00 ps l\n $\nTABLE 4 -2   (continued)\n\n91\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  91\n4\nNotice that while many of the output columns are the same as when we used the Unix-style \nparameters, some different ones appear as well:\n ■\nVSZ: The size in kilobytes of the process in memory\n ■\nRSS: The physical memory that a process has used that isn’t swapped out\n ■\nSTAT: A two-character state code representing the current process state\nMany system administrators like the BSD-style \nl parameter because it produces a more \ndetailed state code for processes (the STAT column). The two-character code more precisely \ndefi nes exactly what’s happening with the process than the single-character Unix-style \noutput.\nThe fi rst character uses the same values as the Unix-style \nS output column, showing when \na process is sleeping, running, or waiting. The second character further defi nes the pro-\ncess’s status:\n ■\n<: The process is running at high priority.\n ■\nN: The process is running at low priority.\n ■\nL: The process has pages locked in memory.\n ■\ns: The process is a session leader.\n ■\nl: The process is multi-threaded.\n ■\n+: The process is running in the foreground.\nFrom the simple example shown previously, you can see that the \nbash command is sleep-\ning, but it is a session leader (it’s the main process in my session), whereas the \nps command \nwas running in the foreground on the system.\nThe GNU long parameters\nFinally, the GNU developers put their own touches on the new, improved ps command by \nadding a few more options to the parameter mix. Some of the GNU long parameters copy \nexisting Unix- or BSD-style parameters, while others provide new features. Table 4-3 lists \nthe available GNU long parameters.\nTABLE 4 -3    The ps Command GNU Parameters\nParameterDescription\n--deselect\nShows all processes except those listed in the command line\n--Group grplist\nShows processes whose group ID is listed in grplist\n--User\n userlistShows processes whose user ID is listed in userlist\n--group\n grplistShows processes whose effective group ID is listed in grplist\nContinues\n\n92\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  92\nParameterDescription\n--pid pidlist\nShows processes whose process ID is listed in pidlist\n--ppid\n pidlistShows processes whose parent process ID is listed in pidlist\n--sid\n sidlistShows processes whose session ID is listed in sidlist\n--tty\n ttylistShows processes whose terminal device ID is listed in ttylist\n--user\n userlistShows processes whose effective user ID is listed in userlist\n--format format\nDisplays only columns specifi ed in the format\n--context\nDisplays additional security information\n--cols nSets screen width to n columns\n--columns n\nSets screen width to n columns\n--cumulative\nIncludes stopped child process information\n--forest\nDisplays processes in a hierarchical listing showing parent processes\n--headers\nRepeats column headers on each page of output\n--no-headers\nPrevents display of column headers\n--lines nSets the screen height to n lines\n--rows nSets the screen height to n rows\n--sort orderDefi nes the column(s) to use for sorting the output\n--width nSets the screen width to n columns\n--help\nDisplays the help information\n--info\nDisplays debugging information\n--version\nDisplays the version of the ps program\nYou can combine GNU long parameters with either Unix- or BSD-style parameters to really \ncustomize your display. One cool feature of GNU long parameters that we really like is the \n--forest parameter. It displays the hierarchical process information, but using ASCII \ncharacters to draw cute charts:\n  1981 ?        00:00:00 sshd\n  3078 ?        00:00:00  \\_ sshd\n  3080 ?        00:00:00      \\_ sshd\n  3081 pts/0    00:00:00          \\_ bash\n 16676 pts/0    00:00:00              \\_ ps\nThis format makes tracing child and parent processes a snap!\nReal-time process monitoring\nThe ps command is great for gleaning information about processes running on the system, \nbut it has one drawback. The \nps command can display information only for a specifi c point \nTABLE 4 -3   (continued)\n\n93\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  93\n4\nin time. If you’re trying to fi nd trends about processes that are frequently swapped in and \nout of memory, it’s hard to do that with the \nps command.\nInstead, the \ntop command can solve this problem. The top command displays process \ninformation similarly to the \nps command, but it does it in real-time mode. Figure 4-1 is a \nsnapshot of the \ntop command in action.\nFIGURE 4-1\nThe output of the top command while it is running\nThe fi rst section of the output shows general system information. The fi rst line shows the \ncurrent time, how long the system has been up, the number of users logged in, and the load \naverage on the system.\nThe load average appears as three numbers: the 1-minute, 5-minute, and 15-minute load \naverages. The higher the values, the more load the system is experiencing. It’s not uncom-\nmon for the 1-minute load value to be high for short bursts of activity. If the 15-minute \nload value is high, your system may be in trouble.\n\n94\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  94\nThe trick in Linux system administration is defi ning what exactly a high load average value is. This value depends on \nwhat’s normally running on your system and the hardware confi guration. What’s high for one system might be normal \nfor another. Usually, if your load averages start getting over 2, things are getting busy on your system.\nThe second line shows general process information (called tasks in top): how many \nprocesses are running, sleeping, stopped, and zombie (have fi nished but their parent \nprocess hasn’t responded).\nThe next line shows general CPU information. The \ntop display breaks down the CPU \nutilization into several categories depending on the owner of the process (user versus \nsystem processes) and the state of the processes (running, idle, or waiting).\nFollowing that are two lines that detail the status of the system memory. The fi rst line \nshows the status of the physical memory in the system, how much total memory there is, \nhow much is currently being used, and how much is free. The second memory line shows \nthe status of the swap memory area in the system (if any is installed), with the same \ninformation.\nFinally, the next section shows a detailed list of the currently running processes, with \nsome information columns that should look familiar from the \nps command output:\n ■\nPID: The process ID of the process\n ■\nUSER: The user name of the owner of the process\n ■\nPR: The priority of the process\n ■\nNI: The nice value of the process\n ■\nVIRT: The total amount of virtual memory used by the process\n ■\nRES: The amount of physical memory the process is using\n ■\nSHR: The amount of memory the process is sharing with other processes\n ■\nS: The process status (D = interruptible sleep, R = running, S = sleeping, T = traced \nor stopped, or \nZ = zombie)\n ■\n%CPU: The share of CPU time that the process is using\n ■\n%MEM: The share of available physical memory the process is using\n ■\nTIME+: The total CPU time the process has used since starting\n ■\nCOMMAND: The command line name of the process (program started)\nBy default, when you start \ntop, it sorts the processes based on the %CPU value. You can \nchange the sort order by using one of several interactive commands while \ntop is running. \nEach interactive command is a single character that you can press while \ntop is running \nand changes the behavior of the program. Pressing \nf allows you to select the fi eld to use \n\n95\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  95\n4\nto sort the output, and pressing d allows you to change the polling interval. Press q to exit \nthe top display.\nYou have lots of control over the output of the \ntop command. Using this tool, you can \noften fi nd offending processes that have taken over your system. Of course, after you fi nd \none, the next job is to stop it, which brings us to the next topic.\nStopping processes\nA crucial part of being a system administrator is knowing when and how to stop a process. \nSometimes, a process gets hung up and needs a gentle nudge to either get going again \nor stop. Other times, a process runs away with the CPU and refuses to give it up. In both \ncases, you need a command that allows you to control a process. Linux follows the Unix \nmethod of interprocess communication.\nIn Linux, processes communicate with each other using signals. A process signal is a \npredefi ned message that processes recognize and may choose to ignore or act on. The \ndevelopers program how a process handles signals. Most well-written applications have the \nability to receive and act on the standard Unix process signals. Table 4-4 shows these signals.\nTABLE 4 - 4    Linux  Process  Signals\nSignalNameDescription\n1HUPHangs up\n2INTInterrupts\n3QUITStops running\n9KILLUnconditionally terminates\n11SEGVProduces segment violation\n15TERMTerminates if possible\n17STOPStops unconditionally, but doesn’t terminate\n18TSTPStops or pauses, but continues to run in background\n19CONTResumes execution after STOP or TSTP\nTwo commands available in Linux allow you to send process signals to running processes.\nThe kill command\nThe kill command allows you to send signals to processes based on their process ID (PID). \nBy default, the \nkill command sends a TERM signal to all the PIDs listed on the command \nline. Unfortunately, you can only use the process PID instead of its command name, making \nthe \nkill command diffi cult to use sometimes.\n\n96\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  96\nTo send a process signal, you must either be the owner of the process or be logged in as the \nroot user.\n $ kill 3940\n -bash: kill: (3940) - Operation not permitted\n $\nThe TERM signal tells the process to kindly stop running. Unfortunately, if you have a \nrunaway process, most likely it ignores the request. When you need to get forceful, the \n-s \nparameter allows you to specify other signals (either using their name or signal number).\nAs you can see from the following example, no output is associated with the \nkill \ncommand.\n # kill -s HUP 3940\n #\nTo see if the command was effective, you must perform another ps or top command to see \nif the offending process stopped.\nThe killall command\nThe killall command is a powerful way to stop processes by using their names rather \nthan the PID numbers. The \nkillall command allows you to use wildcard characters as \nwell, making it a very useful tool when you have a system that’s gone awry:\n# killall http*\n#\nThis example kills all the processes that start with http, such as the httpd services for the \nApache web server.\nBe extremely careful using the killall command when logged in as the root user. It’s easy to get carried away with \nwildcard characters and accidentally stop important system processes. This could lead to a damaged fi lesystem.\nMonitoring Disk Space\nAnother important task of the system administrator is to keep track of the disk usage on \nthe system. Whether you’re running a simple Linux desktop or a large Linux server, you \nneed to know how much space you have for your applications.\nSome command line commands can help you manage the media environment on your Linux \nsystem. This section describes the core commands you’ll likely run into during your system \nadministration duties.\n\n97\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  97\n4\nMounting media\nAs discussed in Chapter 3, the Linux fi lesystem combines all media disks into a single \nvirtual directory. Before you can use a new media disk on your system, you must place it in \nthe virtual directory. This task is called mounting.\nIn today’s graphical desktop world, most Linux distributions have the ability to automati-\ncally mount specifi c types of removable media. A removable media device is a medium that \n(obviously) can be easily removed from the PC, such as CD-ROMs and USB memory sticks.\nIf you’re not using a distribution that automatically mounts and unmounts removable \nmedia, you have to do it yourself. This section describes the Linux command line com-\nmands to help you manage your removable media devices.\nThe mount command\nOddly enough, the command used to mount media is called mount. By default, the mount \ncommand displays a list of media devices currently mounted on the system:\n $ mount\n /dev/mapper/VolGroup00-LogVol00 on / type ext3 (rw)\n proc on /proc type proc (rw)\n sysfs on /sys type sysfs (rw)\n devpts on /dev/pts type devpts (rw,gid=5,mode=620)\n /dev/sda1 on /boot type ext3 (rw)\n tmpfs on /dev/shm type tmpfs (rw)\n none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)\n /dev/sdb1 on /media/disk type vfat\n (rw,nosuid,nodev,uhelper=hal,shortname=lower,uid=503)\n $\nThe mount command provides four pieces of information:\n ■\nThe device fi lename of the media\n ■\nThe mount point in the virtual directory where the media is mounted\n ■\nThe fi lesystem type\n ■\nThe access status of the mounted media\nThe last entry in the preceding example is a USB memory stick that the GNOME desktop \nautomatically mounted at the \n/media/disk mount point. The vfat fi lesystem type shows \nthat it was formatted on a Microsoft Windows PC.\nTo manually mount a media device in the virtual directory, you must be logged in as the \nroot user or use the \nsudo command to run the command as the root user. The following is \nthe basic command for manually mounting a media device:\n mount -t type device directory\n\n98\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  98\nThe type parameter defi nes the fi lesystem type under which the disk was formatted. Linux \nrecognizes lots of different fi lesystem types. If you share removable media devices with \nyour Windows PCs, you are most likely to run into these types:\n ■\nvfat: Windows long fi lesystem\n ■\nntfs: Windows advanced fi lesystem used in Windows NT, XP, and Vista\n ■\niso9660: The standard CD-ROM fi lesystem\nMost USB memory sticks and fl oppies are formatted using the vfat fi lesystem. If you need \nto mount a data CD, you must use the iso9660 fi lesystem type.\nThe next two parameters defi ne the location of the device fi le for the media device and the \nlocation in the virtual directory for the mount point. For example, to manually mount the \nUSB memory stick at device \n/dev/sdb1 at location /media/disk, you use the following \ncommand:\n mount -t vfat /dev/sdb1 /media/disk\nAfter a media device is mounted in the virtual directory, the root user has full access to \nthe device, but access by other users is restricted. You can control who has access to the \ndevice using directory permissions (discussed in Chapter 7).\nIn case you need to use some of the more exotic features of the \nmount command, Table 4-5 \nshows the available parameters .\nTABLE 4 -5    The mount Command Parameters\nParameterDescription\n-a\nMounts all fi lesystems specifi ed in the /etc/fstab fi le\n-f\nCauses the mount command to simulate mounting a device, but not actually \nmount it\n-F\nMounts all fi lesystems at the same time when used with the -a parameter\n-v\nExplains all the steps required to mount the device; stands for verbose mode\n-I\nTells you not to use any fi lesystem helper fi les under /sbin/mount\n.filesystem\n-l\nAdds the fi lesystem labels automatically for ext2, ext3, or XFS fi lesystems\n-n\nMounts the device without registering it in the /etc/mstab mounted device \nfi le\n-p numFor encrypted mounting, reads the passphrase from the fi le descriptor num\n-s\nIgnores mount options not supported by the fi lesystem\n-r\nMounts the device as read-only\n\n99\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  99\n4\n-w\nMounts the device as read-write (the default)\n-L label\nMounts the device with the specifi ed label\n-U\n uuidMounts the device with the specifi ed uuid\n-O\nWhen used with the -a parameter, limits the set of fi lesystems applied\n-o\nAdds specifi c options to the fi lesystem\nThe -o option allows you to mount the fi lesystem with a comma-separated list of additional \noptions. These are popular options to use:\n ■\nro: Mounts as read-only\n ■\nrw: Mounts as read-write\n ■\nuser: Allows an ordinary user to mount the fi lesystem\n ■\ncheck=none: Mounts the fi lesystem without performing an integrity check\n ■\nloop: Mounts a fi le\nThe unmount command\nTo remove a removable media device, you should never just remove it from the system. \nInstead, you should always unmount it fi rst.\nLinux doesn’t allow you to eject a mounted CD. If you ever have trouble removing a CD from the drive, most likely it \nmeans the CD is still mounted in the virtual directory. Unmount it fi rst, and then try to eject it.\nThe command used to unmount devices is umount (yes, there’s no “n” in the command, \nwhich gets confusing sometimes). The format for the \numount command is pretty simple:\n umount [directory | device ]\nThe umount command gives you the choice of defi ning the media device by either its \ndevice location or its mounted directory name. If any program has a fi le open on a device, \nthe system won’t let you unmount it.\n [root@testbox mnt]# umount /home/rich/mnt\n umount: /home/rich/mnt: device is busy\n umount: /home/rich/mnt: device is busy\n [root@testbox mnt]# cd /home/rich\n [root@testbox rich]# umount /home/rich/mnt\n [root@testbox rich]# ls -l mnt\n total 0\n [root@testbox rich]#\n\n100\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  100\nIn this example, the command prompt was still in a directory within the fi lesystem struc-\nture, so the \numount command couldn’t unmount the image fi le. After the command prompt \nwas moved out of the image fi le fi lesystem, the \numount command successfully unmounted \nthe image fi le.\nUsing the df command\nSometimes, you need to see how much disk space is available on an individual device. The \ndf command allows you to easily see what’s happening on all the mounted disks:\n $ df\n Filesystem           1K-blocks      Used Available Use% Mounted on\n /dev/sda2             18251068   7703964   9605024  45% /\n /dev/sda1               101086     18680     77187  20% /boot\n tmpfs                   119536         0    119536   0% /dev/shm\n /dev/sdb1               127462    113892     13570  90% /media/disk\n $\nThe df command shows each mounted fi lesystem that contains data. As you can see from \nthe \nmount command earlier, some mounted devices are used for internal system purposes. \nThe command displays the following:\n ■\nThe device location of the device\n ■\nHow many 1024-byte blocks of data it can hold\n ■\nHow many 1024-byte blocks are used\n ■\nHow many 1024-byte blocks are available\n ■\nThe amount of used space as a percentage\n ■\nThe mount point where the device is mounted\nA few different command line parameters are available with the \ndf command, most of \nwhich you’ll never use. One popular parameter is \n-h, which shows the disk space in human-\nreadable form, usually as an M for megabytes or a G for gigabytes:\n $ df -h\n Filesystem            Size  Used Avail Use% Mounted on\n /dev/sdb2              18G  7.4G  9.2G  45% /\n /dev/sda1              99M   19M   76M  20% /boot\n tmpfs                 117M     0  117M   0% /dev/shm\n /dev/sdb1             125M  112M   14M  90% /media/disk\n $\nNow instead of having to decode those ugly block numbers, all the disk sizes are shown \nusing “normal” sizes. The \ndf command is invaluable in troubleshooting disk space problems \non the system.\n\n101\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  101\n4\nRemember that the Linux system always has processes running in the background that handle fi les. The values \nfrom the \ndf command refl ect what the Linux system thinks are the current values at that point in time. It’s possible \nthat you have a process running that has created or deleted a fi le but has not released the fi le yet. This value is not \nincluded in the free space calculation.\nUsing the du command\nWith the df command, you can easily see when a disk is running out of space. The next \nproblem for the system administrator is to know what to do when that happens.\nAnother useful command to help you is the \ndu command. The du command shows the disk \nusage for a specifi c directory (by default, the current directory). This is a quick way to \ndetermine if you have any obvious disk hogs on the system.\nBy default, the \ndu command displays all the fi les, directories, and subdirectories under \nthe current directory, and it shows how many disk blocks each fi le or directory takes. For a \nstandard-sized directory, this can be quite a listing. Here’s a partial listing of using the \ndu \ncommand:\n $ du\n 484     ./.gstreamer-0.10\n 8       ./Templates\n 8       ./Download\n 8       ./.ccache/7/0\n 24      ./.ccache/7\n 368     ./.ccache/a/d\n 384     ./.ccache/a\n 424     ./.ccache\n 8       ./Public\n 8       ./.gphpedit/plugins\n 32      ./.gphpedit\n 72      ./.gconfd\n 128     ./.nautilus/metafiles\n 384     ./.nautilus\n 72      ./.bittorrent/data/metainfo\n 20      ./.bittorrent/data/resume\n 144     ./.bittorrent/data\n 152     ./.bittorrent\n 8       ./Videos\n 8       ./Music\n 16      ./.config/gtk-2.0\n 40      ./.config\n 8       ./Documents\n\n102\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  102\nThe number at the left of each line is the number of disk blocks that each fi le or \ndirectory takes. Notice that the listing starts at the bottom of a directory and works its \nway up through the fi les and subdirectories contained within the directory.\nThe \ndu command by itself can be somewhat useless. It’s nice to be able to see how much \ndisk space each individual fi le and directory takes up, but it can be meaningless when \nyou have to wade through pages and pages of information before you fi nd what you’re \nlooking for.\nYou can use a few command line parameters with the \ndu command to make things a little \nmore legible:\n ■\n-c: Produces a grand total of all the fi les listed\n ■\n-h: Prints sizes in human-readable form, using K for kilobyte, M for megabyte, and \nG for gigabyte\n ■\n-s: Summarizes each argument\nThe next step for the system administrator is to use some fi le-handling commands for \nmanipulating large amounts of data. That’s exactly what the next section covers.\nWorking with Data Files\nWhen you have a large amount of data, handling the information and making it useful can \nbe diffi cult. As you saw with the \ndu command in the previous section, it’s easy to get data \noverload when working with system commands.\nThe Linux system provides several command line tools to help you manage large amounts of \ndata. This section covers the basic commands that every system administrator — as well as \nany everyday Linux user — should know how to use to make their lives easier.\nSorting data\nThe sort command is a popular function that comes in handy when working with large \namounts of data. The \nsort command does what it says: It sorts data.\nBy default, the \nsort command sorts the data lines in a text fi le using standard sorting \nrules for the language you specify as the default for the session.\n $ cat file1\n one\n two\n three\n four\n\n103\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  103\n4\n five\n $ sort file1\n five\n four\n one\n three\n two\n $\nIt’s pretty simple, but things aren’t always as easy as they appear. Look at this example:\n $ cat file2\n 1\n 2\n 100\n 45\n 3\n 10\n 145\n 75\n $ sort file2\n 1\n 10\n 100\n 145\n 2\n 3\n 45\n 75\n $\nIf you were expecting the numbers to sort in numerical order, you were disappointed. By \ndefault, the \nsort command interprets numbers as characters and performs a standard \ncharacter sort, producing output that might not be what you want. To solve this problem, \nuse the \n-n parameter, which tells the sort command to recognize numbers as numbers \ninstead of characters and to sort them based on their numerical values:\n $ sort -n file2\n 1\n 2\n 3\n 10\n 45\n 75\n 100\n 145\n $\n\n104\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  104\nNow, that’s much better! Another common parameter that’s used is -M, the month sort. \nLinux log fi les usually contain a timestamp at the beginning of the line to indicate when \nthe event occurred:\n Sep 13 07:10:09 testbox smartd[2718]: Device: /dev/sda, opened\nIf you sort a fi le that uses timestamp dates using the default sort, you get something like \nthis:\n $ sort file3\n Apr\n Aug\n Dec\n Feb\n Jan\n Jul\n Jun\n Mar\n May\n Nov\n Oct\n Sep\n $\nIt’s not exactly what you wanted. If you use the -M parameter, the sort command recog-\nnizes the three-character month nomenclature and sorts appropriately:\n $ sort -M file3\n Jan\n Feb\n Mar\n Apr\n May\n Jun\n Jul\n Aug\n Sep\n Oct\n Nov\n Dec\n $\nTable 4-6 shows other handy sort parameters you can use.\n\n105\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  105\n4\nTABLE 4 - 6    The sort Command Parameters\nSingle DashDouble DashDescription\n-b\n--ignore-leading-blanksIgnores leading blanks when sorting\n-C\n--check = quietDoesn’t sort, but doesn’t report if data is out of sort \norder\n-c\n--checkDoesn’t sort, but checks if the input data is already \nsorted, and reports if not sorted\n-d\n--dictionary-orderConsiders only blanks and alphanumeric charac-\nters; doesn’t consider special characters\n-f\n--ignore-caseBy default, sort orders capitalized letters fi rst; \nignores case\n-g\n--general-numeric-sortUses general numerical value to sort\n-i\n--ignore-nonprintingIgnores nonprintable characters in the sort\n-k\n--key = POS1[,POS2]Sorts based on position POS1, and ends at POS2 if \nspecifi ed\n-M\n--month-sortSorts by month order using three-character month \nnames\n-m\n--mergeMerges two already sorted data fi les\n-n\n--numeric-sortSorts by string numerical value\n-o\n--output = fileWrites results to fi le specifi ed\n-R\n--random-sortSorts by a random hash of keys\n--random-source = \nFILESpecifi es the fi le for random bytes used by the -R \nparameter\n-r\n--reverseReverses the sort order (descending instead of \nascending\n-S\n--buffer-size = SIZESpecifi es the amount of memory to use\n-s\n--stableDisables last-resort comparison\n-T\n--temporary-direction = \nDIR\nSpecifi es a location to store temporary working fi les\n-t\n--field-separator\n = \nSEP\nSpecifi es the character used to distinguish key \npositions\n-u          --unique\nWith the -c parameter, checks for strict ordering; \nwithout the \n-c parameter, outputs only the fi rst \noccurrence of two similar lines\n-z          --zero-terminated\nEnds all lines with a NULL character instead of a \nnew line\n\n106\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  106\nThe -k and -t parameters are handy when sorting data that uses fi elds, such as the /etc/\npasswd\n fi le. Use the -t parameter to specify the fi eld separator character, and use the -k \nparameter to specify which fi eld to sort on. For example, to sort the password fi le based on \nnumerical userid, just do this:\n $ sort -t ':' -k 3 -n /etc/passwd\n root:x:0:0:root:/root:/bin/bash\n bin:x:1:1:bin:/bin:/sbin/nologin\n daemon:x:2:2:daemon:/sbin:/sbin/nologin\n adm:x:3:4:adm:/var/adm:/sbin/nologin\n lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\n sync:x:5:0:sync:/sbin:/bin/sync\n shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\n halt:x:7:0:halt:/sbin:/sbin/halt\n mail:x:8:12:mail:/var/spool/mail:/sbin/nologin\n news:x:9:13:news:/etc/news:\n uucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\n operator:x:11:0:operator:/root:/sbin/nologin\n games:x:12:100:games:/usr/games:/sbin/nologin\n gopher:x:13:30:gopher:/var/gopher:/sbin/nologin\n ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\nNow the data is perfectly sorted based on the third fi eld, which is the numerical userid \nvalue.\nThe \n-n parameter is great for sorting numerical outputs, such as the output of the du \ncommand:\n $ du -sh * | sort -nr\n 1008k   mrtg-2.9.29.tar.gz\n 972k    bldg1\n 888k    fbs2.pdf\n 760k    Printtest\n 680k    rsync-2.6.6.tar.gz\n 660k    code\n 516k    fig1001.tiff\n 496k    test\n 496k    php-common-4.0.4pl1-6mdk.i586.rpm\n 448k    MesaGLUT-6.5.1.tar.gz\n 400k    plp\nNotice that the -r option also sorts the values in descending order, so you can easily see \nwhat fi les are taking up the most space in your directory.\nThe pipe command (|) used in this example redirects the output of the du command to the sort command. That’s \ndiscussed in more detail in Chapter 11.\n\n107\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  107\n4\nSearching for data\nOften in a large fi le, you must look for a specifi c line of data buried somewhere in the \nmiddle of the fi le. Instead of manually scrolling through the entire fi le, you can let the \ngrep command search for you. The command line format for the grep command is:\n grep [options] pattern [file]\nThe grep command searches either the input or the fi le you specify for lines that contain \ncharacters that match the specifi ed pattern. The output from \ngrep is the lines that contain \nthe matching pattern.\nHere are two simple examples of using the \ngrep command with the file1 fi le used in the \n“Sorting data” section:\n $ grep three file1\n three\n $ grep t file1\n two\n three\n $\nThe fi rst example searches the fi le file1 for text matching the pattern three. The grep \ncommand produces the line that contains the matching pattern. The next example searches \nthe fi le \nfile1 for the text matching the pattern t. In this case, two lines matched the \nspecifi ed pattern, and both are displayed.\nBecause of the popularity of the \ngrep command, it has undergone lots of development \nchanges over its lifetime. Lots of features have been added to the \ngrep command. If you \nlook over the man pages for the \ngrep command, you’ll see how versatile it really is.\nIf you want to reverse the search (output lines that don’t match the pattern), use the \n-v \nparameter:\n $ grep -v t file1\n one\n four\n five\n $\nIf you need to fi nd the line numbers where the matching patterns are found, use the -n \nparameter:\n $ grep -n t file1\n 2:two\n 3:three\n $\n\n108\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  108\nIf you just need to see a count of how many lines contain the matching pattern, use the -c \nparameter:\n $ grep -c t file1\n 2\n $\nIf you need to specify more than one matching pattern, use the -e parameter to specify \neach individual pattern:\n $ grep -e t -e f file1\n two\n three\n four\n five\n $\nThis example outputs lines that contain either the string t or the string f.\nBy default, the \ngrep command uses basic Unix-style regular expressions to match patterns. \nA Unix-style regular expression uses special characters to defi ne how to look for matching \npatterns.\nFor a more detailed explanation of regular expressions, see Chapter 20.\nHere’s a simple example of using a regular expression in a \ngrep search:\n $ grep [tf] file1\n two\n three\n four\n five\n $\nThe square brackets in the regular expression indicate that grep should look for matches \nthat contain either a t or an f character. Without the regular expression, \ngrep would search \nfor text that would match the string \ntf.\nThe \negrep command is an offshoot of grep, which allows you to specify POSIX extended \nregular expressions, which contain more characters for specifying the matching pattern \n(again, see Chapter 20 for more details). The \nfgrep command is another version that allows \nyou to specify matching patterns as a list of fi xed-string values, separated by newline char-\nacters. This allows you to place a list of strings in a fi le and then use that list in the \nfgrep \ncommand to search for the strings in a larger fi le.\nCompressing data\nIf you’ve done any work in the Microsoft Windows world, no doubt you’ve used zip fi les. It \nbecame such a popular feature that Microsoft eventually incorporated it into the Windows \n\n109\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  109\n4\noperating system starting with XP. The zip utility allows you to easily compress large fi les \n(both text and executable) into smaller fi les that take up less space.\nLinux contains several fi le compression utilities. Although this may sound great, it often \nleads to confusion and chaos when trying to download fi les. Table 4-7 lists the fi le compres-\nsion utilities available for Linux.\nTABLE 4 -7    Linux File Compression Utilities\nUtilityFile ExtensionDescription\nbzip2        .bz2\nUses the Burrows-Wheeler block sorting text compression \nalgorithm and Huffman coding\ncompress     .Z\nOriginal Unix fi le compression utility; starting to fade away \ninto obscurity\ngzip         .gz\nThe GNU Project’s compression utility; uses Lempel-Ziv \ncoding\nzip          .zip\nThe Unix version of the PKZIP program for Windows\nThe compress fi le compression utility is not often found on Linux systems. If you down-\nload a fi le with a \n.Z extension, you can usually install the compress package (called \nncompress in many Linux distributions) using the software installation methods dis-\ncussed in Chapter 9 and then uncompress the fi le with the \nuncompress command. The \ngzip utility is the most popular compression tool used in Linux.\nThe \ngzip package is a creation of the GNU Project, in their attempt to create a free version \nof the original Unix compress utility. This package includes these fi les:\n ■\ngzip for compressing fi les\n ■\ngzcat for displaying the contents of compressed text fi les\n ■\ngunzip for uncompressing fi les\nThese utilities work the same way as the bzip2 utilities:\n $ gzip myprog\n $ ls -l my*\n-rwxrwxr-x 1 rich rich 2197 2007-09-13 11:29 myprog.gz\n $\nThe gzip command compresses the fi le you specify on the command line. You can also \nspecify more than one fi lename or even use wildcard characters to compress multiple fi les \nat once:\n$ gzip my*\n$ ls -l my*\n\n110\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  110\n -rwxr--r--    1 rich     rich          103 Sep  6 13:43 myprog.c.gz\n -rwxr-xr-x    1 rich     rich         5178 Sep  6 13:43 myprog.gz\n -rwxr--r--    1 rich     rich           59 Sep  6 13:46 myscript.gz\n -rwxr--r--    1 rich     rich           60 Sep  6 13:44 myscript2.gz\n$\nThe gzip command compresses every fi le in the directory that matches the wildcard \npattern.\nArchiving data\nAlthough the zip command works great for compressing and archiving data into a single \nfi le, it’s not the standard utility used in the Unix and Linux worlds. By far the most popular \narchiving tool used in Unix and Linux is the \ntar command.\nThe \ntar command was originally used to write fi les to a tape device for archiving. \nHowever, it can also write the output to a fi le, which has become a popular way to archive \ndata in Linux.\nThe following is the format of the \ntar command:\n tar function [options] object1 object2 ...\nThe function parameter defi nes what the tar command should do, as shown in Table 4-8.\nTABLE 4 -8    The tar Command Functions\nFunctionLong NameDescription\n-A          --concatenate\nAppends an existing tar archive fi le to another existing \ntar archive fi le\n-c          --create\nCreates a new tar archive fi le\n-d          --diff\nChecks the differences between a tar archive fi le and the \nfi lesystem\n--delete\nDeletes from an existing tar archive fi le\n-r          --append\nAppends fi les to the end of an existing tar archive fi le\n-t          --list\nLists the contents of an existing tar archive fi le\n-u          --update\nAppends fi les to an existing tar archive fi le that are newer \nthan a fi le with the same name in the existing archive\n-x          --extract\nExtracts fi les from an existing archive fi le\nEach function uses options to defi ne a specifi c behavior for the tar archive fi le. Table 4-9 \nlists the common options that you can use with the \ntar command.\n\n111\nChapter 4: More bash Shell Commands\nc04.indd  12/03/2014  Page  111\n4\nTABLE 4 -9    The tar Command Options\nOptionDescription\n-C dirChanges to the specifi ed directory\n-f fileOutputs results to fi le (or device) file\n-j\nRedirects output to the bzip2 command for compression\n-p\nPreserves all fi le permissions\n-v\nLists fi les as they are processed\n-z\nRedirects the output to the gzip command for compression\nThese options are usually combined to create the following scenarios. First, you want to \ncreate an archive fi le using this command:\n tar -cvf test.tar test/ test2/\nThe above command creates an archive fi le called test.tar containing the contents of \nboth the \ntest directory and the test2 directory. Next, this command:\n tar -tf test.tar\nlists (but doesn’t extract) the contents of the tar fi le test.tar. Finally, this command:\n tar -xvf test.tar\nextracts the contents of the tar fi le test.tar. If the tar fi le was created from a directory \nstructure, the entire directory structure is re-created starting at the current directory.\nAs you can see, using the \ntar command is a simple way to create archive fi les of entire \ndirectory structures. This is a common method for distributing source code fi les for open \nsource applications in the Linux world.\nIf you download open source software, often you see fi lenames that end in .tgz. These are gzipped tar fi les, which can \nbe extracted using the command \ntar -zxvf filename.tgz.\nSummary\nThis chapter discussed some of the more advanced bash commands used by Linux system \nadministrators and programmers. The \nps and top commands are vital in determining the \nstatus of the system, allowing you to see what applications are running and how many \nresources they are consuming.\n\n112\nPart I: The Linux Command Line\nc04.indd  12/03/2014  Page  112\nIn this day of removable media, another popular topic for system administrators is \nmounting storage devices. The \nmount command allows you to mount a physical storage \ndevice into the Linux virtual directory structure. To remove the device, use the \numount \ncommand.\nFinally, the chapter discussed various utilities used for handling data. The \nsort utility \neasily sorts large data fi les to help you organize data, and the \ngrep utility allows you to \nquickly scan through large data fi les looking for specifi c information. Several fi le compres-\nsion utilities are available in Linux, including \ngzip and zip. Each one allows you to com-\npress large fi les to help save space on your fi lesystem. The Linux \ntar  utility is a popular \nway to archive directory structures into a single fi le that can easily be ported to another \nsystem.\nThe next chapter discusses Linux shells and how to interact with them. Linux allows \nyou to communicate between shells, which can come in handy when creating subshells in \nyour scripts.\n\n113\nc05.indd  12/10/2014  Page  113\nCHAPTER \n5\nUnderstanding the Shell\nIN THIS CHAPTER\nInvestigating Shell Types \nUnderstanding the Parent/Child Shell Relationship\nUsing Subshells Creatively\nInvestigating Built-in Shell Commands\nN\now that you know a few shell basics, such as reaching the shell and rudimentary shell com-\nmands, it is time to explore the actual shell process. To understand the shell, you need to \nunderstand a few CLI basics.\nA shell is not just a CLI. It is a complicated interactive running program. Entering commands and \nusing the shell to run scripts can raise some interesting and confusing issues. Understanding the \nshell process and its relationships helps you resolve these issues or avoid them altogether.\nThis chapter takes you through learning about the shell process. You see how subshells are created \nand their relationship to the parent shell. The varied commands that create child processes are \nexplored as well as built-in commands. You even read about some shell tips and tricks to try.\nExploring Shell Types\nThe shell program that the system starts depends on your user ID confi guration. In the /etc/\npasswd\n fi le, the user ID has its default shell program listed in fi eld #7 of its record. The default \nshell program is started whenever the user logs into a virtual console terminal or starts a terminal \nemulator in the GUI.\nIn the following example, user \nchristine has the GNU bash shell as her default shell program:\n$ cat /etc/passwd\n[...]\nChristine:x:501:501:Christine B:/home/Christine:/bin/bash\n$\n\n114\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  114\nThe bash shell program resides in the /bin directory. A long listing reveals /bin/bash \n(the bash shell) is an executable program:\n$ ls -lF /bin/bash\n-rwxr-xr-x. 1 root root 938832 Jul 18  2013 /bin/bash*\n$\nSeveral other shell programs are on this particular CentOS distribution. They include tcsh, \nwhich is based off the original C shell:\n$ ls -lF /bin/tcsh\n-rwxr-xr-x. 1 root root 387328 Feb 21  2013 /bin/tcsh*\n$\nAlso, the Debian based version of the ash shell, dash, is included:\n$ ls -lF /bin/dash\n-rwxr-xr-x. 1 root root 109672 Oct 17  2012 /bin/dash*\n$\nFinally, a soft link (see Chapter 3) of the C shell points to the tcsh shell: \n$ ls -lF /bin/csh\nlrwxrwxrwx. 1 root root 4 Mar 18 15:16 /bin/csh -> tcsh*\n$\nEach of these different shell programs could be set as a user’s default shell. However, due to \nthe bash shell’s popularity, it’s rare to use any other shell as a default shell. \nA brief description of various shells was included in Chapter 1. You may be interested in learning even more about \nshells other than the GNU bash shell. Additional alternative shell information is in Chapter 23.\nThe default interactive shell starts whenever a user logs into a virtual console terminal or \nstarts a terminal emulator in the GUI. However, another default shell, \n/bin/sh, is the \ndefault system shell. The default system shell is used for system shell scripts, such as those \nneeded at startup. \nOften, you see a distribution with its default system shell set to the bash shell using a soft \nlink as shown here on this CentOS distribution:\n$ ls -l /bin/sh\nlrwxrwxrwx. 1 root root 4 Mar 18 15:05 /bin/sh -> bash\n$\nHowever, be aware that on some distributions, the default system shell is different than \nthe default interactive shell, such as on this Ubuntu distribution:\n\n115\nChapter 5: Understanding the Shell\nc05.indd  12/10/2014  Page  115\n5\n$ cat /etc/passwd\n[...]\nchristine:x:1000:1000:Christine,,,:/home/christine:/bin/bash\n$\n$ ls -l /bin/sh\nlrwxrwxrwx 1 root root 4 Apr 22 12:33 /bin/sh -> dash\n$\nNote that the user, christine, has her default interactive shell set to /bin/bash, the \nbash shell. But the default system shell, \n/bin/sh, is set to the dash shell.\nFor bash shell scripts, these two different shells, default interactive shell and default system shell, can cause prob-\nlems. Be sure to read about the important syntax needed for a bash shell script’s fi rst line in Chapter 11 to avoid \nthese issues.\nYou are not forced to stick with your default interactive shell. You can start any shell avail-\nable on your distribution, simply by typing its fi lename. For example, to start the dash \nshell, you can run it directly by typing the command /bin/dash:\n$ /bin/dash\n$\nIt doesn’t look like anything happened, but the dash shell program started. The $ prompt \nis a CLI prompt for the dash shell. You can leave the dash shell program by typing the com-\nmand exit:\n$ exit\nexit\n$\nAgain, it looks like nothing happened. However, the dash shell program was exited. To \nunderstand this process, the next section explores the relationship between a login shell \nprogram and a newly started shell program.\nExploring Parent and Child Shell Relationships\nThe default interactive shell started when a user logs into a virtual console terminal or \nstarts a terminal emulator in the GUI is a parent shell. As you have read so far in this book, \na parent shell process provides a CLI prompt and waits for commands to be entered.\nWhen the \n/bin/bash command or the equivalent bash command is entered at the CLI \nprompt, a new shell program is created. This is a child shell. A child shell also has a CLI \nprompt and waits for commands to be entered.\n\n116\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  116\nBecause you do not see any relevant messages when you type bash and spawn a child shell, \nanother command can help bring clarity. The \nps command was covered in Chapter 4. Using \nthis with the \n-f option before and after entering a child shell is useful:\n$ ps -f\nUID        PID  PPID  C STIME TTY          TIME CMD\n501       1841  1840  0 11:50 pts/0    00:00:00 -bash\n501       2429  1841  4 13:44 pts/0    00:00:00 ps -f\n$\n$ bash\n$\n$ ps -f\nUID        PID  PPID  C STIME TTY          TIME CMD\n501       1841  1840  0 11:50 pts/0    00:00:00 -bash\n501       2430  1841  0 13:44 pts/0    00:00:00 bash\n501       2444  2430  1 13:44 pts/0    00:00:00 ps -f\n$\nThe fi rst use of ps  -f shows two processes. One process has a process ID of 1841 (second \ncolumn) and is running the bash shell program (last column). The second process (process \nID 2429) is the actual \nps  -f command running.\nA process is a running program. The bash shell is a program, and when it runs, it is a process. A running shell is \nsimply one type of process. Therefore, when reading about running a bash shell, you often see the word “shell” and \nthe word “process” used interchangeably.\nAfter the command bash is entered, a child shell is created. The second ps  -f is exe-\ncuted from within the child shell. From this display, you can see that two bash shell pro-\ngrams are running. The fi rst bash shell program, the parent shell process, has the original \nprocess ID (PID) of 1841. The second bash shell program, the child shell process, has a PID \nof 2430. Note that the child shell has a parent process ID (PPID) of 1841, denoting that the \nparent shell process is its parent. Figure 5-1 diagrams this relationship.\nFIGURE 5-1\nParent and child bash shell processes\nCreates\nsubshell\nParent shell\nissues command:\nbash\nChild subshell\nissues command:\nps -f\n\n117\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  117\nWhen a child shell process is spawned, only some of the parent’s environment is copied to \nthe child shell environment. This can cause problems with items such as variables, and it is \ncovered in Chapter 6. \nA child shell is also called a subshell. A subshell can be created from a parent shell, and a \nsubshell can be created from another subshell: \n$ ps -f\nUID        PID  PPID  C STIME TTY          TIME CMD\n501       1841  1840  0 11:50 pts/0    00:00:00 -bash\n501       2532  1841  1 14:22 pts/0    00:00:00 ps -f\n$\n$ bash\n$\n$ bash\n$\n$ bash\n$\n$ ps --forest\n  PID TTY          TIME CMD\n 1841 pts/0    00:00:00 bash\n 2533 pts/0    00:00:00  \\_ bash\n 2546 pts/0    00:00:00      \\_ bash\n 2562 pts/0    00:00:00          \\_ bash\n 2576 pts/0    00:00:00              \\_ ps\n$\nIn the preceding example, the bash shell command was entered three times. Effectively, \nthis created three subshells. The \nps  --forest command shows the nesting of these sub-\nshells. Figure 5-2 also shows this subshell nesting.\nThe \nps  -f command can be useful in subshell nesting, because it displays who is whose \nparent via the PPID column:\n$ ps -f\nUID        PID  PPID  C STIME TTY          TIME CMD\n501       1841  1840  0 11:50 pts/0    00:00:00 -bash\n501       2533  1841  0 14:22 pts/0    00:00:00 bash\n501       2546  2533  0 14:22 pts/0    00:00:00 bash\n501       2562  2546  0 14:24 pts/0    00:00:00 bash\n501       2585  2562  1 14:29 pts/0    00:00:00 ps -f\n$\nThe bash shell program can use command line parameters to modify the shell start. \nTable 5-1 lists the command line parameters available in bash.\n\n118\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  118\nFIGURE 5-2\nSubshell nesting\nCreates\nsubshell\nCreates\nsubshell\nParent shell\nissues command:\nbash\nissues command:\nbash\nbash child subshell\nCreates\nsubshell\nbash great-\ngrandchild subshell\nbash grandchild\nsubshell\nissues command:\nps --forest\nissues command:\nbash\nTABLE 5 -1    The bash Command Line Parameters\nParameterDescription\n-c stringReads commands from string and processes them\n-iStarts an interactive shell, allowing input from the user\n-lActs as if invoked as a login shell\n-rStarts a restricted shell, limiting the user to the default directory\n-sReads commands from the standard input\nYou can fi nd more help on the bash command and even more command line parameters by \ntyping man bash. The \nbash --help command provides additional assistance as well.\nYou can gracefully exit out of each subshell by entering the \nexit command:\n$ exit\nexit\n$\n$ ps --forest\n  PID TTY          TIME CMD\n 1841 pts/0    00:00:00 bash\n\n119\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  119\n 2533 pts/0    00:00:00  \\_ bash\n 2546 pts/0    00:00:00      \\_ bash\n 2602 pts/0    00:00:00          \\_ ps\n$\n$ exit\nexit\n$\n$ exit\nexit\n$\n$ ps --forest\n  PID TTY          TIME CMD\n 1841 pts/0    00:00:00 bash\n 2604 pts/0    00:00:00  \\_ ps\n$\nNot only does the exit command allow you to leave child subshells, but you can also log \nout of your current virtual console terminal or terminal emulation software as well. Just \ntype exit in the parent shell, and you gracefully exit the CLI.\nAnother time a subshell can be created is when you run a shell script. You learn more about \nthat topic in Chapter 11.\nAlso, you can spawn subshells without using the \nbash shell command or running a shell \nscript. One way is by using a process list.\nLooking at process lists\nOn a single line, you can designate a list of commands to be run one after another. This is \ndone by entering a command list using a semicolon (;) between commands:\n$ pwd ; ls ; cd /etc ; pwd ; cd ; pwd ; ls\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n/etc\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n$\nIn the preceding example, the commands all executed one after another with no problems. \nHowever, this is not a process list. For a command list to be considered a process list, the \ncommands must be encased in parentheses:\n\n120\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  120\n$ (pwd ; ls ; cd /etc ; pwd ; cd ; pwd ; ls)\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n/etc\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n$\nThough the parentheses addition may not appear to be a big difference, they do cause a \nvery different effect. Adding parentheses and turning the command list into a process list \ncreated a subshell to execute the commands.\nA process list is a command grouping type. Another command grouping type puts the commands between curly \nbrackets and ends the command list with a semicolon (;). The syntax is as follows: \n{ command; }. Using curly \nbrackets for command grouping does not create a subshell as a process list does.\nTo indicate if a subshell was spawned, a command using an environment variable is needed \nhere. (Environment variables are covered in detail in Chapter 6). The command needed is \necho $BASH_SUBSHELL. If it returns a 0, then there is no subshell. If it returns 1 or more, \nthen there is a subshell.\nFirst, the example using just a command list is executed with the \necho $BASH_SUBSHELL \ntacked onto the end:\n$ pwd ; ls ; cd /etc ; pwd ; cd ; pwd ; ls ; echo $BASH_SUBSHELL\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n/etc\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n0\nAt the very end of the commands’ output, you can see the number zero (0) is displayed. \nThis indicates a subshell was not created to execute these commands.\nThe results are different using a process list. The list is executed with \necho $BASH_SUBSHELL tacked onto the end:\n$ (pwd ; ls ; cd /etc ; pwd ; cd ; pwd ; ls ; echo $BASH_SUBSHELL)\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n/etc\n\n121\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  121\n/home/Christine\nDesktop    Downloads  Music     Public     Videos\nDocuments  junk.dat   Pictures  Templates\n1\nIn this case, the number one (1) displayed at the output’s end. This indicates a subshell \nwas indeed created and used for executing these commands.\nThus, a process list is a command grouping enclosed with parentheses, which creates a sub-\nshell to execute the command(s). You can even create a grandchild subshell by embedding \nparentheses within a process list:\n$ ( pwd ; echo $BASH_SUBSHELL)\n/home/Christine\n1\n$ ( pwd ; (echo $BASH_SUBSHELL))\n/home/Christine\n2\nNotice in the fi rst process list, the number one (1) is displayed indicating a child subshell \nas you would expect. However in the example’s second process list, additional parentheses \nwere added around the \necho $BASH_SUBSHELL command. These additional parentheses \ncaused a grandchild subshell to be created for the command’s execution. Thus, a number \ntwo (\n2) was displayed indicating a subshell within a subshell.\nSubshells are often used for multi-processing in shell scripts. However, entering into a sub-\nshell is an expensive method and can signifi cantly slow down processing. Subshell issues \nexist also for an interactive CLI shell session. It is not truly multi-processing, because the \nterminal gets tied up with the subshell’s I/O.\nCreatively using subshells\nAt the interactive shell CLI, you have more productive ways to use subshells. Process lists, \nco-processes, and pipes (covered in Chapter 11) use subshells. They all can be used effec-\ntively within the interactive shell.\nOne productive subshell method in the interactive shell uses background mode. Before \ndiscussing how to use background mode and subshells together, you need to understand \nbackground mode itself.\nInvestigating background mode\nRunning a command in background mode allows the command to be processed and frees up \nyour CLI for other use. A classic command to demonstrate background mode is the \nsleep \ncommand. \n\n122\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  122\nThe sleep command accepts as a parameter the number of seconds you want the process to \nwait (sleep). This command is often used to introduce pauses in shell scripts. The command \nsleep 10 causes the session to pause for 10 seconds and then return a shell CLI prompt:\n$ sleep 10\n$\nTo put a command into background mode, the & character is tacked onto its end. Putting \nthe \nsleep command into background mode allows a little investigation with the ps \ncommand:\n$ sleep 3000&\n[1] 2396\n$ ps -f\nUID        PID  PPID  C STIME TTY          TIME CMD\nchristi+  2338  2337  0 10:13 pts/9    00:00:00 -bash\nchristi+  2396  2338  0 10:17 pts/9    00:00:00 sleep 3000\nchristi+  2397  2338  0 10:17 pts/9    00:00:00 ps -f\n$\nThe sleep command was told to sleep for 3000 seconds (50 minutes) in the background \n(\n&). When it was put into the background, two informational items were displayed before \nthe shell CLI prompt was returned. The fi rst informational item is the background job’s \nnumber (\n1) displayed in brackets. The second item is the background job’s process ID \n(\n2396). \nThe \nps command was used to display the various processes. Notice that the sleep 3000 \ncommand is listed. Also note that its process ID (PID) in the second column is the same PID \ndisplayed when the command went into the background, \n2396.\nIn addition to the \nps command, you can use the jobs command to display background job \ninformation. The \njobs command displays any user’s processes (jobs) currently running in \nbackground mode:\n$ jobs\n[1]+  Running                 sleep 3000 &\n$\nThe jobs command shows the job number (1) in brackets. It also displays the job’s current \nstatus (\nrunning) as well as the command itself, (sleep 3000 &).\nYou can see even more information by using the \n-l (lowercase L) parameter on the \njobs command. The -l parameter displays the command’s PID in addition to the other \ninformation:\n$ jobs -l\n[1]+  2396 Running                 sleep 3000 &\n$\n\n123\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  123\nWhen the background job is fi nished, its completion status is displayed:\n[1]+  Done                 sleep 3000 &\n$\nBe aware that a background job’s completion status won’t necessarily wait till a convenient time to display itself. \nDon’t let it surprise you when a job’s completion status just suddenly appears on your screen.\nBackground mode is very handy. And it provides a method for creating useful subshells at \nthe CLI.\nPutting process lists into the background\nAs stated earlier, a process list is a command or series of commands executed within a sub-\nshell. Using a process list including \nsleep commands and displaying the BASH_SUBSHELL \nvariable operates as you would expect:\n$ (sleep 2 ; echo $BASH_SUBSHELL ; sleep 2)\n1\n$\nIn the preceding example, a two-second pause occurs, the number one (1) is displayed indi-\ncating a single subshell level (child subshell), and then another two-second pause occurs \nbefore the prompt returns. Nothing too dramatic here.\nPutting the same process list into background mode can cause a slightly different effect \nwith command output:\n$ (sleep 2 ; echo $BASH_SUBSHELL ; sleep 2)&\n[2] 2401\n$ 1\n[2]+  Done                  ( sleep 2; echo $BASH_SUBSHELL; sleep 2 )\n$\nPutting the process list into the background causes a job number and process ID to appear, \nand the prompt returns. However, the odd event is that the displayed number one (\n1), indi-\ncating a single-level subshell, is displayed by the prompt! Don’t let this confuse you. Simply \npress the Enter key, and you get another prompt back.\nUsing a process list in background mode is one creative method for using subshells at the \nCLI. You can do large amounts of processing within a subshell and not have your terminal \ntied up with the subshell’s I/O.\n\n124\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  124\nOf course, the process list of sleep and echo commands are just for example purposes. \nCreating backup fi les with \ntar (see Chapter 4) is a more practical example of using back-\nground process lists effectively:\n$ (tar -cf Rich.tar /home/rich ; tar -cf My.tar /home/christine)&\n[3] 2423\n$\nPutting a process list in background mode is not the only way to use subshells creatively at \nthe CLI. Co-processing is another method.\nLooking at co-processing\nCo-processing does two things at the same time. It spawns a subshell in background mode \nand executes a command within that subshell. \nTo perform co-processing, the \ncoproc command is used along with the command to be \nexecuted in the subshell:\n$ coproc sleep 10\n[1] 2544\n$\nCo-processing performs almost identically to putting a command in background mode, \nexcept for the fact that it creates a subshell. You’ll notice that when the \ncoproc command \nand its parameters were entered, a background job was started. The background job number \n(\n1) and process ID (2544) were displayed on the screen. \nThe \njobs command allows you to display the co-processing status: \n$ jobs\n[1]+  Running                 coproc COPROC sleep 10 &\n$\nFrom the preceding example, you can see the background command executing in the \nsubshell is \ncoproc COPROC sleep 10. The COPROC is a name given to the process by \nthe \ncoproc command. You can set the name yourself by using extended syntax for the \ncommand:\n$ coproc My_Job { sleep 10; }\n[1] 2570\n$\n$ jobs\n[1]+  Running                 coproc My_Job { sleep 10; } &\n$\nBy using the extended syntax, the co-processing name was set to My_Job. Be careful here, \nbecause the extended syntax is a little tricky. You have to make sure that a space appears \nafter the fi rst curly bracket (\n{) and before the start of your command. Also, you have to \n\n125\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  125\nmake sure the command ends with a semicolon (;). And you have to ensure that a space \nappears after the semicolon and before the closing curly bracket (\n}).\nCo-processing allows you to get very fancy and send/receive information to the process running in the subshell. The \nonly time you need to name a co-process is when you have multiple co-processes running, and you need to communi-\ncate with them all. Otherwise, just let the \ncoproc command set the name to the default, COPROC.\nYou can be really clever and combine co-processing with process lists creating nested sub-\nshells. Just type your process list and put the command \ncoproc in front of it:\n$ coproc ( sleep 10; sleep 2 )\n[1] 2574\n$\n$ jobs\n[1]+  Running     coproc COPROC ( sleep 10; sleep 2 ) &\n$\n$ ps --forest\n  PID TTY          TIME CMD\n 2483 pts/12   00:00:00 bash\n 2574 pts/12   00:00:00  \\_ bash\n 2575 pts/12   00:00:00  |   \\_ sleep\n 2576 pts/12   00:00:00  \\_ ps\n$\nJust remember that spawning a subshell can be expensive and slow. Creating nested \nsubshells is even more so!\nUsing subshells can provide fl exibility at the command line as well as convenience. \nUnderstanding their behavior is important to obtaining this fl exibility and convenience. \nCommand behavior is also important to understand. In the next section, the behavior \ndifferences between built-in and external commands are explored.\nUnderstanding Shell Built-In Commands\nWhile learning about the GNU bash shell, you likely have heard the term built-in command. \nIt is important to understand both shell built-in and non-built-in (external) commands. \nBuilt-in commands and non-built-in commands operate very differently. \nLooking at external commands\nAn external command, sometimes called a fi lesystem command, is a program that exists \noutside of the bash shell. They are not built into the shell program. An external command \nprogram is typically located in \n/bin, /usr/bin, /sbin, or /usr/sbin. \n\n126\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  126\nThe ps command is an external command. You can fi nd its fi lename by using both the \nwhich and the type commands:\n$ which ps\n/bin/ps\n$\n$ type -a ps\nps is /bin/ps\n$\n$ ls -l /bin/ps\n-rwxr-xr-x 1 root root 93232 Jan  6 18:32 /bin/ps\n$\nWhenever an external command is executed, a child process is created. This action is \ntermed forking. Conveniently, the external command \nps displays its current parent as well \nas its own forked child processes:\n$ ps -f\nUID        PID  PPID  C STIME TTY          TIME CMD\nchristi+  2743  2742  0 17:09 pts/9    00:00:00 -bash\nchristi+  2801  2743  0 17:16 pts/9    00:00:00 ps -f\n$\nBecause it is an external command, when the ps command executes, a child process is \ncreated. In this case, the \nps command’s PID is 2801 and the parent PID is 2743. The bash \nshell process, which is the parent, has a PID of \n2743. Figure 5-3 illustrates the forking that \noccurs when an external command is executed.\nFIGURE 5-3\nExternal command forking\nForks child\nprocess\nParent process\nissues external\ncommand:\nps -f\nexecutes external\ncommand:\nps -f\nChild process\nWhenever a process must fork, it takes time and effort to set up the new child process’s \nenvironment. Thus, external commands can be a little expensive. \n\n127\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  127\nIf you fork a child process or create a subshell, you can still communicate with it via signaling, which is extremely \nhelpful in both the command line and in writing shell scripts. Signaling allows process communication via signals. \nSignals and signaling are covered in Chapter 16.\nWhen using a built-in command, no forking is required. Therefore, built-in commands are \nless expensive.\nLooking at built-in commands\nBuilt-in commands are different in that they do not need a child process to execute. They \nwere compiled into the shell and thus are part of the shell’s toolkit. No external \nprogram fi le exists to run them.\nBoth the \ncd and exit commands are built into the bash shell. You can tell a command is \nbuilt-in by using the \ntype command:\n$ type cd\ncd is a shell builtin\n$\n$ type exit\nexit is a shell builtin\n$\nBecause they do not need to fork a child process to execute or open a program fi le, built-in \ncommands are faster and more effi cient. A list of GNU bash shell built-in commands is \nprovided in Appendix A.\nBe aware that some commands have multiple fl avors. For example, both \necho and pwd have \na built-in command fl avor as well as an external command fl avor. These fl avors are slightly \ndifferent. To see multiple fl avors for commands, use the \n-a option on the type command:\n$ type -a echo\necho is a shell builtin\necho is /bin/echo\n$\n$ which echo\n/bin/echo\n$\n$ type -a pwd\npwd is a shell builtin\npwd is /bin/pwd\n\n128\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  128\n$\n$ which pwd\n/bin/pwd\n$\nUsing the type -a command shows both types for each of the two commands. Note that \nthe \nwhich command shows only the external command fi le.\nTo use the external command for a command that has multiple fl avors, directly reference the fi le. For example, to use \nthe \npwd external command, type /bin/pwd. \nUsing the history command\nA useful built-in command is the history command. The bash shell keeps track of the \ncommands you have used. You can recall these commands and even reuse them. \nTo see a recently used commands list, just type the \nhistory command with no options:\n$ history\n    1  ps -f\n    2  pwd\n    3  ls\n    4  coproc ( sleep 10; sleep 2 )\n    5  jobs\n    6  ps --forest\n    7  ls\n    8  ps -f\n    9  pwd\n   10  ls -l /bin/ps\n   11  history\n   12  cd /etc\n   13  pwd\n   14  ls\n   15  cd\n   16  type pwd\n   17  which pwd\n   18  type echo\n   19  which echo\n   20  type -a pwd\n   21  type -a echo\n   22  pwd\n   23  history\nIn this example, only the last 23 commands are shown. Typically, the last 1,000 commands \nare kept in history. That is lots of commands!\n\n129\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  129\nYou can set the number of commands to keep in the bash history. To do so, you need to modify an environment \nvariable called \nHISTSIZE (see Chapter 6). \nYou can recall and reuse the last command in your history list. This can save time and \ntyping. To recall and reuse your last command, type \n!! and press the Enter key: \n$ ps --forest\n  PID TTY          TIME CMD\n 2089 pts/0    00:00:00 bash\n 2744 pts/0    00:00:00  \\_ ps\n$\n$ !!\nps --forest\n  PID TTY          TIME CMD\n 2089 pts/0    00:00:00 bash\n 2745 pts/0    00:00:00  \\_ ps\n$\nWhen !! was entered, the bash shell fi rst displayed the command it was recalling from the \nshell’s history. After the command was displayed, it was executed. \nCommand history is kept in the hidden \n.bash_history fi le, which is located in the user’s \nhome directory. Be careful here. The bash command history is stored in memory and then \nwritten out into the history fi le when the shell is exited:\n$ history\n[...]\n   25  ps --forest\n   26  history\n   27  ps --forest\n   28  history\n$\n$ cat .bash_history\npwd\nls\nhistory\nexit\n$\nNotice when the history command is run, 28 commands are listed. In the example, the \nlisting is snipped for brevity. However, when the \n.bash_history fi le is displayed, only \nfour commands are listed, and they don’t match the \nhistory command’s list.\n\n130\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  130\nYou can force the command history to be written to the .bash_history fi le before \nleaving a shell session. In order to force this write, use the \n-a option on the history \ncommand:\n$ history -a\n$\n$ history\n[...]\n   25  ps --forest\n   26  history\n   27  ps --forest\n   28  history\n   29  ls -a\n   30  cat .bash_history\n   31  history -a\n   32  history\n$\n$ cat .bash_history\n[...]\nps --forest\nhistory\nps --forest\nhistory\nls -a\ncat .bash_history\nhistory -a\nThis time both listings need to be snipped because they are so long. Notice that contents \nfrom both the \nhistory command and the .bash_history fi le match, except for the very \nlast command listed for the \nhistory command, because it came after the history  -a \ncommand was issued.\nIf you have multiple terminal sessions open, you can still append the .bash_history in each open session using the \nhistory -a command. However, the histories are not automatically updated for your other open terminal sessions. \nThis is because the \n.bash_history fi le is read only when a terminal session is fi rst started. To force the \n.bash_history fi le to be reread and a terminal session’s history to be updated, use the history -n command. \nYou can recall any command from the history list. Just enter an exclamation point and the \ncommand’s number from the history list:\n$ history\n[...]\n   13  pwd\n   14  ls\n   15  cd\n\n131\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  131\n   16  type pwd\n   17  which pwd\n   18  type echo\n   19  which echo\n   20  type -a pwd\n   21  type -a echo\n[...]\n   32  history -a\n   33  history\n   34  cat .bash_history\n   35  history\n$\n$ !20\ntype -a pwd\npwd is a shell builtin\npwd is /bin/pwd\n$\nCommand number 20 was pulled from command history. Notice that similar to executing \nthe last command in history, the bash shell fi rst displays the command it is recalling from \nthe shell’s history. After the command is displayed, it is executed.\nUsing bash shell command history can be a great timesaver. You can do even more with the \nbuilt-in \nhistory command. Be sure to view the bash manual pages for history, by typing \nman history. \nUsing command aliases\nThe alias command is another shell built-in command. A command alias allows you to \n create an alias name for common commands (along with their parameters) to help keep your \ntyping to a minimum.\nMost likely, your Linux distribution has already set some common command aliases for you. \nTo see a list of the active aliases, use the \nalias command with the -p parameter:\n$ alias -p\n[...]\nalias egrep='egrep --color=auto'\nalias fgrep='fgrep --color=auto'\nalias grep='grep --color=auto'\nalias l='ls -CF'\nalias la='ls -A'\nalias ll='ls -alF'\nalias ls='ls --color=auto'\n$\nNotice that, on this Ubuntu Linux distribution, an alias is used to override the standard ls \ncommand. It automatically provides the \n--color parameter, indicating that the terminal \nsupports color mode listings.\n\n132\nPart I: The Linux Command Line\nc05.indd  12/10/2014  Page  132\nYou can create your own aliases using the alias command:\n$ alias li='ls -li'\n$\n$ li\ntotal 36\n529581 drwxr-xr-x. 2 Christine Christine 4096 May 19 18:17 Desktop\n529585 drwxr-xr-x. 2 Christine Christine 4096 Apr 25 16:59 Documents\n529582 drwxr-xr-x. 2 Christine Christine 4096 Apr 25 16:59 Downloads\n529586 drwxr-xr-x. 2 Christine Christine 4096 Apr 25 16:59 Music\n529587 drwxr-xr-x. 2 Christine Christine 4096 Apr 25 16:59 Pictures\n529584 drwxr-xr-x. 2 Christine Christine 4096 Apr 25 16:59 Public\n529583 drwxr-xr-x. 2 Christine Christine 4096 Apr 25 16:59 Templates\n532891 -rwxrw-r--. 1 Christine Christine   36 May 30 07:21 test.sh\n529588 drwxr-xr-x. 2 Christine Christine 4096 Apr 25 16:59 Videos\n$\nAfter you defi ne an alias value, you can use it at any time in your shell, including in shell \nscripts. Be aware that because command aliases are built-in commands, an alias is valid \nonly for the shell process in which it is defi ned:\n$ alias li='ls -li'\n$\n$ bash\n$\n$ li\nbash: li: command not found\n$\n$ exit\nexit\n$\nFortunately, you can make an alias value permanent across subshells. The next chapter \ncovers how to do that, along with environment variables.\nSummary\nThis chapter discussed the complicated interactive program, the GNU bash shell. It covered \nunderstanding the shell process and its relationships, including how subshells are spawned \nand their relationship to the parent shell. We also explored commands that create child \nprocesses and commands that don’t.\nThe default interactive shell is normally started whenever a user logs in to a terminal. The \nshell that the system starts depends upon a user ID confi guration. Typically, it is \n/bin/\nbash\n. The default system shell, /bin/sh, is used for system shell scripts, such as those \nneeded at startup.\n\n133\nChapter 5: Understanding the Shell\n5\nc05.indd  12/10/2014  Page  133\nA subshell or child shell can be spawned using the bash command. They are also created \nwhen a process list or the \ncoproc  command is used. Using subshells at the command line \ncan allow for creative and productive use of the CLI. Subshells can be nested, spawning \ngrandchild shells and great-grandchild shells. Creating a subshell is an expensive process as \na new environment for the shell must be created as well.\nFinally, the chapter looked at two different types of shell commands: built-in and external \ncommands. External commands create a child process with a new environment, but a built-\nin command does not. This causes external commands to be more expensive to use. Because \na new environment is not needed, built-in commands are more effi cient and not affected by \nany environment changes.\nShells, subshells, processes, and forked processes are all affected by environment variables. \nHow the variables affect and can be used within these different contexts are explored in \nthe next chapter. \n\n\n\n135\nc06.indd  12/03/2014  Page  135\nCHAPTER \n6\nUsing Linux Environment \nVariables\nIN THIS CHAPTER\nLooking at environment variables\nCreating your own local variables\nRemoving variables\nExploring default shell environment variables\nSetting the PATH environment variable\nLocating environment fi les\nUsing variable arrays \nL\ninux environment variables help defi ne your Linux shell experience. Many programs and \nscripts use environment variables to obtain system information and store temporary data \nand confi guration information. Environment variables are set in lots of places on the Linux \n system, and you should know where these places are.\nThis chapter walks you through the world of Linux environment variables, showing where they are, \nhow to use them, and even how to create your own. The chapter fi nishes off with how to use vari-\nable arrays.\nExploring Environment Variables\nThe bash shell uses a feature called environment variables to store information about the shell \nsession and the working environment (thus the name environment variables). This feature also \nallows you to store data in memory that can be easily accessed by any program or script running \nfrom the shell. It is a handy way to store needed persistent data.\nThere are two environment variable types in the bash shell:\n ■\nGlobal variables\n ■\nLocal variables\n\n136\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  136\nThis section describes each type of environment variable and shows how to see and \nuse them.\nEven though the bash shell uses specifi c environment variables that are consistent, different Linux distributions often \nadd their own environment variables. The environment variable examples you see in this chapter may differ slightly \nfrom what’s available on your specifi c distribution. If you run into an environment variable not covered here, check \nyour Linux distribution’s documentation.\nLooking at global environment variables\nGlobal environment variables are visible from the shell session and from any spawned child \nsubshells. Local variables are available only in the shell that creates them. This makes \nglobal environment variables useful in applications that create child subshells, which \nrequire parent shell information.\nThe Linux system sets several global environment variables when you start your bash ses-\nsion. (For more details about what variables are started at that time, see the “Locating \nSystem Environment Variables” section later in this chapter.) The system environment vari-\nables almost always use all capital letters to differentiate them from normal user environ-\nment variables.\nTo view global environment variables, use the \nenv or the printenv command:\n$ printenv\nHOSTNAME=server01.class.edu\nSELINUX_ROLE_REQUESTED=\nTERM=xterm\nSHELL=/bin/bash\nHISTSIZE=1000\n[...]\nHOME=/home/Christine\nLOGNAME=Christine\n[...]\nG_BROKEN_FILENAMES=1\n_=/usr/bin/printenv\nSo many global environment variables get set for the bash shell that the display had to \nbe snipped. Not only are many set during the login process, but how you log in can affect \nwhich ones are set as well.\nTo display an individual environment variable’s value, you can use the \nprintenv com-\nmand, but not the \nenv command:\n$ printenv HOME\n/home/Christine\n\n137\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  137\n6\n$\n$ env HOME\nenv: HOME: No such file or directory\n$\nYou can also use the echo command to display a variable’s value. When referencing an \nenvironment variable in this case, you must place a dollar sign (\n$) before the environment \nvariable name:\n$ echo $HOME\n/home/Christine\n$\nUsing the dollar sign along with the variable name does more than just display its \ncurrent defi nition when used with the \necho command. The dollar sign before a variable \nname allows the variable to be passed as a command parameter:\n$ ls $HOME\nDesktop    Downloads  Music     Public     test.sh\nDocuments  junk.dat   Pictures  Templates  Videos\n$\n$ ls /home/Christine\nDesktop    Downloads  Music     Public     test.sh\nDocuments  junk.dat   Pictures  Templates  Videos\n$\nAs mentioned earlier, global environment variables are also available to any process’s \nsubshells:\n$ bash\n$\n$ ps -f\nUID        PID  PPID  C STIME TTY          TIME CMD\n501       2017  2016  0 16:00 pts/0    00:00:00 -bash\n501       2082  2017  0 16:08 pts/0    00:00:00 bash\n501       2095  2082  0 16:08 pts/0    00:00:00 ps -f\n$\n$ echo $HOME\n/home/Christine\n$\n$ exit\nexit\n$\nIn this example, after spawning a subshell using the bash command, the HOME envi-\nronment variable’s current value is shown. It is set to the exact same value, \n/home/\nChristine\n, as it was in the parent shell.\n\n138\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  138\nLooking at local environment variables\nLocal environment variables, as their name implies, can be seen only in the local process \nin which they are defi ned. Even though they are local, they are just as important as global \nenvironment variables. In fact, the Linux system also defi nes standard local environment \nvariables for you by default. However, you can also defi ne your own local variables. These, \nas you would assume, are called user-defi ned local variables.\nTrying to see the local variables list is a little tricky at the CLI. Unfortunately, there isn’t \na command that displays only these variables. The \nset command displays all variables \ndefi ned for a specifi c process, including both local and global environment variables and \nuser-defi ned variables:\n$ set\nBASH=/bin/bash\n[...]\nBASH_ALIASES=()\nBASH_ARGC=()\nBASH_ARGV=()\nBASH_CMDS=()\nBASH_LINENO=()\nBASH_SOURCE=()\n[...]\ncolors=/etc/DIR_COLORS\nmy_variable='Hello World'\n[...]\n$\nAll global environment variables displayed using the env or printenv commands appear \nin the \nset command’s output. The additional environment variables are the local environ-\nment and user-defi ned variables.\nThe differences between the commands env, printenv, and set are subtle. The set command displays both \nglobal and local environment variables and user-defi ned variables. It also sorts the display alphabetically. The \nenv \nand \nprintenv are different from set in that they do not sort the variables, nor do they include local environment \nor local user-defi ned variables. Used in this context, \nenv and printenv produce duplicate listings. However, the \nenv command has additional functionality that printenv does not have, making it the slightly more powerful \ncommand.\nSetting User-Defi ned Variables\nYou can set your own variables directly from the bash shell. This section shows you how to cre-\nate your own variables and reference them from an interactive shell or shell script program.\n\n139\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  139\n6\nSetting local user-defi ned variables\nAfter you start a bash shell (or spawn a shell script), you’re allowed to create local user-defi ned \nvariables that are visible within your shell process. You can assign either a numeric or a string \nvalue to an environment variable by assigning the variable to a value using the equal sign:\n$ echo $my_variable\n$ my_variable=Hello\n$\n$ echo $my_variable\nHello\nThat was simple! Now, any time you need to reference the my_variable user-defi ned vari-\nable’s value, just reference it by the name \n$my_variable.\nIf you need to assign a string value that contains spaces, you need to use a single or double \nquotation mark to delineate the beginning and the end of the string:\n$ my_variable=Hello World\n-bash: World: command not found\n$ \n$ my_variable=\"Hello World\"\n$\n$ echo $my_variable\nHello World\n$\nWithout the quotation marks, the bash shell assumes that the next word is another command \nto process. Notice that for the local variable you defi ned, you used lowercase letters, while the \nsystem environment variables you’ve seen so far have all used uppercase letters.\nThe standard bash shell convention is for all environment variables to use uppercase letters. If you are creating a \nlocal variable for yourself and your own shell scripts, use lowercase letters. Variables are case sensitive. By keep-\ning your user-defi ned local variables lowercase, you avoid the potential disaster of redefi ning a system environment \nvariable.\nIt’s extremely important that you not use spaces between the variable name, the equal \nsign, and the value. If you put any spaces in the assignment, the bash shell interprets the \nvalue as a separate command:\n$ my_variable = \"Hello World\"\n-bash: my_variable: command not found\n$\nAfter you set a local variable, it’s available for use anywhere within your shell process. \nHowever, if you spawn another shell, it’s not available in the child shell:\n\n140\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  140\n$ my_variable=\"Hello World\"\n$\n$ bash\n$\n$ echo $my_variable\n$ exit\nexit\n$\n$ echo $my_variable\nHello World\n$\nIn this example, a child shell was spawned. The user-defi ned my_variable was not avail-\nable in the child shell. This is demonstrated by the blank line returned after the \necho $my_variable command. After the child shell was exited and returned to the \noriginal shell, the local variable was still available.\nSimilarly, if you set a local variable in a child process, after you leave the child process, the \nlocal variable is no longer available:\n$ echo $my_child_variable\n$ bash\n$\n$ my_child_variable=\"Hello Little World\"\n$\n$ echo $my_child_variable\nHello Little World\n$\n$ exit\nexit\n$\n$ echo $my_child_variable\n$\nThe local variable set within the child shell doesn’t exist after a return to the parent shell. \nYou can change this behavior by turning your local user-defi ned variable into a global envi-\nronment variable.\nSetting global environment variables\nGlobal environment variables are visible from any child processes created by the parent pro-\ncess that sets the variable. The method used to create a global environment variable is to \nfi rst create a local variable and then export it to the global environment.\n\n141\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  141\n6\nThis is done by using the export command and the variable name minus the dollar sign:\n$ my_variable=\"I am Global now\"\n$\n$ export my_variable\n$\n$ echo $my_variable\nI am Global now\n$\n$ bash\n$\n$ echo $my_variable\nI am Global now\n$\n$ exit\nexit\n$\n$ echo $my_variable\nI am Global now\n$\nAfter defi ning and exporting the local variable my_variable, a child shell was started by \nthe \nbash command. The child shell was able to properly display the my_variable vari-\nable’s value. The variable kept its value, because the \nexport command made it a global \nenvironment variable.\nChanging a global environment variable within a child shell does not affect the variable’s \nvalue in the parent shell:\n$ my_variable=\"I am Global now\"\n$ export my_variable\n$\n$ echo $my_variable\nI am Global now\n$\n$ bash\n$\n$ echo $my_variable\nI am Global now\n$\n$ my_variable=\"Null\"\n$\n$ echo $my_variable\nNull\n$\n$ exit\nexit\n$\n\n142\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  142\n$ echo $my_variable\nI am Global now\n$\nAfter defi ning and exporting the variable my_variable, a subshell was started by the \nbash command. The subshell properly displayed the value of the my_variable global \nenvironment variable. The variable’s value was then changed by the child shell. However, \nthe variable’s value was modifi ed only within the child shell and not in the parent’s shell.\nA child shell cannot even use the \nexport command to change the parent shell’s global \nenvironment variable’s value:\n$ my_variable=\"I am Global now\"\n$ export my_variable\n$\n$ echo $my_variable\nI am Global now\n$\n$ bash\n$\n$ echo $my_variable\nI am Global now\n$\n$ my_variable=\"Null\"\n$\n$ export my_variable\n$\n$ echo $my_variable\nNull\n$\n$ exit\nexit\n$\n$ echo $my_variable\nI am Global now\n$\nEven though the child shell redefi ned and exported the variable my_variable, the parent \nshell’s \nmy_variable variable kept its original value.\nRemoving Environment Variables\nOf course, if you can create a new environment variable, it makes sense that you can also remove \nan existing environment variable. You can do this with the \nunset command. When referencing \nthe environment variable in the \nunset command, remember not to use the dollar sign:\n$ echo $my_variable\nI am Global now\n\n143\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  143\n6\n$\n$ unset my_variable\n$\n$ echo $my_variable\n$\nIt can be confusing to remember when to use and when not to use the dollar sign with environment variables. Just \nremember this: If you are doing anything with the variable, use the dollar sign. If you are doing anything to the vari-\nable, don’t use the dollar sign. The exception to this rule is using \nprintenv to display a variable’s value.\nWhen dealing with global environment variables, things get a little tricky. If you’re in a \nchild process and unset a global environment variable, it applies only to the child process. \nThe global environment variable is still available in the parent process:\n$ my_variable=\"I am Global now\"\n$\n$ export my_variable\n$\n$ echo $my_variable\nI am Global now\n$\n$ bash\n$\n$ echo $my_variable\nI am Global now\n$\n$ unset my_variable\n$\n$ echo $my_variable\n$ exit\nexit\n$\n$ echo $my_variable\nI am Global now\n$\nJust as with modifying a variable, you cannot unset it in a child shell and have the vari-\nable be unset in the parent’s shell.\nUncovering Default Shell Environment Variables\nThe bash shell uses specifi c environment variables by default to defi ne the system envi-\nronment. You can always count on these variables being set or available to be set on your \n\n144\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  144\nLinux system. Because the bash shell is a derivative of the original Unix Bourne shell, it \nalso includes environment variables originally defi ned in that shell.\nTable 6-1 shows the environment variables that the bash shell provides that are compatible \nwith the original Unix Bourne shell.\nTABLE 6 -1    The bash Shell Bourne Variables\nVariableDescription\nCDPATH\nA colon-separated list of directories used as a search path for the cd \ncommand\nHOME\nThe current user’s home directory\nIFS\nA list of characters that separate fi elds used by the shell to split text strings\nMAIL\nThe fi lename for the current user’s mailbox (The bash shell checks this fi le for \nnew mail.)\nMAILPATH\nA colon-separated list of multiple fi lenames for the current user’s mailbox (The \nbash shell checks each fi le in this list for new mail.)\nOPTARG\nThe value of the last option argument processed by the getopt command\nOPTIND\nThe index value of the last option argument processed by the getopt \ncommand\nPATH\nA colon-separated list of directories where the shell looks for commands\nPS1\nThe primary shell command line interface prompt string\nPS2\nThe secondary shell command line interface prompt string\nBesides the default Bourne environment variables, the bash shell also provides a few vari-\nables of its own, as shown in Table 6-2.\nTABLE 6 -2    The bash Shell Environment Variables\nVariableDescription\nBASH\nThe full pathname to execute the current instance of the bash shell\nBASH_ALIASES\nAn associative array of currently set aliases\nBASH_ARGC\nA variable array that contains the number of parameters being \npassed to a subroutine or shell script\nBASH_ARCV\nA variable array that contains the parameters being passed to a \nsubroutine or shell script\nBASH_CMDS\nAn associative array of locations of commands the shell has \nexecuted\n\n145\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  145\n6\nBASH_COMMAND\nThe shell command currently being or about to be executed\nBASH_ENV\nWhen set, each bash script attempts to execute a startup fi le \ndefi ned by this variable before running.\nBASH_EXECUTION_STRING\nThe command(s) passed using the bash -c option\nBASH_LINENO\nA variable array containing the source code line number of the \ncurrently executing shell function\nBASH_REMATCH\nA read-only variable array containing patterns and their sub-\npatterns for positive matches using the regular expression \ncomparison operator, \n=~\nBASH_SOURCE\nA variable array containing the source code fi lename of the \ncurrently executing shell function\nBASH_SUBSHELL\nThe current nesting level of a subshell environment (The initial \nvalue is 0.)\nBASH_VERSINFO\nA variable array that contains the individual major and minor \nversion numbers of the current instance of the bash shell\nBASH_VERSION\nThe version number of the current instance of the bash shell\nBASH_XTRACEFD\nIf set to a valid fi le descriptor (0,1,2), trace output generated from \nthe \n'set -x' debugging option can be redirected. This is often \nused to separate trace output into a fi le.\nBASHOPTS\nA list of bash shell options that are currently enabled\nBASHPID\nProcess ID of the current bash process\nCOLUMNS\nContains the terminal width of the terminal used for the current \ninstance of the bash shell\nCOMP_CWORD\nAn index into the variable COMP_WORDS, which contains the \ncurrent cursor position\nCOMP_LINE\nThe current command line\nCOMP_POINT\nThe index of the current cursor position relative to the beginning \nof the current command\nCOMP_KEY\nThe fi nal key used to invoke the current completion of a shell \nfunction\nCOMP_TYPE\nAn integer value representing the type of completion attempted \nthat caused a completion shell function to be invoked\nCOMP_WORDBREAKS\nThe Readline library word separator characters for performing \nword completion\nCOMP_WORDS\nAn array variable that contains the individual words on the \ncurrent command line\nCOMPREPLY\nAn array variable that contains the possible completion codes \ngenerated by a shell function\nContinues\n\n146\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  146\nVariableDescription\nCOPROC\nAn array variable that holds an unnamed coprocess’ I/O fi le \ndescriptors\nDIRSTACK\nAn array variable that contains the current contents of the direc-\ntory stack\nEMACS\nIndicates the emacs shell buffer is executing and line editing is \ndisabled, when set to \n't'\nENV\nWhen set, executes the startup fi le defi ned before a bash shell \nscript runs (It is used only when the bash shell has been invoked \nin POSIX mode.)\nEUID\nThe numeric effective user ID of the current user\nFCEDIT\nThe default editor used by the fc command\nFIGNORE\nA colon-separated list of suffi xes to ignore when performing fi le \nname completion\nFUNCNAME\nThe name of the currently executing shell function\nFUNCNEST\nSets the maximum allowed function nesting level, when set to a \nnumber greater than zero (If it is exceeded, the current com-\nmand aborts.)\nGLOBIGNORE\nA colon-separated list of patterns defi ning the set of fi lenames \nto be ignored by fi le name expansion\nGROUPS\nA variable array containing the list of groups of which the current \nuser is a member\nhistchars\nUp to three characters, which control history expansion\nHISTCMD\nThe history number of the current command\nHISTCONTROL\nControls what commands are entered in the shell history list\nHISTFILE\nThe name of the fi le in which to save the shell history list (.bash_\nhistory\n by default)\nHISTFILESIZE\nThe maximum number of lines to save in the history fi le\nHISTTIMEFORMAT\nUsed as a formatting string to print each command’s timestamp \nin bash history, if set and not null\nHISTIGNORE\nA colon-separated list of patterns used to decide which com-\nmands are ignored for the history fi le\nHISTSIZE\nThe maximum number of commands stored in the history fi le\nHOSTFILE\nContains the name of the fi le that should be read when the shell \nneeds to complete a hostname\nHOSTNAME\nThe name of the current host\nHOSTTYPE\nA string describing the machine the bash shell is running on\nTABLE 6 -2   (continued)\n\n147\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  147\n6\nIGNOREEOF\nThe number of consecutive EOF characters the shell must \nreceive before exiting (If this value doesn’t exist, the default is 1.)\nINPUTRC\nThe name of the Readline initialization fi le (The default is \n.inputrc.)\nLANG\nThe locale category for the shell\nLC_ALL\nOverrides the LANG variable, defi ning a locale category\nLC_COLLATE\nSets the collation order used when sorting string values\nLC_CTYPE\nDetermines the interpretation of characters used in fi lename \nexpansion and pattern matching\nLC_MESSAGES\nDetermines the locale setting used when interpreting double-\nquoted strings preceded by a dollar sign\nLC_NUMERIC\nDetermines the locale setting used when formatting numbers\nLINENO\nThe line number in a script currently executing\nLINES\nDefi nes the number of lines available on the terminal\nMACHTYPE\nA string defi ning the system type in cpu-company-system format\nMAPFILE\nAn array variable that holds read-in text from the mapfile com-\nmand when no array variable name is given\nMAILCHECK\nHow often (in seconds) the shell should check for new mail (The \ndefault is 60.)\nOLDPWD\nThe previous working directory used in the shell\nOPTERR\nIf set to 1, the bash shell displays errors generated by the \ngetopts command.\nOSTYPE\nA string defi ning the operating system the shell is running on\nPIPESTATUS\nA variable array containing a list of exit status values from the \nprocesses in the foreground process\nPOSIXLY_CORRECT\nIf set, bash starts in POSIX mode.\nPPID\nThe process ID (PID) of the bash shell’s parent process\nPROMPT_COMMAND\nIf set, the command to execute before displaying the primary \nprompt\nPROMPT_DIRTRIM\nAn integer used to indicate the number of trailing directory \nnames to display when using the \\w and \\W prompt string \nescapes (The directory names removed are replaced with one \nset of ellipses.) \nPS3\nThe prompt to use for the select command\nPS4\nThe prompt displayed before the command line is echoed if the \nbash \n-x parameter is used\nPWD\nThe current working directory\nContinues\n\n148\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  148\nVariableDescription\nRANDOM\nReturns a random number between 0 and 32767 (Assigning a \nvalue to this variable seeds the pseudo-random number \ngenerator.)\nREADLINE_LINE\nReadline buffer contents when using bind -x command\nREADLINE_POINT\nReadline buffer content insertion point’s current position when \nusing \nbind -x command\nREPLY\nThe default variable for the read command\nSECONDS\nThe number of seconds since the shell was started (Assigning a \nvalue resets the timer to the value.)\nSHELL\nThe full pathname to the bash shell\nSHELLOPTS\nA colon-separated list of enabled bash shell options\nSHLVL\nIndicates the shell level, incremented by one each time a new \nbash shell is started\nTIMEFORMAT\nA format specifying how the shell displays time values\nTMOUT\nThe value of how long (in seconds) the select and read com-\nmands should wait for input (The default of zero indicates to wait \nindefi nitely.)\nTMPDIR\nDirectory name where the bash shell creates temporary fi les for \nits use\nUID\nThe numeric real user ID of the current user\nYou may notice that not all default environment variables are shown when the set com-\nmand is used. When not in use, the default environment variables are not all required to \ncontain a value.\nSetting the PATH Environment Variable\nWhen you enter an external command (see Chapter 5) in the shell command line interface \n(CLI), the shell must search the system to fi nd the program. The \nPATH environment vari-\nable defi nes the directories it searches looking for commands and programs. On this Ubuntu \nLinux system, the \nPATH environment variable looks like this:\n$ echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:\n/sbin:/bin:/usr/games:/usr/local/games\n$\nThis shows that there are eight directories where the shell looks for commands and \nprograms. The directories in the \nPATH are separated by colons.\nTABLE 6 -2   (continued)\n\n149\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  149\n6\nIf a command’s or program’s location is not included in the PATH variable, the shell cannot \nfi nd it without an absolute directory reference. If the shell cannot fi nd the command or \nprogram, it produces an error message:\n$ myprog\n-bash: myprog: command not found\n$\nThe problem is that often applications place their executable programs in directories that \naren’t in the \nPATH environment variable. The trick is ensuring that your PATH environment \nvariable includes all the directories where your applications reside.\nYou can add new search directories to the existing \nPATH environment variable without \nhaving to rebuild it from scratch. The individual directories listed in the \nPATH are sepa-\nrated by colons. All you need to do is reference the original \nPATH value and add any new \ndirectories to the string. This looks something like this:\n$ echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:\n/sbin:/bin:/usr/games:/usr/local/games\n$\n$ PATH=$PATH:/home/christine/Scripts\n$\n$ echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/\n     games:/usr/local/games:/home/christine/Scripts\n$\n$ myprog\nThe factorial of 5 is 120.\n$\nBy adding the directory to the PATH environment variable, you can now execute your \nprogram from anywhere in the virtual directory structure:\n$ cd /etc\n$\n$ myprog\nThe factorial of 5 is 120\n$\nIf you want your program’s location to be available to subshells, be sure to export your modifi ed PATH environment \nvariable. \nA common trick for programmers is to include the single dot symbol in their PATH environment \nvariable. The single dot symbol represents the current directory (see Chapter 3):\n$ PATH=$PATH:.\n$\n\n150\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  150\n$ cd /home/christine/Old_Scripts\n$\n$ myprog2\nThe factorial of 6 is 720\n$\nChanges to the PATH variable last only until you exit the system or the system reboots. \nThe changes are not persistent. In the next section, you see how you can make changes to \nenvironment variables permanent.\nLocating System Environment Variables\nThe Linux system uses environment variables for many purposes. You know now how to \nmodify system environment variables and create your own variables. The trick is in how \nthese environment variables are made persistent.\nWhen you start a bash shell by logging in to the Linux system, by default bash checks \nseveral fi les for commands. These fi les are called startup files or environment files. The \nstartup fi les that bash processes depend on the method you use to start the bash shell. \nYou can start a bash shell in three ways:\n ■\nAs a default login shell at login time\n ■\nAs an interactive shell that is started by spawning a subshell\n ■\nAs a non-interactive shell to run a script\nThe following sections describe the startup fi les the bash shell executes in each of these \nstartup methods.\nUnderstanding the login shell process\nWhen you log in to the Linux system, the bash shell starts as a login shell. The login shell \ntypically looks for fi ve different startup fi les to process commands from:\n ■\n/etc/profile\n ■\n$HOME/.bash_profile\n ■\n$HOME/.bashrc\n ■\n$HOME/.bash_login\n ■\n$HOME/.profile\nThe /etc/profile fi le is the main default startup fi le for the bash shell on the system. \nAll users on the system execute this startup fi le when they log in.\n\n151\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  151\n6\nBe aware that some Linux distributions use Pluggable Authentication Modules (PAM). In this case, before the bash \nshell is started, PAM fi les are processed, including ones that may contain environment variables. PAM fi le examples \ninclude the \n/etc/environment fi le and the $HOME/.pam_environment fi le. Find more information about \nPAM at \nhttp://linux-pam.org. \nThe other four startup fi les are specifi c for each user and can be customized for an indi-\nvidual user’s requirements. Let’s look closer at these fi les. \nViewing the /etc/profile file\nThe /etc/profile fi le is the main default startup fi le for the bash shell. Whenever you \nlog in to the Linux system, bash executes the commands in the \n/etc/profile startup fi le \nfi rst. Different Linux distributions place different commands in this fi le. On this Ubuntu \nLinux system, the fi le looks like this:\n$ cat /etc/profile\n# /etc/profile: system-wide .profile file for the Bourne shell (sh(1))\n# and Bourne compatible shells (bash(1), ksh(1), ash(1), ...).\nif [ \"$PS1\" ]; then\n  if [ \"$BASH\" ] && [ \"$BASH\" != \"/bin/sh\" ]; then\n    # The file bash.bashrc already sets the default PS1.\n    # PS1='\\h:\\w\\$ '\n    if [ -f /etc/bash.bashrc ]; then\n      . /etc/bash.bashrc\n    fi\n  else\n    if [ \"`id -u`\" -eq 0 ]; then\n      PS1='# '\n    else\n      PS1='$ '\n    fi\n  fi\nfi \n# The default umask is now handled by pam_umask.\n# See pam_umask(8) and /etc/login.defs.\nif [ -d /etc/profile.d ]; then\n  for i in /etc/profile.d/*.sh; do\n    if [ -r $i ]; then\n      . $i\n    fi\n  done\n  unset i\nfi\n$\n\n152\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  152\nMost of the commands and syntax you see in this fi le are covered in more detail in Chapter \n12 and beyond. Each distribution’s \n/etc/profile fi le has different settings and com-\nmands. For example, notice that a fi le is mentioned in this Ubuntu distribution’s \n/etc/\nprofile\n fi le above, called /etc/bash.bashrc. It contains system environment variables.\nHowever, in this CentOS distribution’s \n/etc/profile fi le listed below, no /etc/bash\n.bashrc\n fi le is called. Also note that it sets and exports some system environment \nvariables within itself:\n$ cat /etc/profile\n# /etc/profile\n# System wide environment and startup programs, for login setup\n# Functions and aliases go in /etc/bashrc\n# It's NOT a good idea to change this file unless you know what you\n# are doing. It's much better to create a custom.sh shell script in\n# /etc/profile.d/ to make custom changes to your environment, to\n# prevent the need for merging in future updates.\npathmunge () {\n    case \":${PATH}:\" in\n        *:\"$1\":*)\n            ;;\n        *)\n            if [ \"$2\" = \"after\" ] ; then\n                PATH=$PATH:$1\n            else\n                PATH=$1:$PATH\n            fi\n    esac\n}\nif [ -x /usr/bin/id ]; then\n    if [ -z \"$EUID\" ]; then\n        # ksh workaround\n        EUID=`id -u`\n        UID=`id -ru`\n    fi\n    USER=\"`id -un`\"\n    LOGNAME=$USER\n    MAIL=\"/var/spool/mail/$USER\"\nfi\n# Path manipulation\nif [ \"$EUID\" = \"0\" ]; then\n    pathmunge /sbin\n\n153\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  153\n6\n    pathmunge /usr/sbin\n    pathmunge /usr/local/sbin\nelse\n    pathmunge /usr/local/sbin after\n    pathmunge /usr/sbin after\n    pathmunge /sbin after\nfi\nHOSTNAME=`/bin/hostname 2>/dev/null`\nHISTSIZE=1000\nif [ \"$HISTCONTROL\" = \"ignorespace\" ] ; then\n    export HISTCONTROL=ignoreboth\nelse\n    export HISTCONTROL=ignoredups\nfi\nexport PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL\n# By default, we want umask to get set. This sets it for login shell\n# Current threshold for system reserved uid/gids is 200\n# You could check uidgid reservation validity in\n# /usr/share/doc/setup-*/uidgid file\nif [ $UID -gt 199 ] && [ \"`id -gn`\" = \"`id -un`\" ]; then\n    umask 002\nelse\n    umask 022\nfi\nfor i in /etc/profile.d/*.sh ; do\n    if [ -r \"$i\" ]; then\n        if [ \"${-#*i}\" != \"$-\" ]; then\n            . \"$i\"\n        else\n            . \"$i\" >/dev/null 2>&1\n        fi\n    fi\ndone\nunset i\nunset -f pathmunge\n$\nBoth distributions’ /etc/profile fi les use a certain feature. It is a for statement that \niterates through any fi les located in the \n/etc/profile.d directory. (for statements are \ndiscussed in detail in Chapter 13.) This provides a place for the Linux system to place \napplication-specifi c startup fi les that is executed by the shell when you log in. On this \nUbuntu Linux system, the following fi les are in the \nprofile.d directory:\n\n154\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  154\n$ ls -l /etc/profile.d\ntotal 12\n-rw-r--r-- 1 root root   40 Apr 15 06:26 appmenu-qt5.sh\n-rw-r--r-- 1 root root  663 Apr  7 10:10 bash_completion.sh\n-rw-r--r-- 1 root root 1947 Nov 22  2013 vte.sh\n$\nYou can see that this CentOs system has quite a few more fi les in /etc/profile.d:\n$ ls -l /etc/profile.d\ntotal 80\n-rw-r--r--. 1 root root 1127 Mar  5 07:17 colorls.csh\n-rw-r--r--. 1 root root 1143 Mar  5 07:17 colorls.sh\n-rw-r--r--. 1 root root   92 Nov 22  2013 cvs.csh\n-rw-r--r--. 1 root root   78 Nov 22  2013 cvs.sh\n-rw-r--r--. 1 root root  192 Feb 24 09:24 glib2.csh\n-rw-r--r--. 1 root root  192 Feb 24 09:24 glib2.sh\n-rw-r--r--. 1 root root   58 Nov 22  2013 gnome-ssh-askpass.csh\n-rw-r--r--. 1 root root   70 Nov 22  2013 gnome-ssh-askpass.sh\n-rwxr-xr-x. 1 root root  373 Sep 23  2009 kde.csh\n-rwxr-xr-x. 1 root root  288 Sep 23  2009 kde.sh\n-rw-r--r--. 1 root root 1741 Feb 20 05:44 lang.csh\n-rw-r--r--. 1 root root 2706 Feb 20 05:44 lang.sh\n-rw-r--r--. 1 root root  122 Feb  7  2007 less.csh\n-rw-r--r--. 1 root root  108 Feb  7  2007 less.sh\n-rw-r--r--. 1 root root  976 Sep 23  2011 qt.csh\n-rw-r--r--. 1 root root  912 Sep 23  2011 qt.sh\n-rw-r--r--. 1 root root 2142 Mar 13 15:37 udisks-bash-completion.sh\n-rw-r--r--. 1 root root   97 Apr  5  2012 vim.csh\n-rw-r--r--. 1 root root  269 Apr  5  2012 vim.sh\n-rw-r--r--. 1 root root  169 May 20  2009 which2.sh\n$\nNotice that several fi les are related to specifi c applications on the system. Most applications \ncreate two startup fi les — one for the bash shell (using the \n.sh extension) and one for the \nc shell (using the \n.csh extension).\nThe \nlang.csh and lang.sh fi les attempt to determine the default language character set \nused on the system and set the \nLANG environment variable appropriately.\nViewing the $HOME startup files\nThe remaining startup fi les are all used for the same function — to provide a user-specifi c \nstartup fi le for defi ning user-specifi c environment variables. Most Linux distributions use \nonly one or two of these four startup fi les:\n ■\n$HOME/.bash_profile\n ■\n$HOME/.bashrc\n\n155\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  155\n6\n ■\n$HOME/.bash_login\n ■\n$HOME/.profile\nNotice that all four fi les start with a dot, making them hidden fi les (they don’t appear in a \nnormal \nls command listing). Because they are in the user’s HOME directory, each user can \nedit the fi les and add his or her own environment variables that are active for every bash \nshell session they start. \nEnvironment fi les are one area where Linux distributions vary greatly. Not every $HOME fi le listed in this section \nexists for every user. For example, some users may have only the \n$HOME/.bash_profile fi le. This is normal. \nThe fi rst fi le found in the following ordered list is run, and the rest are ignored:\n$HOME/.bash_profile\n$HOME/.bash_login\n$HOME/.profile\nNotice that $HOME/.bashrc is not in this list. This is because it is typically run from one \nof the other fi les.\nRemember that $HOME represents a user’s home directory. Also, the tilde (~) is used to represent a user’s home \ndirectory.\nThis CentOS Linux system contains the following .bash_profile fi le:\n$ cat $HOME/.bash_profile\n# .bash_profile\n# Get the aliases and functions\nif [ -f ~/.bashrc ]; then\n        . ~/.bashrc\nfi\n# User specific environment and startup programs\nPATH=$PATH:$HOME/bin\nexport PATH\n$\nThe .bash_profile startup fi le fi rst checks to see if the startup fi le, .bashrc, is present \nin the \nHOME directory. If it’s there, the startup fi le executes the commands in it.\n\n156\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  156\nUnderstanding the interactive shell process\nIf you start a bash shell without logging into a system (if you just type bash at a CLI \nprompt, for example), you start what’s called an interactive shell. The interactive shell doesn’t \nact like the login shell, but it still provides a CLI prompt for you to enter commands.\nIf bash is started as an interactive shell, it doesn’t process the \n/etc/profile fi le. Instead, \nit only checks for the \n.bashrc fi le in the user’s HOME directory.\nOn this Linux CentOS distribution, this fi le looks like this:\n$ cat .bashrc\n# .bashrc\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n# User specific aliases and functions\n$\nThe .bashrc fi le does two things. First, it checks for a common bashrc fi le in the /etc \ndirectory. Second, it provides a place for the user to enter personal command aliases \n(discussed in Chapter 5) and private script functions (described in Chapter 17).\nUnderstanding the non-interactive shell process\nThe last type of shell is a non-interactive subshell. This is the shell where the \nsystem can start to execute a shell script. This is different in that there isn’t a CLI prompt \nto worry about. However, you may want to run specifi c startup commands each time you \nstart a script on your system.\nScripts can be executed in different ways. Only some execution methods start a subshell. You learn about the differ-\nent shell execution methods in Chapter 11.\nTo accommodate that situation, the bash shell provides the BASH_ENV environment vari-\nable. When the shell starts a non-interactive subshell process, it checks this environment \nvariable for the startup fi le name to execute. If one is present, the shell executes the fi le’s \ncommands, which typically include variables set for the shell scripts.\nOn this CentOS Linux distribution, this environment value is not set by default. When a \nvariable is not set, the \nprintenv command simply returns the CLI prompt:\n$ printenv BASH_ENV\n$\n\n157\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  157\n6\nOn this Ubuntu distribution, the BASH_ENV variable isn’t set either. Remember that, when \na variable is not set, the \necho command displays a blank line and returns the CLI prompt:\n$ echo $BASH_ENV\n$\nSo if the BASH_ENV variable isn’t set, how do the shell scripts get their environment vari-\nables? Remember that some shell script execution methods start a subshell, also called a \nchild shell (see Chapter 5). A child shell inherits its parent shell’s exported variables. \nFor example, if the parent shell was a login shell and had variables set and exported in the \n/etc/profile fi le, /etc/profile.d/*.sh fi les, and the $HOME/.bashrc fi le, the \nchild shell for the script inherits these variables. \nHowever, remember that any variables set but not exported by the parent shell are local \nvariables. Local variables are not inherited by a subshell. \nFor scripts that do not start a subshell, the variables are already available in the current \nshell. Thus, even if \nBASH_ENV is not set, both the current shell’s local and global variables \nare present to be used.\nMaking environment variables persistent\nNow that you know you way around the various shell process types and their various \nenvironment fi les, locating the permanent environment variables is much easier. You can \nalso set your own permanent global or local variables using these fi les.\nFor global environment variables (those variables needed by all the users on a Linux \nsystem), it may be tempting to put new or modifi ed variable settings in the \n/etc/\nprofile\n, but this is a bad idea. The fi le could be changed when your distribution is \nupgraded, and you would lose all the customized variable settings.\nIt is a better idea to create a fi le ending with \n.sh in the /etc/profile.d directory. In \nthat fi le, place all your new or modifi ed global environment variable settings.\nOn most distributions, the best place to store an individual user’s persistent bash shell \nvariables is in the \n$HOME/.bashrc fi le. This is true for all shell process types. However, if \nthe \nBASH_ENV variable is set, keep in mind that unless it points to $HOME/.bashrc, you \nmay need to store a user’s variables for non-interactive shell types elsewhere.\nKeep in mind that user environment variables for graphical interface elements, such as the GUI client, may need to \nbe set in different confi guration fi les than where bash shell environment variables are set.\nRecall back in Chapter 5 that command alias settings are also not persistent. You can also store \nyour personal \nalias settings in the $HOME/.bashrc startup fi le to make them permanent.\n\n158\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  158\nLearning about Variable Arrays\nA really cool feature of environment variables is that they can be used as arrays. An array \nis a variable that can hold multiple values. Values can be referenced either individually or \nas a whole for the entire array.\nTo set multiple values for an environment variable, just list them in parentheses, with \nvalues separated by spaces:\n$ mytest=(one two three four five)\n$\nNot much excitement there. If you try to display the array as a normal environment \nvariable, you’ll be disappointed:\n$ echo $mytest\none\n$\nOnly the fi rst value in the array appears. To reference an individual array element, you \nmust use a numerical index value, which represents its place in the array. The numeric \nvalue is enclosed in square brackets:\n$ echo ${mytest[2]}\nthree\n$\nEnvironment variable arrays start with an index value of zero. This can be confusing.\nTo display an entire array variable, you use the asterisk wildcard character as the index \nvalue:\n$ echo ${mytest[*]}\none two three four five\n$\nYou can also change the value of an individual index position:\n$ mytest[2]=seven\n$\n$ echo ${mytest[*]}\none two seven four five\n$\nYou can even use the unset command to remove an individual value within the array, but \nbe careful, because this gets tricky. Watch this example:\n\n159\nChapter 6: Using Linux Environment Variables\nc06.indd  12/03/2014  Page  159\n6\n$ unset mytest[2]\n$\n$ echo ${mytest[*]}\none two four five\n$\n$ echo ${mytest[2]}\n$ echo ${mytest[3]}\nfour\n$\nThis example uses the unset command to remove the value at index value 2. When you \ndisplay the array, it appears that the other index values just dropped down one. However, if \nyou specifi cally display the data at index value 2, you see that that location is empty.\nFinally, you can remove the entire array just by using the array name in the \nunset \ncommand:\n$ unset mytest\n$\n$ echo ${mytest[*]}\n$\nSometimes variable arrays just complicate matters, so they’re often not used in shell script \nprogramming. They’re not very portable to other shell environments, which is a downside if \nyou do lots of shell programming for different shells. Some bash system environment vari-\nables use arrays (such as \nBASH_VERSINFO), but overall you probably won’t run into them \nvery often.\nSummary\nThis chapter examined the world of Linux environment variables. Global environment vari-\nables can be accessed from any child shell spawned by the parent shell in which they’re \ndefi ned. Local environment variables can be accessed only from the process in which \nthey’re defi ned.\nThe Linux system uses both global and local environment variables to store information \nabout the system environment. You can access this information from the shell command \nline interface, as well as within shell scripts. The bash shell uses the system environment \nvariables defi ned in the original Unix Bourne shell, as well as lots of new environment \nvariables. The \nPATH environment variable defi nes the search pattern the bash shell \ntakes to fi nd an executable command. You can modify the \nPATH environment variable \nto add your own directories, or even the current directory symbol, to make running your \nprograms easier.\n\n160\nPart I: The Linux Command Line\nc06.indd  12/03/2014  Page  160\nYou can also create your own global and local environment variables for your own use. \nAfter you create an environment variable, it’s accessible for the entire duration of your \nshell session.\nThe bash shell executes several startup fi les when it starts up. These startup fi les can con-\ntain environment variable defi nitions to set standard environment variables for each bash \nsession. When you log in to the Linux system, the bash shell accesses the \n/etc/profile \nstartup fi le and three local startup fi les for each user, \n$HOME/.bash_profile, $HOME/\n.bash_login\n, and $HOME/.profile . Users can customize these fi les to include environ-\nment variables and startup scripts for their own use.\nFinally, this chapter discussed the use of environment variable arrays. These environment \nvariables can contain multiple values in a single variable. You can access the values either \nindividually by referencing an index value or as a whole by referencing the entire environ-\nment variable array name.\nThe next chapter dives into the world of Linux fi le permissions. This is possibly the most \ndiffi cult topic for novice Linux users. However, to write good shell scripts, you need to \nunderstand how fi le permissions work and be able to use them on your Linux system. \n\n161\nc07.indd  12/16/2014  Page  161\nCHAPTER \n7\nUnderstanding Linux File \nPermissions\nIN THIS CHAPTER\nUnderstanding Linux security\nDecoding the permissions\nWorking with Linux groups\nN\no system is complete without some form of security. There must be a mechanism available to \nprotect fi les from unauthorized viewing or modifi cation. The Linux system follows the Unix \nmethod of fi le permissions, allowing individual users and groups access to fi les based on a \nset of security settings for each fi le and directory. This chapter discusses how to use the Linux fi le \nsecurity system to protect data when necessary and share data when desired.\nLinux Security\nThe core of the Linux security system is the user account. Each individual who accesses a Linux \nsystem should have a unique user account assigned. The users’ permissions to objects on the system \ndepend on the user account they log in with.\nUser permissions are tracked using a user ID (often called a UID), which is assigned to an account \nwhen it’s created. The UID is a numerical value, unique for each user. However, you don’t log in to \na Linux system using your UID. Instead, you use a login name. The login name is an alphanumeric \ntext string of eight characters or fewer that the user uses to log in to the system (along with an \nassociated password).\nThe Linux system uses special fi les and utilities to track and manage user accounts on the system. \nBefore we can discuss fi le permissions, we need to discuss how Linux handles user accounts. This \nsection describes the fi les and utilities required for user accounts so that you can understand how \nto use them when working with fi le permissions.\n\n162\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  162\nThe /etc/passwd fi le\nThe Linux system uses a special fi le to match the login name to a corresponding UID \nvalue. This fi le is the \n/etc/passwd fi le. The /etc/passwd fi le contains several pieces of \ninformation about the user. Here’s what a typical \n/etc/passwd fi le looks like on a Linux \nsystem:\n $ cat /etc/passwd\n root:x:0:0:root:/root:/bin/bash\n bin:x:1:1:bin:/bin:/sbin/nologin\n daemon:x:2:2:daemon:/sbin:/sbin/nologin\n adm:x:3:4:adm:/var/adm:/sbin/nologin\n lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin\n sync:x:5:0:sync:/sbin:/bin/sync\n shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown\n halt:x:7:0:halt:/sbin:/sbin/halt\n mail:x:8:12:mail:/var/spool/mail:/sbin/nologin\n news:x:9:13:news:/etc/news:\n uucp:x:10:14:uucp:/var/spool/uucp:/sbin/nologin\n operator:x:11:0:operator:/root:/sbin/nologin\n games:x:12:100:games:/usr/games:/sbin/nologin\n gopher:x:13:30:gopher:/var/gopher:/sbin/nologin\n ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin\n nobody:x:99:99:Nobody:/:/sbin/nologin\n rpm:x:37:37::/var/lib/rpm:/sbin/nologin\n vcsa:x:69:69:virtual console memory owner:/dev:/sbin/nologin\n mailnull:x:47:47::/var/spool/mqueue:/sbin/nologin\n smmsp:x:51:51::/var/spool/mqueue:/sbin/nologin\n apache:x:48:48:Apache:/var/www:/sbin/nologin\n rpc:x:32:32:Rpcbind Daemon:/var/lib/rpcbind:/sbin/nologin\n ntp:x:38:38::/etc/ntp:/sbin/nologin\n nscd:x:28:28:NSCD Daemon:/:/sbin/nologin\n tcpdump:x:72:72::/:/sbin/nologin\n dbus:x:81:81:System message bus:/:/sbin/nologin\n avahi:x:70:70:Avahi daemon:/:/sbin/nologin\n hsqldb:x:96:96::/var/lib/hsqldb:/sbin/nologin\n sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin\n rpcuser:x:29:29:RPC Service User:/var/lib/nfs:/sbin/nologin\n nfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin\n haldaemon:x:68:68:HAL daemon:/:/sbin/nologin\n xfs:x:43:43:X Font Server:/etc/X11/fs:/sbin/nologin\n gdm:x:42:42::/var/gdm:/sbin/nologin\n rich:x:500:500:Rich Blum:/home/rich:/bin/bash\n mama:x:501:501:Mama:/home/mama:/bin/bash\n katie:x:502:502:katie:/home/katie:/bin/bash\n jessica:x:503:503:Jessica:/home/jessica:/bin/bash\n mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/bash\n $\n\n163\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  163\n7\n7\nThe root user account is the administrator for the Linux system and is always assigned \nUID 0. As you can see, the Linux system creates lots of user accounts for various functions \nthat aren’t actual users. These are called system accounts. A system account is a special \naccount that services running on the system use to gain access to resources on the system. \nAll services that run in background mode need to be logged in to the Linux system under a \nsystem user account.\nBefore security became a big issue, these services often just logged in using the \nroot user \naccount. Unfortunately, if an unauthorized person broke into one of these services, he \ninstantly gained access to the system as the \nroot user. To prevent this, now just about \nevery service that runs in background on a Linux server has its own user account to log in \nwith. This way, if a troublemaker compromises a service, he still can’t necessarily get access \nto the whole system.\nLinux reserves UIDs below 500 for system accounts. Some services even require specifi c \nUIDs to work properly. When you create accounts for normal users, most Linux systems \nassign the fi rst available UID starting at 500 (although this is not necessarily true for all \nLinux distributions).\nYou probably noticed that the \n/etc/passwd fi le contains much more than just the login \nname and UID for the user. The fi elds of the \n/etc/passwd fi le contain the following \ninformation:\n ■\nThe login username\n ■\nThe password for the user\n ■\nThe numerical UID of the user account\n ■\nThe numerical group ID (GID) of the user account\n ■\nA text description of the user account (called the comment fi eld)\n ■\nThe location of the HOME directory for the user\n ■\nThe default shell for the user\nThe password fi eld in the \n/etc/passwd fi le is set to an x. This doesn’t mean that all the \nuser accounts have the same password. In the old days of Linux, the \n/etc/passwd fi le \ncontained an encrypted version of the user’s password. However, because lots of programs \nneed to access the \n/etc/passwd fi le for user information, this became a security prob-\nlem. With the advent of software that could easily decrypt encrypted passwords, the bad \nguys had a fi eld day trying to break user passwords stored in the \n/etc/passwd fi le. Linux \ndevelopers needed to rethink that policy.\nNow, most Linux systems hold user passwords in a separate fi le (called the shadow fi le, \nlocated at \n/etc/shadow). Only special programs (such as the login program) are allowed \naccess to this fi le.\n\n164\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  164\nThe /etc/passwd fi le is a standard text fi le. You can use any text editor to manually \nperform user management functions (such as adding, modifying, or removing user \naccounts) directly in the \n/etc/passwd fi le. However, this is an extremely dangerous \npractice. If the \n/etc/passwd fi le becomes corrupt, the system can’t read it, and it \nprevents anyone (even the root user) from logging in. Instead, it’s safer to use the standard \nLinux user management utilities to perform all user management functions.\nThe /etc/shadow fi le\nThe /etc/shadow fi le provides more control over how the Linux system manages pass-\nwords. Only the root user has access to the \n/etc/shadow fi le, making it more secure than \nthe \n/etc/passwd fi le.\nThe \n/etc/shadow fi le contains one record for each user account on the system. A record \nlooks like this:\n rich:$1$.FfcK0ns$f1UgiyHQ25wrB/hykCn020:11627:0:99999:7:::\nThere are nine fi elds in each /etc/shadow fi le record:\n ■\nThe login name corresponding to the login name in the /etc/passwd fi le\n ■\nThe encrypted password\n ■\nThe number of days since January 1, 1970, that the password was last changed\n ■\nThe minimum number of days before the password can be changed\n ■\nThe number of days before the password must be changed\n ■\nThe number of days before password expiration that the user is warned to change \nthe password\n ■\nThe number of days after a password expires before the account will be disabled\n ■\nThe date (stored as the number of days since January 1, 1970) since the user \naccount was disabled\n ■\nA fi eld reserved for future use\nUsing the shadow password system, the Linux system has much fi ner control over user \npasswords. It can control how often a user must change his or her password and when to \ndisable the account if the password hasn’t been changed.\nAdding a new user\nThe primary tool used to add new users to your Linux system is useradd. This command \nprovides an easy way to create a new user account and set up the user’s \nHOME directory \nstructure all at once. The \nuseradd command uses a combination of system default values \nand command line parameters to defi ne a user account. The system defaults are set in the \n\n165\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  165\n7\n/etc/default/useradd fi le. To see the system default values used on your Linux distri-\nbution, enter the \nuseradd command with the -D parameter:\n # /usr/sbin/useradd -D\n GROUP=100\n HOME=/home\n INACTIVE=-1\n EXPIRE=\n SHELL=/bin/bash\n SKEL=/etc/skel\n CREATE_MAIL_SPOOL=yes\n #\nSome Linux distributions place the Linux user and group utilities in the /usr/sbin directory, which may not be in \nyour \nPATH environment variable. If that’s the case in your Linux distribution, either add the directory to your PATH or \nuse the absolute fi le path to run it.\nThe -D parameter shows what defaults the useradd command uses if you don’t specify \nthem in the command line when creating a new user account. This example shows the \nfollowing default values:\n ■\nThe new user is added to a common group with group ID 100.\n ■\nThe new user has a HOME account created in the directory /home/loginname.\n ■\nThe account can’t be disabled when the password expires.\n ■\nThe new account can’t be set to expire at a set date.\n ■\nThe new account uses the bash shell as the default shell.\n ■\nThe system copies the contents of the /etc/skel directory to the user’s HOME \ndirectory.\n ■\nThe system creates a fi le in the mail directory for the user account to receive mail.\nThe penultimate value is interesting. The \nuseradd command allows an administrator to \ncreate a default \nHOME directory confi guration and then uses that as a template to create the \nnew user’s \nHOME directory. This allows you to place default fi les for the system in every new \nuser’s \nHOME directory automatically. In the Ubuntu Linux system, the /etc/skel directory \nhas the following fi les:\n$ ls -al /etc/skel\ntotal 32\ndrwxr-xr-x   2 root root  4096 2010-04-29 08:26 .\ndrwxr-xr-x 135 root root 12288 2010-09-23 18:49 ..\n-rw-r--r--   1 root root   220 2010-04-18 21:51 .bash_logout\n\n166\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  166\n-rw-r--r--   1 root root  3103 2010-04-18 21:51 .bashrc\n-rw-r--r--   1 root root   179 2010-03-26 08:31 examples.desktop\n-rw-r--r--   1 root root   675 2010-04-18 21:51 .profile\n$\nYou should recognize these fi les from Chapter 6. These are the standard startup fi les for \nthe bash shell environment. The system automatically copies these default fi les into every \nuser’s \nHOME directory you create.\nYou can test this by creating a new user account using the default system parameters and \nthen looking at the \nHOME directory for the new user:\n# useradd -m test\n# ls -al /home/test\ntotal 24\ndrwxr-xr-x 2 test test 4096 2010-09-23 19:01 .\ndrwxr-xr-x 4 root root 4096 2010-09-23 19:01 ..\n-rw-r--r-- 1 test test  220 2010-04-18 21:51 .bash_logout\n-rw-r--r-- 1 test test 3103 2010-04-18 21:51 .bashrc\n-rw-r--r-- 1 test test  179 2010-03-26 08:31 examples.desktop\n-rw-r--r-- 1 test test  675 2010-04-18 21:51 .profile\n#\nBy default, the useradd command doesn’t create a HOME directory, but the –m command \nline option tells it to create the \nHOME directory. As you can see in the example, the \nuseradd command created the new HOME directory, using the fi les contained in the /etc/\nskel\n directory.\nTo run the user account administration commands in this chapter, you either need to be logged in as the special root \nuser account or use the \nsudo command to run the commands as the root user account.\nIf you want to override a default value or behavior when creating a new user, you can do \nthat with command line parameters. These are shown in Table 7-1.\nTABLE 7-1    The useradd Command Line Parameters\nParameterDescription\n-c commentAdds text to the new user’s comment fi eld\n-d home_dirSpecifi es a different name for the HOME directory other than the \nlogin name\n\n167\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  167\n7\n-e expire_dateSpecifi es a date, in YYYY-MM-DD format, when the account will \nexpire\n-f inactive_daysSpecifi es the number of days after a password expires when the \naccount will be disabled. A value of \n0 disables the account as soon \nas the password expires; a value of \n-1 disables this feature.\n-g initial_groupSpecifi es the group name or GID of the user’s login group\n-G group . . .Specifi es one or more supplementary groups the user belongs to\n-k\nCopies the /etc/skel directory contents into the user’s HOME direc-\ntory (must use \n-m as well)\n-m\nCreates the user’s HOME directory\n-M\nDoesn’t create a user’s HOME directory (used if the default setting is \nto create one)\n-n\nCreates a new group using the same name as the user’s login name\n-r\nCreates a system account\n-p passwdSpecifi es a default password for the user account\n-s shellSpecifi es the default login shell\n-u uid\nSpecifi es a unique UID for the account\nAs you can see, you can override all the system default values when creating a new user \naccount just by using command line parameters. However, if you fi nd yourself having to \noverride a value all the time, it’s easier to just change the system default value.\nYou can change the system default new user values by using the \n-D parameter, along with \na parameter representing the value you need to change. These parameters are shown in \nTable 7-2.\nTABLE 7-2    The useradd Change Default Values Parameters\nParameterDescription\n-b default_homeChanges the location where users’ HOME directories are created\n-e expiration_dateChanges the expiration date on new accounts\n-f inactiveChanges the number of days after a password has expired \nbefore the account is disabled\n-g groupChanges the default group name or GID used\n-s shellChanges the default login shell\n\n168\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  168\nChanging the default values is a snap:\n # useradd -D -s /bin/tsch\n # useradd -D\n GROUP=100\n HOME=/home\n INACTIVE=-1\n EXPIRE=\n SHELL=/bin/tsch\n SKEL=/etc/skel\n CREATE_MAIL_SPOOL=yes\n #\nNow, the useradd command uses the tsch shell as the default login shell for all new user \naccounts you create.\nRemoving a user\nIf you want to remove a user from the system, the userdel command is what you need. By \ndefault, the \nuserdel command removes only the user information from the /etc/passwd \nfi le. It doesn’t remove any fi les the account owns on the system.\nIf you use the \n-r parameter, userdel removes the user’s HOME directory, along with the \nuser’s mail directory. However, other fi les owned by the deleted user account may still be \non the system. This can be a problem in some environments.\nHere’s an example of using the \nuserdel command to remove an existing user account:\n # /usr/sbin/userdel -r test\n # ls -al /home/test\n ls: cannot access /home/test: No such file or directory\n #\nAfter using the -r parameter, the user’s old /home/test directory no longer exists.\nBe careful when using the -r parameter in an environment with lots of users. You never know if a user had important \nfi les stored in his or her \nHOME directory that are used by someone else or another program. Always check before \nremoving a user’s \nHOME directory!\nModifying a user\nLinux provides a few different utilities for modifying the information for existing user \naccounts. Table 7-3 shows these utilities.\n\n169\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  169\n7\nTABLE 7-3    User  Account  Modifi cation Utilities\nCommandDescription\nusermod\nEdits user account fi elds, as well as specifying primary and secondary group \nmembership\npasswd\nChanges the password for an existing user\nchpasswd\nReads a fi le of login name and password pairs, and updates the passwords\nchage\nChanges the password’s expiration date\nchfn\nChanges the user account’s comment information\nchsh\nChanges the user account’s default shell\nEach of these utilities provides a specifi c function for changing information about user \naccounts. The following sections describe each of these utilities.\nusermod\nThe usermod command is the most robust of the user account modifi cation utilities. It \nprovides options for changing most of the fi elds in the \n/etc/passwd fi le. To do that, you \njust need to use the command line parameter that corresponds to the value you want to \nchange. The parameters are mostly the same as the \nuseradd parameters (such as -c to \nchange the comment fi eld, \n-e to change the expiration date, and -g to change the default \nlogin group). However, a couple of additional parameters might come in handy:\n ■\n-l changes the login name of the user account.\n ■\n-L locks the account so the user can’t log in.\n ■\n-p changes the password for the account.\n ■\n-U unlocks the account so the user can log in.\nThe \n-L parameter is especially handy. Use this to lock an account so a user can’t log in \nwithout having to remove the account and the user’s data. To return the account to normal, \njust use the \n-U parameter.\npasswd and chpasswd\nA quick way to change just the password for a user is the passwd command:\n # passwd test\n Changing password for user test.\n New UNIX password:\n\n170\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  170\n Retype new UNIX password:\n passwd: all authentication tokens updated successfully.\n #\nIf you just use the passwd command by itself, it changes your own password. Any user in \nthe system can change his or her own password, but only the root user can change someone \nelse’s password.\nThe \n-e option is a handy way to force a user to change the password on the next log in. \nThis allows you to set the user’s password to a simple value and forces them to change it to \nsomething harder that they can remember.\nIf you ever need to do a mass password change for lots of users on the system, the \nchpasswd command can be a lifesaver. The chpasswd command reads a list of login name \nand password pairs (separated by a colon) from the standard input, automatically encrypts \nthe password, and sets it for the user account. You can also use the redirection command to \nredirect a fi le of \nuserid:password pairs into the command:\n# chpasswd < users.txt\n#\nchsh, chfn, and chage\nThe chsh, chfn, and chage utilities are specialized for specifi c account modifi cation func-\ntions. The \nchsh command allows you to quickly change the default login shell for a user. \nYou must use the full pathname for the shell, and not just the shell name:\n #  chsh -s /bin/csh test\n Changing shell for test.\n Shell changed.\n #\nThe chfn command provides a standard method for storing information in the comments \nfi eld in the \n/etc/passwd fi le. Instead of just inserting random text, such as names or \nnicknames, or even just leaving the comment fi eld blank, the \nchfn command uses specifi c \ninformation used in the Unix \nfinger command to store information in the comment fi eld. \nThe \nfinger command allows you to easily fi nd information about people on your Linux \nsystem:\n # finger rich\n Login: rich                             Name: Rich Blum\n Directory: /home/rich                   Shell: /bin/bash\n On since Thu Sep 20 18:03 (EDT) on pts/0 from 192.168.1.2\n No mail.\n No Plan.\n #\n\n171\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  171\n7\nBecause of security concerns, many Linux system administrators disable the finger command on their systems, \nand many Linux distributions don’t even install it by default.\nIf you use the chfn command with no parameters, it queries you for the appropriate values \nto enter in to the comment fi eld:\n # chfn test\n Changing finger information for test.\n Name []: Ima Test\n Office []: Director of Technology\n Office Phone []: (123)555-1234\n Home Phone []: (123)555-9876\n Finger information changed.\n # finger test\n Login: test                             Name: Ima Test\n Directory: /home/test                   Shell: /bin/csh\n Office: Director of Technology          Office Phone: (123)555-1234\n Home Phone: (123)555-9876\n Never logged in.\n No mail.\n No Plan.\n #\nIf you now look at the entry in the /etc/passwd fi le, it looks like this:\n # grep test /etc/passwd\n test:x:504:504:Ima Test,Director of Technology,(123)555-\n 1234,(123)555-9876:/home/test:/bin/csh\n #\nAll the fi nger information is neatly stored away in the /etc/passwd fi le entry.\nFinally, the \nchage command helps you manage the password aging process for user \naccounts. You need to set several parameters to individual values, shown in Table 7-4.\nTABLE 7- 4    The chage Command Parameters\nParameterDescription\n-d\nSets the number of days since the password was last changed\n-E\nSets the date the password expires\nContinues\n\n172\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  172\nParameterDescription\n-I\nSets the number of days of inactivity after the password expires to lock the \naccount\n-m\nSets the minimum number of days between password changes\n-W\nSets the number of days before the password expires that a warning message \nappears\nThe chage date values can be expressed using one of two methods:\n ■\nA date in YYYY-MM-DD format\n ■\nA numerical value representing the number of days since January 1, 1970\nOne neat feature of the \nchage command is that it allows you to set an expiration date for \nan account. Using this feature, you can create temporary user accounts that automatically \nexpire on a set date, without your having to remember to delete them! Expired accounts are \nsimilar to locked accounts. The account still exists, but the user can’t log in with it.\nUsing Linux Groups\nUser accounts are great for controlling security for individual users, but they aren’t so good \nat allowing groups of users to share resources. To accomplish this, the Linux system uses \nanother security concept, called groups.\nGroup permissions allow multiple users to share a common set of permissions for an object \non the system, such as a fi le, directory, or device (more on that later in the “Decoding File \nPermissions” section).\nLinux distributions differ somewhat on how they handle default group memberships. Some \nLinux distributions create just one group that contains all the user accounts as members. \nYou need to be careful if your Linux distribution does this, because your fi les may be read-\nable by all other users on the system. Other distributions create a separate group account \nfor each user to provide a little more security.\nEach group has a unique GID, which, like UIDs, is a unique numerical value on the system. \nAlong with the GID, each group has a unique group name. You can use some group utilities \nto create and manage your own groups on the Linux system. This section discusses how \ngroup information is stored and how to use the group utilities to create new groups and \nmodify existing groups.\nTABLE 7- 4   (continued)\n\n173\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  173\n7\nThe /etc/group fi le\nJust like user accounts, group information is stored in a fi le on the system. The /etc/\ngroup\n fi le contains information about each group used on the system. These are examples \nfrom a typical \n/etc/group fi le on a Linux system:\n root:x:0:root\n bin:x:1:root,bin,daemon\n daemon:x:2:root,bin,daemon\n sys:x:3:root,bin,adm\n adm:x:4:root,adm,daemon\n rich:x:500:\n mama:x:501:\n katie:x:502:\n jessica:x:503:\n mysql:x:27:\n test:x:504:\nLike UIDs, GIDs are assigned using a special format. Groups used for system accounts are \nassigned GIDs below 500, and user groups are assigned GIDs starting at 500. The \n/etc/\ngroup\n fi le uses four fi elds:\n ■\nThe group name\n ■\nThe group password\n ■\nThe GID\n ■\nThe list of user accounts that belong to the group\nThe group password allows a non-group member to temporarily become a member of the \ngroup by using the password. This feature is not used all that commonly, but it does exist.\nYou should never add users to groups by editing the \n/etc/group fi le. Instead, use \nthe \nusermod command (discussed earlier in the “Linux Security” section) to add a \nuser account to a group. Before you can add users to different groups, you must create \nthe groups.\nThe list of user accounts is somewhat misleading. You’ll notice that there are several groups in the list that don’t have \nany users listed. This isn’t because they don’t have any members. When a user account uses a group as the default \ngroup in the \n/etc/passwd fi le, the user account doesn’t appear in the /etc/group fi le as a member. This has \ncaused confusion for more than one system administrator over the years!\n\n174\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  174\nCreating new groups\nThe groupadd command allows you to create new groups on your system:\n # /usr/sbin/groupadd shared\n # tail /etc/group\n haldaemon:x:68:\n xfs:x:43:\n gdm:x:42:\n rich:x:500:\n mama:x:501:\n katie:x:502:\n jessica:x:503:\n mysql:x:27:\n test:x:504:\n shared:x:505:\n #\nWhen you create a new group, no users are assigned to it by default. The groupadd com-\nmand doesn’t provide an option for adding user accounts to the group. Instead, to add new \nusers, use the \nusermod command:\n # /usr/sbin/usermod -G shared rich\n # /usr/sbin/usermod -G shared test\n # tail /etc/group\n haldaemon:x:68:\n xfs:x:43:\n gdm:x:42:\n rich:x:500:\n mama:x:501:\n katie:x:502:\n jessica:x:503:\n mysql:x:27:\n test:x:504:\n shared:x:505:rich, test\n #\nThe shared group now has two members, test and rich. The -G parameter in usermod \nappends the new group to the list of groups for the user account.\nIf you change the user groups for an account that is currently logged into the system, the user must log out and then \nlog back in for the group changes to take effect.\n\n175\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  175\n7\nBe careful when assigning groups for user accounts. If you use the -g parameter, the group name you specify \nreplaces the default group for the user account. The \n-G parameter adds the group to the list of groups the user \nbelongs to, keeping the default group intact.\nModifying groups\nAs you can see from the /etc/group fi le, you don’t need to modify much information \nabout a group. The \ngroupmod command allows you to change the GID (using the -g param-\neter) or the group name (using the \n-n parameter) of an existing group:\n # /usr/sbin/groupmod -n sharing shared\n # tail /etc/group\n haldaemon:x:68:\n xfs:x:43:\n gdm:x:42:\n rich:x:500:\n mama:x:501:\n katie:x:502:\n jessica:x:503:\n mysql:x:27:\n test:x:504:\n sharing:x:505:test,rich\n #\nWhen changing the name of a group, the GID and group members remain the same, only the \ngroup name changes. Because all security permissions are based on the GID, you can change \nthe name of a group as often as you wish without adversely affecting fi le security.\nDecoding File Permissions\nNow that you know about users and groups, it’s time to decode the cryptic fi le permissions \nyou’ve seen when using the \nls command. This section describes how to decipher the per-\nmissions and where they come from.\n\n176\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  176\nUsing fi le permission symbols\nIf you remember from Chapter 3, the ls command allows you to see the fi le permissions for \nfi les, directories, and devices on the Linux system:\n $ ls -l\n total 68\n -rw-rw-r-- 1 rich rich   50 2010-09-13 07:49 file1.gz\n -rw-rw-r-- 1 rich rich   23 2010-09-13 07:50 file2\n -rw-rw-r-- 1 rich rich   48 2010-09-13 07:56 file3\n -rw-rw-r-- 1 rich rich   34 2010-09-13 08:59 file4\n -rwxrwxr-x 1 rich rich 4882 2010-09-18 13:58 myprog\n -rw-rw-r-- 1 rich rich  237 2010-09-18 13:58 myprog.c\n drwxrwxr-x 2 rich rich 4096 2010-09-03 15:12 test1\n drwxrwxr-x 2 rich rich 4096 2010-09-03 15:12 test2\n $\nThe fi rst fi eld in the output listing is a code that describes the permissions for the fi les and \ndirectories. The fi rst character in the fi eld defi nes the type of the object:\n ■\n- for fi les\n ■\nd for directories\n ■\nl for links\n ■\nc for character devices\n ■\nb for block devices\n ■\nn for network devices\nAfter that, you see three sets of three characters. Each set of three characters defi nes an \naccess permission triplet:\n ■\nr for read permission for the object\n ■\nw for write permission for the object\n ■\nx for execute permission for the object\nIf a permission is denied, a dash appears in the location. The three sets relate the three \nlevels of security for the object:\n ■\nThe owner of the object\n ■\nThe group that owns the object\n ■\nEveryone else on the system\nThis is broken down in Figure 7-1.\n\n177\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  177\n7\nFIGURE 7-1\nThe Linux file permissions\n-rwxrwxr-x  1 rich rich   4882   2010-09-18  13:58  myprog\npermissions for everyone else\npermissions for group members\npermissions for the file owner\nThe easiest way to discuss this is to take an example and decode the fi le permissions one \nby one:\n -rwxrwxr-x 1 rich rich 4882 2010-09-18 13:58 myprog\nThe fi le myprog has the following sets of permissions:\n ■\nrwx for the fi le owner (set to the login name rich)\n ■\nrwx for the fi le group owner (set to the group name rich)\n ■\nr-x for everyone else on the system\nThese permissions indicate that the user login name rich can read, write, and execute the \nfi le (considered full permissions). Likewise, members in the group rich can also read, write, \nand execute the fi le. However, anyone else not in the rich group can only read and execute \nthe fi le; the w is replaced with a dash, indicating that write permissions are not assigned to \nthis security level.\nDefault fi le permissions\nYou may be wondering about where these fi le permissions come from. The answer is umask. \nThe \numask command sets the default permissions for any fi le or directory you create:\n $ touch newfile\n $ ls -al newfile\n -rw-r--r--    1 rich     rich            0 Sep 20 19:16 newfile\n $\nThe touch command created the fi le using the default permissions assigned to my user \naccount. The \numask command shows and sets the default permissions:\n $ umask\n 0022\n $\n\n178\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  178\nUnfortunately, the umask command setting isn’t overtly clear, and trying to understand \nexactly how it works makes things even muddier. The fi rst digit represents a special secu-\nrity feature called the \nsticky bit. We’ll talk more about that later on in this chapter in \nthe “Sharing Files” section.\nThe next three digits represent the octal values of the \numask for a fi le or directory. To \nunderstand how \numask works, you fi rst need to understand octal mode security settings.\nOctal mode security settings take the three rwx permission values and convert them into \na 3-bit binary value, represented by a single octal value. In the binary representation, each \nposition is a binary bit. Thus, if the read permission is the only permission set, the value \nbecomes \nr--, relating to a binary value of 100, indicating the octal value of 4. Table 7-5 \nshows the possible combinations you’ll run into.\nTABLE 7-5    Linux File Permission Codes\nPermissionsBinaryOctalDescription\n---\n0000No permissions\n--x\n0011Execute-only permission\n-w-\n0102Write-only permission\n-wx\n0113Write and execute permissions\nr--\n1004Read-only permission\nr-x\n1015Read and execute permissions\nrw-\n1106Read and write permissions\nrwx\n1117Read, write, and execute permissions\nOctal mode takes the octal permissions and lists three of them in order for the three secu-\nrity levels (user, group, and everyone). Thus, the octal mode value 664 represents read and \nwrite permissions for the user and group, but read-only permission for everyone else.\nNow that you know about octal mode permissions, the \numask value becomes even more \nconfusing. The octal mode shown for the default umask on my Linux system is 0022, but \nthe fi le I created had an octal mode permission of 644. How did that happen?\nThe umask value is just that, a mask. It masks out the permissions you don’t want to give \nto the security level. Now we have to dive into some octal arithmetic to fi gure out the rest \nof the story.\nThe umask value is subtracted from the full permission set for an object. The full permis-\nsion for a fi le is mode 666 (read/write permission for all), but for a directory it’s 777 (read/\nwrite/execute permission for all).\n\n179\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  179\n7\nThus, in the example, the fi le starts out with permissions 666, and the umask of 022 is \napplied, leaving a fi le permission of 644.\nThe umask value is normally set in the \n/etc/profile startup fi le in most Linux distribu-\ntions (see Chapter 6), but some prefer to set it in the \n/etc/login.defs fi le (such as in \nUbuntu). You can specify a different default umask setting using the \numask command:\n $ umask 026\n $ touch newfile2\n $ ls -l newfile2\n -rw-r-----    1 rich     rich            0 Sep 20 19:46 newfile2\n $\nBy setting the umask value to 026, the default fi le permissions become 640, so the new fi le \nnow is restricted to read-only for the group members, and everyone else on the system has \nno permissions to the fi le.\nThe umask value also applies to making new directories:\n $ mkdir newdir\n $ ls -l\n drwxr-x--x    2 rich     rich         4096 Sep 20 20:11 newdir/\n $\nBecause the default permissions for a directory are 777, the resulting permissions from the \numask are different from those of a new fi le. The 026 umask value is subtracted from 777, \nleaving the 751 directory permission setting.\nChanging Security Settings\nIf you’ve already created a fi le or directory and need to change the security settings on it, \nLinux has a few different utilities available for this. This section shows you how to change \nthe existing permissions, the default owner, and the default group settings for a fi le or \ndirectory.\nChanging permissions\nThe chmod command allows you to change the security settings for fi les and directories. \nThe format of the \nchmod command is:\n chmod options mode file\nThe mode parameter allows you to set the security settings using either octal or symbolic \nmode. The octal mode settings are pretty straightforward; just use the standard three-digit \noctal code you want the fi le to have:\n\n180\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  180\n $ chmod 760 newfile\n $ ls -l newfile\n -rwxrw----    1 rich     rich            0 Sep 20 19:16 newfile\n$\nThe octal fi le permissions are automatically applied to the fi le indicated. The symbolic \nmode permissions are not so easy to implement.\nInstead of using the normal string of three sets of three characters, the \nchmod command \ntakes a different approach. The following is the format for specifying a permission in sym-\nbolic mode:\n [ugoa...][[+-=][rwxXstugo...]\nMakes perfectly good sense, doesn’t it? The fi rst group of characters defi nes to whom the \nnew permissions apply:\n ■\nu for the user\n ■\ng for the group\n ■\no for others (everyone else)\n ■\na for all of the above\nNext, a symbol is used to indicate whether you want to add the permission to the existing \npermissions (+), subtract the permission from the existing permission (−), or set the per-\nmissions to the value (=).\nFinally, the third symbol is the permission used for the setting. You may notice that there \nare more than the normal \nrwx values here. These are the additional settings:\n ■\nX assigns execute permissions only if the object is a directory or if it already had \nexecute permissions.\n ■\ns sets the UID or GID on execution.\n ■\nt saves program text.\n ■\nu sets the permissions to the owner’s permissions.\n ■\ng sets the permissions to the group’s permissions.\n ■\no sets the permissions to the other’s permissions.\nUsing these permissions looks like this:\n $ chmod o+r newfile\n $ ls -lF newfile\n -rwxrw-r--    1 rich     rich            0 Sep 20 19:16 newfile*\n$\nThe o+r entry adds the read permission to whatever permissions the everyone security \nlevel already had.\n\n181\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  181\n7\n $ chmod u-x newfile\n $ ls -lF newfile\n -rw-rw-r--    1 rich     rich            0 Sep 20 19:16 newfile\n $\nThe u-x entry removes the execute permission that the user already had. Note that the –F \noption for the \nls command indicates whether a fi le has execution permissions by adding an \nasterisk to the fi lename.\nThe options parameters provide a few additional features to augment the behavior of the \nchmod command. The -R parameter performs the fi le and directory changes recursively. \nYou can use wildcard characters for the fi lename specifi ed, changing the permissions on \nmultiple fi les with just one command.\nChanging ownership\nSometimes, you need to change the owner of a fi le, such as when someone leaves an orga-\nnization or a developer creates an application that needs to be owned by a system account \nwhen it’s in production. Linux provides two commands for doing that. The \nchown command \nmakes it easy to change the owner of a fi le, and the \nchgrp command allows you to change \nthe default group of a fi le.\nThe format of the \nchown command is:\n chown options owner[.group] file\nYou can specify either the login name or the numeric UID for the new owner of the fi le:\n # chown dan newfile\n # ls -l newfile\n -rw-rw-r--    1 dan      rich            0 Sep 20 19:16 newfile\n#\nSimple. The chown command also allows you to change both the user and group of a fi le:\n # chown dan.shared newfile\n # ls -l newfile\n -rw-rw-r--    1 dan      shared             0 Sep 20 19:16 newfile\n#\nIf you really want to get tricky, you can just change the default group for a fi le:\n # chown .rich newfile\n # ls -l newfile\n -rw-rw-r--    1 dan      rich            0 Sep 20 19:16 newfile\n#\nFinally, if your Linux system uses individual group names that match user login names, you \ncan change both with just one entry:\n\n182\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  182\n # chown test. newfile\n # ls -l newfile\n -rw-rw-r--    1 test    test             0 Sep 20 19:16 newfile\n#\nThe chown command uses a few different option parameters. The -R parameter allows you \nto make changes recursively through subdirectories and fi les, using a wildcard character. \nThe \n-h parameter also changes the ownership of any fi les that are symbolically linked to \nthe fi le.\nOnly the root user can change the owner of a fi le. Any user can change the default group of a fi le, but the user must \nbe a member of the groups the fi le is changed from and to.\nThe chgrp command provides an easy way to change just the default group for a fi le or \ndirectory:\n $ chgrp shared newfile\n $ ls -l newfile\n -rw-rw-r--    1 rich     shared          0 Sep 20 19:16 newfile\n$\nThe user account must own the fi le, and be a member of the new group as well to be able \nto change the group. Now any member in the shared group can write to the fi le. This is one \nway to share fi les on a Linux system. However, sharing fi les among a group of people on the \nsystem can get tricky. The next section discusses how to do this.\nSharing Files\nAs you’ve probably already fi gured out, creating groups is the way to share access to fi les \non the Linux system. However, for a complete fi le-sharing environment, things are more \ncomplicated.\nAs you’ve already seen in the “Decoding File Permissions” section, when you create a \nnew fi le, Linux assigns the fi le permissions of the new fi le using your default UID and \nGID. To allow others access to the fi le, you need to either change the security \npermissions for the everyone security group or assign the fi le a different default group \nthat contains other users.\nThis can be a pain in a large environment if you want to create and share documents among \nseveral people. Fortunately, there’s a simple solution for how to solve this problem.\n\n183\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  183\n7\nThere are three additional bits of information that Linux stores for each fi le and directory:\n ■\nThe set user id (SUID): When a fi le is executed by a user, the program runs under \nthe permissions of the fi le owner.\n ■\nThe set group id (SGID): For a fi le, the program runs under the permissions of the \nfi le group. For a directory, new fi les created in the directory use the directory group \nas the default group.\n ■\nThe sticky bit: The fi le remains (sticks) in memory after the process ends.\nThe SGID bit is important for sharing fi les. By enabling the SGID bit, you can force all new \nfi les created in a shared directory to be owned by the directory’s group and now the indi-\nvidual user’s group.\nThe SGID is set using the \nchmod command. It’s added to the beginning of the standard \nthree-digit octal value (making a four-digit octal value), or you can use the symbol \ns in \nsymbolic mode.\nIf you’re using octal mode, you’ll need to know the arrangement of the bits, shown in \nTable 7-6.\nTABLE 7- 6:  The chmod SUID, SGID, and Sticky Bit Octal Values\nBinaryOctalDescription\n0000All bits are cleared.\n0011The sticky bit is set.\n0102The SGID bit is set.\n0113The SGID and sticky bits are set.\n1004The SUID bit is set.\n1015The SUID and sticky bits are set.\n1106The SUID and SGID bits are set.\n1117All bits are set.\nSo, to create a shared directory that always sets the directory group for all new fi les, all you \nneed to do is set the SGID bit for the directory:\n $ mkdir testdir\n $ ls -l\n drwxrwxr-x    2 rich     rich         4096 Sep 20 23:12 testdir/\n\n184\nPart I: The Linux Command Line\nc07.indd  12/16/2014  Page  184\n $ chgrp shared testdir\n $ chmod g+s testdir\n $ ls -l\n drwxrwsr-x    2 rich     shared       4096 Sep 20 23:12 testdir/\n $ umask 002\n $ cd testdir\n $ touch testfile\n $ ls -l\n total 0\n -rw-rw-r--    1 rich     shared          0 Sep 20 23:13 testfile\n $\nThe fi rst step is to create a directory that you want to share using the mkdir command. \nNext, use the \nchgrp command to change the default group for the directory to a group that \ncontains the members who need to share fi les (you must be a member of that group for this \nto work). Finally, set the SGID bit for the directory to ensure that any fi les created in the \ndirectory use the shared group name as the default group.\nFor this environment to work properly, all the group members must have their umask \nvalues set to make fi les writable by group members. In the preceding example, the \numask is \nchanged to \n002 so the fi les are writable by the group.\nAfter all that’s done, any member of the group can go to the shared directory and create \na new fi le. As expected, the new fi le uses the default group of the directory, not the user \naccount’s default group. Now any user in the shared group can access this fi le.\nSummary\nThis chapter discussed the command line commands you need to know to manage the Linux \nsecurity on your system. Linux uses a system of user IDs and group IDs to protect access to \nfi les, directories, and devices. Linux stores information about user accounts in the \n/etc/\npasswd\n fi le and information about groups in the /etc/group fi le. Each user is assigned \na unique numeric user ID, along with a text login name to identify the user in the system. \nGroups are also assigned unique numerical group IDs and text group names. A group can \ncontain one or more users to allowed shared access to system resources.\nSeveral commands are available for managing user accounts and groups. The \nuseradd \ncommand allows you to create new user accounts, and the \ngroupadd command allows you \nto create new group accounts. To modify an existing user account, use the \nusermod com-\nmand. Similarly, use the \ngroupmod command to modify group account information.\nLinux uses a complicated system of bits to determine access permissions for fi les and direc-\ntories. Each fi le contains three security levels of protection: the fi le’s owner, a default \ngroup that has access to the fi le, and a level for everyone else on the system. Each security \nlevel is defi ned by three access bits: read, write, and execute. The combination of three \n\n185\nChapter 7: Understanding Linux File Permissions\nc07.indd  12/16/2014  Page  185\n7\nbits is often referred to by the symbols rwx, for read, write, and execute. If a permission is \ndenied, its symbol is replaced with a dash (such as \nr-- for read-only permission).\nThe symbolic permissions are often referred to as octal values, with the three bits combined \ninto one octal value and three octal values representing the three security levels. Use the \numask command to set the default security settings for fi les and directories created on the \nsystem. The system administrator normally sets a default umask value in the \n/etc\n/profile\n fi le, but you can use the umask command to change your umask value at any \ntime.\nUse the \nchmod command to change security settings for fi les and directories. Only the fi le’s \nowner can change permissions for a fi le or directory. However, the root user can change the \nsecurity settings for any fi le or directory on the system. You can use the \nchown and chgrp  \ncommands to change the default owner and group of the fi le.\nThe chapter closed with a discussion on how to use the set GID bit to create a shared \ndirectory. The SGID bit forces any new fi les or directories created in a directory to use the \ndefault group name of the parent directory, not that of the user who created them. This \nprovides an easy way to share fi les between users on the system.\nNow that you’re up to speed with fi le permissions, it’s time to take a closer look at how to \nwork with the actual fi lesystem in Linux. The next chapter shows you how to create new \npartitions in Linux from the command line and then how to format the new partitions so \nthat they can be used in the Linux virtual directory. \n\n\n\n187\nc08.indd  12/10/2014  Page  187\nCHAPTER \n8\nManaging Filesystems\nIN THIS CHAPTER\nUnderstanding fi lesystem basics\nExploring journaling and copy-on-write fi lesystems\nManaging fi lesystems\nInvestigating the logical volume layout\nUsing the Linux Logical Volume Manager\nW\nhen you’re working with your Linux system, one of the decisions you’ll need to make is \nwhat fi lesystem to use for the storage devices. Most Linux distributions kindly provide a \ndefault fi lesystem for you at installation time, and most beginning Linux users just use it \nwithout giving the topic another thought.\nAlthough using the default fi lesystem choice isn’t necessarily a bad thing, sometimes it helps to \nknow the other options available to you. This chapter discusses the different fi lesystem options \nyou have available in the Linux world and shows you how to create and manage them from the \nLinux command line.\nExploring Linux Filesystems\nChapter 3 discussed how Linux uses a filesystem to store fi les and folders on a storage device. The \nfi lesystem provides a way for Linux to bridge the gap between the ones and zeroes stored in the \nhard drive and the fi les and folders you work with in your applications.\nLinux supports several types of fi lesystems to manage fi les and folders. Each fi lesystem implements \nthe virtual directory structure on storage devices using slightly different features. This section \nwalks you through the strengths and weaknesses of the more common fi lesystems used in the \nLinux environment.\n\n188\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  188\nUnderstanding the basic Linux fi lesystems\nThe original Linux system used a simple fi lesystem that mimicked the functionality of the \nUnix fi lesystem. This section discusses the evolution of that fi lesystem.\nLooking at the ext Filesystem\nThe original fi lesystem introduced with the Linux operating system is called the extended \nfilesystem (or just ext for short). It provides a basic Unix-like fi lesystem for Linux, using \nvirtual directories to handle physical devices, and storing data in fi xed-length blocks on \nthe physical devices.\nThe ext fi lesystem uses a system called inodes to track information about the fi les stored \nin the virtual directory. The inode system creates a separate table on each physical device, \ncalled the inode table, to store fi le information. Each stored fi le in the virtual directory has \nan entry in the inode table. The extended part of the name comes from the additional data \nthat it tracks on each fi le, which consists of these items:\n ■\nThe fi lename\n ■\nThe fi le size\n ■\nThe owner of the fi le\n ■\nThe group the fi le belongs to\n ■\nAccess permissions for the fi le\n ■\nPointers to each disk block that contains data from the fi le\nLinux references each inode in the inode table using a unique number (called the inode \nnumber), assigned by the fi lesystem as data fi les are created. The fi lesystem uses the inode \nnumber to identify the fi le rather than having to use the full fi lename and path.\nLooking at the ext2 Filesystem\nThe original ext fi lesystem had quite a few limitations, such as restraining fi les to only 2GB \nin size. Not too long after Linux was fi rst introduced, the ext fi lesystem was upgraded to \ncreate the second extended fi lesystem, called ext2.\nAs you can guess, the ext2 fi lesystem is an expansion of the basic abilities of the ext fi le-\nsystem, but maintains the same structure. The ext2 fi lesystem expands the inode table \nformat to track additional information about each fi le on the system.\nThe ext2 inode table adds the created, modifi ed, and last accessed time values for fi les \nto help system administrators track fi le access on the system. The ext2 fi lesystem also \nincreases the maximum fi le size allowed to 2TB (then in later versions of ext2, that was \nincreased to 32TB) to help accommodate large fi les commonly found in database servers.\nIn addition to expanding the inode table, the ext2 fi lesystem also changed the way in \nwhich fi les are stored in the data blocks. A common problem with the ext fi lesystem was \nthat as a fi le is written to the physical device, the blocks used to store the data tend to be \n\n189\nChapter 8: Managing Filesystems\nc08.indd  12/10/2014  Page  189\n8\nscattered throughout the device (called fragmentation). Fragmentation of data blocks can \nreduce the fi lesystem performance, because it takes longer to search the storage device to \naccess all the blocks for a specifi c fi le.\nThe ext2 fi lesystem helps reduce fragmentation by allocating disk blocks in groups when \nyou save a fi le. By grouping the data blocks for a fi le, the fi lesystem doesn’t have to search \nall over the physical device for the data blocks to read the fi le.\nThe ext2 fi lesystem was the default fi lesystem used in Linux distributions for many years, \nbut it, too, had its limitations. The inode table, although a nice feature that allows the fi le-\nsystem to track additional information about fi les, can cause problems that can be fatal to \nthe system. Each time the fi lesystem stores or updates a fi le, it must modify the inode table \nwith the new information. The problem is that this isn’t always a fl uid action.\nIf something should happen to the computer system between the fi le being stored and the \ninode table being updated, the two would become out of sync. The ext2 fi lesystem is notori-\nous for easily becoming corrupted due to system crashes and power outages. Even if the fi le \ndata is stored just fi ne on the physical device, if the inode table entry isn’t completed, the \next2 fi lesystem doesn’t even know that the fi le existed!\nIt wasn’t long before developers were exploring a different avenue of Linux fi lesystems.\nUnderstanding journaling fi lesystems\nJournaling filesystems provide a new level of safety to the Linux system. Instead of writing \ndata directly to the storage device and then updating the inode table, journaling fi lesys-\ntems write fi le changes into a temporary fi le (called the journal) fi rst. After data is success-\nfully written to the storage device and the inode table, the journal entry is deleted.\nIf the system should crash or suffer a power outage before the data can be written to the \nstorage device, the journaling fi lesystem just reads through the journal fi le and processes \nany uncommitted data left over.\nLinux commonly uses three different methods of journaling, each with different levels of \nprotection. These are shown in Table 8-1.\nTABLE 8-1    Journaling  Filesystem  Methods\nMethodDescription\nData modeBoth inode and fi le data are journaled. Low risk of losing data, but poor \nperformance.\nOrdered modeOnly inode data is written to the journal, but not removed until fi le data is \nsuccessfully written. Good compromise between performance and safety.\nWriteback \nmode\nOnly inode data is written to the journal, no control over when the fi le data is \nwritten. Higher risk of losing data, but still better than not using journaling.\n\n190\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  190\nThe data mode journaling method is by far the safest for protecting data, but it is also the \nslowest. All the data written to a storage device must be written twice, once to the journal \nand again to the actual storage device. This can cause poor performance, especially for \nsystems that do lots of data writing.\nOver the years, a few different journaling fi lesystems have appeared in Linux. The following \nsections describe the popular Linux journaling fi lesystems available.\nLooking at the ext3 Filesystem\nThe ext3 fi lesystem was added to the Linux kernel in 2001, and up until recently was the \ndefault fi lesystem used by just about all Linux distributions. It uses the same inode table \nstructure as the ext2 fi lesystem, but adds a journal fi le to each storage device to journal \nthe data written to the storage device.\nBy default, the ext3 fi lesystem uses the ordered mode method of journaling, only writing \nthe inode information to the journal fi le, but not removing it until the data blocks have \nbeen successfully written to the storage device. You can change the journaling method \nused in the ext3 fi lesystem to either data or writeback modes with a simple command line \noption when creating the fi lesystem.\nAlthough the ext3 fi lesystem added basic journaling to the Linux fi lesystem, it still lacked \na few things. For example, the ext3 fi lesystem doesn’t provide any recovery from accidental \ndeletion of fi les, no built-in data compression is available (although a patch can be installed \nseparately that provides this feature), and the ext3 fi lesystem doesn’t support encrypting \nfi les. For those reasons, developers in the Linux project chose to continue work on improv-\ning the ext3 fi lesystem.\nLooking at the ext4 Filesystem\nThe result of expanding the ext3 fi lesystem was (as you probably guessed) the ext4 fi lesys-\ntem. The ext4 fi lesystem was offi cially supported in the Linux kernel in 2008 and is now \nthe default fi lesystem used in popular Linux distributions, such as Ubuntu.\nIn addition to supporting compression and encryption, the ext4 fi lesystem also supports a \nfeature called extents. Extents allocate space on a storage device in blocks and only store \nthe starting block location in the inode table. This helps save space in the inode table by \nnot having to list all the data blocks used to store data from the fi le.\nThe ext4 fi lesystem also incorporates block preallocation. If you want to reserve space on a \nstorage device for a fi le that you know will grow in size, with the ext4 fi lesystem it’s possi-\nble to allocate all the expected blocks for the fi le, not just the blocks that physically exist. \nThe ext4 fi lesystem fi lls in the reserved data blocks with zeroes and knows not to allocate \nthem for any other fi le.\nLooking at the Reiser Filesystem\nIn 2001, Hans Reiser created the fi rst journaling fi lesystem for Linux, called ReiserFS. The \nReiserFS fi lesystem supports only writeback journaling mode, writing only the inode table \n\n191\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  191\ndata to the journal fi le. Because it writes only the inode table data to the journal, the \nReiserFS fi lesystem is one of the faster Linux journaling fi lesystems.\nTwo interesting features incorporated into the ReiserFS fi lesystem are that you can resize \nan existing fi lesystem while it’s still active and that it uses a technique called tailpacking, \nwhich stuffs data from one fi le into empty space in a data block from another fi le. The \nactive fi lesystem resizing feature is great if you have to expand an already created fi lesys-\ntem to accommodate more data.\nThe ReiserFS development team began working on a new version called Reiser4 in 2004. The \nReiser4 fi lesystem has several improvements over ResierFS, including extremely effi cient \nhandling of small fi les. However, most current mainstream Linux distributions don’t use \nthe Reiser4 fi lesystem. Yet, you may still run into a Linux system that employs it.\nLooking at the Journaled Filesystem\nPossibly one of the oldest journaling fi lesystems around, the Journaled File System (JFS) was \ndeveloped by IBM in 1990 for its AIX fl avor of Unix. However, it wasn’t until its second ver-\nsion that it was ported to the Linux environment.\nThe offi cial IBM name of the second version of the JFS fi lesystem is JFS2, but most Linux systems refer to it \nas just JFS.\nThe JFS fi lesystem uses the ordered journaling method, storing only the inode table data in \nthe journal, but not removing it until the actual fi le data is written to the storage device. \nThis method is a compromise between the speed of the Reiser4 and the integrity of the data \nmode journaling method.\nThe JFS fi lesystem uses extent-based fi le allocation, allocating a group of blocks for each \nfi le written to the storage device. This method provides for less fragmentation on the stor-\nage device.\nOutside of the IBM Linux offerings, the JFS fi lesystem isn’t popularly used, but you may run \ninto it in your Linux journey.\nLooking at the XFS Filesystem\nThe XFS journaling fi lesystem is yet another fi lesystem originally created for a commercial \nUnix system that made its way into the Linux world. Silicon Graphics Incorporated (SGI) orig-\ninally created XFS in 1994 for its commercial IRIX Unix system. It was released to the Linux \nenvironment for common use in 2002. The XFS fi lesystem has recently become more popular \nand is used as the default fi lesystem in mainstream Linux distributions, such as RHEL. \nThe XFS fi lesystem uses the writeback mode of journaling, which provides high perfor-\nmance but does introduce an amount of risk because the actual data isn’t stored in the \n\n192\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  192\njournal fi le. The XFS fi lesystem also allows online resizing of the fi lesystem, similar to the \nReiser4 fi lesystem, except XFS fi lesystems can only be expanded and not shrunk.\nUnderstanding the copy-on-write fi lesystems\nWith journaling, you must choose between safety and performance. Although data mode \njournaling provides the highest safety, performance suffers because both inode and data \nis journaled. With writeback mode journaling, performance is acceptable, but safety is \ncompromised.\nFor fi lesystems, an alternative to journaling is a technique called copy-on-write (COW). COW \noffers both safety and performance via snapshots. For modifying data, a clone or writable-\nsnapshot is used. Instead of writing modifi ed data over current data, the modifi ed data is \nput in a new fi lesystem location. Even when data modifi cation is completed, the old data is \nnever overwritten. \nCOW fi lesystems are gaining in popularity. Two of the most popular, Btrfs and ZFS, are \nbriefl y reviewed in the following sections.\nLooking at the ZFS Filesystem\nThe COW fi lesystem ZFS was developed in 2005 by Sun Microsystems for the OpenSolaris \noperating system. It began being ported to Linux in 2008 and was fi nally available for \nLinux production use in 2012. \nZFS is a stable fi lesystem and competes well against Resier4, Btrfs, and ext4. Its biggest \ndetractor is that ZFS does not have a GPL license. The OpenZFS project was launched in \n2013, which may help to change this situation. However, it’s possible that until a GPL \nlicense is obtained, ZFS will never be a default Linux fi lesystem.\nLooking at the Btrfs Filesystem\nThe COW newcomer is the Btrfs fi lesystem, also called the B-tree fi lesystem. Oracle started \ndevelopment on Btrfs in 2007. It was based on many of Reiser4’s features, but offered \nimprovements in reliability. Additional developers eventually joined in and helped Btrfs \nquickly rise toward the top of the popular fi lesystems list. This popularity is due to \nstability, ease of use, as well as the ability to dynamically resize a mounted fi lesystem. The \nopenSUSE Linux distribution recently established Btrfs as its default fi lesystem. It is also \noffered in other Linux distributions, such as RHEL, although not as the default fi lesystem.\nWorking with Filesystems\nLinux provides a few different utilities that make it easier to work with fi lesystems from \nthe command line. You can add new fi lesystems or change existing fi lesystems from the \ncomfort of your own keyboard. This section walks you through the commands for interact-\ning with fi lesystems from a command line environment.\n\n193\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  193\nCreating partitions\nTo start out, you need to create a partition on the storage device to contain the fi lesystem. \nThe partition can be an entire disk or a subset of a disk that contains a portion of the vir-\ntual directory.\nThe \nfdisk utility is used to help you organize partitions on any storage device installed on \nthe system. The \nfdisk command is an interactive program that allows you to enter com-\nmands to walk through the steps of partitioning a hard drive.\nTo start the \nfdisk command, you need to specify the device name of the storage device \nyou want to partition and you need to have superuser privileges. When you don’t have \nsuperuser privileges and attempt to use \nfdisk, you’ll receive some sort of error message, \nlike this one: \n$ fdisk /dev/sdb\nUnable to open /dev/sdb\n$\nSometimes, the hardest part of creating a new disk partition is trying to fi nd the physical disk on your Linux system. \nLinux uses a standard format for assigning device names to hard drives, but you need to be familiar with the format. \nFor older IDE drives, Linux uses \n/dev/hdx, where x is a letter based on the order the drive is detected (a for the \nfi rst drive, b for the second, and so on). For both the newer SATA drives and SCSI drives, Linux uses \n/dev/sdx, \nwhere \nx is a letter based on the order the drive is detected (again, a for the fi rst drive, b for the second, and so on). \nIt’s always a good idea to double-check to make sure you are referencing the correct drive before formatting the \npartition!\nIf you do have superuser privileges and the correct device name, the fdisk command \nallows you entrance into the utility as demonstrated here on a CentOS distribution:\n$ sudo fdisk /dev/sdb\n[sudo] password for Christine:\nDevice contains neither a valid DOS partition table, \nnor Sun, SGI or OSF disklabel\nBuilding a new DOS disklabel with disk identifier 0xd3f759b5.\nChanges will remain in memory only \nuntil you decide to write them.\nAfter that, of course, the previous content won't be recoverable.\nWarning: invalid flag 0x0000 of partition table 4 will \nbe corrected by w(rite)\n[...]\nCommand (m for help):\n\n194\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  194\nIf this is the fi rst time you’re partitioning the storage device, fdisk gives you a warning that a partition table is not \non the device.\nThe fdisk interactive command prompt uses single letter commands to instruct fdisk \nwhat to do. Table 8-2 shows the commands available at the \nfdisk command prompt.\nTABLE 8-2    The  fdisk  Commands\nCommandDescription\na\nToggles a fl ag indicating if the partition is bootable\nb\nEdits the disklabel used by BSD Unix systems\nc\nToggles the DOS compatibility fl ag\nd\nDeletes the partition\nl\nLists the available partition types\nm\nDisplays the command options\nn\nAdds a new partition\no\nCreates a DOS partition table\np\nDisplays the current partition table\nq\nQuits without saving changes\ns\nCreates a new disklabel for Sun Unix systems\nt\nChanges the partition system ID\nu\nChanges the storage units used\nv\nVerifi es the partition table\nw\nWrites the partition table to the disk\nx\nAdvanced functions\nAlthough this list may look intimidating, usually you need just a few basic commands in \nday-to-day work.\nFor starters, you can display the details of a storage device using the \np command:\nCommand (m for help): p\nDisk /dev/sdb: 5368 MB, 5368709120 bytes\n255 heads, 63 sectors/track, 652 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\n\n195\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  195\nDisk identifier: 0x11747e88\n   Device Boot      Start         End      Blocks   Id  System\nCommand (m for help):\nThe output shows that the storage device has 5368MB of space on it (5GB). The listing under \nthe storage device details shows whether there are any existing partitions on the device. \nThe listing in this example doesn’t show any partitions, so the device is not partitioned yet.\nNext, you’ll want to create a new partition on the storage device. Use the \nn command for that:\nCommand (m for help): n\nCommand action\n   e   extended\n   p   primary partition (1-4)\np\nPartition number (1-4): 1\nFirst cylinder (1-652, default 1): 1\nLast cylinder, +cylinders or +size{K,M,G} (1-652, default 652): +2G\nCommand (m for help): \nPartitions can be created as either a primary partition or an extended partition. Primary \npartitions can be formatted with a fi lesystem directly, whereas extended partitions can only \ncontain other primary partitions. The reason for extended partitions is that there can only be \nfour partitions on a single storage device. You can extend that by creating multiple extended \npartitions and then creating primary partitions inside the extended partitions. This example \ncreates a primary storage device, assigns it partition number 1, and then allocates 2GB of the \nstorage device space to it. You can see the results using the \np command again:\nCommand (m for help): p\nDisk /dev/sdb: 5368 MB, 5368709120 bytes\n255 heads, 63 sectors/track, 652 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x029aa6af\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1         262     2104483+  83  Linux\nCommand (m for help):\nNow in the output there’s a partition shown on the storage device (called /dev/sdb1). The \nId entry defi nes how Linux treats the partition. fdisk allows you to create lots of parti-\ntion types. Using the \nl command lists the different types available. The default is type 83, \nwhich defi nes a Linux fi lesystem. If you want to create a partition for a different fi lesystem \n(such as a Windows NTFS partition), just select a different partition type.\n\n196\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  196\nYou can repeat the process to allocate the remaining space on the storage device to another \nLinux partition. After you’ve created the partitions you want, use the \nw command to save \nthe changes to the storage device:\nCommand (m for help): w\nThe partition table has been altered!\nCalling ioctl() to re-read partition table.\nSyncing disks.\n$\nThe storage device partition information was written to the partition table, and Linux was \ninformed of the new partition via the \nioctl() call. Now that you have set up a partition \non the storage device, you’re ready to format it with a Linux fi lesystem.\nSome distributions and older distribution versions do not automatically inform your Linux system of a new partition \nafter it is made. In this case, you need to use either the \npartprobe or hdparm command (see their man pages), \nor reboot your system so it reads the updated partition table.\nCreating a fi lesystem\nBefore you can store data on the partition, you must format it with a fi lesystem so Linux \ncan use it. Each fi lesystem type uses its own command line program to format partitions. \nTable 8-3 lists the utilities used for the different fi lesystems discussed in this chapter.\nTABLE 8-3    Command Line Programs to Create Filesystems\nUtilityPurpose\nmkefs\nCreates an ext fi lesystem\nmke2fs\nCreates an ext2 fi lesystem\nmkfs.ext3\nCreates an ext3 fi lesystem\nmkfs.ext4\nCreates an ext4 fi lesystem\nmkreiserfs\nCreates a ReiserFS fi lesystem\njfs_mkfs\nCreates a JFS fi lesystem\nmkfs.xfs\nCreates an XFS fi lesystem\nmkfs.zfs\nCreates a ZFS fi lesystem\nmkfs.btrfs\nCreates a Btrfs fi lesystem\nNot all fi lesystem utilities are installed by default. To determine whether you have a \nparticular fi lesystem utility, use the \ntype command:\n\n197\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  197\n$ type mkfs.ext4\nmkfs.ext4 is /sbin/mkfs.ext4\n$\n$ type mkfs.btrfs\n-bash: type: mkfs.btrfs: not found\n$\nThe preceding example on an Ubuntu system shows that the mkfs.ext4 utility is \navailable. However, the Btrfs utility is not. See Chapter 9 on how to install additional \nsoftware and utilities on your Linux distribution.\nEach fi lesystem utility command has lots of command line options that allow you to \ncustomize just how the fi lesystem is created in the partition. To see all the command line \noptions available, use the \nman command to display the manual pages for the fi lesystem \ncommand (see Chapter 3). All the fi lesystem commands allow you to create a default \nfi lesystem with just the simple command with no options:\n$ sudo mkfs.ext4 /dev/sdb1\n[sudo] password for Christine:\nmke2fs 1.41.12 (17-May-2010)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n131648 inodes, 526120 blocks\n26306 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=541065216\n17 block groups\n32768 blocks per group, 32768 fragments per group\n7744 inodes per group\nSuperblock backups stored on blocks:\n        32768, 98304, 163840, 229376, 294912\nWriting inode tables: done\nCreating journal (16384 blocks): done\nWriting superblocks and filesystem accounting information: done\nThis filesystem will be automatically checked every 23 mounts or\n180 days, whichever comes first. Use tune2fs -c or -i to override.\n$\nThe new fi lesystem uses the ext4 fi lesystem type, which is a journaling fi lesystem in Linux. \nNotice that part of the creation process was to create the new journal.\nAfter you create the fi lesystem for a partition, the next step is to mount it on a virtual \ndirectory mount point so you can store data in the new fi lesystem. You can mount the new \nfi lesystem anywhere in your virtual directory where you need the extra space.\n\n198\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  198\n$ ls /mnt\n$\n$ sudo mkdir /mnt/my_partition\n$\n$ ls -al /mnt/my_partition/\n$\n$ ls -dF /mnt/my_partition\n/mnt/my_partition/\n$\n$ sudo  mount -t ext4  /dev/sdb1  /mnt/my_partition\n$\n$ ls -al /mnt/my_partition/\ntotal 24\ndrwxr-xr-x. 3 root root  4096 Jun 11 09:53 .\ndrwxr-xr-x. 3 root root  4096 Jun 11 09:58 ..\ndrwx------. 2 root root 16384 Jun 11 09:53 lost+found\n$\nThe mkdir command (Chapter 3) creates the mount point in the virtual directory, and the \nmount command adds the new hard drive partition to the mount point. The -t option on \nthe \nmount command indicates what fi lesystem type, ext4, you are mounting. Now you can \nsave new fi les and folders on the new partition!\nThis method of mounting a fi lesystem only temporarily mounts the fi lesystem. When you reboot your Linux system, the \nfi lesystem doesn’t automatically mount. To force Linux to automatically mount the new fi lesystem at boot time, add \nthe new fi lesystem to the \n/etc/fstab  fi  l e .\nNow that the fi lesystem is mounted within the virtual directory system, it can start to be \nused on a regular basis. Unfortunately, with regular use comes the potential for serious prob-\nlems, such as fi lesystem corruption. The next section looks at how to deal with these issues. \nChecking and repairing a fi lesystem\nEven with modern fi lesystems, things can go wrong if power is unexpectedly lost, or if a \nwayward application locks up the system while fi le access is in progress. Fortunately, some \ncommand line tools are available to help you make an attempt to restore the fi lesystem \nback to order.\nEach fi lesystem has its own recovery command for interacting with the fi lesystem. That \nhas the potential of getting ugly, because more and more fi lesystems are available in the \nLinux environment, making for lots of individual commands you have to know. Fortunately, \na common front-end program available can determine the fi lesystem on the storage device \nand use the appropriate fi lesystem recovery command based on the fi lesystem being \nrecovered.\n\n199\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  199\nThe fsck command is used to check and repair most Linux fi lesystem types, including ones \ndiscussed earlier in this chapter — ext, ext2, ext3, ext4, Reiser4, JFS, and XFS. The format \nof the command is:\nfsck options filesystem\nYou can list multiple filesystem entries on the command line to check. Filesystems can \nbe referenced using either the device name, the mount point in the virtual directory, or a \nspecial Linux UUID value assigned to the fi lesystem.\nAlthough journaling fi lesystems users do need the fsck command, it is arguable as to whether COW fi lesystems \nusers do. In fact, the ZFS fi lesystem does not even have an interface to the \nfsck utility.\nThe fsck command uses the /etc/fstab fi le to automatically determine the fi lesystem on \na storage device that’s normally mounted on the system. If the storage device isn’t normally \nmounted (such as if you just created a fi lesystem on a new storage device), you need to use \nthe \n-t command line option to specify the fi lesystem type. Table 8-4 lists the other com-\nmand line options available.\nTABLE 8- 4    The fsck Command Line Options\nOptionDescription\n-a\nAutomatically repairs the fi lesystem if errors are detected\n-A\nChecks all the fi lesystems listed in the /etc/fstab fi le\n-C\nDisplays a progress bar for fi lesystems that support that feature (only ext2 and \next3)\n-N\nDoesn’t run the check, only displays what checks would be performed\n-r\nPrompts to fi x if errors found\n-R\nSkips the root fi lesystem if using the -A option\n-s\nIf checking multiple fi lesystems, performs the checks one at a time\n-t\nSpecifi es the fi lesystem type to check\n-T\nDoesn’t show the header information when starting\n-V\nProduces verbose output during the checks\n-y\nAutomatically repairs the fi lesystem if errors detected\nYou may notice that some of the command line options are redundant. That’s part of the \nproblem of trying to implement a common front-end for multiple commands. Some of the \nindividual fi lesystem repair commands have additional options that can be used. If you \n\n200\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  200\nneed to do more advanced error checking, you’ll need to check the man pages for the indi-\nvidual fi lesystem repair tool to see if there are extended options specifi c to that fi lesystem.\nYou can run the fsck command on unmounted fi lesystems only. For most fi lesystems, you can just unmount the \nfi lesystem to check it and then remount it when you’re fi nished. However, because the root fi lesystem contains all the \ncore Linux commands and log fi les, you can’t unmount it on a running system.\nThis is a time where having a Linux LiveCD comes in handy! Just boot your system with the LiveCD, and then run the \nfsck command on the root fi lesystem!\nThis chapter has showed how to handle fi lesystems contained in physical storage devices. \nLinux also provides a couple of different ways to create logical storage devices for fi le-\nsystems. The next section examines how you can use a logical storage device for your \nfi lesystems.\nManaging Logical Volumes\nIf you create your fi lesystems using standard partitions on hard drives, trying to add \nadditional space to an existing fi lesystem can be somewhat of a painful experience. You \ncan only expand a partition to the extent of the available space on the same physical hard \ndrive. If no more space is available on that hard drive, you’re stuck having to get a larger \nhard drive and manually moving the existing fi lesystem to the new drive.\nWhat would come in handy is a way to dynamically add more space to an existing fi le-\nsystem by just adding a partition from another hard drive to the existing fi lesystem. The \nLinux Logical Volume Manager (LVM) software package allows you to do just that. It provides \nan easy way for you to manipulate disk space on a Linux system without having to rebuild \nentire fi lesystems.\nExploring logical volume management layout\nThe core of logical volume management is how it handles the physical hard drive partitions \ninstalled on the system. In the logical volume management world, hard drives are called \nphysical volumes (PV). Each PV maps to a specifi c physical partition created on a hard drive.\nMultiple PV elements are pooled together to create a volume group (VG). The logical volume \nmanagement system treats the VG like a physical hard drive, but in reality the VG may \nconsist of multiple physical partitions spread across multiple hard drives. The VG provides a \nplatform to create the logical partitions, which actually contain the fi lesystem.\nThe fi nal layer in the structure is the logical volume (LV). The LV creates the partition envi-\nronment for Linux to create a fi lesystem, acting similar to a physical hard disk partition as \nfar as Linux is concerned. The Linux system treats the LV just like a physical partition. \n\n201\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  201\nYou can format the LV using any one of the standard Linux fi lesystems and then add it to \nthe Linux virtual directory at a mount point.\nFigure 8-1 shows the basic layout of a typical Linux logical volume management \nenvironment.\nFIGURE 8-1\nThe logical volume management environment\nLogical Volume 1\nHard Drive 1Hard Drive 2Hard Drive 3\nLogical Volume 2\nVolume Group\nPhysical\nVolume 1\npartition\n1\npartition\n2\npartition\n1\npartition\n2\npartition\n1\nunused\nspace\nPhysical\nVolume 2\nPhysical\nVolume 3\nPhysical\nVolume 4\nPhysical\nVolume 5\nThe volume group, shown in Figure 8-1, spans across three separate physical hard drives, \nwhich contain fi ve separate physical partitions. Inside the volume group are two separate \nlogical volumes. The Linux system treats each logical volume just like a physical partition. \nEach logical volume can be formatted as an ext4 fi lesystem and then mounted to a specifi c \nlocation in the virtual directory.\nNotice in Figure 8-1 that the third physical hard drive has an unused partition. Using \nlogical volume management, you can easily assign this unused partition to the existing \nvolume group at a later time, and then either use it to create a new logical volume or add it \nto expand one of the existing logical volumes when you need more space.\nLikewise, if you add a new hard drive to the system, the local volume management system \nallows you to add it to the existing volume group, and then create more space for one of the \nexisting logical volumes, or start a new logical volume to be mounted. That’s a much better \nway of handling expanding fi lesystems!\nUsing the LVM in Linux\nThe Linux LVM was developed by Heinz Mauelshagen and released to the Linux community \nin 1998. It allows you to manage a complete logical volume management environment in \nLinux using simple command line commands.\nTwo versions of Linux LVM are available:\n\n202\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  202\n ■\nLVM1: The original LVM package released in 1998 and available in only the 2.4 \nLinux kernels. It provides only basic logical volume management features.\n ■\nLVM2: An updated version of the LVM, available in the 2.6 Linux kernels. It provides \nadditional features over the standard LVM1 features.\nMost modern Linux distributions using the 2.6 kernel version or above provide support for \nLVM2. Besides the standard logical volume management features, LVM2 provides a few other \nnice things for you to use in your Linux system.\nTaking a Snapshot\nThe original Linux LVM allows you to copy an existing logical volume to another device \nwhile the logical volume is active. This feature is called a snapshot. Snapshots are great \nfor backing up important data that can’t be locked due to high availability requirements. \nTraditional backup methods usually lock fi les as they’re being copied to the backup media. \nThe snapshot allows you to continue running mission critical web or database servers while \nperforming the copy. Unfortunately, LVM1 allows you to create only a read-only snapshot. \nAfter you create the snapshot, you can’t write to it.\nLVM2 allows you to create a read-write snapshot of an active logical volume. With the \nread-write copy, you can remove the original logical volume and mount the snapshot as a \nreplacement. This feature is great for fast fail-overs or for experimenting with applications \nthat modify data that may need to be restored if something fails.\nStriping\nAnother interesting feature that LVM2 provides is striping. With striping, a logical volume is \ncreated across multiple physical hard drives. When the Linux LVM writes a fi le to the logical \nvolume, the data blocks in the fi le are spread across the multiple hard drives. Each succes-\nsive block of data is written to the next hard drive.\nStriping helps improve disk performance, because Linux can write the multiple data blocks \nfor a fi le to the multiple hard drives simultaneously, rather than having to wait for a single \nhard drive to move the read/write head to different locations. This improvement also \napplies to reading sequentially accessed fi les, because the LVM can read data from the mul-\ntiple hard drives simultaneously.\nLVM striping is not the same as RAID striping. LVM striping doesn’t provide a parity entry, which creates the fault-\ntolerant environment. In fact, LVM striping may increase the chance of a fi le being lost due to a hard drive failure. A \nsingle disk failure can result in multiple logical volumes being inaccessible.\nMirroring\nJust because you install a fi lesystem using LVM doesn’t mean that things can’t still go \nwrong in the fi lesystem. Just as in a physical partition, LVM logical volumes are susceptible \n\n203\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  203\nto power outages and disk crashes. After a fi lesystem becomes corrupt, there’s always a \npossibility that you won’t be able to recover it.\nThe LVM snapshot process provides some comfort knowing that you can create a backup \ncopy of a logical volume at any time, but for some environments that may not be enough. \nSystems that have lots of data changes, such as database servers, may store hundreds or \nthousands of records since the last snapshot.\nA solution to this problem is the LVM mirror. A mirror is a complete copy of a logical volume \nthat’s updated in real time. When you create the mirror logical volume, LVM synchronizes \nthe original logical volume to the mirror copy. Depending on the size of the original logical \nvolume, this may take some time to complete.\nAfter the original synchronization is complete, LVM performs two writes for each write \nprocess in the fi lesystem — one to the main logical volume and one to the mirrored copy. \nAs you can guess, this process does slow down write performance on the system. However, \nif the original logical volume should become corrupt for some reason, you have a complete \nup-to-date copy at your fi ngertips!\nUsing the Linux LVM\nNow that you’ve seen what the Linux LVM can do, this section discusses how to implement \nit to help organize the disk space on your system. The Linux LVM package only provides \ncommand line programs for creating and managing all the components in the logical \nvolume management system. Some Linux distributions include graphical front-ends to the \ncommand line commands, but for complete control of your LVM environment, it’s best to get \ncomfortable working directly with the commands.\nDefining Physical Volumes\nThe fi rst step in the process is to convert the physical partitions on the hard drive into \nphysical volume extents used by the Linux LVM. Our friend the \nfdisk command helps us \nhere. After creating the basic Linux partition, you need to change the partition type using \nthe \nt command:\n[...]\nCommand (m for help): t\nSelected partition 1\nHex code (type L to list codes): 8e\nChanged system type of partition 1 to 8e (Linux LVM)\nCommand (m for help): p\nDisk /dev/sdb: 5368 MB, 5368709120 bytes\n255 heads, 63 sectors/track, 652 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\n\n204\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  204\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0xa8661341\n   Device Boot      Start         End      Blocks   Id  System\n/dev/sdb1               1         262     2104483+  8e  Linux LVM\nCommand (m for help): w\nThe partition table has been altered!\nCalling ioctl() to re-read partition table.\nSyncing disks.\n$ \nThe 8e partition type denotes that the partition will be used as part of a Linux LVM system \nand not as a direct fi lesystem, as you saw with the \n83 partition type earlier.\nIf the pvcreate command in the next step does not work for you, it’s most likely due to the LVM2 package not \nbeing installed by default. To install the package, use the package name lvm2 and see Chapter 9 for how to install \nsoftware packages.\nThe next step is to use the partition to create the actual physical volume. That’s done using \nthe \npvcreate command. The pvcreate command defi nes the physical partition to use for \nthe PV. It simply tags the partition as a physical volume in the Linux LVM system:\n$ sudo pvcreate /dev/sdb1\n  dev_is_mpath: failed to get device for 8:17\n  Physical volume \"/dev/sdb1\" successfully created\n$\nDon’t let the daunting message dev_is_mpath: failed to get device for 8:17 or similar messages \nfrighten you. As long as you receive the \nsuccessfully created message, all is well. The pvcreate com-\nmand checks to see whether the partition is a multi-path (mpath) device. If it is not, it issues the daunting message.\nYou can use the pvdisplay command to display a list of physical volumes you’ve created if \nyou’d like to see your progress along the way:\n$ sudo pvdisplay /dev/sdb1\n  \"/dev/sdb1\" is a new physical volume of \"2.01 GiB\"\n  --- NEW Physical volume ---\n  PV Name               /dev/sdb1\n  VG Name\n  PV Size               2.01 GiB\n\n205\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  205\n  Allocatable           NO\n  PE Size               0\n  Total PE              0\n  Free PE               0\n  Allocated PE          0\n  PV UUID               0FIuq2-LBod-IOWt-8VeN-tglm-Q2ik-rGU2w7\n$\nThe pvdisplay command shows that /dev/sdb1 is now tagged as a PV. Notice, however, \nthat in the output, the \nVG Name is blank. The PV does not yet belong to a volume group.\nCreating Volume Groups\nThe next step in the process is to create one or more volume groups from the physical \nvolumes. There are no set rules for how many volume groups you need to create for your \nsystem — you can add all the available physical volumes to a single volume group, or you \ncan create multiple volume groups by combining different physical volumes.\nTo create the volume group from the command line, you need to use the \nvgcreate \ncommand. The \nvgcreate command requires a few command line parameters to defi ne \nthe volume group name, as well as the name of the physical volumes you’re using to \ncreate the volume group:\n$ sudo vgcreate Vol1 /dev/sdb1\n  Volume group \"Vol1\" successfully created\n$\nThat’s not all too exciting for output! If you’d like to see some details about the newly cre-\nated volume group, use the \nvgdisplay command:\n$ sudo vgdisplay Vol1\n  --- Volume group ---\n  VG Name               Vol1\n  System ID\n  Format                lvm2\n  Metadata Areas        1\n  Metadata Sequence No  1\n  VG Access             read/write\n  VG Status             resizable\n  MAX LV                0\n  Cur LV                0\n  Open LV               0\n  Max PV                0\n  Cur PV                1\n  Act PV                1\n  VG Size               2.00 GiB\n  PE Size               4.00 MiB\n\n206\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  206\n  Total PE              513\n  Alloc PE / Size       0 / 0\n  Free  PE / Size       513 / 2.00 GiB\n  VG UUID               oe4I7e-5RA9-G9ti-ANoI-QKLz-qkX4-58Wj6e\n$\nThis example creates a volume group named Vol1, using the physical volume created on \nthe \n/dev/sdb1 partition.\nNow that you have one or more volume groups created, you’re ready to create the logical volume.\nCreating Logical Volumes\nThe logical volume is what the Linux system uses to emulate a physical partition, and it \nholds the fi lesystem. The Linux system handles the logical volumes just like a physical \npartition, allowing you to defi ne fi lesystems in the logical volume and then mount the \nfi lesystem into the virtual directory.\nTo create the logical volume, use the \nlvcreate command. Although you can usually get \naway without using command line options in the other Linux LVM commands, the \nlvcreate command requires at least some options to be entered. Table 8-5 shows the \navailable command line options.\nTABLE 8-5    The  lvcreate  Options\nOptionLong Option NameDescription\n-c       --chunksize\nSpecifi es the chunksize of the snapshot logical volume\n-C       --contiguous\nSets or resets the contiguous allocation policy\n-i       --stripes\nSpecifi es the number of stripes\n-I       --stripsize\nSpecifi es the size of each stripe\n-l       --extents\nSpecifi es the number of logical extents to allocate \nto a new logical volume or the percent of the logical \nextents to use\n-L       --size\nSpecifi es the disk size to allocate to a new logical volume\n--minor\nSpecifi es the minor number of the device\n-m       --mirrors\nCreates a mirrored logical volume\n-M       --persistent\nMakes the minor number persistent\n-n       --name\nSpecifi es the name of the new logical volume\n-p       --permission\nSets read/write permission for the logical volume\n-r       --readahead\nSets the read ahead sector count\n\n207\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  207\n-R       --regionsize\nSpecifi es the size to divide the mirror regions into\n-s       --snapshot\nCreates a snapshot logical volume\n-Z       --zero\nSets the fi rst 1KB of data on the new logical volume to \nzeros\nAlthough the command line options may look intimidating, for most situations, you can get \nby with a minimal amount of options:\n$ sudo lvcreate -l 100%FREE -n lvtest Vol1\n  Logical volume \"lvtest\" created\n$\nIf you want to see the details of what you created, use the lvdisplay command:\n$ sudo lvdisplay Vol1\n  --- Logical volume ---\n  LV Path                /dev/Vol1/lvtest\n  LV Name                lvtest\n  VG Name                Vol1\n  LV UUID                4W2369-pLXy-jWmb-lIFN-SMNX-xZnN-3KN208\n  LV Write Access        read/write\n  LV Creation host, time ... -0400\n  LV Status              available\n  # open                 0\n  LV Size                2.00 GiB\n  Current LE             513\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     256\n  Block device           253:2\n$\nNow you can see just what you created! Notice that the volume group name (Vol1) is used \nto identify the volume group to use when creating the new logical volume.\nThe \n-l parameter defi nes how much of the available space on the volume group specifi ed to \nuse for the logical volume. Notice that you can specify the value as a percent of the free space \nin the volume group. This example used all (100%) of the free space for the new logical volume.\nYou can use the \n-l parameter to specify the size as a percentage of the available space \nor the \n-L parameter to specify the actual size in bytes, kilobytes (KB), megabytes (MB), \nor gigabytes (GB). The \n-n parameter allows you to provide a name for the logical volume \n(called \nlvtest in this example).\n\n208\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  208\nCreating the Filesystem\nAfter you run the lvcreate command, the logical volume exists but doesn’t have a fi lesys-\ntem. To do that, you need to use the appropriate command line program for the fi lesystem \nyou want to create:\n$ sudo mkfs.ext4 /dev/Vol1/lvtest\nmke2fs 1.41.12 (17-May-2010)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n131376 inodes, 525312 blocks\n26265 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=541065216\n17 block groups\n32768 blocks per group, 32768 fragments per group\n7728 inodes per group\nSuperblock backups stored on blocks:\n        32768, 98304, 163840, 229376, 294912\nWriting inode tables: done\nCreating journal (16384 blocks): done\nWriting superblocks and filesystem accounting information: done\nThis filesystem will be automatically checked every 28 mounts or\n180 days, whichever comes first.Use tune2fs -c or -i to override.\n$\nAfter you’ve created the new fi lesystem, you can mount the volume in the virtual directory \nusing the standard Linux mount command, just as if it were a physical partition. The only \ndifference is that you use a special path that identifi es the logical volume:\n$ sudo mount /dev/Vol1/lvtest /mnt/my_partition\n$\n$ mount\n/dev/mapper/vg_server01-lv_root on / type ext4 (rw)\n[...]\n/dev/mapper/Vol1-lvtest on /mnt/my_partition type ext4 (rw)\n$\n$ cd /mnt/my_partition\n$\n$ ls -al\ntotal 24\ndrwxr-xr-x. 3 root root  4096 Jun 12 10:22 .\ndrwxr-xr-x. 3 root root  4096 Jun 11 09:58 ..\ndrwx------. 2 root root 16384 Jun 12 10:22 lost+found\n$\n\n209\nChapter 8: Managing Filesystems\n8\nc08.indd  12/10/2014  Page  209\nNotice that the path used in both the mkfs.ext4 and mount commands is a little odd. \nInstead of a physical partition path, the path uses the volume group name, along with the \nlogical volume name. After the fi lesystem is mounted, you can access the new area in the \nvirtual directory.\nModifying the LVM\nBecause the benefi t of using the Linux LVM is to dynamically modify fi lesystems, you’d \nexpect that some tools would allow you to do that. Some tools are available in Linux that \nallow you to modify the existing logical volume management confi guration.\nIf you don’t have access to a fancy graphical interface for managing your Linux LVM envi-\nronment, all is not lost. You’ve already seen some of the Linux LVM command line programs \nin action in this chapter. You can use a host of other command line programs to manage \nthe LVM setup after you’ve installed it. Table 8-6 lists the common commands that are \navailable in the Linux LVM package.\nTABLE 8- 6    The Linux LVM Commands\nCommandFunction\nvgchange\nActivates and deactivates a volume group\nvgremove\nRemoves a volume group\nvgextend\nAdds physical volumes to a volume group\nvgreduce\nRemoves physical volumes from a volume group\nlvextend\nIncreases the size of a logical volume\nlvreduce\nDecreases the size of a logical volume\nUsing these command line programs, you have full control over your Linux LVM \nenvironment.\nBe careful when manually increasing or decreasing the size of a logical volume. The fi lesystem stored in the logical \nvolume must be manually fi xed to handle the change in size. Most fi lesystems include command line programs for \nreformatting the fi lesystem, such as the \nresize2fs program for the ext2, ext3, and ext4 fi lesystems.\n\n210\nPart I: The Linux Command Line\nc08.indd  12/10/2014  Page  210\nSummary\nWorking with storage devices in Linux requires that you know a little bit about fi lesys-\ntems. Knowing how to create and work with fi lesystems from the command line can come \nin handy as you work on Linux systems. This chapter discussed how to handle fi lesystems \nfrom the Linux command line.\nThe Linux system is different from Windows in that it supports lots of different methods \nfor storing fi les and folders. Each fi lesystem method has different features that make it \nideal for different situations. Also, each fi lesystem method uses different commands for \ninteracting with the storage device.\nBefore you can install a fi lesystem on a storage device, you must fi rst prepare the \ndevice. The \nfdisk  command is used to partition storage devices to get them ready for \nthe fi lesystem. When you partition the storage device, you must defi ne what type of \nfi lesystem will be used on it.\nAfter you partition a storage device, you can use one of several different fi lesystems for the \npartition. Popular Linux fi lesystems include ext4 and XFS. Both of these fi lesystems provide \njournaling fi lesystem features, making them less prone to errors and problems if the Linux \nsystem should crash.\nOne limiting factor to creating fi lesystems directly on a storage device partition is that you \ncan’t easily change the size of the fi lesystem if you run out of disk space. However, Linux \nsupports logical volume management, a method of creating virtual partitions across multi-\nple storage devices. This method allows you to easily expand an existing fi lesystem without \nhaving to completely rebuild it. The Linux LVM package provides command line commands \nto create logical volumes across multiple storage devices on which to build fi lesystems.\nNow that you’ve seen the core Linux command line commands, it’s close to the time to start \ncreating some shell script programs. However, before you start coding, we need to discuss \nanother element: installing software. If you plan to write shell scripts, you need an envi-\nronment in which to create your masterpieces. The next chapter discusses how to install \nand manage software packages from the command line in different Linux environments. \n\n211\nc09.indd  12/23/2014  Page  211\nCHAPTER \n9\nInstalling Software\nIN THIS CHAPTER\nInstalling software\nUsing Debian packages\nWorking with Red Hat packages\nI\nn the old days of Linux, installing software could be a painful experience. Fortunately, the \nLinux developers have made life a little easier for us by bundling software into pre-built pack-\nages that are much easier to install. However, you still have a little work to do to get the soft-\nware packages installed, especially if you want to do that from the command line. This chapter \nlooks at the various Package Management Systems available in Linux and the command line tools \nused for software installation, management, and removal.\nPackage Management Primer\nBefore diving into the world of Linux software package management, this chapter goes through \na few of the basics fi rst. Each of the major Linux distributions utilizes some form of a Package \nManagement System (PMS) to control installing software applications and libraries. A PMS utilizes \na database that keeps track of these items:\n ■\nWhat software packages are installed on the Linux system\n ■\nWhat fi les have been installed for each package\n ■\nVersions of each of the software packages installed\nSoftware packages are stored on servers, called repositories, and are accessed across the Internet via \nPMS utilities running on your local Linux system. You can use the PMS utilities to search for new \nsoftware packages or even updates to software packages already installed on the system.\nA software package often has dependencies or other packages that must be installed fi rst for the \nsoftware to run properly. The PMS utilities detect these dependencies and offer to install any addi-\ntionally needed software packages before installing the desired package.\n\n212\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  212\nThe downside to PMS is that there isn’t a single standard utility. Whereas all the bash shell \ncommands discussed so far in this book work no matter which Linux distribution you use, \nthis is not true with software package management.\nThe PMS utilities and their associated commands are vastly different between the various \nLinux distributions. The two primary PMS base utilities commonly used in the Linux world \nare \ndpkg and rpm.\nDebian-based distributions such as Ubuntu and Linux Mint use, at the base of their PMS \nutilities, the \ndpkg command. This command interacts directly with the PMS on the Linux \nsystem and is used for installing, managing, and removing software packages.\nThe Red Hat–based distributions, such as Fedora, openSUSE, and Mandriva, use the \nrpm \ncommand at the base of their PMS. Similar to the \ndpkg command, the rpm command can \nlist installed packages, install new packages, and remove existing software.\nNote that these two commands are the core of their respective PMS, not the entire PMS \nitself. Many Linux distributions that use the \ndpkg or rpm methods have built additional \nspecialty PMS utilities upon these base commands to help make your life much easier. The \nfollowing sections walk through various PMS utility commands you’ll run into in the popu-\nlar Linux distributions.\nThe Debian-Based Systems\nThe dpkg command is at the core of the Debian-based family of PMS tools. These other \ntools are included in this PMS:\n ■\napt-get\n ■\napt-cache\n ■\naptitude\nBy far the most common command line tool is aptitude, and for good reason. The aptitude \ntool is essentially a front-end for both the apt tools and \ndpkg. Whereas dpkg is a PMS tool, \naptitude is a complete Package Management System.\nUsing the \naptitude command at the command line helps you avoid common software \ninstallation problems, such as missing software dependencies, unstable system environ-\nments, and just a whole lot of unnecessary hassle. This section looks at how to use the \naptitude command tool from the Linux command line.\nManaging packages with aptitude\nA common task faced by Linux system administrators is to determine what packages are \nalready installed on the system. Fortunately, aptitude has a handy interactive interface \nthat makes this task an easy one.\n\n213\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  213\n9\n9\nIf you have aptitude installed in your Linux distribution, at the shell prompt just type \naptitude and press Enter. You are thrown into aptitude’s full-screen mode, as you can see \nin Figure 9-1.\nFIGURE 9.1\nThe aptitude main window\nUse the arrow keys to maneuver around the menu. Select the menu option Installed \nPackages to see what packages are installed. You will see several groups of software pack-\nages, such as editors, and so on. A number in parentheses follows each group, which indi-\ncates the number of packages the group contains.\nUse the arrow keys to highlight a group, and press Enter to see each subgroup of packages. \nYou then see the individual package names and their version numbers. Press Enter on indi-\nvidual packages to get very detailed information, such as the package’s description, home \npage, size, maintainer, and so on.\nWhen you’re fi nished viewing the installed packages, press \nq to quit the display. You can \nthen go back to the arrow keys. and use Enter to toggle open or closed the packages and \ntheir subgroups. When you are all fi nished, just press \nq multiple times until you receive the \npop-up screen “Really quit Aptitude?”\nIf you already know the packages on your system and want to quickly display detailed \ninformation about a particular package, you don’t need to go into aptitude’s interactive \ninterface. You can use aptitude as a single command at the command line:\naptitude show package_name\n\n214\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  214\nHere’s an example of displaying the details of the package mysql-client:\n$ aptitude show mysql-client\nPackage: mysql-client             \nState: not installed\nVersion: 5.5.38-0ubuntu0.14.04.1\nPriority: optional\nSection: database\nMaintainer: Ubuntu Developers <ubuntu-devel-discuss@lists.ubuntu.com>\nArchitecture: all\nUncompressed Size: 129 k\nDepends: mysql-client-5.5\nProvided by: mysql-client-5.5\nDescription: MySQL database client (metapackage depending on the latest version)\n This is an empty package that depends on the current \"best\" version of\n mysql-client (currently mysql-client-5.5), as determined by the MySQL\n maintainers.  Install this package if in doubt about which MySQL version you\n want, as this is the one considered to be in the best shape by the Maintainers.\nHomepage: http://dev.mysql.com/\n$\nThe aptitude show command indicates that the package is not installed on the system. It also shows detailed \npackage information from the software repository.\nOne detail you cannot get with aptitude is a listing of all the fi les associated with a par-\nticular software package. To get this list, you must go to the \ndpkg tool itself:\ndpkg -L package_name\nHere’s an example of using dpkg to list all the fi les installed as part of the vim-common \npackage:\n$\n$ dpkg -L vim-common\n/.\n/usr\n/usr/bin\n/usr/bin/xxd\n/usr/bin/helpztags\n/usr/lib\n/usr/lib/mime\n/usr/lib/mime/packages\n/usr/lib/mime/packages/vim-common\n/usr/share\n/usr/share/man\n/usr/share/man/ru\n\n215\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  215\n9\n/usr/share/man/ru/man1\n/usr/share/man/ru/man1/vim.1.gz\n/usr/share/man/ru/man1/vimdiff.1.gz\n/usr/share/man/ru/man1/xxd.1.gz\n/usr/share/man/it\n/usr/share/man/it/man1\n[...]\n$\nYou can also do the reverse — fi nd what package a particular fi le belongs to:\ndpkg --search absolute_file_name\nNote that you need to use an absolute fi le reference for this to work:\n$\n$ dpkg --search /usr/bin/xxd\nvim-common: /usr/bin/xxd\n$\nThe output shows the /usr/bin/xxd fi le was installed as part of the vim-common \npackage.\nInstalling software packages with aptitude\nNow that you know more about listing software package information on your system, this \nsection walks through a software package install. First, you’ll want to determine the pack-\nage name to install. How do you fi nd a particular software package? Use the \naptitude \ncommand with the search option:\naptitude search package_name\nThe beauty of the search option is that you do not need to insert wildcards around \npackage_name. Wildcards are implied. Here’s an example of using \naptitude to look \nfor the wine software package:\n$\n$ aptitude search wine\np   gnome-wine-icon-theme           - red variation of the GNOME- ...\nv   libkwineffects1-api             - \np   libkwineffects1a                - library used by effects...\np   q4wine                          - Qt4 GUI for wine (W.I.N.E) \np   shiki-wine-theme                - red variation of the Shiki- ...\np   wine                            - Microsoft Windows Compatibility ...\np   wine-dev                        - Microsoft Windows Compatibility ...\np   wine-gecko                      - Microsoft Windows Compatibility ...\np   wine1.0                         - Microsoft Windows Compatibility ...\np   wine1.0-dev                     - Microsoft Windows Compatibility ...\n\n216\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  216\np   wine1.0-gecko                   - Microsoft Windows Compatibility ...\np   wine1.2                         - Microsoft Windows Compatibility ...\np   wine1.2-dbg                     - Microsoft Windows Compatibility ...\np   wine1.2-dev                     - Microsoft Windows Compatibility ...\np   wine1.2-gecko                   - Microsoft Windows Compatibility ...\np   winefish                        - LaTeX Editor based on Bluefish\n$\nNotice that before each package name is either a p or i. If you see an i u, the package is \ncurrently installed on your system. If you see a \np or v, it is available but not installed. As \nyou can see from the preceding listing, this system does not have wine currently installed, \nbut the package is available from the software repository.\nInstalling a software package on a system from a repository using \naptitude is as easy as \nthis:\naptitude install package_name\nAfter you fi nd the software package name from the search option, just plug it into the \naptitude command using the install option:\n$\n$ sudo aptitude install wine\nThe following NEW packages will be installed:\n  cabextract{a} esound-clients{a} esound-common{a} gnome-exe-thumbnailer\n{a}\n  icoutils{a} imagemagick{a} libaudio2{a} libaudiofile0{a} libcdt4{a}\n  libesd0{a} libgraph4{a} libgvc5{a} libilmbase6{a} libmagickcore3-extra\n{a}\n  libmpg123-0{a} libnetpbm10{a} libopenal1{a} libopenexr6{a}\n  libpathplan4{a} libxdot4{a} netpbm{a} ttf-mscorefonts-installer{a}\n  ttf-symbol-replacement{a} winbind{a} wine wine1.2{a} wine1.2-gecko{a}\n0 packages upgraded, 27 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 0B/27.6MB of archives. After unpacking 121MB will be used.\nDo you want to continue? [Y/n/?] Y\nPreconfiguring packages ...              \n[...]\nAll done, no errors.\nAll fonts downloaded and installed.\nUpdating fontconfig cache for /usr/share/fonts/truetype/msttcorefonts\nSetting up winbind (2:3.5.4~dfsg-1ubuntu7) ...\n * Starting the Winbind daemon winbind                                  \n [ OK ]\nSetting up wine (1.2-0ubuntu5) ...\nSetting up gnome-exe-thumbnailer (0.6-0ubuntu1) ...\nProcessing triggers for libc-bin ...\nldconfig deferred processing now taking place\n                                         \n$\n\n217\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  217\n9\nBefore the aptitude command in the preceding listing, the sudo command is used. The sudo command allows \nyou to run a command as the root user. You can use the \nsudo command to run administrative tasks, such as install-\ning software. \nTo check if the installation processed properly, just use the search option again. This time \nyou should see an \ni u listed in front of the wine software package, indicating it is installed.\nYou may also notice that there are additional packages with the \ni u in front of them. This \nis because \naptitude automatically resolved any necessary package dependencies for us \nand installs the needed additional library and software packages. This is a wonderful fea-\nture included in many Package Management Systems.\nUpdating software with aptitude\nWhile aptitude helps protect you from problems installing software, trying to coordinate a \nmultiple-package update with dependencies can get tricky. To safely update all the software \npackages on a system with any new versions in the repository, use the \nsafe-upgrade option:\naptitude safe-upgrade\nNotice that this command doesn’t take a software package name as an argument. That’s \nbecause the \nsafe-upgrade option upgrades all the installed packages to the most recent \nversion available in the repository, which is safer for system stabilization.\nHere’s a sample output from running the \naptitude safe-update command:\n$ \n$ sudo aptitude safe-upgrade\nThe following packages will be upgraded:\n  evolution evolution-common evolution-plugins gsfonts libevolution\n  xserver-xorg-video-geode\n6 packages upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 9,312kB of archives. After unpacking 0B will be used.\nDo you want to continue? [Y/n/?] Y\nGet:1 http://us.archive.ubuntu.com/ubuntu/ maverick/main\n libevolution i386 2.30.3-1ubuntu4 [2,096kB]\n[...]\nPreparing to replace xserver-xorg-video-geode 2.11.9-2 \n(using .../xserver-xorg-video-geode_2.11.9-3_i386.deb) ...\nUnpacking replacement xserver-xorg-video-geode ...\nProcessing triggers for man-db ...\nProcessing triggers for desktop-file-utils ...\nProcessing triggers for python-gmenu ...\n[...]\nCurrent status: 0 updates [-6].\n$\n\n218\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  218\nYou can also use less-conservative options for software upgrades:\n ■\naptitude full-upgrade\n ■\naptitude dist-upgrade\nThese options perform the same task, upgrading all the software packages to the latest ver-\nsions. Where they differ from \nsafe-upgrade is that they do not check dependencies between \npackages. The whole package dependency issue can get real ugly. If you’re not exactly sure of \nthe dependencies for the various packages, stick with the \nsafe-upgrade option.\nObviously, running aptitude’s safe-upgrade option is something you should do on a regular basis to keep your \nsystem up to date. However, it is especially important to run it after a fresh distribution installation. Usually, lots of \nsecurity patches and updates have been released since the last full release of a distribution. \nUninstalling software with aptitude\nGetting rid of software packages with aptitude is as easy as installing and upgrading them. \nThe only real choice you have to make is whether to keep the software’s data and confi gura-\ntion fi les around afterward.\nTo remove a software package, but not the data and confi guration fi les, use the \nremove \noption of aptitude. To remove a software package and the related data and confi guration \nfi les, use the \npurge option:\n$ sudo aptitude purge wine\n[sudo] password for user:\nThe following packages will be REMOVED:  \n  cabextract{u} esound-clients{u} esound-common{u} gnome-exe-thumbnailer\n{u}\n  icoutils{u} imagemagick{u} libaudio2{u} libaudiofile0{u} libcdt4{u}\n  libesd0{u} libgraph4{u} libgvc5{u} libilmbase6{u} libmagickcore3-extra\n{u}\n  libmpg123-0{u} libnetpbm10{u} libopenal1{u} libopenexr6{u}\n  libpathplan4{u} libxdot4{u} netpbm{u} ttf-mscorefonts-installer{u}\n  ttf-symbol-replacement{u} winbind{u} wine{p} wine1.2{u} wine1.2-gecko\n{u}\n0 packages upgraded, 0 newly installed, 27 to remove and 6 not upgraded.\nNeed to get 0B of archives. After unpacking 121MB will be freed.\nDo you want to continue? [Y/n/?] Y\n(Reading database ... 120968 files and directories currently installed.)\nRemoving ttf-mscorefonts-installer ...\n[...]\nProcessing triggers for fontconfig ...\nProcessing triggers for ureadahead ...\nProcessing triggers for python-support ...\n$\n\n219\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  219\n9\nTo see if the package has been removed, you can use the aptitude search option again. \nIf you see a \nc in front of the package name, it means the software has been removed, but \nthe confi guration fi les have not been purged from the system. A \np in front indicates the \nconfi guration fi les have also been removed.\nThe aptitude repositories\nThe default software repository locations for aptitude are set up for you when you \ninstall your Linux distribution. The repository locations are stored in the fi le \n/etc/apt/\nsources.list\n.\nIn many cases, you never need to add/remove a software repository so you don’t need to \ntouch this fi le. However, \naptitude pulls software from only these repositories. Also, when \nsearching for software to install or update, aptitude checks only these repositories. If you \nneed to include some additional software repositories for your PMS, this is the place to do it.\nThe Linux distribution developers work hard to make sure package versions added to the repositories don’t confl ict \nwith one another. Usually it’s safest to upgrade or install a software package from the repository. Even if a newer \nversion is available elsewhere, you may want to hold off installing it until that version is available in your Linux distri-\nbution’s repository. \nThe following is an example of a sources.list fi le from an Ubuntu system:\n$ cat /etc/apt/sources.list\n#deb cdrom:[Ubuntu 14.04 LTS _Trusty Tahr_ - Release i386 (20140417)]/\n trusty main restricted\n# See http://help.ubuntu.com/community/UpgradeNotes for how to upgrade to\n# newer versions of the distribution.\ndeb http://us.archive.ubuntu.com/ubuntu/ trusty main restricted\ndeb-src http://us.archive.ubuntu.com/ubuntu/ trusty main restricted\n## Major bug fix updates produced after the final release of the\n## distribution.\ndeb http://us.archive.ubuntu.com/ubuntu/ trusty-updates main restricted\ndeb-src http://us.archive.ubuntu.com/ubuntu/ trusty-updates main restricted\n## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu\n## team. Also, please note that software in universe WILL NOT receive any\n## review or updates from the Ubuntu security team.\ndeb http://us.archive.ubuntu.com/ubuntu/ trusty universe\ndeb-src http://us.archive.ubuntu.com/ubuntu/ trusty universe\ndeb http://us.archive.ubuntu.com/ubuntu/ trusty-updates universe\n\n220\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  220\ndeb-src http://us.archive.ubuntu.com/ubuntu/ trusty-updates universe\n[...]\n## Uncomment the following two lines to add software from Canonical's\n## 'partner' repository.\n## This software is not part of Ubuntu, but is offered by Canonical and the\n## respective vendors as a service to Ubuntu users.\n# deb http://archive.canonical.com/ubuntu trusty partner\n# deb-src http://archive.canonical.com/ubuntu trusty partner\n## This software is not part of Ubuntu, but is offered by third-party\n## developers who want to ship their latest software.\ndeb http://extras.ubuntu.com/ubuntu trusty main\ndeb-src http://extras.ubuntu.com/ubuntu trusty main\n$\nFirst, notice that the fi le is full of helpful comments and warnings. The repository sources \nspecifi ed use the following structure:\ndeb (or deb-src) address  distribution_name  package_type_list\nThe deb or deb-src value indicates the software package type. The deb value indicates \nit is a source of compiled programs, whereas the \ndeb-src value indicates it is a source of \nsource code.\nThe \naddress entry is the software repository’s web address. The distribution_name entry \nis the name of this particular software repository’s distribution’s version. In the example, the \ndistribution name is \ntrusty. This does not necessarily mean that the distribution you are \nrunning is Ubuntu’s Trusty Tahr; it just means the Linux distribution is using the Ubuntu \nTrusty Tahr software repositories! For example, in Linux Mint’s \nsources.list fi le, you see a \nmix of Linux Mint and Ubuntu software repositories.\nFinally, the \npackage_type_list entry may be more than one word and indicates what \ntype of packages the repository has in it. For example, you may see values such as main, \nrestricted, universe, or partner.\nWhen you need to add a software repository to your sources fi le, you can try to wing it \nyourself, but that more than likely will cause problems. Often, software repository sites \nor various package developer sites have an exact line of text that you can copy from their \nwebsite and paste into your \nsources.list fi le. It’s best to choose the safer route and \njust copy/paste.\nThe front-end interface, \naptitude, provides intelligent command line options for working \nwith the Debian-based \ndpkg utility. Now it’s time to look at the Red Hat–based distribu-\ntions’ \nrpm utility and its various front-end interfaces.\n\n221\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  221\n9\nThe Red Hat–Based Systems\nLike the Debian-based distributions, the Red Hat–based systems have several different \nfront-end tools that are available. These are the common ones:\n ■\nyum: Used in Red Hat and Fedora\n ■\nurpm: Used in Mandriva\n ■\nzypper: Used in openSUSE\nThese front-ends are all based on the \nrpm command line tool. The following section dis-\ncusses how to manage software packages using these various rpm-based tools. The focus is \non \nyum, but information is also included for zypper and urpm.\nListing installed packages\nTo fi nd out what is currently installed on your system, at the shell prompt, type the follow-\ning command:\nyum list installed\nThe information will probably whiz by you on the display screen, so it’s best to redirect the \ninstalled software listing into a fi le. You can then use the \nmore or less command (or a GUI \neditor) to look at the list in a controlled manner.\nyum list installed > installed_software\nTo list out the installed packages on your openSUSE or Mandriva distribution, see the com-\nmands in Table 9-1. Unfortunately, the \nurpm tool used in Mandriva cannot produce a cur-\nrently installed software listing. Thus, you need to revert to the underlying \nrpm tool.\nTABLE 9-1    How to List Installed Software with zypper and urpm\nDistributionFront-End ToolCommand\nMandriva\nurpmrpm -qa > installed_software\nopenSUSE\nzypperzipper search -I > installed_software\nTo fi nd out detailed information for a particular software package, yum really shines. It \ngives you a very verbose description of the package, and with another simple command, \nyou can see whether the package is installed:\n# yum list xterm \nLoaded plugins: langpacks, presto, refresh-packagekit \nAdding en_US to language list \nAvailable Packages \n\n222\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  222\nxterm.i686 253-1.el6 \n# \n# yum list installed xterm \nLoaded plugins: refresh-packagekit \nError: No matching Packages to list \n#\nThe commands to list detailed software package information using urpm and zypper are \nin Table 9-2. You can acquire an even more detailed set of package information from the \nrepository, using the \ninfo option on the zypper command.\nTABLE 9-2    How to See Various Package Details with zypper and urpm\nDetail TypeFront-End ToolCommand\nPackage Information\nurpmurpmq -i package_name\nInstalled?\nurpmrpm -q package_name\nPackage Information\nzypperzypper search -s package_name\nInstalled?\nzypper\nSame command, but look for an i in the \nStatus column\nFinally, if you need to fi nd out what software package provides a particular fi le on your \nfi lesystem, the versatile \nyum can do that, too! Just enter the command:\nyum provides file_name\nHere’s an example of trying to fi nd what software provided the confi guration fi le /etc/\nyum.conf\n:\n# \n# yum provides /etc/yum.conf\nLoaded plugins: fastestmirror, refresh-packagekit, security\nDetermining fastest mirrors\n * base: mirror.web-ster.com\n * extras: centos.chi.host-engine.com\n * updates: mirror.umd.edu\nyum-3.2.29-40.el6.centos.noarch : RPM package installer/updater/manager\nRepo        : base\nMatched from:\nFilename    : /etc/yum.conf\nyum-3.2.29-43.el6.centos.noarch : RPM package installer/updater/manager\nRepo        : updates\nMatched from:\n\n223\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  223\n9\nFilename    : /etc/yum.conf\nyum-3.2.29-40.el6.centos.noarch : RPM package installer/updater/manager\nRepo        : installed\nMatched from:\nOther       : Provides-match: /etc/yum.conf\n#\n#\nyum checked three separate repositories: base, updates, and installed. From both, the \nanswer is: the \nyum software package provides this fi le!\nInstalling software with yum\nInstallation of a software package using yum is incredibly easy. The following is the basic \ncommand for installing a software package, all its needed libraries, and package dependen-\ncies from a repository:\nyum install package_name\nHere’s an example of installing the xterm package that we talked about in Chapter 2:\n$ su -\nPassword: \n# yum install xterm \nLoaded plugins: fastestmirror, refresh-packagekit, security\nDetermining fastest mirrors\n * base: mirrors.bluehost.com\n * extras: mirror.5ninesolutions.com\n * updates: mirror.san.fastserv.com\nSetting up Install Process\nResolving Dependencies\n--> Running transaction check\n---> Package xterm.i686 0:253-1.el6 will be installed\n--> Finished Dependency Resolution\nDependencies Resolved\n[...]\nInstalled:\n  xterm.i686 0:253-1.el6 \nComplete!\n#\n\n224\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  224\nBefore the yum command in the preceding listing, the su - command is used. This command allows you to switch \nto the root user. On this Linux system, the # denotes you are logged in as root. You should only switch to root user \ntemporarily in order to run administrative tasks, such as installing and updating software. The \nsudo command is \nanother option as well.\nYou can also manually download an rpm installation fi le and install it using yum. This is \ncalled a local installation. This is the basic command:\nyum localinstall package_name.rpm\nYou can begin to see that one of yum’s strengths is that it uses very logical and user-\nfriendly commands.\nTable 9-3 shows how to perform a package install with \nurpm and zypper. You should note that \nif you are not logged in as root, you get a “command not found” error message using \nurpm.\nTABLE 9-3    How to Install Software with zypper and urpm\nFront-End ToolCommand\nurpmurpmi package_name\nzypperzypper install package_name\nUpdating software with yum\nIn most Linux distributions, when you’re working away in the GUI, you get those nice little \nnotifi cation icons telling you that an update is needed. Here at the command line, it takes \na little more work.\nTo see the list of all the available updates for your installed packages, type the following \ncommand:\nyum list updates\nIt’s always nice to get no response to this command because it means you have nothing to \nupdate! However, if you do discover a particular software package needs updating, type the \nfollowing command:\nyum update package_name\nIf you’d like to update all the packages listed in the update list, just enter the following \ncommand:\nyum update\n\n225\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  225\n9\nCommands for updating software packages on Mandriva and openSUSE are listed in \nTable 9-4. When urpm is used, the repository database is automatically refreshed as well as \nsoftware packages updated.\nTABLE 9- 4    How to Update Software with zypper and urpm\nFront-End ToolCommand\nurpmurpmi --auto-update --update\nzypperzypper update\nUninstalling software with yum\nThe yum tool also provides an easy way to uninstall software you no longer want on your \nsystem. As with \naptitude, you need to choose whether to keep the software package’s \ndata and confi guration fi les.\nTo just remove the software package and keep any confi guration and data fi les, use the fol-\nlowing command:\nyum remove package_name\nTo uninstall the software and all its fi les, use the erase option:\nyum erase package_name\nIt is equally easy to remove software using urpm and zypper in Table 9-5. Both of these \ntools perform a function similar to \nyum’s erase option.\nTABLE 9-5    How to Uninstall Software with zypper and urpm\nFront-End ToolCommand\nurpmurpme package_name\nzypperzypper remove package_name\nAlthough life is considerably easier with PMS packages, it’s not always problem-free. \nOccasionally, things do go wrong. Fortunately, there’s help.\nDealing with broken dependencies\nSometimes, as multiple software packages get loaded, a software dependency for one pack-\nage can get overwritten by the installation of another package. This is called a broken \ndependency.\n\n226\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  226\nIf this should happen on your system, fi rst try the following command:\nyum clean all\nThen try to use the update option in the yum command. Sometimes, just cleaning up any \nmisplaced fi les can help.\nIf that doesn’t solve the problem, try the following command:\nyum deplist package_name\nThis command displays all the package’s library dependencies and what software package \nprovides them. After you know the libraries required for a package, you can then install \nthem. Here’s an example of determining the dependencies for the xterm package:\n# yum deplist xterm \nLoaded plugins: fastestmirror, refresh-packagekit, security\nLoading mirror speeds from cached hostfile\n * base: mirrors.bluehost.com\n * extras: mirror.5ninesolutions.com\n * updates: mirror.san.fastserv.com\nFinding dependencies: \npackage: xterm.i686 253-1.el6\n  dependency: libncurses.so.5\n   provider: ncurses-libs.i686 5.7-3.20090208.el6\n  dependency: libfontconfig.so.1\n   provider: fontconfig.i686 2.8.0-3.el6\n  dependency: libXft.so.2\n   provider: libXft.i686 2.3.1-2.el6\n  dependency: libXt.so.6\n   provider: libXt.i686 1.1.3-1.el6\n  dependency: libX11.so.6\n   provider: libX11.i686 1.5.0-4.el6\n  dependency: rtld(GNU_HASH)\n   provider: glibc.i686 2.12-1.132.el6\n   provider: glibc.i686 2.12-1.132.el6_5.1\n   provider: glibc.i686 2.12-1.132.el6_5.2\n  dependency: libICE.so.6\n   provider: libICE.i686 1.0.6-1.el6\n  dependency: libXaw.so.7\n   provider: libXaw.i686 1.0.11-2.el6\n  dependency: libtinfo.so.5\n   provider: ncurses-libs.i686 5.7-3.20090208.el6\n  dependency: libutempter.so.0\n   provider: libutempter.i686 1.1.5-4.1.el6\n  dependency: /bin/sh\n   provider: bash.i686 4.1.2-15.el6_4\n  dependency: libc.so.6(GLIBC_2.4)\n\n227\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  227\n9\n   provider: glibc.i686 2.12-1.132.el6\n   provider: glibc.i686 2.12-1.132.el6_5.1\n   provider: glibc.i686 2.12-1.132.el6_5.2\n  dependency: libXmu.so.6\n   provider: libXmu.i686 1.1.1-2.el6\n#\nIf that doesn’t solve your problem, you have one last tool:\nyum update --skip-broken\nThe --skip-broken option allows you to just ignore the package with the broken depen-\ndency and update the other software packages. This may not help the broken package, but \nat least you can update the remaining packages on the system!\nIn Table 9-6, the commands to try for broken dependencies with \nurpm and zypper are \nlisted. With \nzypper, there is only the one command to verify and fi x a broken dependency. \nWith \nurpm, if the clean option does not work, you can skip updates on the offensive pack-\nage. To do this, you must add the name of the offending package to the fi le \n/etc/urpmi/\nskip.list\n.\nTABLE 9- 6    Broken Dependencies with zypper and urpm\nFront End ToolCommand\nurpmurpmi --clean\nzypperzypper verify\nyum repositories\nJust like the aptitude systems, yum has its software repositories set up at installation. \nFor most purposes, these pre-installed repositories work just fi ne for your needs. But if and \nwhen the time comes that you need to install software from a different repository, here are \nsome things you need to know.\nA wise system administrator sticks with approved repositories. An approved repository is one that is sanctioned by \nthe distribution’s offi cial site. If you start adding unapproved repositories, you lose the guarantee of stability. And you \nwill be heading into broken dependencies territory.\nTo see what repositories you are currently pulling software from, type the following \ncommand:\nyum repolist\n\n228\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  228\nIf you don’t fi nd a repository you need software from, you need to do a little confi guration \nfi le editing. The \nyum repository defi nition fi les are located in /etc/yum.repos.d. You \nneed to add the proper URL and gain access to any necessary encryption keys.\nGood repository sites such as \nrpmfusion.org lay out all the steps necessary to use them. \nSometimes, these repository sites offer an \nrpm fi le that you can download and install using \nthe \nyum localinstall command. The installation of the rpm fi le does all the repository \nsetup work for you. Now that’s convenient!\nurpm calls its repositories media. The commands for looking at urpm media and zypper’s \nrepositories are in Table 9-7. Notice with both of these front-end tools that you do not edit \na confi guration fi le. Instead, to add media or a repository, you just type the command.\nTABLE 9-7    zypper and urpm Repositories\nActionFront-End ToolCommand\nDisplay repository\nurpmurpmq --list-media\nAdd repository\nurpmurpmi.addmedia path_name\nDisplay repository\nzypperzypper repos\nAdd repository\nzypperzypper addrepo path_name\nBoth Debian–based and Red Hat–based systems use Package Management Systems to ease \nthe process of managing software. Now we are going to step out of the world of Package \nManagement Systems and look at something a little more diffi cult: installing directly from \nsource code.\nInstalling from Source Code\nChapter 4 discussed tarball packages — how to create them using the tar command line \ncommand and how to unpack them. Before the fancy \nrpm and dpkg tools, administrators \nhad to know how to unpack and install software from tarballs.\nIf you work in the open source software environment much, there’s a good chance you will \nstill fi nd software packed up as a tarball. This section walks you through the process of \nunpacking and installing a tarball software package.\nFor this example, the software package \nsysstat is used. The sysstat utility is a very \nnice software package that provides a variety of system monitoring tools.\nFirst, you need to download the sysstat tarball to your Linux system. You can often fi nd \nthe \nsysstat package available on different Linux sites, but it’s usually best to go straight \n\n229\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  229\n9\nto the source of the program. In this case, it’s the website http://sebastien.godard\n.pagesperso-orange.fr/\n.\nIf you click the Download link, you go to the page that contains the fi les for downloading. \nThe current version at the time of this writing is 11.1.1, and the distribution fi le name is \nsysstat-11.1.1.tar.gz.\nClick the link to download the fi le to your Linux system. After you have downloaded the \nfi le, you can unpack it.\nTo unpack a software tarball, use the standard \ntar command:\n#\n# tar -zxvf sysstat-11.1.1.tar.gz\nsysstat-11.1.1/\nsysstat-11.1.1/cifsiostat.c\nsysstat-11.1.1/FAQ\nsysstat-11.1.1/ioconf.h\nsysstat-11.1.1/rd_stats.h\nsysstat-11.1.1/COPYING\nsysstat-11.1.1/common.h\nsysstat-11.1.1/sysconfig.in\nsysstat-11.1.1/mpstat.h\nsysstat-11.1.1/rndr_stats.h\n[...]\nsysstat-11.1.1/activity.c\nsysstat-11.1.1/sar.c\nsysstat-11.1.1/iostat.c\nsysstat-11.1.1/rd_sensors.c\nsysstat-11.1.1/prealloc.in\nsysstat-11.1.1/sa2.in\n# \n#\nNow that the tarball is unpacked and the fi les have neatly put themselves into a directory \ncalled \nsysstat-11.1.1, you can dive down into that directory and continue.\nFirst, use the \ncd command to get into the new directory and list the contents of the \ndirectory:\n$ cd sysstat-11.1.1\n$ ls\nactivity.c    iconfig              prealloc.in   sa.h\nbuild         INSTALL              pr_stats.c    sar.c\nCHANGES       ioconf.c             pr_stats.h    sa_wrap.c\ncifsiostat.c  ioconf.h             rd_sensors.c  sysconfig.in\ncifsiostat.h  iostat.c             rd_sensors.h  sysstat-11.1.1.lsm\ncommon.c      iostat.h             rd_stats.c    sysstat-11.1.1.spec\n\n230\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  230\ncommon.h      json_stats.c         rd_stats.h    sysstat.in\nconfigure     json_stats.h         README        sysstat.ioconf\nconfigure.in  Makefile.in          rndr_stats.c  sysstat.service.in\ncontrib       man                  rndr_stats.h  sysstat.sysconfig.in\nCOPYING       mpstat.c             sa1.in        version.in\ncount.c       mpstat.h             sa2.in        xml\ncount.h       nfsiostat-sysstat.c  sa_common.c   xml_stats.c\nCREDITS       nfsiostat-sysstat.h  sadc.c        xml_stats.h\ncron          nls                  sadf.c\nFAQ           pidstat.c            sadf.h\nformat.c      pidstat.h            sadf_misc.c\n$\nIn the listing of the directory, you should typically see a README or AAAREADME fi le. It is \nvery important to read this fi le. The actual instructions you need to fi nish the software’s \ninstallation are in this fi le.\nFollowing the advice contained in the \nREADME fi le, the next step is to configure \nsysstat for your system. This checks your Linux system to ensure it has the proper \nlibrary dependencies, in addition to the proper compiler to compile the source code:\n# ./configure\nCheck programs:\n.\nchecking for gcc... gcc\nchecking whether the C compiler works... yes\nchecking for C compiler default output file name... a.out\n[...]\nchecking for ANSI C header files... (cached) yes\nchecking for dirent.h that defines DIR... yes\nchecking for library containing opendir... none required\nchecking ctype.h usability... yes\nchecking ctype.h presence... yes\nchecking for ctype.h... yes\nchecking errno.h usability... yes\nchecking errno.h presence... yes\nchecking for errno.h... yes\n[...]\nCheck library functions:\n.\nchecking for strchr... yes\nchecking for strcspn... yes\nchecking for strspn... yes\nchecking for strstr... yes\nchecking for sensors support... yes\nchecking for sensors_get_detected_chips in -lsensors... no\nchecking for sensors lib... no\n.\n\n231\nChapter 9: Installing Software\nc09.indd  12/23/2014  Page  231\n9\nCheck system services:\n.\nchecking for special C compiler options needed for large files... no\nchecking for _FILE_OFFSET_BITS value needed for large files... 64\n.\nCheck configuration:\n[...]\nNow create files:\n[...]\nconfig.status: creating Makefile\n   Sysstat version:             11.1.1\n   Installation prefix:         /usr/local\n   rc directory:                /etc/rc.d\n   Init directory:              /etc/rc.d/init.d\n   Systemd unit dir:\n   Configuration directory:      /etc/sysconfig\n   Man pages directory:          ${datarootdir}/man\n   Compiler:               gcc\n   Compiler flags:         -g -O2\n#\nIf anything does go wrong, the configure step displays an error message explaining \nwhat’s missing. If you don’t have the GNU C compiler installed in your Linux distribution, \nyou get a single error message, but for all other issues you should see multiple messages \nindicating what’s installed and what isn’t.\nThe next stage is to build the various binary fi les using the \nmake command. The make com-\nmand compiles the source code and then the linker to create the fi nal executable fi les for \nthe package. As with the \nconfigure command, the make command produces lots of out-\nput as it goes through the steps of compiling and linking all the source code fi les:\n# make\n–gcc -o sadc.o -c -g -O2 -Wall -Wstrict-prototypes -pipe -O2 \n -DSA_DIR=\\\"/var/log/sa\\\" -DSADC_PATH=\\\"/usr/local/lib/sa/sadc\\\"  \n -DUSE_NLS -DPACKAGE=\\\"sysstat\\\" \n -DLOCALEDIR=\\\"/usr/local/share/locale\\\" sadc.c\ngcc -o act_sadc.o -c -g -O2 -Wall -Wstrict-prototypes -pipe -O2 \n -DSOURCE_SADC  -DSA_DIR=\\\"/var/log/sa\\\" \n -DSADC_PATH=\\\"/usr/local/lib/sa/sadc\\\"  \n -DUSE_NLS -DPACKAGE=\\\"sysstat\\\" \n -DLOCALEDIR=\\\"/usr/local/share/locale\\\" activity.c\n[...]\n#\nWhen make is fi nished, you have the actual sysstat software program available in the \ndirectory! However, it’s somewhat inconvenient to have to run it from that directory. \n\n232\nPart I: The Linux Command Line\nc09.indd  12/23/2014  Page  232\nInstead, you’ll want to install it in a common location on your Linux system. To do that, \nyou need to log in as the root user account (or use the \nsudo command if your Linux distri-\nbution prefers) and then use the \ninstall option of the make command:\n# make install\nmkdir -p /usr/local/share/man/man1\nmkdir -p /usr/local/share/man/man5\nmkdir -p /usr/local/share/man/man8\nrm -f /usr/local/share/man/man8/sa1.8*\ninstall -m 644 -g man man/sa1.8 /usr/local/share/man/man8\nrm -f /usr/local/share/man/man8/sa2.8*\ninstall -m 644 -g man man/sa2.8 /usr/local/share/man/man8\nrm -f /usr/local/share/man/man8/sadc.8*\n[...]\ninstall -m 644 -g man man/sadc.8 /usr/local/share/man/man8\ninstall -m 644 FAQ /usr/local/share/doc/sysstat-11.1.1\ninstall -m 644 *.lsm /usr/local/share/doc/sysstat-11.1.1\n#\nNow the sysstat package is installed on the system! Although it’s not quite as easy as \ninstalling a software package via a PMS, installing software using tarballs is not that diffi cult.\nSummary\nThis chapter discussed how to work with a Package Management Systems (PMS) to install, \nupdate, or remove software from the command line. Although most of the Linux distribu-\ntions use fancy GUI tools for software package management, you can also perform package \nmanagement from the command line.\nThe Debian-based Linux distributions use the \ndpkg utility to interface with the PMS from \nthe command line. A front-end to the \ndpkg utility is aptitude. It provides simple com-\nmand line options for working with software packages in the dpkg format.\nThe Red Hat–based Linux distributions are based on the \nrpm utility but use different front-\nend tools at the command line. Red Hat and Fedora use \nyum for installing and managing \nsoftware packages. The openSUSE distribution uses \nzypper for managing software, while \nthe Mandriva distribution uses \nurpm.\nThe chapter closed with a discussion on how to install software packages that are only dis-\ntributed in source code tarballs. The \ntar command allows you to unpack the source code \nfi les from the tarball, and \nconfigure and make  allow you to build the fi nal executable \nprogram from the source code.\nThe next chapter looks at the different editors available in Linux distributions. As you \nget ready to start working on shell scripts, it will come in handy to know what editors are \navailable to use! \n\n233\nc10.indd  12/05/2014  Page  233\nCHAPTER \n10\nWorking with Editors\nIN THIS CHAPTER\nWorking with the vim editor\nExploring nano\nUnderstanding emacs\nGetting comfortable with kwrite\nLooking at Kate\nUsing the GNOME editor\nB\nefore you can start your shell scripting career, you need to know how to use at least one text \neditor in Linux. The more you know about how to use features such as searching, cutting, \nand pasting, the quicker you can develop your shell scripts.\nYou can choose from several editors. Many individuals fi nd a particular editor whose features they \nlove and exclusively use that text editor. This chapter discusses just a few of the text editors you’ll \nsee in the Linux world.\nVisiting the vim Editor\nThe vi editor was the original editor used on Unix systems. It used the console graphics mode to \nemulate a text-editing window, allowing you to see the lines of your fi le, move around within the \nfi le, and insert, edit, and replace text.\nAlthough it was quite possibly the most complicated editor in the world (at least in the opinion of \nthose who hate it), it provides many features that have made it a staple for Unix administrators for \ndecades.\nWhen the GNU Project ported the vi editor to the open source world, they chose to make some \nimprovements to it. Because it no longer resembled the original vi editor found in the Unix world, \nthe developers also renamed it, to vi improved, or vim.\nThis section walks you through the basics of using the vim editor to edit your text shell script fi les.\n\n234\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  234\nChecking your vim package\nBefore you begin your exploration of the vim editor, it’s a good idea to understand what \nvim package your Linux system has installed. On some distributions, you will have the \nfull vim package installed and an alias for the \nvi command, as shown on this CentOS \ndistribution:\n$ alias vi\nalias vi='vim'\n$\n$ which vim\n/usr/bin/vim\n$\n$ ls -l /usr/bin/vim\n-rwxr-xr-x. 1 root root 1967072 Apr  5  2012 /usr/bin/vim\n$\nNotice that the program fi le’s long listing does not show any linked fi les (see Chapter 3 for \nmore information on linked fi les). If the vim program is linked, it may be linked to a less \nthan full-featured editor. Thus, it’s a good idea to check for linked fi les.\nOn other distributions, you will fi nd various fl avors of the vim editor. Notice on this \nUbuntu distribution that not only is there no alias for the \nvi command, but the /usr/\nbin/vi\n program fi le belongs to a series of fi le links:\n$ alias vi\n-bash: alias: vi: not found\n$\n$ which vi\n/usr/bin/vi\n$\n$ ls -l /usr/bin/vi\nlrwxrwxrwx 1 root root 20 Apr 22 12:39 \n/usr/bin/vi -> /etc/alternatives/vi\n$\n$ ls -l /etc/alternatives/vi\nlrwxrwxrwx 1 root root 17 Apr 22 12:33 \n/etc/alternatives/vi -> /usr/bin/vim.tiny\n$\n$ ls -l /usr/bin/vim.tiny\n-rwxr-xr-x 1 root root 884360 Jan  2 14:40 \n/usr/bin/vim.tiny\n$\n$ readlink -f /usr/bin/vi\n/usr/bin/vim.tiny\n\n235\nChapter 10: Working with Editors\nc10.indd  12/05/2014  Page  235\n10\nThus, when the vi command is entered, the /usr/bin/vim.tiny program is executed. \nThe \nvim.tiny program provides only a few vim editor features. If you are serious about \nusing the vim editor and have Ubuntu, you should install at least the basic vim package.\nNotice in the preceding example that, instead of having to use the ls -l command multiple times to fi nd a series \nof linked fi les’ fi nal object, you can use the \nreadlink -f command. It immediately produces the linked fi le series’ \nfi nal object. \nSoftware installations were covered in detail in Chapter 9. Installing the basic vim package \non this Ubuntu distribution is fairly straightforward:\n$ sudo apt-get install vim\n[...]\nThe following extra packages will be installed:\n  vim-runtime\nSuggested packages:\n  ctags vim-doc vim-scripts\nThe following NEW packages will be installed:\n  vim vim-runtime\n[...]\n$\n$ readlink -f /usr/bin/vi\n/usr/bin/vim.basic\n$\nThe basic vim editor is now installed on this Ubuntu distribution, and the /usr/bin/vi \nprogram fi le’s link was automatically changed to point to \n/usr/bin/vim.basic. Thus, \nwhen the \nvi command is entered on this Ubuntu system, the basic vim editor is used \ninstead of tiny vim. \nExploring vim basics\nThe vim editor works with data in a memory buffer. To start the vim editor, just type the \nvim command (or vi if there’s an alias or linked fi le) and the name of the fi le you want to \nedit:\n$ vim myprog.c\nIf you start vim without a fi lename, or if the fi le doesn’t exist, vim opens a new buffer area \nfor editing. If you specify an existing fi le on the command line, vim reads the entire fi le’s \ncontents into a buffer area, where it is ready for editing, as shown in Figure 10-1.\n\n236\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  236\nFIGURE 10-1\nThe vim main window.\nThe vim editor detects the terminal type for the session (see Chapter 2) and uses a full-\nscreen mode to use the entire console window for the editor area.\nThe initial vim edit window shows the contents of the fi le (if there are any) along with \na message line at the bottom of the window. If the fi le contents don’t take up the entire \nscreen, vim places a tilde on lines that are not part of the fi le (as shown in Figure 10-1).\nThe message line at the bottom indicates information about the edited fi le, depending on \nthe fi le’s status, and the default settings in your vim installation. If the fi le is new, the \nmessage \n[New File] appears.\nThe vim editor has two modes of operation:\n ■\nNormal mode\n ■\nInsert mode\nWhen you fi rst open a fi le (or start a new fi le) for editing, the vim editor enters normal \nmode. In normal mode, the vim editor interprets keystrokes as commands (more on those \nlater).\nIn insert mode, vim inserts every key you type at the current cursor location in the buffer. \nTo enter insert mode, press the i key. To get out of insert mode and go back into normal \nmode, press the Escape key on the keyboard.\n\n237\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  237\nIn normal mode, you can move the cursor around the text area by using the arrow keys \n(as long as your terminal type is detected properly by vim). If you happen to be on a fl aky \nterminal connection that doesn’t have the arrow keys defi ned, all hope is not lost. The vim \ncommands include commands for moving the cursor:\n ■\nh to move left one character\n ■\nj to move down one line (the next line in the text)\n ■\nk to move up one line (the previous line in the text)\n ■\nl to move right one character\nMoving around within large text fi les line by line can get tedious. Fortunately, vim pro-\nvides a few commands to help speed things along:\n ■\nPageDown (or Ctrl+F) to move forward one screen of data\n ■\nPageUp (or Ctrl+B) to move backward one screen of data\n ■\nG to move to the last line in the buffer\n ■\nnum G to move to the line number num in the buffer\n ■\ngg to move to the fi rst line in the buffer\nThe vim editor has a special feature within normal mode called command line mode. The \ncommand line mode provides an interactive command line where you can enter additional \ncommands to control the actions in vim. To get to command line mode, press the colon key \nin normal mode. The cursor moves to the message line, and a colon (\n:) appears, waiting for \nyou to enter a command.\nWithin the command line mode are several commands for saving the buffer to the fi le and \nexiting vim:\n ■\nq to quit if no changes have been made to the buffer data\n ■\nq! to quit and discard any changes made to the buffer data\n ■\nw filename to save the fi le under a different fi lename\n ■\nwq to save the buffer data to the fi le and quit\nAfter seeing just a few basic vim commands, you might understand why some people \nabsolutely hate the vim editor. To be able to use vim to its fullest, you must know plenty \nof obscure commands. However, after you get a few of the basic vim commands down, you \ncan quickly edit fi les directly from the command line, no matter what type of environ-\nment you’re in. Plus, after you get comfortable typing commands, it almost seems second \nnature to type both data and editing commands, and it becomes odd having to jump back \nto using a mouse!\n\n238\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  238\nEditing data\nWhile in insert mode, you can insert data into the buffer; however, sometimes you need to \nadd or remove data after you’ve already entered it into the buffer. While in normal mode, \nthe vim editor provides several commands for editing the data in the buffer. Table 10-1 lists \nsome common editing commands for vim.\nTABLE 10 -1    vim  Editing  Commands\nCommandDescription\nx\nDeletes the character at the current cursor position\ndd\nDeletes the line at the current cursor position\ndw\nDeletes the word at the current cursor position\nd$\nDeletes to the end of the line from the current cursor position\nJ\nDeletes the line break at the end of the line at the current cursor position \n(joins lines)\nu\nUndoes the previous edit command\na\nAppends data after the current cursor position\nA\nAppends data to the end of the line at the current cursor position\nr charReplaces a single character at the current cursor position with char\nR\n textOverwrites the data at the current cursor position with text, until you press \nEscape\nSome of the editing commands also allow you to use a numeric modifi er to indicate how \nmany times to perform the command. For example, the command \n2x deletes two characters, \nstarting from the current cursor position, and the command \n5dd deletes fi ve lines, starting \nat the line from the current cursor position.\nBe careful when trying to use the keyboard Backspace or Delete keys while in the vim editor’s normal mode. The vim \neditor usually recognizes the Delete key as the functionality of the \nx command, deleting the character at the current \ncursor location. Usually, the vim editor doesn’t recognize the Backspace key in normal mode.\nCopying and pasting\nA standard editor feature is the ability to cut or copy data and paste it elsewhere in the \ndocument. The vim editor provides a way to do this.\n\n239\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  239\nCutting and pasting is relatively easy. You’ve already seen the commands in Table 10-1 that \ncan remove data from the buffer. However, when vim removes data, it actually keeps it \nstored in a separate register. You can retrieve that data by using the \np command.\nFor example, you can use the \ndd command to delete a line of text, move the cursor to the \nbuffer location where you want to place it, and then use the \np command. The p command \ninserts the text after the line at the current cursor position. You can do this with any com-\nmand that removes text.\nCopying text is a little bit trickier. The copy command in vim is \ny (for yank). You can \nuse the same second character with \ny as with the d command (yw to yank a word, y$ to \nyank to the end of a line). After you yank the text, move the cursor to the location where \nyou want to place the text and use the \np command. The yanked text now appears at that \nlocation.\nYanking is tricky in that you can’t see what happened because you’re not affecting the text \nthat you yank. You never know for sure what you yanked until you paste it somewhere. But \nthere’s another feature in vim that helps you out with yanking.\nThe visual mode highlights text as you move the cursor. You use visual mode to select text \nto yank for pasting. To enter visual mode, move the cursor to the location where you want \nto start yanking, and press \nv. Notice that the text at the cursor position is now high-\nlighted. Next, move the cursor to cover the text you want to yank (you can even move \ndown lines to yank more than one line of text). As you move the cursor, vim highlights the \ntext in the yank area. After you’ve covered the text you want to copy, press the y key to \nactivate the yank command. Now that you have the text in the register, just move the cur-\nsor to where you want to paste and use the \np command.\nSearching and substituting\nYou can easily search for data in the buffer using the vim search command. To enter a \nsearch string, press the forward slash (/) key. The cursor goes to the message line, and vim \ndisplays a forward slash. Enter the text you want to fi nd, and press the Enter key. The vim \neditor responds with one of three actions:\n ■\nIf the word appears after the current cursor location, it jumps to the fi rst location \nwhere the text appears.\n ■\nIf the word doesn’t appear after the current cursor location, it wraps around the \nend of the fi le to the fi rst location in the fi le where the text appears (and indicates \nthis with a message).\n ■\nIt produces an error message stating that the text was not found in the fi le.\nTo continue searching for the same word, press the forward slash character and then press \nthe Enter key, or you can use the n key, for next.\n\n240\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  240\nThe substitute command allows you to quickly replace (substitute) one word for another in \nthe text. To get to the substitute command, you must be in command line mode. The for-\nmat for the substitute command is:\n:s/old/new/\nThe vim editor jumps to the fi rst occurrence of the text old and replaces it with the text \nnew. You can make a few modifi cations to the substitute command to substitute more than \none occurrence of the text:\n ■\n:s/old/new/g to replace all occurrences of old in a line\n ■\n:n,ms/old/new/g to replace all occurrences of old between line numbers n and m\n ■\n:%s/old/new/g to replace all occurrences of old in the entire fi le\n ■\n:%s/old/new/gc to replace all occurrences of old in the entire fi le, but prompt \nfor each occurrence\nAs you can see, for a console mode text editor, vim contains quite a few advanced features. \nBecause every Linux distribution includes it, it’s a good idea to at least know the basics of \nthe vim editor so you can always edit scripts, no matter where you are or what you have \navailable.\nNavigating the nano Editor\nAlthough vim is a very complicated editor with many powerful features, nano is a very \nsimple editor. For individuals who need a simple console mode text editor that is easy to \nnavigate, nano is the tool to use. It’s also a great text editor for kids who are starting on \ntheir Linux command line adventure. \nThe nano text editor is a clone of the Unix systems’ Pico editor. Although Pico also is a \nlight and simple text editor, it is not licensed under the GPL. Not only is the nano text edi-\ntor licensed under the GPL, it is also part of the GNU project. \nThe nano text editor is installed on most Linux distributions by default. Everything about \nthe nano text editor is simple. To open a fi le at the command line with nano:\n$ nano myprog.c\nIf you start nano without a fi lename, or if the fi le doesn’t exist, nano simply opens a new \nbuffer area for editing. If you specify an existing fi le on the command line, nano reads \nthe entire contents of the fi le into a buffer area, where it is ready for editing, as shown in \nFigure 10-2.\n\n241\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  241\nFIGURE 10-2\nThe nano editor window\nNotice at the bottom of the nano editor window various commands with a brief description \nare shown. These commands are the nano control commands. The caret (\n^) symbol shown \nrepresents the Ctrl key. Therefore, \n^X stands for the keyboard sequence Ctrl+X. \nThough the nano control commands list capital letters in the keyboard sequences, you can use either lowercase or \nuppercase characters for control commands.\nHaving all the basic commands listed right in front of you is great. No need to memorize \nwhat control command does what. Table 10-2 presents the various nano control commands.\nTABLE 10 -2    nano  Control  Commands\nCommandDescription\nCTRL+CDisplays the cursor’s position within the text editing buffer\nCTRL+GDisplays nano’s main help window\nCTRL+JJustifi es the current text paragraph\nCTRL+KCuts the text line and stores it in cut buffer\nCTRL+OWrites out the current text editing buffer to a fi le\nContinues\n\n242\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  242\nCommandDescription\nCTRL+RReads a fi le into the current text editing buffer\nCTRL+TStarts the available spell checker\nCTRL+UPastes text stored in cut buffer and places in current line\nCTRL+VScrolls text editing buffer to next page\nCTRL+WSearches for word or phrases within text editing buffer\nCTRL+XCloses the current text editing buffer, exits nano, and returns to the shell\nCTRL+YScrolls text editing buffer to previous page\nThe control commands listed in Table 10-2 are really all you need. However, if you desire \nmore powerful control features than those listed, nano has them. To see more control com-\nmands, type Ctrl+G in the nano text editor to display its main help window containing \nadditional control commands. \nIf you try to use the nano spell checker via the Ctrl+T command and get the error message \nSpell checking failed: Error invoking 'Spell', there are some potential solutions. Install the \nspell checker software package, \naspell, on your Linux distribution using Chapter 9 as a guide.\nIf installing the \naspell software package does not solve the problem, as superuser edit the /etc/nanorc fi le, \nusing your favorite text editor. Find the line, \n# set speller \"aspell -x -c\" and delete the hash mark (#) \nfrom the line. Save and exit the fi le.\nAdditional powerful features are available at the command line. You can use command line \noptions to control nano editor features, such as creating a backup fi le before editing. Type \nman nano to see these additional command line options for starting nano.\nThe vim and nano text editors offer a choice between powerful and simple console mode \ntext editors. However, neither offers the ability to use graphical features for editing. Some \ntext editors can operate in both worlds, as explored in the next section.\nExploring the emacs Editor\nThe emacs editor is an extremely popular editor that appeared before even Unix was \naround. Developers liked it so much that they ported it to the Unix environment, and now \nit’s been ported to the Linux environment. The emacs editor started out life as a console \neditor, much like vi, but has migrated to the graphical world.\nTABLE 10 -2   (continued)\n\n243\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  243\nThe emacs editor still provides the original console mode editor, and now it also has \nthe ability to use a graphical window to allow editing text in a graphical environment. \nTypically, when you start the emacs editor from a command line, the editor determines \nwhether you have an available graphical session and starts in graphical mode. If you don’t, \nit starts in console mode.\nThis section describes both the console mode and graphical mode emacs editors so that \nyou’ll know how to use either one if you want (or need) to.\nChecking your emacs package\nMany distributions do not come with the emacs editor installed by default. You can check \nyour Red Hat-based distribution, by using the \nwhich and/or yum list command as shown \non this CentOS distribution:\n$ which emacs\n/usr/bin/which: no emacs in (/usr/lib64/qt-3.3\n/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:\n/usr/sbin:/sbin:/home/Christine/bin)\n$\n$ yum list emacs\n[...]\nAvailable Packages\nemacs.x86_64               1:23.1-25.el6                base\nThe emacs editor package is not currently installed on this CentOS distribution. However, \nit is available to be installed. (For a more thorough discussion on displaying installed soft-\nware, see Chapter 9).\nFor a Debian-based distribution, check for the emacs editor package by using the \nwhich \nand/or \napt-cache show command as shown on this Ubuntu distribution:\n$ which emacs\n$\n$ sudo apt-cache show emacs\nPackage: emacs\nPriority: optional\nSection: editors\nInstalled-Size: 25\n[...]\nDescription-en: GNU Emacs editor (metapackage)\n GNU Emacs is the extensible self-documenting text editor.\n This is a metapackage that will always depend on the latest\n recommended Emacs release.\nDescription-md5: 21fb7da111336097a2378959f6d6e6a8\nBugs: https://bugs.launchpad.net/ubuntu/+filebug\n\n244\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  244\nOrigin: Ubuntu\nSupported: 5y\n$\nThe which command operates a little differently here. When it does not fi nd the installed \ncommand, it simply returns the bash shell prompt. The emacs editor package is optional for \nthis Ubuntu distribution, but is available to be installed. The following shows the emacs \neditor being installed on Ubuntu:\n$ sudo apt-get install emacs\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\nThe following extra packages will be installed:\n[...]\nInstall emacsen-common for emacs24\nemacsen-common: Handling install of emacsen flavor emacs24\nWrote /etc/emacs24/site-start.d/00debian-vars.elc\nWrote /usr/share/emacs24/site-lisp/debian-startup.elc\nSetting up emacs (45.0ubuntu1) ...\nProcessing triggers for libc-bin (2.19-0ubuntu6) ...\n$\n$ which emacs\n/usr/bin/emacs\n$\nNow when the which command is used, it points to the emacs program fi le. The emacs edi-\ntor is ready to be used on this Ubuntu distribution.\nFor the CentOS distribution, install the emacs editor using the \nyum install command:\n$ sudo yum install emacs\n[sudo] password for Christine:\n[...]\nSetting up Install Process\nResolving Dependencies\n[...]\nInstalled:\n  emacs.x86_64 1:23.1-25.el6\nDependency Installed:\n  emacs-common.x86_64 1:23.1-25.el6\n  libotf.x86_64 0:0.9.9-3.1.el6\n  m17n-db-datafiles.noarch 0:1.5.5-1.1.el6\nComplete!\n$\n$ which emacs\n/usr/bin/emacs\n$\n$ yum list emacs\n\n245\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  245\n[...]\nInstalled Packages\nemacs.x86_64               1:23.1-25.el6               @base\n$\nWith the emacs editor successfully installed on your Linux distribution, you can begin to \nexplore its different features, staring with using it on the console.\nUsing emacs on the console\nThe console mode version of emacs is another editor that uses lots of key commands to per-\nform editing functions. The emacs editor uses key combinations involving the Control key \n(the Ctrl key on the keyboard) and the Meta key. In most terminal emulator packages, the \nMeta key is mapped to the Alt key. The offi cial emacs documents abbreviate the Ctrl key as \nC- and the Meta key as M-. Thus, if you enter a Ctrl+x key combination, the document shows \nC-x. This chapter does the same so as not to confuse you.\nExploring the basics of emacs\nTo edit a fi le using emacs, from the command line, enter:\n$ emacs myprog.c\nThe emacs console mode window appears with a short introduction and help screen. Don’t \nbe alarmed; as soon as you press a key, emacs loads the fi le into the active buffer and dis-\nplays the text, as shown in Figure 10-3.\nFIGURE 10-3\nEditing a file using the emacs editor in console mode\n\n246\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  246\nYou’ll notice that the top of the console mode window shows a typical menu bar. \nUnfortunately, you can’t use the menu bar in console mode, only in graphical mode.\nSome commands in this section work differently than described, if you run emacs in a graphical desktop environ-\nment. To use emac’s console mode in a graphical desktop environment, use the \nemacs -nw command. If you want to \nuse emacs’ graphical features, see the section “Using emacs in a GUI.”\nUnlike the vim editor, where you have to move into and out of insert mode to switch \nbetween entering commands and inserting text, the emacs editor has only one mode. If you \ntype a printable character, emacs inserts it at the current cursor position. If you type a \ncommand, emacs executes the command.\nTo move the cursor around the buffer area, you can use the arrow keys and the PageUp and \nPageDown keys, assuming that emacs detected your terminal emulator correctly. If not, \nthese commands move the cursor around:\n ■\nC-p moves up one line (the previous line in the text).\n ■\nC-b moves left (back) one character.\n ■\nC-f moves right (forward) one character.\n ■\nC-n moves down one line (the next line in the text).\nThese commands make longer jumps with the cursor within the text:\n ■\nM-f moves right (forward) to the next word.\n ■\nM-b moves left (backward) to the previous word.\n ■\nC-a moves to the beginning of the current line.\n ■\nC-e moves to the end of the current line.\n ■\nM-a moves to the beginning of the current sentence.\n ■\nM-e moves to the end of the current sentence.\n ■\nM-v moves back one screen of data.\n ■\nC-v moves forward one screen of data.\n ■\nM-< moves the fi rst line of the text.\n ■\nM-> moves to the last line of the text.\nYou should know these commands for saving the editor buffer back into the fi le and exiting \nemacs:\n ■\nC-x C-s saves the current buffer contents to the fi le.\n ■\nC-z exits emacs but keeps it running in your session so you can come back to it.\n ■\nC-x C-c exits emacs and stops the program.\n\n247\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  247\nYou’ll notice that two of these features require two key commands. The C-x command is \ncalled the extend command. This provides yet another whole set of commands to work with.\nEditing data\nThe emacs editor is pretty robust about inserting and deleting text in the buffer. To insert \ntext, just move the cursor to the location where you want to insert the text and start typ-\ning. To delete text, emacs uses the Backspace key to delete the character before the current \ncursor position and the Delete key to delete the character at the current cursor location.\nThe emacs editor also has commands for killing text. The difference between deleting text \nand killing text is that when you kill text, emacs places it in a temporary area where you \ncan retrieve it (see the next section, “Copying and pasting”). Deleted text is gone forever.\nThese commands are for killing text in the buffer:\n ■\nM-Backspace kills the word before the current cursor position.\n ■\nM-d kills the word after the current cursor position.\n ■\nC-k kills from the current cursor position to the end of the line.\n ■\nM-k kills from the current cursor position to the end of the sentence.\nThe emacs editor also includes a fancy way of mass-killing text. Just move the cursor to \nthe start of the area you want to kill, and press either the \nC-@ or C-Spacebar keys. Then \nmove the cursor to the end of the area you want to kill, and press the \nC-w command keys. \nAll the text between the two locations is killed.\nIf you happen to make a mistake when killing text, the \nC-/ command undoes the kill com-\nmand and returns the data to the state it was in before you killed it.\nCopying and pasting\nYou’ve seen how to cut data from the emacs buffer area; now it’s time to see how to paste \nit somewhere else. Unfortunately, if you use the vim editor, this process may confuse you \nwhen you use the emacs editor.\nIn an unfortunate coincidence, pasting data in emacs is called yanking. In the vim editor, \ncopying is called yanking, which is what makes this a diffi cult thing to remember if you \nhappen to use both editors.\nAfter you kill data using one of the kill commands, move the cursor to the location where \nyou want to paste the data, and use the \nC-y command. This yanks the text out of the \ntemporary area and pastes it at the current cursor position. The \nC-y command yanks the \ntext from the last kill command. If you’ve performed multiple kill commands, you can cycle \nthrough them using the \nM-y command.\nTo copy text, just yank it back into the same location you killed it from and then move to \nthe new location and use the \nC-y command again. You can yank text back as many times \nas you desire.\n\n248\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  248\nSearching and Replacing\nSearching for text in the emacs editor is done by using the C-s and C-r commands. The \nC-s command performs a forward search in the buffer area from the current cursor position \nto the end of the buffer, whereas the \nC-r command performs a backward search in the buf-\nfer area from the current cursor position to the start of the buffer.\nWhen you enter either the \nC-s or C-r command, a prompt appears in the bottom line, \nquerying you for the text to search. You can perform two types of searches in emacs.\nIn an incremental search, the emacs editor performs the text search in real-time mode as \nyou type the word. When you type the fi rst letter, it highlights all the occurrences of that \nletter in the buffer. When you type the second letter, it highlights all the occurrences of \nthe two-letter combination in the text and so on until you complete the text you’re search-\ning for.\nIn a non-incremental search, press the Enter key after the \nC-s or C-r commands. This locks \nthe search query into the bottom line area and allows you to type the search text in full \nbefore searching.\nTo replace an existing text string with a new text string, you must use the \nM-x command. \nThis command requires a text command, along with parameters.\nThe text command is \nreplace-string. After typing the command, press the Enter key, \nand emacs queries you for the existing text string. After entering that, press the Enter key \nagain and emacs queries you for the new replacement text string.\nUsing buffers in emacs\nThe emacs editor allows you to edit multiple fi les at the same time by having multiple buf-\nfer areas. You can load fi les into a buffer and switch between buffers while editing.\nTo load a new fi le into a buffer while you’re in emacs, use the \nC-x C-f key combination. \nThis is the emacs Find a File mode. It takes you to the bottom line in the window and \nallows you to enter the name of the fi le you want to start to edit. If you don’t know the \nname or location of the fi le, just press the Enter key. This brings up a fi le browser in the \nedit window, as shown in Figure 10-4.\nFrom here, you can browse to the fi le you want to edit. To traverse up a directory level, go \nto the double dot entry and press the Enter key. To traverse down a directory, go to the \ndirectory entry and press the Enter key. When you’ve found the fi le you want to edit, press \nthe Enter key and emacs loads it into a new buffer area.\nYou can list the active buffer areas by pressing the \nC-x C-b extended command combina-\ntion. The emacs editor splits the editor window and displays a list of buffers in the bottom \nwindow. emacs provides two buffers in addition to your main editing buffer:\n ■\nA scratch area called *scratch*\n ■\nA message area called *Messages*\n\n249\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  249\nFIGURE 10-4\nThe emacs Find a File mode browser\nThe scratch area allows you to enter LISP programming commands as well as enter notes \nto yourself. The message area shows messages generated by emacs while operating. If any \nerrors occur while using emacs, they appear in the message area.\nYou can switch to a different buffer area in the window in two ways:\n ■\nUse C-x o to switch to the buffer listing window. Use the arrow keys to move to \nthe buffer area you want and press the Enter key.\n ■\nUse C-x b to type in the name of the buffer area you want to switch to.\nWhen you select the option to switch to the buffer listing window, emacs opens the buffer \narea in the new window area. The emacs editor allows you to have multiple windows open \nin a single session. The following section discusses how to manage multiple windows in \nemacs.\nUsing windows in console mode emacs\nThe console mode emacs editor was developed many years before the idea of graphical \nwindows appeared. However, it was advanced for its time, in that it could support multiple \nediting windows within the main emacs window.\nYou can split the emacs editing window into multiple windows by using one of two \ncommands:\n ■\nC-x 2 splits the window horizontally into two windows.\n ■\nC-x 3 splits the window vertically into two windows.\n\n250\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  250\nTo move from one window to another, use the C-x o command. Notice that when you cre-\nate a new window, emacs uses the buffer area from the original window in the new window. \nAfter you move into the new window, you can use the \nC-x C-f command to load a new fi le \nor use one of the commands to switch to a different buffer area in the new window.\nTo close a window, move to it and use the \nC-x 0 (that’s a zero) command. If you want \nto close all the windows except the one you’re in, use the \nC-x 1 (that’s a numerical one) \ncommand.\nUsing emacs in a GUI\nIf you use emacs from a GUI environment (such as the Unity or GNOME desktops), it starts \nin graphical mode, as shown in Figure 10-5.\nFIGURE 10-5\nThe emacs graphical window\nIf you’ve already used emacs in console mode, you should be fairly familiar with the graphi-\ncal mode. All the key commands are available as menu bar items. The emacs menu bar con-\ntains the following items:\n ■\nFile allows you to open fi les in the window, create new windows, close windows, \nsave buffers, and print buffers.\n ■\nEdit allows you to cut and copy selected text to the clipboard, paste clipboard data \nto the current cursor position, search for text, and replace text.\n\n251\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  251\n ■\nOptions provides settings for many more emacs features, such as highlighting, \nword wrap, cursor type, and setting fonts.\n ■\nBuffers lists the current buffers available and allows you to easily switch between \nbuffer areas.\n ■\nTools provides access to the advanced features in emacs, such as the command line \ninterface access, spell checking, comparing text between fi les (called diff), sending \nan e-mail message, calendar, and the calculator.\n ■\nHelp provides the emacs manual online for access to help on specifi c emacs \nfunctions.\nIn addition to the normal graphical emacs menu bar items, there is often a separate item \nspecifi c to the fi le type in the editor buffer. Figure 10-5 shows opening a C program, so \nemacs provided a C menu item, allowing advanced settings for highlighting C syntax, and \ncompiling, running, and debugging the code from a command prompt.\nThe graphical emacs window is an example of an older console application making the \nmigration to the graphical world. Now that many Linux distributions provide graphical \ndesktops (even on servers that don’t need them), graphical editors are becoming more com-\nmonplace. Popular Linux desktop environments (such as KDE and GNOME) have also pro-\nvided graphical text editors specifi cally for their environments, which are covered in the \nrest of this chapter.\nExploring the KDE Family of Editors\nIf you’re using a Linux distribution that uses the KDE desktop (see Chapter 1), you have \na couple of options when it comes to text editors. The KDE project offi cially supports two \npopular text editors:\n ■\nKWrite: A single-screen text-editing package\n ■\nKate: A full-featured, multi-window text-editing package\nBoth of these editors are graphical text editors that contain many advanced features. The \nKate editor provides more advanced features, plus extra niceties not often found in stan-\ndard text editors. This section describes each of the editors and shows some of the features \nyou can use to help with your shell script editing.\nLooking at the KWrite editor\nThe basic editor for the KDE environment is KWrite. It provides simple word-processing–\nstyle text editing, along with support for code syntax highlighting and editing. The default \nKWrite editing window is shown in Figure 10-6.\n\n252\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  252\nFIGURE 10-6\nThe default KWrite window editing a shell script program\nYou can’t tell from Figure 10-6, but the KWrite editor recognizes several types of program-\nming languages and uses color coding to distinguish constants, functions, and comments. \nAlso, notice that the \nfor loop has an icon that links the opening and closing braces. This \nis called a folding marker. By clicking the icon, you can collapse the function into a single \nline. This is a great feature when working through large applications.\nThe KWrite editing window provides full cut and paste capabilities, using the mouse and \nthe arrow keys. As in a word processor, you can highlight and cut (or copy) text anywhere \nin the buffer area and paste it at any other place.\nTo edit a fi le using KWrite, you can either select KWrite from the KDE menu system on your \ndesktop (some Linux distributions even create a Panel icon for it) or start it from the com-\nmand line prompt:\n$ kwrite factorial.sh\nThe kwrite command has several command line parameters you can use to customize how \nit starts:\n ■\n--stdin causes KWrite to read data from the standard input device instead of a \nfi le.\n ■\n--encoding specifi es a character encoding type to use for the fi le.\n\n253\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  253\n ■\n--line specifi es a line number in the fi le to start at in the editor window.\n ■\n--column specifi es a column number in the fi le to start at in the editor window.\nThe KWrite editor provides both a menu bar and a toolbar at the top of the edit window, \nallowing you to select features and change confi guration settings of the KWrite editor.\nThe menu bar contains these items:\n ■\nFile loads, saves, prints, and exports text from fi les.\n ■\nEdit manipulates text in the buffer area.\n ■\nView manages how the text appears in the editor window.\n ■\nBookmarks handle pointers to return to specifi c locations in the text; this option \nmay need to be enabled in the confi gurations.\n ■\nTools contains specialized features to manipulate the text.\n ■\nSettings confi gures the way the editor handles text.\n ■\nHelp gives you information about the editor and commands.\nThe Edit menu bar item provides commands for all your text-editing needs. Instead of hav-\ning to remember cryptic key commands (which by the way, KWrite also supports), you can \njust select items in the Edit menu bar, as shown in Table 10-3.\nTABLE 10 -3    The KWrite Edit Menu Items\nItemDescription\nUndoReverses the last action or operation\nRedoReverses the last undo action\nCutDeletes the selected text and places it in the clipboard\nCopyCopies the selected text to the clipboard\nPasteInserts the current contents of the clipboard at the current cursor \nposition\nSelect AllSelects all text in the editor\nDeselectDeselects any text that is currently selected\nOverwrite ModeToggles insert mode to overwrite mode, replacing text with new \ntyped text instead of just inserting the new text\nFindProduces the Find Text dialog box, which allows you to customize a \ntext search\nFind NextRepeats the last fi nd operation forward in the buffer area\nFind PreviousRepeats the last fi nd operation backwards in the buffer area\nContinues\n\n254\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  254\nItemDescription\nReplaceProduces the Replace With dialog box, which allows you to cus-\ntomize a text search and replace\nFind SelectedFinds the next occurrence of the selected text\nFind Selected \nBackwards\nFinds the previous occurrence of the selected text\nGo to LineProduces the Goto dialog box, which allows you to enter a line \nnumber. The cursor moves to the specifi ed line\nThe Find feature has two modes. Normal mode performs simple text searches and power \nsearches. Replace mode lets you do advanced searching and replacing if necessary. You \ntoggle between the two modes using the green arrow in the Find section, as shown in \nFigure 10-7.\nFIGURE 10-7\nThe KWrite Find section\nTABLE 10 -3   (continued)\n\n255\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  255\nThe Find power mode allows you to search not only with words, but with a regular expres-\nsion (discussed in Chapter 20) for the search. You can use some other options to custom-\nize the search as well, indicating, for example, whether or not to perform a case-sensitive \nsearch or to look only for whole words instead of fi nding the text within words.\nThe Tools menu bar item provides several handy features for working with the text in the \nbuffer area. Table 10-4 describes the tools available in KWrite.\nTABLE 10 - 4    The  KWrite  Tools\nToolDescription\nRead Only ModeLocks the text so no changes can be made while in the editor\nEncodingSets the character set encoding used by the text\nSpellingStarts the spell-check program at the start of the text\nSpelling (from cursor)Starts the spell-check program from the current cursor position\nSpellcheck SelectionStarts the spell-check program only on the selected section of text\nIndentIncreases the paragraph indentation by one\nUnindentDecreases the paragraph indentation by one\nClean IndentationReturns all paragraph indentation to the original settings\nAlignForces the current line or the selected lines to return to the default \nindentation settings\nUppercaseSets the selected text, or the character at the current cursor position, \nto uppercase\nLowercaseSets the selected text, or the character at the current cursor position, \nto lowercase\nCapitalizeCapitalizes the fi rst letter of the selected text or the word at the cur-\nrent cursor position\nJoin LinesCombines the selected lines, or the line at the current cursor position \nand the next line, into one line\nWord Wrap \nDocument\nEnables word wrapping in the text. If a line extends past the editor \nwindow edge, the line continues on the next line.\nThere are lots of tools for a simple text editor!\nThe Settings menu includes the Confi gure Editor dialog box, shown in Figure 10-8.\n\n256\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  256\nFIGURE 10-8\nThe KWrite Configure Editor dialog box\nThe Confi guration dialog box uses icons on the left side for you to select the feature in \nKWrite to confi gure. When you select an icon, the right side of the dialog box shows the \nconfi guration settings for the feature.\nThe Appearance feature allows you to set several features that control how the text appears \nin the text editor window. You can enable word wrap, line numbers (great for programmers), \nand the folder markers from here. With the Fonts & Colors feature, you can customize the \ncomplete color scheme for the editor, determining what colors to make each category of \ntext in the program code.\nLooking at the Kate editor\nThe Kate editor is the fl agship editor for the KDE Project. It uses the same text editor as the \nKWrite application (so most of those features are the same), but it incorporates lots of other \nfeatures into a single package.\n\n257\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  257\nIf you fi nd that the Kate editor has not been installed with your KDE desktop environment, you can easily install it \n(see Chapter 9). The package name that contains Kate is \nkdesdk.\nWhen you start the Kate editor from the KDE menu system, the fi rst thing you notice is \nthat the editor doesn’t start! Instead, you get a dialog box, as shown in Figure 10-9.\nFIGURE 10-9\nThe Kate session dialog box\nThe Kate editor handles fi les in sessions. You can have multiple fi les open in a session, and \nyou can have multiple sessions saved. When you start Kate, it provides you with the choice \nof which session to return to. When you close your Kate session, it remembers the docu-\nments you had open and displays them the next time you start Kate. This allows you to \neasily manage fi les from multiple projects by using separate workspaces for each project.\nAfter selecting a session, you see the main Kate editor window, shown in Figure 10-10.\nThe left side frame shows the documents currently open in the session. You can switch \nbetween documents just by clicking the document name. To edit a new fi le, click the \nFilesystem Browser tab on the left side. The left frame is now a full graphical fi lesystem \nbrowser, allowing you to graphically browse to locate your fi les.\nA great feature of the Kate editor is the built-in terminal window, shown in Figure 10-11.\n\n258\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  258\nFIGURE 10-10\nThe main Kate editing window\nThe terminal tab at the bottom of the text editor window starts the built-in terminal emu-\nlator in Kate (using the KDE Konsole terminal emulator). This feature horizontally splits the \ncurrent editing window, creating a new window with Konsole running in it. You can now \nenter command line commands, start programs, or check on system settings without having \nto leave the editor! To close the terminal window, just type exit at the command prompt.\nAs you can tell from the terminal feature, Kate also supports multiple windows. The \nWindow menu bar item (\nView) provides options to perform these tasks:\n ■\nCreate a new Kate window using the current session\n ■\nSplit the current window vertically to create a new window\n ■\nSplit the current window horizontally to create a new window\n ■\nClose the current window\n\n259\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  259\nFIGURE 10-11\nThe Kate built-in terminal window\nTo set the confi guration settings in Kate, select the Confi gure Kate item under the Settings \nmenu bar item. The Confi guration dialog box, shown in Figure 10-12, appears.\nNotice that the Editor settings area is exactly the same as for KWrite. This is because the \ntwo editors share the same text editor engine. The Application settings area allows you to \nconfi gure settings for the Kate items, such as controlling sessions (shown in Figure 10-12), \nthe documents list, and the fi lesystem browser. Kate also supports external plug-in applica-\ntions, which can be activated here.\n\n260\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  260\nFIGURE 10-12\nThe Kate configuration settings dialog box\nExploring the GNOME Editor\nIf you’re working on a Linux system using the GNOME or Unity desktop environment, there’s \na graphical text editor that you can use as well. The gedit text editor is a basic text editor, \nwith a few advanced features thrown in just for fun. This section walks you through the \nfeatures of gedit and demonstrates how to use it for your shell script programming.\nStarting gedit\nMost GNOME desktop environments include gedit in the Accessories Panel menu item. For \nthe Unity desktop environment, go to Dash  ➪  Search and type gedit. If you can’t fi nd \ngedit via the menu system, you can start it from the command line prompt in a GUI termi-\nnal emulator:\n$ gedit factorial.sh myprog.c\n\n261\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  261\nWhen you start gedit with multiple fi les, it loads all the fi les into separate buffers and dis-\nplays each one as a tabbed window within the main editor window, as shown in \nFigure 10-13.\nFIGURE 10-13\nThe gedit main editor window\nThe left frame in the gedit main editor window shows the documents you’re currently edit-\ning. If your gedit doesn’t show the left frame when started, you can press the F9 function \nkey or enable \nSide Pane from the View menu. \nDifferent desktops may have gedit options that are available in slightly different menu locations than shown in these \nfi gures. Additional options may also be available. Consult your distribution’s gedit Help menu for more assistance.\nThe right side shows the tabbed windows that contain the buffer text. If you hover your \nmouse pointer over each tab, a dialog box appears, showing the full pathname of the fi le, \nthe MIME type, and the character set encoding it uses.\n\n262\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  262\nUnderstanding basic gedit features\nIn addition to the editor windows, gedit uses both a menu bar and toolbar that allow you \nto set features and confi gure settings. The toolbar provides quick access to menu bar items. \nThese menu bar items are available:\n ■\nFile handles new fi les, saves existing fi les, and prints fi les.\n ■\nEdit manipulates text in the active buffer area and sets the editor preferences.\n ■\nView sets the editor features to display in the window and sets the text highlight-\ning mode.\n ■\nSearch fi nds and replaces text in the active editor buffer area.\n ■\nTools accesses plug-in tools installed in gedit.\n ■\nDocuments manages fi les open in the buffer areas.\n ■\nHelp provides access to the full gedit manual.\nThere shouldn’t be anything too surprising here. The Edit menu contains the standard cut, \ncopy, and paste functions, along with a neat feature that allows you to easily enter the \ndate and time in the text in several different formats. The Search menu provides a stan-\ndard fi nd function, which produces a dialog box where you can enter the text to fi nd, along \nwith the capability to select how the fi nd feature should work (matching case, matching \nthe whole word, and the search direction). It also provides an incremental search feature, \nwhich works in real-time mode, fi nding text as you type the characters of the word.\nSetting preferences\nThe Edit menu contains a Preferences item, which produces the gedit Preferences dialog \nbox, shown in Figure 10-14.\nThis is where you can customize the operation of the gedit editor. The Preferences dialog \nbox contains fi ve tabbed areas for setting the features and behavior of the editor.\nSetting View preferences\nThe View tab provides options for how gedit displays the text in the editor window:\n ■\nText Wrapping: Determines how to handle long lines of text in the editor. The \nEnabling text wrapping option wraps long lines to the next line of the editor. The \nDo Not Split Words Over Two Lines option prevents the auto-inserting of hyphens \ninto long words, to prevent them being split between two lines.\n ■\nLine Numbers: Displays line numbers in the left margin in the editor window.\n\n263\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  263\n ■\nCurrent Line: Highlights the line where the cursor is currently positioned, \nenabling you to easily fi nd the cursor position.\n ■\nRight Margin: Enables the right margin and allows you to set how many columns \nshould be in the editor window. The default value is 80 columns.\n ■\nBracket Matching: When enabled, highlights bracket pairs in programming code, \nallowing you to easily match brackets in \nif-then statements, for and while \nloops, and other coding elements that use brackets.\nThe line-numbering and bracket-matching features provide an environment for program-\nmers to troubleshoot code that’s not often found in text editors.\nFIGURE 10-14\nThe GNOME desktop gedit Preferences dialog box\nSetting Editor preferences\nThe Editor tab provides options for how the gedit editor handles tabs and indentation, \nalong with how fi les are saved:\n\n264\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  264\n ■\nTab Stops: Sets the number of spaces skipped when you press the Tab key. The \ndefault value is eight. This feature also includes a check box that, when selected, \ninserts spaces instead of a tab skip.\n ■\nAutomatic Indentation: When enabled, causes gedit to automatically indent lines \nin the text for paragraphs and code elements (such as \nif-then statements and \nloops).\n ■\nFile Saving: Provides two features for saving fi les: whether or not to create a \nbackup copy of the fi le when opened in the edit window, and whether or not to \nautomatically save the fi le at a preselected interval.\nThe auto-save feature is a great way to ensure that your changes are saved on a regular \nbasis to prevent catastrophes from crashes or power outages.\nSetting Font & Color preferences\nThe Font & Colors tab allows you to confi gure (not surprisingly) two items:\n ■\nFont: Allows you to select the default font, or to select a customized font and font \nsize from a dialog box.\n ■\nColor Scheme: Allows you to select the default color scheme used for text, back-\nground, selected text, and selection colors, or choose a custom color for each \ncategory.\nThe default colors for gedit normally match the standard GNOME desktop theme selected for \nthe desktop. These colors will change to match the scheme you select for the desktop.\nManaging plug-ins\nThe Plugins tab provides control over the plug-ins used in gedit. Plug-ins are separate \nprograms that can interface with gedit to provide additional functionality. \nSeveral plug-ins are available for gedit, but not all of them are installed by default. \nTable 10-5 describes the plug-ins that are currently available in the GNOME desktop’s gedit.\nTABLE 10 -5    The GNOME desktop gedit Plug-ins\nPlug-InDescription\nChange CaseChanges the case of selected text\nDocument \nStatistics\nReports the number of words, lines, characters, and non-space \ncharacters\nExternal ToolsProvides a shell environment in the editor to execute commands and \nscripts\nFile Browser PaneProvides a simple fi le browser to make selecting fi les for editing easier\n\n265\nChapter 10: Working with Editors\n10\nc10.indd  12/05/2014  Page  265\nIndent LinesProvides selected lines to be indented or un-indented\nInsert Date/TimeInserts the current date and time in several formats at the current cursor \nposition\nModelinesProvides emacs-style message lines at the bottom of the editor window\nPython ConsoleProvides an interactive console at the bottom of the editor window for \nentering commands using the Python programming language\nQuick OpenOpens fi les directly in the gedit edit window\nSnippetsAllows you to store often-used pieces of text for easy retrieval anywhere \nin the text\nSortQuickly sorts the entire fi le or selected text\nSpell CheckerProvides dictionary spellchecking for the text fi le\nTag ListProvides a list of commonly used strings you can easily enter into your \ntext\nPlug-ins that are enabled show a check mark in the check box next to their name. Some \nplug-ins, such as the External Tools plug-in, also provide additional confi guration features \nafter you select them. It allows you to set a shortcut key to start the terminal, where gedit \ndisplays output, and the command to use to start the shell session.\nUnfortunately, not all plug-ins are installed in the same place in the gedit menu bar. Some \nplug-ins appear in the Tools menu bar item (such as the Spell Checker and External Tools \nplug-ins), while others appear in the Edit menu bar item (such as the Change Case and \nInsert Date/Time plug-ins).\nThis chapter has covered just a few of the text editors available on Linux. If you fi nd that \nthe text editors described here don’t meet your needs, you have options. Many more Linux \neditors are available, such as geany, Eclipse, jed, Bluefi sh, and leafpad to name a few. All \nthese editors can help you as you begin your \nbash shell script writing journey.\nSummary\n When it comes to creating shell scripts, you need some type of text editor. Several popu-\nlar text editors are available for the Linux environment. The most popular editor in the \nUnix world, vi, has been ported to the Linux world as the vim editor. The vim editor \nprovides simple text editing from the console, using a rudimentary full-screen graphical \nmode. The vim editor provides many advanced editor features, such as text searching and \nreplacement.\nAnother editor that has been ported from the Unix world to Linux is the nano text editor. \nThe vim editor can be rather complex, but the nano editor offers simplicity. The nano editor \nallows quick text editing in console mode. \n\n266\nPart I: The Linux Command Line\nc10.indd  12/05/2014  Page  266\nAnother popular Unix editor — emacs — has also made its way to the Linux world. The \nLinux version of emacs has both console and a graphical mode, making it the bridge \nbetween the old world and the new. The emacs editor provides multiple buffer areas, allow-\ning you to edit multiple fi les simultaneously.\nThe KDE Project created two editors for use in the KDE desktop. The KWrite editor is a sim-\nple editor that provides the basic text-editing features, along with a few advanced features, \nsuch as syntax highlighting for programming code, line numbering, and code folding. The \nKate editor provides more advanced features for programmers. One great feature in Kate is \na built-in terminal window. You can open a command line interface session directly in the \nKate editor without having to open a separate terminal emulator window. The Kate editor \nalso allows you to open multiple fi les, providing different windows for each opened fi le.\nThe GNOME Project also provides a simple text editor for programmers. The gedit editor is \na basic text editor that provides some advanced features such as code syntax highlighting \nand line numbering, but it was designed to be a bare-bones editor. To spruce up the gedit \neditor, developers created plug-ins, which expand the features available in gedit. Current \nplug-ins include a spell-checker, a terminal emulator, and a fi le browser.\nThis wraps up the background chapters on working with the command line in Linux. The \nnext part of the book dives into the shell-scripting world. The next chapter starts off by \nshowing you how to create a shell script fi le and how to run it on your Linux system. It also \nshows you the basics of shell scripts, allowing you to create simple programs by stringing \nmultiple commands together into a script you can run. \n\nc11.indd  12/23/2014  Page  267\nIN THIS PART\nChapter 11\nBasic Script Building\nChapter 12\nUsing Structured Commands\nChapter 13\nMore Structured Commands\nChapter 14\nHandling User Input\nChapter 15\nPresenting Data\nChapter 16\nScript Control\nPart II\nShell Scripting Basics\n\nc11.indd  12/23/2014  Page  268\n\n269\nc11.indd  12/23/2014  Page  269\nBasic Script Building\nIN THIS CHAPTER\nUsing multiple commands\nCreating a script fi le\nDisplaying messages\nUsing variables\nRedirecting input and output\nPipes\nPerforming math\nExiting the script\nN\now that we’ve covered the basics of the Linux system and the command line, it’s time to \nstart coding. This chapter discusses the basics of writing shell scripts. You need to know \nthese basic concepts before you can start writing your own shell script masterpieces.\nUsing Multiple Commands\nSo far you’ve seen how to use the command line interface (CLI) prompt of the shell to enter com-\nmands and view the command results. The key to shell scripts is the ability to enter multiple \ncommands and process the results from each command, even possibly passing the results of one \ncommand to another. The shell allows you to chain commands together into a single step.\nIf you want to run two commands together, you can enter them on the same prompt line, separated \nwith a semicolon: \n$ date ; who\nMon Feb 21 15:36:09 EST 2014\nChristine tty2         2014-02-21 15:26\nSamantha tty3         2014-02-21 15:26\nTimothy  tty1         2014-02-21 15:26\nuser     tty7         2014-02-19 14:03 (:0)\nCHAPTER \n11\n\n270\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  270\nuser     pts/0        2014-02-21 15:21 (:0.0)\n$\nCongratulations, you just wrote a shell script! This simple script uses just two bash shell \ncommands. The \ndate command runs fi rst, displaying the current date and time, followed \nby the output of the \nwho command, showing who is currently logged on to the system. \nUsing this technique, you can string together as many commands as you wish, up to the \nmaximum command line character count of 255 characters.\nUsing this technique is fi ne for small scripts, but it has a major drawback: You must enter \nthe entire command at the command prompt every time you want to run it. Instead of hav-\ning to manually enter the commands onto a command line, you can combine the commands \ninto a simple text fi le. When you need to run the commands, just simply run the text fi le.\nCreating a Script File\nTo place shell commands in a text fi le, fi rst you need to use a text editor (see Chapter 10) to \ncreate a fi le and then enter the commands into the fi le.\nWhen creating a shell script fi le, you must specify the shell you are using in the fi rst line of \nthe fi le. Here’s the format for this: \n#!/bin/bash\nIn a normal shell script line, the pound sign (#) is used as a comment line. A comment line \nin a shell script isn’t processed by the shell. However, the fi rst line of a shell script fi le is \na special case, and the pound sign followed by the exclamation point tells the shell what \nshell to run the script under (yes, you can be using a bash shell and run your script using \nanother shell).\nAfter indicating the shell, commands are entered onto each line of the fi le, followed by a \ncarriage return. As mentioned, comments can be added by using the pound sign. An exam-\nple looks like this: \n#!/bin/bash\n# This script displays the date and who's logged on\ndate\nwho\nAnd that’s all there is to it. You can use the semicolon and put both commands on the same \nline if you want to, but in a shell script, you can list commands on separate lines. The shell \nprocesses commands in the order in which they appear in the fi le.\nAlso notice that another line was included that starts with the pound symbol and adds \na comment. Lines that start with the pound symbol (other than the fi rst \n#! line) aren’t \n\n271\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  271\n11\ninterpreted by the shell. This is a great way to leave comments for yourself about what’s \nhappening in the script, so when you come back to it two years later, you can easily remem-\nber what you did.\nSave this script in a fi le called \ntest1, and you are almost ready. You need to do a couple of \nthings before you can run your new shell script fi le.\nIf you try running the fi le now, you’ll be somewhat disappointed to see this: \n$ test1\nbash: test1: command not found\n$\nThe fi rst hurdle to jump is getting the bash shell to fi nd your script fi le. If you remember \nfrom Chapter 6, the shell uses an environment variable called \nPATH to fi nd commands. A \nquick look at the \nPATH environment variable demonstrates our problem:\n$ echo $PATH\n/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:/usr/bin\n:/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/user/bin $\nThe PATH environment variable is set to look for commands only in a handful of directo-\nries. To get the shell to fi nd the \ntest1 script, we need to do one of two things: \n ■\nAdd the directory where our shell script fi le is located to the PATH environment \nvariable.\n ■\nUse an absolute or relative fi le path to reference our shell script fi le in the prompt.\nSome Linux distributions add the $HOME/bin directory to the PATH environment variable. This creates a place in \nevery user’s \nHOME directory to place fi les where the shell can fi nd them to execute.\nFor this example, we use the second method to tell the shell exactly where the script fi le is \nlocated. Remember that to reference a fi le in the current directory, you can use the single \ndot operator in the shell: \n$ ./test1\nbash: ./test1: Permission denied\n$\nThe shell found the shell script fi le just fi ne, but there’s another problem. The shell indi-\ncated that you don’t have permission to execute the fi le. A quick look at the fi le permis-\nsions should show what’s going on here: \n$ ls -l test1\n-rw-rw-r--    1 user     user           73 Sep 24 19:56 test1\n$\n\n272\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  272\nWhen the new test1 fi le was created, the umask value determined the default permis-\nsion settings for the new fi le. Because the \numask variable is set to 002 (see Chapter 7) in \nUbuntu, the system created the fi le with only read/write permissions for the fi le’s owner \nand group.\nThe next step is to give the fi le owner permission to execute the fi le, using the \nchmod com-\nmand (see Chapter 7):\n$ chmod u+x test1\n$ ./test1\nMon Feb 21 15:38:19 EST 2014\nChristine tty2         2014-02-21 15:26\nSamantha tty3         2014-02-21 15:26\nTimothy  tty1         2014-02-21 15:26\nuser     tty7         2014-02-19 14:03 (:0)\nuser     pts/0        2014-02-21 15:21 (:0.0) $\nSuccess! Now all the pieces are in the right places to execute the new shell script fi le.\nDisplaying Messages\nMost shell commands produce their own output, which is displayed on the console moni-\ntor where the script is running. Many times, however, you will want to add your own text \nmessages to help the script user know what is happening within the script. You can do this \nwith the \necho command. The echo command can display a simple text string if you add \nthe string following the command: \n$ echo This is a test\nThis is a test\n$\nNotice that by default you don’t need to use quotes to delineate the string you’re display-\ning. However, sometimes this can get tricky if you are using quotes within your string: \n$ echo Let's see if this'll work\nLets see if thisll work\n$\nThe echo command uses either double or single quotes to delineate text strings. If you use \nthem within your string, you need to use one type of quote within the text and the other \ntype to delineate the string: \n$ echo \"This is a test to see if you're paying attention\"\nThis is a test to see if you're paying attention\n$ echo 'Rich says \"scripting is easy\".'\nRich says \"scripting is easy\".\n$\nNow all the quotation marks appear properly in the output.\n\n273\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  273\n11\nYou can add echo statements anywhere in your shell scripts where you need to display \nadditional information: \n$ cat test1\n#!/bin/bash\n# This script displays the date and who's logged on\necho  The time and date are:\ndate\necho \"Let's see who's logged into the system:\"\nwho\n$\nWhen you run this script, it produces the following output: \n$ ./test1\nThe time and date are:\nMon Feb 21 15:41:13 EST 2014\nLet's see who's logged into the system:\nChristine tty2         2014-02-21 15:26\nSamantha tty3         2014-02-21 15:26\nTimothy  tty1         2014-02-21 15:26\nuser     tty7         2014-02-19 14:03 (:0)\nuser     pts/0        2014-02-21 15:21 (:0.0)\n$\nThat’s nice, but what if you want to echo a text string on the same line as a command out-\nput? You can use the \n-n parameter for the echo statement to do that. Just change the fi rst \necho statement line to this: \necho -n \"The time and date are: \"\nYou need to use quotes around the string to ensure that there’s a space at the end of the \nechoed string. The command output begins exactly where the string output stops. The out-\nput now looks like this: \n$ ./test1\nThe time and date are: Mon Feb 21 15:42:23 EST 2014\nLet's see who's logged into the system:\nChristine tty2         2014-02-21 15:26\nSamantha tty3         2014-02-21 15:26\nTimothy  tty1         2014-02-21 15:26\nuser     tty7         2014-02-19 14:03 (:0)\nuser     pts/0        2014-02-21 15:21 (:0.0)\n$\nPerfect! The echo command is a crucial piece of shell scripts that interact with users. You’ll \nfi nd yourself using it in many situations, especially when you want to display the values of \nscript variables. Let’s look at that next.\n\n274\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  274\nUsing Variables\nJust running individual commands from the shell script is useful, but this has its limi-\ntations. Often, you’ll want to incorporate other data in your shell commands to process \ninformation. You can do this by using variables. Variables allow you to temporarily store \ninformation within the shell script for use with other commands in the script. This section \nshows how to use variables in your shell scripts.\nEnvironment variables\nYou’ve already seen one type of Linux variable in action. Chapter 6 described the environ-\nment variables available in the Linux system. You can access these values from your shell \nscripts as well.\nThe shell maintains environment variables that track specifi c system information, such as \nthe name of the system, the name of the user logged in to the system, the user’s system ID \n(called UID), the default home directory of the user, and the search path used by the shell \nto fi nd programs. You can display a complete list of active environment variables available \nby using the \nset command: \n$ set\nBASH=/bin/bash\n[...]\nHOME=/home/Samantha\nHOSTNAME=localhost.localdomain\nHOSTTYPE=i386\nIFS=$' \\t\\n'\nIMSETTINGS_INTEGRATE_DESKTOP=yes\nIMSETTINGS_MODULE=none\nLANG=en_US.utf8\nLESSOPEN='|/usr/bin/lesspipe.sh %s'\nLINES=24\nLOGNAME=Samantha\n[...]\nYou can tap into these environment variables from within your scripts by using the envi-\nronment variable’s name preceded by a dollar sign. This is demonstrated in the following \nscript: \n$ cat test2\n#!/bin/bash\n# display user information from the system.\necho \"User info for userid: $USER\"\necho UID: $UID\necho HOME: $HOME\n$\n\n275\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  275\n11\nThe $USER, $UID, and $HOME environment variables are used to display the pertinent \ninformation about the logged-in user. The output should look something like this: \n$chmod u+x test2\n$ ./test2\nUser info for userid: Samantha\nUID: 1001\nHOME: /home/Samantha\n$\nNotice that the environment variables in the echo commands are replaced by their current \nvalues when the script runs. Also notice that we were able to place the \n$USER system vari-\nable within the double quotation marks in the fi rst string, and the shell script still fi gured \nout what we meant. There is a drawback to using this method, however. Look at what hap-\npens in this example: \n$ echo \"The cost of the item is $15\"\nThe cost of the item is 5\nThat is obviously not what was intended. Whenever the script sees a dollar sign within \nquotes, it assumes you’re referencing a variable. In this example, the script attempted to \ndisplay the variable \n$1 (which was not defi ned) and then the number 5. To display an \nactual dollar sign, you must precede it with a backslash character: \n$ echo \"The cost of the item is \\$15\"\nThe cost of the item is $15\nThat’s better. The backslash allowed the shell script to interpret the dollar sign as an actual \ndollar sign and not a variable. The next section shows how to create your own variables in \nyour scripts. \nYou may also see variables referenced using the format ${variable}. The extra braces around the variable name are \noften used to help identify the variable name from the dollar sign.\nUser variables\nIn addition to the environment variables, a shell script allows you to set and use your own \nvariables within the script. Setting variables allows you to temporarily store data and use it \nthroughout the script, making the shell script more like a real computer program.\nUser variables can be any text string of up to 20 letters, digits, or an underscore character. User \nvariables are case sensitive, so the variable \nVar1 is different from the variable var1. This little \nrule often gets novice script programmers in trouble.\n\n276\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  276\nValues are assigned to user variables using an equal sign. No spaces can appear between the \nvariable, the equal sign, and the value (another trouble spot for novices). Here are a few \nexamples of assigning values to user variables: \nvar1=10\nvar2=-57\nvar3=testing\nvar4=\"still more testing\"\nThe shell script automatically determines the data type used for the variable value. \nVariables defi ned within the shell script maintain their values throughout the life of the \nshell script but are deleted when the shell script completes.\nJust like system variables, user variables can be referenced using the dollar sign: \n$ cat test3\n#!/bin/bash\n# testing variables\ndays=10\nguest=\"Katie\"\necho \"$guest checked in $days days ago\"\ndays=5\nguest=\"Jessica\"\necho \"$guest checked in $days days ago\"\n$\nRunning the script produces the following output: \n$ chmod u+x test3\n$ ./test3\nKatie checked in 10 days ago\nJessica checked in 5 days ago\n$\nEach time the variable is referenced, it produces the value currently assigned to it. It’s \nimportant to remember that when referencing a variable value you use the dollar sign, but \nwhen referencing the variable to assign a value to it, you do not use the dollar sign. Here’s \nan example of what I mean: \n$ cat test4\n#!/bin/bash\n# assigning a variable value to another variable\nvalue1=10\nvalue2=$value1\necho The resulting value is $value2\n$\n\n277\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  277\n11\nWhen you use the value of the value1 variable in the assignment statement, you must still \nuse the dollar sign. This code produces the following output: \n$ chmod u+x test4\n$ ./test4\nThe resulting value is 10\n$\nIf you forget the dollar sign and make the value2 assignment line look like this:\nvalue2=value1\nyou get the following output: \n$ ./test4\nThe resulting value is value1\n$\nWithout the dollar sign, the shell interprets the variable name as a normal text string, \nwhich is most likely not what you wanted.\nCommand substitution\nOne of the most useful features of shell scripts is the ability to extract information from \nthe output of a command and assign it to a variable. After you assign the output to a vari-\nable, you can use that value anywhere in your script. This comes in handy when processing \ndata in your scripts.\nThere are two ways to assign the output of a command to a variable:\n ■\nThe backtick character (`)\n ■\nThe $() format\nBe careful with the backtick character; it is not the normal single quotation mark \ncharacter you are used to using for strings. Because it is not used very often outside \nof shell scripts, you may not even know where to fi nd it on your keyboard. You should \nbecome familiar with it because it’s a crucial component of many shell scripts. Hint: On a \nU.S. keyboard, it is usually on the same key as the tilde symbol (~).\nCommand substitution allows you to assign the output of a shell command to a variable. \nAlthough this doesn’t seem like much, it is a major building block in script programming.\nYou must either surround the entire command line command with two backtick characters: \ntesting='date'\nor use the $() format:\ntesting=$(date)\n\n278\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  278\nThe shell runs the command within the command substitution characters and assigns the \noutput to the variable \ntesting. Notice that there are no spaces between the assignment \nequal sign and the command substitution character. Here’s an example of creating a vari-\nable using the output from a normal shell command: \n$ cat test5\n#!/bin/bash\ntesting=$(date)\necho \"The date and time are: \" $testing\n$\nThe variable testing receives the output from the date command, and it is used in the \necho statement to display it. Running the shell script produces the following output: \n$ chmod u+x test5\n$ ./test5\nThe date and time are:  Mon Jan 31 20:23:25 EDT 2014\n$\nThat’s not all that exciting in this example (you could just as easily just put the command \nin the \necho statement), but after you capture the command output in a variable, you can \ndo anything with it.\nHere’s a popular example of how command substitution is used to capture the current date \nand use it to create a unique fi lename in a script: \n#!/bin/bash\n# copy the /usr/bin directory listing to a log file\ntoday=$(date +%y%m%d)\nls /usr/bin -al > log.$today\nThe today variable is assigned the output of a formatted date command. This is a com-\nmon technique used to extract date information for log fi lenames. The \n+%y%m%d format \ninstructs the date command to display the date as a two-digit year, month, and day: \n$ date +%y%m%d\n140131\n$\nThe script assigns the value to a variable, which is then used as part of a fi lename. The fi le \nitself contains the redirected output (discussed in the “Redirecting Input and Output” section) \nof a directory listing. After running the script, you should see a new fi le in your directory: \n-rw-r--r--    1 user     user          769 Jan 31 10:15 log.140131\nThe log fi le appears in the directory using the value of the $today variable as part of the \nfi lename. The contents of the log fi le are the directory listing from the \n/usr/bin direc-\ntory. If the script runs the next day, the log fi lename is \nlog.140201, thus creating a new \nfi le for the new day.\n\n279\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  279\n11\nCommand substitution creates what’s called a subshell to run the enclosed command. A subshell is a separate \nchild shell generated from the shell that’s running the script. Because of that, any variables you create in the script \naren’t available to the subshell command. \nSubshells are also created if you run a command from the command prompt using the ./ path, but they aren’t cre-\nated if you just run the command without a path. However, if you use a built-in shell command, that doesn’t generate \na subshell. Be careful when running scripts from the command prompt!\nRedirecting Input and Output\nSometimes, you want to save the output from a command instead of just having it dis-\nplayed on the monitor. The bash shell provides a few different operators that allow you to \nredirect the output of a command to an alternative location (such as a fi le). Redirection \ncan be used for input as well as output, redirecting a fi le to a command for input. This sec-\ntion describes what you need to do to use redirection in your shell scripts.\nOutput redirection\nThe most basic type of redirection is sending output from a command to a fi le. The bash \nshell uses the greater-than symbol (\n>) for this: \ncommand > outputfile\nAnything that would appear on the monitor from the command instead is stored in the out-\nput fi le specifi ed: \n$ date > test6\n$ ls -l test6\n-rw-r--r--    1 user     user           29 Feb 10 17:56 test6\n$ cat test6\nThu Feb 10 17:56:58 EDT 2014\n$\nThe redirect operator created the fi le test6 (using the default umask settings) and redi-\nrected the output from the date command to the \ntest6 fi le. If the output fi le already \nexists, the redirect operator overwrites the existing fi le with the new fi le data: \n$ who > test6\n$ cat test6\nuser     pts/0    Feb 10 17:55\n$\nNow the contents of the test6 fi le contain the output from the who command.\n\n280\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  280\nSometimes, instead of overwriting the fi le’s contents, you may need to append output from \na command to an existing fi le — for example, if you’re creating a log fi le to document an \naction on the system. In this situation, you can use the double greater-than symbol (\n>>) to \nappend data: \n$ date >> test6\n$ cat test6\nuser     pts/0    Feb 10 17:55\nThu Feb 10 18:02:14 EDT 2014\n$\nThe test6 fi le still contains the original data from the who command processed earlier — \nand now it contains the new output from the \ndate command.\nInput redirection\nInput redirection is the opposite of output redirection. Instead of taking the output of a \ncommand and redirecting it to a fi le, input redirection takes the content of a fi le and redi-\nrects it to a command.\nThe input redirection symbol is the less-than symbol (\n<): \ncommand < inputfile\nThe easy way to remember this is that the command is always listed fi rst in the command \nline, and the redirection symbol “points” to the way the data is fl owing. The less-than \nsymbol indicates that the data is fl owing from the input fi le to the command.\nHere’s an example of using input redirection with the \nwc command: \n$ wc < test6\n      2      11      60\n$\nThe wc command provides a count of text in the data. By default, it produces three values: \n ■\nThe number of lines in the text\n ■\nThe number of words in the text\n ■\nThe number of bytes in the text\nBy redirecting a text fi le to the \nwc command, you can get a quick count of the lines, words, \nand bytes in the fi le. The example shows that there are 2 lines, 11 words, and 60 bytes in \nthe \ntest6 fi le.\nAnother method of input redirection is called inline input redirection. This method allows \nyou to specify the data for input redirection on the command line instead of in a fi le. This \nmay seem somewhat odd at fi rst, but a few applications are available for this process (such \nas those shown in the “Performing Math” section).\n\n281\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  281\n11\nThe inline input redirection symbol is the double less-than symbol (<<). Besides this sym-\nbol, you must specify a text marker that delineates the beginning and end of the data used \nfor input. You can use any string value for the text marker, but it must be the same at the \nbeginning of the data and the end of the data: \ncommand << marker\ndata\nmarker\nWhen using inline input redirection on the command line, the shell prompts for data using \nthe secondary prompt, defi ned in the \nPS2 environment variable (see Chapter 6). Here’s how \nthis looks when you use it: \n$ wc << EOF\n> test string 1\n> test string 2\n> test string 3\n> EOF\n      3       9      42\n$\nThe secondary prompt continues to prompt for more data until you enter the string value \nfor the text marker. The \nwc command performs the line, word, and byte counts of the data \nsupplied by the inline input redirection.\nPipes\nSometimes, you need to send the output of one command to the input of another command. \nThis is possible using redirection, but somewhat clunky: \n$ rpm -qa > rpm.list \n$ sort < rpm.list \nabrt-1.1.14-1.fc14.i686 \nabrt-addon-ccpp-1.1.14-1.fc14.i686 \nabrt-addon-kerneloops-1.1.14-1.fc14.i686 \nabrt-addon-python-1.1.14-1.fc14.i686 \nabrt-desktop-1.1.14-1.fc14.i686 \nabrt-gui-1.1.14-1.fc14.i686 \nabrt-libs-1.1.14-1.fc14.i686 \nabrt-plugin-bugzilla-1.1.14-1.fc14.i686 \nabrt-plugin-logger-1.1.14-1.fc14.i686 \nabrt-plugin-runapp-1.1.14-1.fc14.i686 \nacl-2.2.49-8.fc14.i686 \n[...]\n\n282\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  282\nThe rpm command manages the software packages installed on systems using the Red Hat \nPackage Management system (RPM), such as the Fedora system as shown. When used with \nthe \n-qa parameters, it produces a list of the existing packages installed, but not necessar-\nily in any specifi c order. If you’re looking for a specifi c package or group of packages, it can \nbe diffi cult to fi nd it using the output of the \nrpm command.\nUsing the standard output redirection, the output was redirected from the \nrpm command \nto a fi le, called \nrpm.list. After the command fi nished, the rpm.list fi le contained a list \nof all the installed software packages on my system. Next, input redirection was used to \nsend the contents of the \nrpm.list fi le to the sort command to sort the package names \nalphabetically.\nThat was useful, but again, a somewhat clunky way of producing the information. Instead \nof redirecting the output of a command to a fi le, you can redirect the output to another \ncommand. This process is called piping. \nLike the command substitution backtick, the symbol for piping is not used often outside of \nshell scripting. The symbol is two vertical lines, one above the other. However, the \npipe \nsymbol often looks like a single vertical line in print (\n|). On a U.S. keyboard, it is usually \non the same key as the backslash (\n\\). The pipe is put between the commands to redirect \nthe output from one to the other:\ncommand1 | command2\nDon’t think of piping as running two commands back to back. The Linux system actually \nruns both commands at the same time, linking them together internally in the system. As \nthe fi rst command produces output, it’s sent immediately to the second command. No inter-\nmediate fi les or buffer areas are used to transfer the data.\nNow, using piping you can easily \npipe the output of the rpm command directly to the \nsort command to produce your results: \n$ rpm -qa | sort\nabrt-1.1.14-1.fc14.i686 \nabrt-addon-ccpp-1.1.14-1.fc14.i686 \nabrt-addon-kerneloops-1.1.14-1.fc14.i686 \nabrt-addon-python-1.1.14-1.fc14.i686 \nabrt-desktop-1.1.14-1.fc14.i686 \nabrt-gui-1.1.14-1.fc14.i686 \nabrt-libs-1.1.14-1.fc14.i686 \nabrt-plugin-bugzilla-1.1.14-1.fc14.i686 \nabrt-plugin-logger-1.1.14-1.fc14.i686 \nabrt-plugin-runapp-1.1.14-1.fc14.i686 \nacl-2.2.49-8.fc14.i686 \n[...]\n\n283\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  283\n11\nUnless you’re a (very) quick reader, you probably couldn’t keep up with the output gener-\nated by this command. Because the piping feature operates in real time, as soon as the \nrpm \ncommand produces data, the \nsort command gets busy sorting it. By the time the rpm com-\nmand fi nishes outputting data, the \nsort command already has the data sorted and starts \ndisplaying it on the monitor.\nThere’s no limit to the number of \npipes you can use in a command. You can continue pip-\ning the output of commands to other commands to refi ne your operation.\nIn this case, because the output of the \nsort command zooms by so quickly, you can use \none of the text paging commands (such as \nless or more) to force the output to stop at \nevery screen of data: \n$ rpm -qa | sort | more\nThis command sequence runs the rpm command, pipes the output to the sort command, \nand then \npipes that output to the more command to display the data, stopping after \nevery screen of information. This now lets you pause and read what’s on the display before \ncontinuing, as shown in Figure 11-1.\nFIGURE 11-1\nUsing piping to send data to the more command\nTo get even fancier, you can use redirection along with piping to save your output to a fi le: \n$ rpm -qa | sort > rpm.list \n$ more rpm.list \n\n284\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  284\nabrt-1.1.14-1.fc14.i686 \nabrt-addon-ccpp-1.1.14-1.fc14.i686 \nabrt-addon-kerneloops-1.1.14-1.fc14.i686 \nabrt-addon-python-1.1.14-1.fc14.i686 \nabrt-desktop-1.1.14-1.fc14.i686 \nabrt-gui-1.1.14-1.fc14.i686 \nabrt-libs-1.1.14-1.fc14.i686 \nabrt-plugin-bugzilla-1.1.14-1.fc14.i686 \nabrt-plugin-logger-1.1.14-1.fc14.i686 \nabrt-plugin-runapp-1.1.14-1.fc14.i686 \nacl-2.2.49-8.fc14.i686 \n[...]\nAs expected, the data in the rpm.list fi le is now sorted!\nBy far one of the most popular uses of piping is piping the results of commands that pro-\nduce long output to the \nmore command. This is especially common with the ls command, \nas shown in Figure 11-2.\nFIGURE 11-2\nUsing the more command with the ls command\nThe ls -l command produces a long listing of all the fi les in the directory. For directories \nwith lots of fi les, this can be quite a listing. By piping the output to the \nmore command, \nyou force the output to stop at the end of every screen of data.\n\n285\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  285\n11\nPerforming Math\nAnother feature crucial to any programming language is the ability to manipulate numbers. \nUnfortunately, for shell scripts this process is a bit awkward. There are two different ways \nto perform mathematical operations in your shell scripts.\nThe expr command\nOriginally, the Bourne shell provided a special command that was used for processing math-\nematical equations. The \nexpr command allowed the processing of equations from the com-\nmand line, but it is extremely clunky: \n$ expr 1 + 5\n6\nThe expr command recognizes a few different mathematical and string operators, shown in \nTable 11-1.\nTABLE 11-1    The expr Command Operators\nOperatorDescription\nARG1 | ARG2\nReturns ARG1 if neither argument is null or zero; otherwise, returns \nARG2\nARG1 & ARG2\nReturns ARG1 if neither argument is null or zero; otherwise, returns \n0\nARG1 < ARG2\nReturns 1 if ARG1 is less than ARG2; otherwise, returns 0\nARG1 <= ARG2\nReturns 1 if ARG1 is less than or equal to ARG2; otherwise, returns 0\nARG1 = ARG2\nReturns 1 if ARG1 is equal to ARG2; otherwise, returns 0\nARG1 != ARG2\nReturns 1 if ARG1 is not equal to ARG2; otherwise, returns 0\nARG1 >= ARG2\nReturns 1 if ARG1 is greater than or equal to ARG2; otherwise, \nreturns 0\nARG1 > ARG2\nReturns 1 if ARG1 is greater than ARG2; otherwise, returns 0\nARG1 + ARG2\nReturns the arithmetic sum of ARG1 and ARG2\nARG1 - ARG2\nReturns the arithmetic difference of ARG1 and ARG2\nARG1 * ARG2\nReturns the arithmetic product of ARG1 and ARG2\nARG1 / ARG2\nReturns the arithmetic quotient of ARG1 divided by ARG2\nARG1 % ARG2\nReturns the arithmetic remainder of ARG1 divided by ARG2\nSTRING : REGEXP\nReturns the pattern match if REGEXP matches a pattern in STRING\nContinues\n\n286\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  286\nOperatorDescription\nmatch STRING REGEXP\nReturns the pattern match if REGEXP matches a pattern in STRING\nsubstr STRING POS \nLENGTH\nReturns the substring LENGTH characters in length, starting at posi-\ntion \nPOS (starting at 1)\nindex STRING CHARS\nReturns position in STRING where CHARS is found; otherwise, \nreturns 0\nlength STRING\nReturns the numeric length of the string STRING\n+ TOKEN\nInterprets TOKEN as a string, even if it’s a keyword\n(EXPRESSION)\nReturns the value of EXPRESSION\nAlthough the standard operators work fi ne in the expr command, the problem occurs when \nusing them from a script or the command line. Many of the \nexpr command operators have \nother meanings in the shell (such as the asterisk). Using them in the \nexpr command pro-\nduces odd results: \n$ expr 5 * 2\nexpr: syntax error\n$\nTo solve this problem, you need to use the shell escape character (the backslash) to identify \nany characters that may be misinterpreted by the shell before being passed to the \nexpr \ncommand: \n$ expr 5 \\* 2\n10\n$\nNow that’s really starting to get ugly! Using the expr command in a shell script is equally \ncumbersome: \n$ cat test6\n#!/bin/bash\n# An example of using the expr command\nvar1=10\nvar2=20\nvar3=$(expr $var2 / $var1)\necho The result is $var3\nTo assign the result of a mathematical equation to a variable, you have to use command \nsubstitution to extract the output from the \nexpr command: \n$ chmod u+x test6\n$ ./test6\nThe result is 2\n$\nTABLE 11-1   (continued)\n\n287\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  287\n11\nFortunately, the bash shell has an improvement for processing mathematical operators as \nyou shall see in the next section.\nUsing brackets\nThe bash shell includes the expr command to stay compatible with the Bourne shell; how-\never, it also provides a much easier way of performing mathematical equations. In bash, \nwhen assigning a mathematical value to a variable, you can enclose the mathematical equa-\ntion using a dollar sign and square brackets (\n$[ operation ]): \n$ var1=$[1 + 5]\n$ echo $var1\n6\n$ var2=$[$var1 * 2]\n$ echo $var2\n12\n$\nUsing brackets makes shell math much easier than with the expr command. This same \ntechnique also works in shell scripts: \n$ cat test7\n#!/bin/bash\nvar1=100\nvar2=50\nvar3=45\nvar4=$[$var1 * ($var2 - $var3)]\necho The final result is $var4\n$\nRunning this script produces the output: \n$ chmod u+x test7\n$ ./test7\nThe final result is 500\n$\nAlso, notice that when using the square brackets method for calculating equations, you \ndon’t need to worry about the multiplication symbol, or any other characters, being mis-\ninterpreted by the shell. The shell knows that it’s not a wildcard character because it is \nwithin the square brackets.\nThere’s one major limitation to performing math in the bash shell script. Look at this \nexample: \n$ cat test8\n#!/bin/bash\nvar1=100\n\n288\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  288\nvar2=45\nvar3=$[$var1 / $var2]\necho The final result is $var3\n$\nNow run it and see what happens: \n$ chmod u+x test8\n$ ./test8\nThe final result is 2\n$\nThe bash shell mathematical operators support only integer arithmetic. This is a huge limi-\ntation if you’re trying to do any sort of real-world mathematical calculations. \nThe z shell (zsh) provides full fl oating-point arithmetic operations. If you require fl oating-point calculations in your \nshell scripts, you might consider checking out the z shell (discussed in Chapter 23).\nA fl oating-point solution\nYou can use several solutions for overcoming the bash integer limitation. The most popular \nsolution uses the built-in bash calculator, called \nbc.\nThe basics of bc\nThe bash calculator is actually a programming language that allows you to enter fl oating-\npoint expressions at a command line and then interprets the expressions, calculates them, \nand returns the result. The bash calculator recognizes these: \n ■\nNumbers (both integer and fl oating point)\n ■\nVariables (both simple variables and arrays)\n ■\nComments (lines starting with a pound sign or the C language /* */ pair)\n ■\nExpressions\n ■\nProgramming statements (such as if-then statements)\n ■\nFunctions\nYou can access the bash calculator from the shell prompt using the \nbc command: \n$ bc\nbc 1.06.95\nCopyright 1991-1994, 1997, 1998, 2000, 2004, 2006 Free Software Foundation, Inc.\nThis is free software with ABSOLUTELY NO WARRANTY.\nFor details type 'warranty'.\n\n289\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  289\n11\n12 * 5.4\n64.8\n3.156 * (3 + 5)\n25.248\nquit\n$\nThe example starts out by entering the expression 12 * 5.4. The bash calculator returns \nthe answer. Each subsequent expression entered into the calculator is evaluated, and the \nresult is displayed. To exit the bash calculator, you must enter \nquit.\nThe fl oating-point arithmetic is controlled by a built-in variable called \nscale. You must set \nthis value to the desired number of decimal places you want in your answers, or you won’t \nget what you were looking for: \n$ bc -q\n3.44 / 5\n0\nscale=4\n3.44 / 5\n.6880\nquit\n$\nThe default value for the scale variable is zero. Before the scale value is set, the bash \ncalculator provides the answer to zero decimal places. After you set the \nscale variable \nvalue to four, the bash calculator displays the answer to four decimal places. The \n-q com-\nmand line parameter suppresses the lengthy welcome banner from the bash calculator.\nIn addition to normal numbers, the bash calculator also understands variables: \n$ bc -q\nvar1=10\nvar1 * 4\n40\nvar2 = var1 / 5\nprint var2\n2\nquit\n$\nAfter a variable value is defi ned, you can use the variable throughout the bash calculator \nsession. The \nprint statement allows you to print variables and numbers.\nUsing bc in scripts\nNow you may be wondering how the bash calculator is going to help you with fl oating-point \narithmetic in your shell scripts. Do you remember command substitution? Yes, you can use \n\n290\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  290\nthe command substitution character to run a bc command and assign the output to a vari-\nable! The basic format to use is this: \nvariable=$(echo \"options; expression\" | bc)\nThe fi rst portion, options, allows you to set variables. If you need to set more than one \nvariable, separate them using the semicolon. The expression parameter defi nes the math-\nematical expression to evaluate using \nbc. Here’s a quick example of doing this in a script: \n$ cat test9\n#!/bin/bash\nvar1=$(echo \"scale=4; 3.44 / 5\" | bc)\necho The answer is $var1\n$\nThis example sets the scale variable to four decimal places and then specifi es a specifi c \ncalculation for the expression. Running this script produces the following output:\n$ chmod u+x test9\n$ ./test9\nThe answer is .6880\n$\nNow that’s fancy! You aren’t limited to just using numbers for the expression value. You can \nalso use variables defi ned in the shell script: \n$ cat test10\n#!/bin/bash\nvar1=100\nvar2=45\nvar3=$(echo \"scale=4; $var1 / $var2\" | bc)\necho The answer for this is $var3\n$\nThe script defi nes two variables, which are used within the expression sent to the bc com-\nmand. Remember to use the dollar sign to signify the value for the variables and not the \nvariables themselves. The output of this script is as follows: \n$ ./test10\nThe answer for this is 2.2222\n$\nAnd of course, after a value is assigned to a variable, that variable can be used in yet \nanother calculation: \n$ cat test11\n#!/bin/bash\nvar1=20\nvar2=3.14159\n\n291\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  291\n11\nvar3=$(echo \"scale=4; $var1 * $var1\" | bc)\nvar4=$(echo \"scale=4; $var3 * $var2\" | bc)\necho The final result is $var4\n$\nThis method works fi ne for short calculations, but sometimes you need to get more involved \nwith your numbers. If you have more than just a couple of calculations, it gets confusing \ntrying to list multiple expressions on the same command line.\nThere’s a solution to this problem. The \nbc command recognizes input redirection, allowing \nyou to redirect a fi le to the \nbc command for processing. However, this also can get confus-\ning, because you’d need to store your expressions in a fi le.\nThe best method is to use inline input redirection, which allows you to redirect data \ndirectly from the command line. In the shell script, you assign the output to a variable: \nvariable=$(bc << EOF\noptions\nstatements\nexpressions\nEOF\n)\nThe EOF text string indicates the beginning and end of the inline redirection data. \nRemember that the command substitution characters are still needed to assign the output \nof the \nbc command to the variable.\nNow you can place all the individual bash calculator elements on separate lines in the script \nfi le. Here’s an example of using this technique in a script: \n$ cat test12\n#!/bin/bash\nvar1=10.46\nvar2=43.67\nvar3=33.2\nvar4=71\nvar5=$(bc << EOF\nscale = 4\na1 = ( $var1 * $var2)\nb1 = ($var3 * $var4)\na1 + b1\nEOF\n)\necho The final answer for this mess is $var5\n$\n\n292\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  292\nPlacing each option and expression on a separate line in your script makes things cleaner \nand easier to read and follow. The \nEOF string indicates the start and end of the data to \nredirect to the \nbc command. Of course, you must use the command substitution characters \nto indicate the command to assign to the variable.\nYou’ll also notice in this example that you can assign variables within the bash calculator. \nIt’s important to remember that any variables created within the bash calculator are valid \nonly within the bash calculator and can’t be used in the shell script.\nExiting the Script\nSo far in our sample scripts, we terminated things pretty abruptly. When we were fi nished \nwith our last command, we just ended the script. There’s a more elegant way of completing \nthings available to us.\nEvery command that runs in the shell uses an \nexit status to indicate to the shell that \nit’s fi nished processing. The exit status is an integer value between 0 and 255 that’s passed \nby the command to the shell when the command fi nishes running. You can capture this \nvalue and use it in your scripts.\nChecking the exit status\nLinux provides the $? special variable that holds the exit status value from the last com-\nmand that executed. You must view or use the \n$? variable immediately after the command \nyou want to check. It changes values to the exit status of the last command executed by \nthe shell: \n$ date\nSat Jan 15 10:01:30 EDT 2014\n$ echo $?\n0\n$\nBy convention, the exit status of a command that successfully completes is zero. If a com-\nmand completes with an error, then a positive integer value is placed in the exit status: \n$ asdfg\n-bash: asdfg: command not found\n$ echo $?\n127\n$\nThe invalid command returns an exit status of 127. There’s not much of a standard \nconvention to Linux error exit status codes. However, you can use the guidelines shown in \nTable 11-2.\n\n293\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  293\n11\nTABLE 11-2    Linux Exit Status Codes\nCodeDescription\n0Successful completion of the command\n1General unknown error\n2Misuse of shell command\n126The command can’t execute\n127Command not found\n128Invalid exit argument\n128+xFatal error with Linux signal x\n130Command terminated with Ctrl+C\n255Exit status out of range\nAn exit status value of 126 indicates that the user didn’t have the proper permissions set to \nexecute the command: \n$ ./myprog.c\n-bash: ./myprog.c: Permission denied\n$ echo $?\n126\n$\nAnother common error you’ll encounter occurs if you supply an invalid parameter to a \ncommand: \n$ date %t\ndate: invalid date '%t'\n$ echo $?\n1\n$\nThis generates the general exit status code of 1, indicating that an unknown error occurred \nin the command.\nThe exit command\nBy default, your shell script exits with the exit status of the last command in your script: \n$ ./test6\nThe result is 2\n$ echo $?\n0\n$\n\n294\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  294\nYou can change that to return your own exit status code. The exit command allows you to \nspecify an exit status when your script ends: \n$ cat test13\n#!/bin/bash\n# testing the exit status\nvar1=10\nvar2=30\nvar3=$[$var1 + $var2]\necho The answer is $var3\nexit 5\n$\nWhen you check the exit status of the script, you get the value used as the parameter of \nthe \nexit command: \n$ chmod u+x test13\n$ ./test13\nThe answer is 40\n$ echo $?\n5\n$\nYou can also use variables in the exit command parameter: \n$ cat test14\n#!/bin/bash\n# testing the exit status\nvar1=10\nvar2=30\nvar3=$[$var1 + $var2]\nexit $var3\n$\nWhen you run this command, it produces the following exit status: \n$ chmod u+x test14\n$ ./test14\n$ echo $?\n40\n$\nYou should be careful with this feature, however, because the exit status codes can only go \nup to 255. Watch what happens in this example: \n$ cat test14b\n#!/bin/bash\n# testing the exit status\nvar1=10\n\n295\nChapter 11: Basic Script Building\nc11.indd  12/23/2014  Page  295\n11\nvar2=30\nvar3=$[$var1 * $var2]\necho The value is $var3\nexit $var3\n$\nNow when you run it, you get the following: \n$ ./test14b\nThe value is 300\n$ echo $?\n44\n$\nThe exit status code is reduced to fi t in the 0 to 255 range. The shell does this by using \nmodulo arithmetic. The \nmodulo of a value is the remainder after a division. The resulting \nnumber is the remainder of the specifi ed number divided by 256. In the case of 300 (the \nresult value), the remainder is 44, which is what appears as the exit status code.\nIn Chapter 12, you’ll see how you can use the \nif-then statement to check the error status \nreturned by a command to see whether the command was successful.\nSummary\nThe bash shell script allows you to string commands together into a script. The most basic \nway to create a script is to separate multiple commands on the command line using a semi-\ncolon. The shell executes each command in order, displaying the output of each command \non the monitor.\nYou can also create a shell script fi le, placing multiple commands in the fi le for the shell \nto execute in order. The shell script fi le must defi ne the shell used to run the script. This \nis done in the fi rst line of the script fi le, using the \n#! symbol, followed by the full path of \nthe shell.\nWithin the shell script you can reference environment variable values by using a dollar sign \nin front of the variable. You can also defi ne your own variables for use within the script, \nand assign values and even the output of a command by using the backtick character or the \n$() format. The variable value can be used within the script by placing a dollar sign in front \nof the variable name.\nThe bash shell allows you to redirect both the input and output of a command from the \nstandard behavior. You can redirect the output of any command from the monitor display \nto a fi le by using the greater-than symbol, followed by the name of the fi le to capture the \noutput. You can append output data to an existing fi le by using two greater-than symbols. \n\n296\nPart II: Shell Scripting Basics\nc11.indd  12/23/2014  Page  296\nThe less-than symbol is used to redirect input to a command. You can redirect input from a \nfi le to a command.\nThe Linux \npipe command (the broken bar symbol) allows you to redirect the output of \na command directly to the input of another command. The Linux system runs both com-\nmands at the same time, sending the output of the fi rst command to the input of the sec-\nond command without using any redirect fi les.\nThe bash shell provides a couple of ways for you to perform mathematical operations in \nyour shell scripts. The \nexpr command is a simple way to perform integer math. In the bash \nshell, you can also perform basic math calculations by enclosing equations in square brack-\nets, preceded by a dollar sign. To perform fl oating-point arithmetic, you need to utilize the \nbc calculator command, redirecting input from inline data and storing the output in a user \nvariable.\nFinally, the chapter discussed how to use the exit status in your shell script. Every com-\nmand that runs in the shell produces an exit status. The exit status is an integer value \nbetween 0 and 255 that indicates if the command completed successfully, and if not, what \nthe reason may have been. An exit status of 0 indicates that the command completed suc-\ncessfully. You can use the \nexit  command in your shell script to declare a specifi c exit sta-\ntus upon the completion of your script.\nSo far in your shell scripts, things have proceeded in an orderly fashion from one command \nto the next. In the next chapter, you’ll see how you can use some logic fl ow control to alter \nwhich commands are executed within the script.\n\n297\nc12.indd  12/23/2014  Page  297\nCHAPTER \n12\nUsing Structured Commands\nIN THIS CHAPTER\nWorking with the if-then statement\nNesting ifs\nUnderstanding the test command\nTesting compound conditions\nUsing double brackets and parentheses\nLooking at case\nI\nn the shell scripts presented in Chapter 11, the shell processed each individual command in the \nshell script in the order it appeared. This works out fi ne for sequential operations, where you want \nall the commands to process in the proper order. However, this isn’t how all programs operate.\nMany programs require some sort of logic fl ow control between the commands in the script. There is \na whole command class that allows the script to skip over executed commands based on tested con-\nditions. These commands are generally referred to as structured commands.\nThe structured commands allow you to alter the operation fl ow of a program. Quite a few structured \ncommands are available in the bash shell, so we’ll look at them individually. In this chapter, we \nlook at \nif-then and case statements.\nWorking with the if-then Statement\nThe most basic type of structured command is the if-then statement. The if-then statement \nhas the following format: \nif command\nthen\n    commands\nfi\nIf you’re using if-then statements in other programming languages, this format may be somewhat \nconfusing. In other programming languages, the object after the \nif statement is an equation that \nis evaluated for a \nTRUE or FALSE value. That’s not how the bash shell if statement works.\n\n298\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  298\nThe bash shell if statement runs the command defi ned on the if line. If the exit status of \nthe command (see Chapter 11) is zero (the command completed successfully), the commands \nlisted under the \nthen section are executed. If the exit status of the command is anything \nelse, the \nthen commands aren’t executed, and the bash shell moves on to the next com-\nmand in the script. The \nfi statement delineates the if-then statement’s end.\nHere’s a simple example to demonstrate this concept: \n$ cat test1.sh\n#!/bin/bash\n# testing the if statement\nif pwd\nthen\n    echo \"It worked\"\nfi\n$\nThis script uses the pwd command on the if line. If the command completes successfully, \nthe \necho statement should display the text string. When you run this script from the com-\nmand line, you get the following results: \n$ ./test1.sh\n/home/Christine\nIt worked\n$\nThe shell executed the pwd command listed on the if line. Because the exit status was \nzero, it also executed the \necho statement listed in the then section.\nHere’s another example: \n$ cat test2.sh\n#!/bin/bash\n# testing a bad command\nif IamNotaCommand\nthen\n   echo \"It worked\"\nfi\necho \"We are outside the if statement\"\n$\n$ ./test2.sh\n./test2.sh: line 3: IamNotaCommand: command not found\nWe are outside the if statement\n$\nIn this example, we deliberately used a command, IamNotaCommand, that does not work \nin the \nif statement line. Because this is a bad command, it produces an exit status that’s \nnon-zero, and the bash shell skips the \necho statement in the then section. Also notice \nthat the error message generated from running the command in the \nif statement still \n\n299\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  299\n12\n12\nappears in the script’s output. There may be times when you don’t want an error statement \nto appear. Chapter 15 discusses how this can be avoided.\nYou might see an alternative form of the if-then statement used in some scripts:\nif command; then\n   commands\nfi\nBy putting a semicolon at the end of the command to evaluate, you can include the then statement on the same \nline, which looks closer to how \nif-then statements are handled in some other programming languages.\nYou are not limited to just one command in the then section. You can list commands just \nas in the rest of the shell script. The bash shell treats the commands as a block, executing \nall of them when the command in the \nif statement line returns a zero exit status or skip-\nping all of them when the command returns a non-zero exit status: \n$ cat test3.sh\n#!/bin/bash\n# testing multiple commands in the then section\n#\ntestuser=Christine\n#\nif grep $testuser /etc/passwd\nthen\n   echo \"This is my first command\"\n   echo \"This is my second command\"\n   echo \"I can even put in other commands besides echo:\"\n   ls -a /home/$testuser/.b*\nfi\n$\nThe if statement line uses the grep comment to search the /etc/passwd fi le to see if a \nspecifi c username is currently used on the system. If there’s a user with that logon name, \nthe script displays some text and then lists the bash fi les in the user’s \nHOME directory:\n$ ./test3.sh\nChristine:x:501:501:Christine B:/home/Christine:/bin/bash\nThis is my first command\nThis is my second command\nI can even put in other commands besides echo:\n/home/Christine/.bash_history  /home/Christine/.bash_profile\n/home/Christine/.bash_logout   /home/Christine/.bashrc\n$\nHowever, if you set the testuser variable to a user that doesn’t exist on the system, \nnothing happens: \n\n300\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  300\n$ cat test3.sh\n#!/bin/bash\n# testing multiple commands in the then section\n#\ntestuser=NoSuchUser\n#\nif grep $testuser /etc/passwd\nthen\n   echo \"This is my first command\"\n   echo \"This is my second command\"\n   echo \"I can even put in other commands besides echo:\"\n   ls -a /home/$testuser/.b*\nfi\n$\n$ ./test3.sh\n$\nIt’s not all that exciting. It would be nice if we could display a little message saying that \nthe username wasn’t found on the system. Well, we can, using another feature of the \nif-then statement.\nExploring the if-then-else Statement\nIn the if-then statement, you have only one option for whether a command is success-\nful. If the command returns a non-zero exit status code, the bash shell just moves on to \nthe next command in the script. In this situation, it would be nice to be able to execute an \nalternate set of commands. That’s exactly what the \nif-then-else statement is for.\nThe \nif-then-else statement provides another group of commands in the statement: \nif command\nthen\n   commands\nelse\n   commands\nfi\nWhen the command in the if statement line returns with a zero exit status code, the com-\nmands listed in the \nthen section are executed, just as in a normal if-then statement. \nWhen the command in the \nif statement line returns a non-zero exit status code, the bash \nshell executes the commands in the \nelse section.\nNow you can copy and modify the test script to include an \nelse section: \n$ cp test3.sh test4.sh\n$\n$ nano test4.sh\n$\n\n301\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  301\n12\n$ cat test4.sh\n#!/bin/bash\n# testing the else section\n#\ntestuser=NoSuchUser\n#\nif grep $testuser /etc/passwd\nthen\n   echo \"The bash files for user $testuser are:\"\n   ls -a /home/$testuser/.b*\n   echo\nelse\n   echo \"The user $testuser does not exist on this system.\"\n   echo\nfi\n$\n$ ./test4.sh\nThe user NoSuchUser does not exist on this system.\n$\nThat’s more user-friendly. Just like the then section, the else section can contain mul-\ntiple commands. The \nfi statement delineates the end of the else section.\nNesting ifs\nSometimes, you must check for several situations in your script code. For these situations, \nyou can nest the \nif-then statements:\nTo check if a logon name is not in the \n/etc/passwd fi le and yet a directory for that user \nstill exists, use a nested \nif-then statement. In this case, the nested if-then statement \nis within the primary \nif-then-else statement’s else code block:\n$ ls -d /home/NoSuchUser/\n/home/NoSuchUser/\n$\n$ cat test5.sh\n#!/bin/bash\n# Testing nested ifs\n#\ntestuser=NoSuchUser\n#\nif grep $testuser /etc/passwd\nthen\n   echo \"The user $testuser exists on this system.\"\nelse\n   echo \"The user $testuser does not exist on this system.\"\n\n302\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  302\n   if ls -d /home/$testuser/\n   then\n      echo \"However, $testuser has a directory.\"\n   fi\nfi\n$\n$ ./test5.sh\nThe user NoSuchUser does not exist on this system.\n/home/NoSuchUser/\nHowever, NoSuchUser has a directory.\n$\nThe script correctly fi nds that although the login name has been removed from the /etc/\npasswd\n fi le, the user’s directory is still on the system. The problem with using this man-\nner of nested \nif-then statements in a script is that the code can get hard to read, and the \nlogic fl ow becomes diffi cult to follow. \nInstead of having to write separate \nif-then statements, you can use an alternative ver-\nsion of the \nelse section, called elif. The elif continues an else section with another \nif-then statement: \nif command1\nthen\n   commands\nelif command2\nthen\n    more commands\nfi\nThe elif statement line provides another command to evaluate, similar to the original if \nstatement line. If the exit status code from the elif command is zero, bash executes the \ncommands in the second \nthen statement section. Using this method of nesting provides \ncleaner code with an easier-to-follow logic fl ow:\n$ cat test5.sh\n#!/bin/bash\n# Testing nested ifs - use elif\n#\ntestuser=NoSuchUser\n#\nif grep $testuser /etc/passwd\nthen\n   echo \"The user $testuser exists on this system.\"\n#\nelif ls -d /home/$testuser\nthen\n   echo \"The user $testuser does not exist on this system.\"\n   echo \"However, $testuser has a directory.\"\n#\n\n303\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  303\n12\nfi\n$\n$ ./test5.sh\n/home/NoSuchUser\nThe user NoSuchUser does not exist on this system.\nHowever, NoSuchUser has a directory.\n$\nYou can even take this script a step further and have it check for both a non-existent user \nwith a directory and a non-existent user without a directory. This is accomplished by add-\ning an \nelse statement within the nested elif:\n$ cat test5.sh\n#!/bin/bash\n# Testing nested ifs - use elif & else\n#\ntestuser=NoSuchUser\n#\nif grep $testuser /etc/passwd\nthen\n   echo \"The user $testuser exists on this system.\"\n#\nelif ls -d /home/$testuser\nthen\n   echo \"The user $testuser does not exist on this system.\"\n   echo \"However, $testuser has a directory.\"\n#\nelse\n   echo \"The user $testuser does not exist on this system.\"\n   echo \"And, $testuser does not have a directory.\"\nfi\n$\n$ ./test5.sh\n/home/NoSuchUser\nThe user NoSuchUser does not exist on this system.\nHowever, NoSuchUser has a directory.\n$\n$ sudo rmdir /home/NoSuchUser\n[sudo] password for Christine:\n$\n$ ./test5.sh\nls: cannot access /home/NoSuchUser: No such file or directory\nThe user NoSuchUser does not exist on this system.\nAnd, NoSuchUser does not have a directory.\n$\nBefore the /home/NoSuchUser directory was removed and the test script executed the \nelif statement, a zero exit status was returned. Thus, the statements within the elif’s \nthen code block were executed. After the /home/NoSuchUser directory was removed, a \n\n304\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  304\nnon-zero exit status was returned for the elif statement. This caused the statements in \nthe \nelse block within the elif block to be executed.\nKeep in mind that, with an elif statement, any else statements immediately following it are for that elif code \nblock. They are not part of a preceding \nif-then statement code block.\nYou can continue to string elif statements together, creating one huge if-then-elif \nconglomeration: \nif command1\nthen\n    command set 1\nelif command2\nthen\n   command set 2\nelif command3\nthen\n   command set 3\nelif command4\nthen\n   command set 4\nfi\nEach block of commands is executed depending on which command returns the zero exit \nstatus code. Remember that the bash shell executes the \nif statements in order, and only \nthe fi rst one that returns a zero exit status results in the \nthen section being executed. \nEven though the code looks cleaner with \nelif statements, it still can be confusing to fol-\nlow the script’s logic. Later in the “Considering the case Command” section, you’ll see how \nto use the \ncase command instead of having to nest lots of if-then statements.\nTrying the test Command\nSo far, all you’ve seen in the if statement line are normal shell commands. You might be \nwondering if the bash \nif-then statement has the ability to evaluate any condition other \nthan a command’s exit status code.\nThe answer is no, it can’t. However, there’s a neat utility available in the bash shell that \nhelps you evaluate other things, using the \nif-then statement.\nThe \ntest command provides a way to test different conditions in an if-then statement. \nIf the condition listed in the \ntest command evaluates to TRUE, the test command exits \nwith a zero exit status code. This makes the \nif-then statement behave in much the same \n\n305\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  305\n12\nway that if-then statements work in other programming languages. If the condition is \nFALSE, the \ntest command exits with a non-zero exit status code, which causes the if-\nthen\n statement to exit.\nThe format of the test command is pretty simple: \ntest condition\nThe condition is a series of parameters and values that the test command evaluates. When \nused in an \nif-then statement, the test command looks like this: \nif test condition\nthen\n   commands\nfi\nIf you leave out the condition portion of the test command statement, it exits with a \nnon-zero exit status code and triggers any \nelse block statements:\n$ cat test6.sh\n#!/bin/bash\n# Testing the test command\n#\nif test\nthen\n   echo \"No expression returns a True\"\nelse\n   echo \"No expression returns a False\"\nfi\n$\n$ ./test6.sh\nNo expression returns a False\n$\nWhen you add in a condition, it is tested by the test command. For example, using the \ntest command, you can determine whether a variable has content. A simple condition \nexpression is needed to determine whether a variable has content:\n$ cat test6.sh\n#!/bin/bash\n# Testing the test command\n#\nmy_variable=\"Full\"\n#\nif test $my_variable\nthen\n   echo \"The $my_variable expression returns a True\"\n#\nelse\n\n306\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  306\n   echo \"The $my_variable expression returns a False\"\nfi\n$\n$ ./test6.sh\nThe Full expression returns a True\n$\nThe variable my_variable contains content (Full), so when the test command checks the \ncondition, the exit status returns a zero. This triggers the statement in the \nthen code block.\nAs you would suspect, the opposite occurs when the variable does not contain content:\n$ cat test6.sh\n#!/bin/bash\n# Testing the test command\n#\nmy_variable=\"\"\n#\nif test $my_variable\nthen\n   echo \"The $my_variable expression returns a True\"\n#\nelse\n   echo \"The $my_variable expression returns a False\"\nfi\n$\n$ ./test6.sh\nThe  expression returns a False\n$\nThe bash shell provides an alternative way of testing a condition without declaring the \ntest command in an if-then statement: \nif [ condition ]\nthen\n   commands\nfi\nThe square brackets defi ne the test condition. Be careful; you must have a space after the \nfi rst bracket and a space before the last bracket, or you’ll get an error message.\nThe \ntest command and test conditions can evaluate three classes of conditions: \n ■\nNumeric comparisons\n ■\nString comparisons\n ■\nFile comparisons\n\n307\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  307\n12\nThe next sections describe how to use each of these test classes in your if-then \nstatements.\nUsing numeric comparisons\nThe most common test evaluation method is to perform a comparison of two numeric val-\nues. Table 12-1 shows the list of condition parameters used for testing two values.\nTABLE 12-1    The test Numeric Comparisons\nComparisonDescription\nn1 -eq n2\nChecks if n1 is equal to n2\nn1 -ge n2\nChecks if n1 is greater than or equal to n2\nn1 -gt n2\nChecks if n1 is greater than n2\nn1 -le n2\nChecks if n1 is less than or equal to n2\nn1 -lt n2\nChecks if n1 is less than n2\nn1 -ne n2\nChecks if n1 is not equal to n2\nThe numeric test conditions can be used to evaluate both numbers and variables. Here’s an \nexample of doing that: \n$ cat numeric_test.sh\n#!/bin/bash\n# Using numeric test evaluations\n#\nvalue1=10\nvalue2=11\n#\nif [ $value1 -gt 5 ]\nthen\n    echo \"The test value $value1 is greater than 5\"\nfi\n#\nif [ $value1 -eq $value2 ]\nthen\n    echo \"The values are equal\"\nelse\n    echo \"The values are different\"\nfi\n#\n$\n\n308\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  308\nThe fi rst test condition: \nif [ $value1 -gt 5 ]\ntests if the value of the variable value1 is greater than 5. The second test condition: \nif [ $value1 -eq $value2 ]\ntests if the value of the variable value1 is equal to the value of the variable value2. Both \nnumeric test conditions evaluate as expected:\n$ ./numeric_test.sh\nThe test value 10 is greater than 5\nThe values are different\n$\nThere is a limitation to the test numeric conditions concerning fl oating-point values:\n$ cat floating_point_test.sh\n#!/bin/bash\n# Using floating point numbers in test evaluations\n#\nvalue1=5.555\n#\necho \"The test value is $value1\"\n#\nif [ $value1 -gt 5 ]\nthen\n    echo \"The test value $value1 is greater than 5\"\nfi\n#\n$ ./floating_point_test.sh\nThe test value is 5.555\n./floating_point_test.sh: line 8: \n[: 5.555: integer expression expected\n$\nThis example uses a fl oating-point value, stored in the value1 variable. Next, it evaluates \nthe value. Something obviously went wrong.\nRemember that the only numbers the bash shell can handle are integers. This works per-\nfectly fi ne if all you need to do is display the result, using an \necho statement. However, \nthis doesn’t work in numeric-oriented functions, such as our numeric test condition. The \nbottom line is that you cannot use fl oating-point values for test conditions.\nUsing string comparisons\nTest conditions also allow you to perform comparisons on string values. Performing com-\nparisons on strings can get tricky, as you’ll see. Table 12-2 shows the comparison functions \nyou can use to evaluate two string values.\n\n309\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  309\n12\nTABLE 12-2    The test String Comparisons\nComparisonDescription\nstr1 = str2\nChecks if str1 is the same as string str2\nstr1 != str2\nChecks if str1 is not the same as str2\nstr1 < str2\nChecks if str1 is less than str2\nstr1 > str2\nChecks if str1 is greater than str2\n-n str1\nChecks if str1 has a length greater than zero\n-z str1\nChecks if str1 has a length of zero\nThe following sections describe the different string comparisons available.\nLooking at string equality\nThe equal and not equal conditions are fairly self-explanatory with strings. It’s pretty easy \nto know when two string values are the same or not: \n$ cat test7.sh\n#!/bin/bash\n# testing string equality\ntestuser=rich\n#\nif [ $USER = $testuser ]\nthen\n   echo \"Welcome $testuser\"\nfi\n$ \n$ ./test7.sh\nWelcome rich\n$\nAlso, using the not equals string comparison allows you to determine if two strings have \nthe same value or not: \n$ cat test8.sh\n#!/bin/bash\n# testing string equality\ntestuser=baduser\n#\nif [ $USER != $testuser ]\nthen\n   echo \"This is not $testuser\"\nelse\n   echo \"Welcome $testuser\"\nfi\n\n310\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  310\n$ \n$ ./test8.sh\nThis is not baduser\n$\nKeep in mind that the test comparison takes all punctuation and capitalization into \naccount when comparing strings for equality.\nLooking at string order\nTrying to determine if one string is less than or greater than another is where things \nstart getting tricky. Two problems often plague shell programmers when trying to use the \ngreater-than or less-than features of test conditions: \n ■\nThe greater-than and less-than symbols must be escaped, or the shell uses them \nas redirection symbols, with the string values as fi lenames.\n ■\nThe greater-than and less-than order is not the same as that used with the \nsort command.\nThe fi rst item can result in a huge problem that often goes undetected when program-\nming your scripts. Here’s an example of what sometimes happens to novice shell script \nprogrammers: \n$ cat badtest.sh\n#!/bin/bash\n# mis-using string comparisons\n#\nval1=baseball\nval2=hockey\n#\nif [ $val1 > $val2 ]\nthen\n   echo \"$val1 is greater than $val2\"\nelse\n   echo \"$val1 is less than $val2\"\nfi\n$ \n$ ./badtest.sh\nbaseball is greater than hockey\n$ ls -l hockey\n-rw-r--r--    1 rich     rich            0 Sep 30 19:08 hockey\n$\nBy just using the greater-than symbol itself in the script, no errors are generated, but the \nresults are wrong. The script interpreted the greater-than symbol as an output redirection \n(see Chapter 15). Thus, it created a fi le called hockey. Because the redirection completed \nsuccessfully, the test condition returns a zero exit status code, which the \nif statement \nevaluates as though things completed successfully!\n\n311\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  311\n12\nTo fi x this problem, you need to properly escape the greater-than symbol: \n$ cat test9.sh\n#!/bin/bash\n# mis-using string comparisons\n#\nval1=baseball\nval2=hockey\n#\nif [ $val1 \\> $val2 ]\nthen\n  echo \"$val1 is greater than $val2\"\nelse\n   echo \"$val1 is less than $val2\"\nfi\n$ \n$ ./test9.sh\nbaseball is less than hockey\n$\nNow that answer is more along the lines of what you would expect from the string \ncomparison.\nThe second issue is a little more subtle, and you may not even run across it unless you are \nworking with uppercase and lowercase letters. The \nsort command handles uppercase let-\nters opposite to the way the test conditions consider them: \n$ cat test9b.sh\n#!/bin/bash\n# testing string sort order\nval1=Testing\nval2=testing\n#\nif [ $val1 \\> $val2 ]\nthen\n   echo \"$val1 is greater than $val2\"\nelse\n   echo \"$val1 is less than $val2\"\nfi\n$ \n$ ./test9b.sh\nTesting is less than testing\n$ \n$ sort testfile\ntesting\nTesting\n$\n\n312\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  312\nCapitalized letters are treated as less than lowercase letters in test comparisons. However, the \nsort command does the opposite. When you put the same strings in a fi le and use the sort \ncommand, the lowercase letters appear fi rst. This is due to different ordering techniques. \nTest comparisons use standard ASCII ordering, using each character’s ASCII numeric value \nto determine the sort order. The \nsort command uses the sorting order defi ned for the \nsystem locale language settings. For the English language, the locale settings specify that \nlowercase letters appear before uppercase letters in sorted order. \nThe test command and test expressions use the standard mathematical comparison symbols for string compari-\nsons and text codes for numerical comparisons. This is a subtle feature that many programmers manage to get \nreversed. If you use the mathematical comparison symbols for numeric values, the shell interprets them as string \nvalues and may not produce the correct results.\nLooking at string size\nThe -n and -z comparisons are handy when trying to evaluate whether a variable \ncontains data: \n$ cat test10.sh\n#!/bin/bash\n# testing string length\nval1=testing\nval2=''\n#\nif [ -n $val1 ]\nthen\n   echo \"The string '$val1' is not empty\"\nelse\n   echo \"The string '$val1' is empty\"\nfi\n#\nif [ -z $val2 ]\nthen\n   echo \"The string '$val2' is empty\"\nelse\n   echo \"The string '$val2' is not empty\"\nfi\n#\nif [ -z $val3 ]\nthen\n   echo \"The string '$val3' is empty\"\nelse\n   echo \"The string '$val3' is not empty\"\nfi\n\n313\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  313\n12\n$ \n$ ./test10.sh\nThe string 'testing' is not empty\nThe string '' is empty\nThe string '' is empty\n$\nThis example creates two string variables. The val1 variable contains a string, and the \nval2 variable is created as an empty string. The following comparisons are made as shown \nbelow: \nif [ -n $val1 ]\nThe preceding code determines whether the val1 variable is non-zero in length, which it is, \nso its \nthen section is processed. \nif [ -z $var2 ]\nThis preceding code determines whether the val2 variable is zero in length, which it is, so \nits \nthen section is processed. \nif [ -z $val3 ]\nThe preceding determines whether the val3 variable is zero in length. This variable was \nnever defi ned in the shell script, so it indicates that the string length is still zero, even \nthough it wasn’t defi ned. \nEmpty and uninitialized variables can have catastrophic effects on your shell script tests. If you’re not sure of the \ncontents of a variable, it’s always best to test if the variable contains a value using \n-n or -z before using it in a \nnumeric or string comparison.\nUsing fi le comparisons\nThe last category of test comparisons is quite possibly the most powerful and most used \ncomparisons in shell scripting. This category allows you to test the status of fi les and direc-\ntories on the Linux fi lesystem. Table 12-3 lists these comparisons.\nTABLE 12-3    The test File Comparisons\nComparisonDescription\n-d file\nChecks if file exists and is a directory\n-e file\nChecks if file exists\n-f file\nChecks if file exists and is a fi le\nContinues\n\n314\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  314\nComparisonDescription\n-r file\nChecks if file exists and is readable\n-s file\nChecks if file exists and is not empty\n-w file\nChecks if file exists and is writable\n-x file\nChecks if file exists and is executable\n-O file\nChecks if file exists and is owned by the current user\n-G file\nChecks if file exists and the default group is the same as the \ncurrent user\nfile1 -nt file2\nChecks if file1 is newer than file2\nfile1 -ot file2\nChecks if file1 is older than file2\nThese conditions give you the ability to check fi lesystem fi les within shell scripts. They are \noften used in scripts that access fi les. Because they’re used so often, let’s look at each of \nthese individually.\nChecking directories\nThe -d test checks to see if a specifi ed directory exists on the system. This is usually a \ngood thing to do if you’re trying to write a fi le to a directory or before you try to change to \na directory location: \n$ cat test11.sh\n#!/bin/bash\n# Look before you leap\n#\njump_directory=/home/arthur\n#\nif [ -d $jump_directory ]\nthen\n   echo \"The $jump_directory directory exists\"\n   cd $jump_directory\n   ls\nelse\n   echo \"The $jump_directory directory does not exist\"\nfi\n#\n$\n$ ./test11.sh\nThe /home/arthur directory does not exist\n$\nThe -d test condition checks to see if the jump_directory variable’s directory exists. If \nit does, it proceeds to use the \ncd command to change to the current directory and performs \na directory listing. If it does not, the script emits a warning message and exits the script. \nTABLE 12.3   (continued)\n\n315\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  315\n12\nChecking whether an object exists\nThe -e comparison allows you to check if either a fi le or directory object exists before you \nattempt to use it in your script: \n$ cat test12.sh\n#!/bin/bash\n# Check if either a directory or file exists\n#\nlocation=$HOME\nfile_name=\"sentinel\"\n#\nif [ -e $location ]\nthen  #Directory does exist\n   echo \"OK on the $location directory.\"\n   echo \"Now checking on the file, $file_name.\"\n   #\n   if [ -e $location/$file_name ]\n   then #File does exist\n       echo \"OK on the filename\"\n       echo \"Updating Current Date...\"\n       date >> $location/$file_name\n   #\n   else #File does not exist\n       echo \"File does not exist\"\n       echo \"Nothing to update\"\n   fi\n#\nelse   #Directory does not exist\n   echo \"The $location directory does not exist.\"\n   echo \"Nothing to update\"\nfi\n#\n$\n$ ./test12.sh\nOK on the /home/Christine directory.\nNow checking on the file, sentinel.\nFile does not exist\nNothing to update\n$\n$ touch sentinel\n$\n$ ./test12.sh\nOK on the /home/Christine directory.\nNow checking on the file, sentinel.\nOK on the filename\nUpdating Current Date...\n$\n\n316\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  316\nThe fi rst check uses the -e comparison to determine whether the user has a $HOME direc-\ntory. If so, the next \n-e comparison checks to determine whether the sentinel fi le exists \nin the \n$HOME directory. If the fi le doesn’t exist, the shell script notes that the fi le is miss-\ning and that there is nothing to update. \nTo ensure that the update will work, the \nsentinel fi le was created and the shell script \nwas run a second time. This time when the conditions are tested, both the \n$HOME and the \nsentinel fi le are found, and the current date and time is appended to the fi le. \nChecking for a file\nThe -e comparison works for both fi les and directories. To be sure that the object specifi ed \nis a fi le and not a directory, you must use the \n-f comparison: \n$ cat test13.sh\n#!/bin/bash\n# Check if either a directory or file exists\n#\nitem_name=$HOME\necho\necho \"The item being checked: $item_name\"\necho\n#\nif [ -e $item_name ]\nthen  #Item does exist\n   echo \"The item, $item_name, does exist.\"\n   echo \"But is it a file?\"\n   echo\n   #\n   if [ -f $item_name ]\n   then #Item is a file\n       echo \"Yes, $item_name is a file.\"\n   #\n   else #Item is not a file\n       echo \"No, $item_name is not a file.\"\n   fi\n#\nelse   #Item does not exist\n   echo \"The item, $item_name, does not exist.\"\n   echo \"Nothing to update\"\nfi\n#\n$ ./test13.sh\nThe item being checked: /home/Christine\nThe item, /home/Christine, does exist.\nBut is it a file?\nNo, /home/Christine is not a file.\n$\n\n317\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  317\n12\nThis little script does lots of checking! First, it uses the -e comparison to test whether \n$HOME exists. If it does, it uses -f to test whether it’s a fi le. If it isn’t a fi le (which of \ncourse it isn’t), a message is displayed stating that it is not a fi le.\nA slight modifi cation to the variable, \nitem_name, replacing the directory $HOME with a \nfi le, \n$HOME/sentinel, causes a different outcome:\n$ nano test13.sh\n$\n$ cat test13.sh\n#!/bin/bash\n# Check if either a directory or file exists\n#\nitem_name=$HOME/sentinel\n[...]\n$\n$ ./test13.sh\nThe item being checked: /home/Christine/sentinel\nThe item, /home/Christine/sentinel, does exist.\nBut is it a file?\nYes, /home/Christine/sentinel is a file.\n$\nThe test13.sh script listing is snipped, because the only item changed in the shell script \nwas the \nitem_name variable’s value. Now when the script is run, the -f test on $HOME/\nsentinel\n exits with a zero status, triggering the then statement, which in turn outputs \nthe message \nYes, /home/Christine/sentinel is a file.\nChecking for read access\nBefore trying to read data from a fi le, it’s usually a good idea to test whether you can read \nfrom the fi le fi rst. You do this with the \n-r comparison: \n$ cat test14.sh\n#!/bin/bash\n# testing if you can read a file\npwfile=/etc/shadow\n#\n# first, test if the file exists, and is a file\nif [ -f $pwfile ]\nthen\n   # now test if you can read it\n   if [ -r $pwfile ]\n\n318\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  318\n   then\n      tail $pwfile\n   else\n      echo \"Sorry, I am unable to read the $pwfile file\"\n   fi\nelse\n   echo \"Sorry, the file $file does not exist\"\nfi\n$ \n$ ./test14.sh\nSorry, I am unable to read the /etc/shadow file\n$\nThe /etc/shadow fi le contains the encrypted passwords for system users, so it’s not read-\nable by normal users on the system. The \n-r comparison determined that read access to the \nfi le wasn’t allowed, so the test command failed and the bash shell executed the \nelse sec-\ntion of the \nif-then statement.\nChecking for empty files\nYou should use -s comparison to check whether a fi le is empty, especially if you don’t want \nto remove a non-empty fi le. Be careful because when the \n-s comparison succeeds, it indi-\ncates that a fi le has data in it: \n$ cat test15.sh\n#!/bin/bash\n# Testing if a file is empty\n#\nfile_name=$HOME/sentinel\n#\nif [ -f $file_name ]\nthen\n   if [ -s $file_name ]\n   then\n      echo \"The $file_name file exists and has data in it.\"\n      echo \"Will not remove this file.\"\n#\n   else\n      echo \"The $file_name file exists, but is empty.\"\n      echo \"Deleting empty file...\"\n      rm $file_name\n   fi\nelse\n   echo \"File, $file_name, does not exist.\"\nfi\n#\n$ \nls -l $HOME/sentinel\n-rw-rw-r--. 1 Christine Christine 29 Jun 25 05:32 /home/Christine/sentinel\n\n319\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  319\n12\n$\n$ \n./test15.sh\nThe /home/Christine/sentinel file exists and has data in it.\nWill not remove this file.\n$\nFirst, the -f comparison tests whether the fi le exists. If it does exist, the -s comparison is trig-\ngered to determine whether the fi le is empty. An empty fi le will be deleted. You can see from \nthe \nls -l that the sentinel fi le is not empty, and therefore the script does not delete it.\nChecking whether you can write to a file\nThe -w comparison determines whether you have permission to write to a fi le. The \ntest16.sh script is simply an update of the test13.sh script. Now instead of just check-\ning whether the \nitem_name exists and is a fi le, the script also checks to see whether it has \npermission to write to the fi le: \n$ cat test16.sh\n#!/bin/bash\n# Check if a file is writable.\n#\nitem_name=$HOME/sentinel\necho\necho \"The item being checked: $item_name\"\necho\n[...]\n       echo \"Yes, $item_name is a file.\"\n       echo \"But is it writable?\"\n       echo\n       #\n       if [ -w $item_name ]\n       then #Item is writable\n            echo \"Writing current time to $item_name\"\n            date +%H%M >> $item_name\n       #\n       else #Item is not writable\n            echo \"Unable to write to $item_name\"\n       fi\n   #\n   else #Item is not a file\n       echo \"No, $item_name is not a file.\"\n   fi\n[...]\n$\n$ ls -l sentinel\n-rw-rw-r--. 1 Christine Christine 0 Jun 27 05:38 sentinel\n$\n\n320\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  320\n$ ./test16.sh\nThe item being checked: /home/Christine/sentinel\nThe item, /home/Christine/sentinel, does exist.\nBut is it a file?\nYes, /home/Christine/sentinel is a file.\nBut is it writable?\nWriting current time to /home/Christine/sentinel\n$\n$ cat sentinel\n0543\n$\nThe item_name variable is set to $HOME/sentinel, and this fi le allows user write access \n(see Chapter 7 for more information on fi le permissions). Thus, when the script is run, the \n-w test expressions returns a non-zero exit status and the then code block is executed, \nwhich writes a time stamp into the \nsentinel fi le.\nWhen the sentinel fi le user’s write access is removed via \nchmod, the -w test expression \nreturns a non-zero status, and a time stamp is not written to the fi le:\n$ chmod u-w sentinel\n$\n$ ls -l sentinel\n-r--rw-r--. 1 Christine Christine 5 Jun 27 05:43 sentinel\n$\n$ ./test16.sh\nThe item being checked: /home/Christine/sentinel\nThe item, /home/Christine/sentinel, does exist.\nBut is it a file?\nYes, /home/Christine/sentinel is a file.\nBut is it writable?\nUnable to write to /home/Christine/sentinel\n$\nThe chmod command could be used again to grant the write permission back for the user. \nThis would make the write test expression return a zero exit status and allow a write \nattempt to the fi le.\n\n321\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  321\n12\nChecking whether you can run a file\nThe -x comparison is a handy way to determine whether you have execute permission for a \nspecifi c fi le. Although this may not be needed for most commands, if you run lots of scripts \nfrom your shell scripts, it could be useful: \n$ cat test17.sh\n#!/bin/bash\n# testing file execution\n#\nif [ -x test16.sh ]\nthen\n   echo \"You can run the script: \"\n   ./test16.sh\nelse\n   echo \"Sorry, you are unable to execute the script\"\nfi\n$ \n$ ./test17.sh\nYou can run the script:\n[...]\n$ \n$ chmod u-x test16.sh\n$ \n$ ./test17.sh\nSorry, you are unable to execute the script\n$\nThis example shell script uses the -x comparison to test whether you have permission to \nexecute the \ntest16.sh script. If so, it runs the script. After successfully running the \ntest16.sh script the fi rst time, the permissions were changed. This time, the -x compari-\nson failed, because execute permission had been removed for the \ntest16.sh script.\nChecking ownership\nThe -O comparison allows you to easily test whether you’re the owner of a fi le: \n$ cat test18.sh\n#!/bin/bash\n# check file ownership\n#\nif [ -O /etc/passwd ]\nthen\n   echo \"You are the owner of the /etc/passwd file\"\nelse\n   echo \"Sorry, you are not the owner of the /etc/passwd file\"\nfi\n\n322\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  322\n$ \n$ ./test18.sh\nSorry, you are not the owner of the /etc/passwd file\n$ \nThe script uses the -O comparison to test whether the user running the script is the owner \nof the \n/etc/passwd fi le. The script is run under a normal user account, so the test fails. \nChecking default group membership\nThe -G comparison checks the default group of a fi le, and it succeeds if it matches the \ngroup of the default group for the user. This can be somewhat confusing because the \n-G comparison checks the default groups only and not all the groups to which the user \nbelongs. Here’s an example of this: \n$ cat test19.sh\n#!/bin/bash\n# check file group test\n#\nif [ -G $HOME/testing ]\nthen\n   echo \"You are in the same group as the file\"\nelse\n   echo \"The file is not owned by your group\"\nfi\n$ \n$ ls -l $HOME/testing\n-rw-rw-r-- 1 rich rich 58 2014-07-30 15:51 /home/rich/testing\n$ \n$ ./test19.sh\nYou are in the same group as the file\n$ \n$ chgrp sharing $HOME/testing\n$ \n$ ./test19\nThe file is not owned by your group\n$\nThe fi rst time the script is run, the $HOME/testing fi le is in the rich group, and the -G \ncomparison succeeds. Next, the group is changed to the \nsharing group, of which the user \nis also a member. However, the \n-G comparison failed, because it compares only the default \ngroups, not any additional group memberships.\nChecking file date\nThe last set of comparisons deal with comparing the creation times of two fi les. This comes \nin handy when writing scripts to install software. Sometimes, you don’t want to install a \nfi le that is older than a fi le already installed on the system.\n\n323\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  323\n12\nThe -nt comparison determines whether a fi le is newer than another fi le. If a fi le is newer, \nit has a more recent fi le creation time. The \n-ot comparison determines whether a fi le is \nolder than another fi le. If the fi le is older, it has an older fi le creation time: \n$ cat test20.sh\n#!/bin/bash\n# testing file dates\n#\nif [ test19.sh -nt test18.sh ]\nthen\n   echo \"The test19 file is newer than test18\"\nelse\n   echo \"The test18 file is newer than test19\"\nfi\nif [ test17.sh -ot test19.sh ]\nthen\n  echo \"The test17 file is older than the test19 file\"\nfi\n$ \n$ ./test20.sh\nThe test19 file is newer than test18\nThe test17 file is older than the test19 file\n$ \n$ ls -l test17.sh test18.sh test19.sh\n-rwxrw-r-- 1 rich rich 167 2014-07-30 16:31 test17.sh\n-rwxrw-r-- 1 rich rich 185 2014-07-30 17:46 test18.sh\n-rwxrw-r-- 1 rich rich 167 2014-07-30 17:50 test19.sh\n$\nThe fi le paths used in the comparisons are relative to the directory from which you run the \nscript. This can cause problems if the fi les being checked are moved around. Another prob-\nlem is that neither of these comparisons checks whether the fi le exists fi rst. Try this test: \n$ cat test21.sh\n#!/bin/bash\n# testing file dates\n#\nif [ badfile1 -nt badfile2 ]\nthen\n   echo \"The badfile1 file is newer than badfile2\"\nelse\n   echo \"The badfile2 file is newer than badfile1\"\nfi\n$ \n$ ./test21.sh\nThe badfile2 file is newer than badfile1\n$\n\n324\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  324\nThis little example demonstrates that if the fi les don’t exist, the -nt comparison just \nreturns a failed condition. It’s imperative to ensure that the fi les exist before trying to use \nthem in the \n-nt or -ot comparison.\nConsidering Compound Testing\nThe if-then statement allows you to use Boolean logic to combine tests. You can use \nthese two Boolean operators: \n ■\n[ condition1 ] && [ condition2 ]\n ■\n[ condition1 ] || [ condition2 ]\nThe fi rst Boolean operation uses the AND Boolean operator to combine two conditions. Both \nconditions must be met for the \nthen section to execute.\nBoolean logic is a method that reduces the potential returned values to be either TRUE or FALSE.\nThe second Boolean operation uses the OR Boolean operator to combine two conditions. If \neither condition evaluates to a \nTRUE condition, the then section is executed. \nThe following shows the \nAND Boolean operator in use:\n$ cat test22.sh\n#!/bin/bash\n# testing compound comparisons\n#\nif [ -d $HOME ] && [ -w $HOME/testing ]\nthen\n   echo \"The file exists and you can write to it\"\nelse\n   echo \"I cannot write to the file\"\nfi\n$ \n$ ./test22.sh\nI cannot write to the file\n$\n$ touch $HOME/testing\n$ \n$ ./test22.sh\nThe file exists and you can write to it\n$\nUsing the AND Boolean operator, both of the comparisons must be met. The fi rst comparison \nchecks to see if the \n$HOME directory exists for the user. The second comparison checks to \n\n325\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  325\n12\nsee if there’s a fi le called testing in the user’s $HOME directory, and if the user has write \npermissions for the fi le. If either of these comparisons fails, the \nif statement fails and the \nshell executes the \nelse section. If both of the comparisons succeed, the if statement suc-\nceeds, and the shell executes the \nthen section.\nWorking with Advanced if-then Features\nTwo additions to the bash shell provide advanced features that you can use in if-then \nstatements: \n ■\nDouble parentheses for mathematical expressions\n ■\nDouble square brackets for advanced string handling functions\nThe following sections describe each of these features in more detail.\nUsing double parentheses\nThe double parentheses command allows you to incorporate advanced mathematical formulas \nin your comparisons. The \ntest command allows for only simple arithmetic operations in \nthe comparison. The double parentheses command provides more mathematical symbols, \nwhich programmers who have used other programming languages may be familiar with \nusing. Here’s the format of the double parentheses command: \n(( expression ))\nThe expression term can be any mathematical assignment or comparison expression. Besides \nthe standard mathematical operators that the \ntest command uses, Table 12-4 shows the \nlist of additional operators available for use in the double parentheses command.\nTABLE 12- 4    The Double Parentheses Command Symbols\nSymbolDescription\nval++\nPost-increment\nval--\nPost-decrement\n++val\nPre-increment\n--val\nPre-decrement\n!\nLogical negation\n~\nBitwise negation\n**\nExponentiation\nContinues\n\n326\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  326\nSymbolDescription\n<<\nLeft bitwise shift\n>>\nRight bitwise shift\n&\nBitwise Boolean AND\n|\nBitwise Boolean OR\n&&\nLogical AND\n||\nLogical OR\nYou can use the double parentheses command in an if statement, as well as in a normal \ncommand in the script for assigning values: \n$ cat test23.sh\n#!/bin/bash\n# using double parenthesis\n#\nval1=10\n#\nif (( $val1 ** 2 > 90 ))\nthen\n   (( val2 = $val1 ** 2 ))\n   echo \"The square of $val1 is $val2\"\nfi\n$ \n$ ./test23.sh\nThe square of 10 is 100\n$\nNotice that you don’t need to escape the greater-than symbol in the expression within the dou-\nble parentheses. This is yet another advanced feature besides the double parentheses command.\nUsing double brackets\nThe double bracket command provides advanced features for string comparisons. Here’s the \ndouble bracket command format: \n[[ expression ]]\nThe double bracketed expression uses the standard string comparison used in the test \nevaluations. However, it provides an additional feature that the test evaluations don’t — \npattern matching.\nDouble brackets work fi ne in the bash shell. Be aware, however, that not all shells support double brackets.\nTABLE 12.4   (continued)\n\n327\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  327\n12\nIn pattern matching, you can defi ne a regular expression (discussed in detail in Chapter 20) \nthat’s matched against the string value: \n$ cat test24.sh\n#!/bin/bash\n# using pattern matching\n#\nif [[ $USER == r* ]]\nthen\n   echo \"Hello $USER\"\nelse\n   echo \"Sorry, I do not know you\"\nfi\n$ \n$ ./test24.sh\nHello rich\n$\nNotice in the preceding script that double equal signs (==) are used. These double equal \nsigns designate the string to the right (\nr*) as a pattern, and pattern matching rules are \napplied. The double bracket command matches the \n$USER environment variable to see \nwhether it starts with the letter \nr. If so, the comparison succeeds, and the shell executes \nthe \nthen section commands.\nConsidering the case Command\nOften, you’ll fi nd yourself trying to evaluate a variable’s value, looking for a specifi c value \nwithin a set of possible values. In this scenario, you end up having to write a lengthy \nif-then-else statement, like this: \n$ cat test25.sh\n#!/bin/bash\n# looking for a possible value\n#\nif [ $USER = \"rich\" ]\nthen\n   echo \"Welcome $USER\"\n   echo \"Please enjoy your visit\"\nelif [ $USER = \"barbara\" ]\nthen\n   echo \"Welcome $USER\"\n   echo \"Please enjoy your visit\"\nelif [ $USER = \"testing\" ]\nthen\n   echo \"Special testing account\"\nelif [ $USER = \"jessica\" ]\n\n328\nPart II: Shell Scripting Basics\nc12.indd  12/23/2014  Page  328\nthen\n   echo \"Do not forget to logout when you're done\"\nelse\n   echo \"Sorry, you are not allowed here\"\nfi\n$ \n$ ./test25.sh\nWelcome rich\nPlease enjoy your visit\n$\nThe elif statements continue the if-then checking, looking for a specifi c value for the \nsingle comparison variable.\nInstead of having to write all the \nelif statements to continue checking the same variable \nvalue, you can use the \ncase command. The case command checks multiple values of a \nsingle variable in a list-oriented format: \ncase variable in\npattern1 | pattern2) commands1;;\npattern3) commands2;;\n*) default commands;;\nesac\nThe case command compares the variable specifi ed against the different patterns. If the \nvariable matches the pattern, the shell executes the commands specifi ed for the pattern. \nYou can list more than one pattern on a line, using the bar operator to separate each pat-\ntern. The asterisk symbol is the catch-all for values that don’t match any of the listed \npatterns. Here’s an example of converting the \nif-then-else program to using the case \ncommand: \n$ cat test26.sh\n#!/bin/bash\n# using the case command\n#\ncase $USER in\nrich | barbara)\n   echo \"Welcome, $USER\"\n   echo \"Please enjoy your visit\";;\ntesting)\n  echo \"Special testing account\";;\njessica)\n   echo \"Do not forget to log off when you're done\";;\n*)\n   echo \"Sorry, you are not allowed here\";;\nesac\n$ \n$ ./test26.sh\n\n329\nChapter 12: Using Structured Commands\nc12.indd  12/23/2014  Page  329\n12\nWelcome, rich\nPlease enjoy your visit\n$\nThe case command provides a much cleaner way of specifying the various options for each \npossible variable value.\nSummary\nStructured commands allow you to alter the normal fl ow of shell script execution. The most \nbasic structured command is the \nif-then statement. This statement provides a command \nevaluation and performs other commands based on the evaluated command’s output.\nYou can expand the \nif-then statement to include a set of commands the bash shell exe-\ncutes if the specifi ed command fails as well. The \nif-then-else statement executes com-\nmands only if the command being evaluated returns a non-zero exit status code.\nYou can also link \nif-then-else statements together, using the elif statement. The \nelif is equivalent to using an else if statement, providing for additional checking of \nwhether the original command that was evaluated failed.\nIn most scripts, instead of evaluating a command, you’ll want to evaluate a condition, such \nas a numeric value, the contents of a string, or the status of a fi le or directory. The \ntest \ncommand provides an easy way for you to evaluate all these conditions. If the condition \nevaluates to a \nTRUE condition, the test command produces a zero exit status code for the \nif-then statement. If the condition evaluates to a FALSE condition, the test command \nproduces a non-zero exit status code for the \nif-then statement.\nThe square bracket is a special bash command that is a synonym for the \ntest command. \nYou can enclose a test condition in square brackets in the \nif-then statement to test for \nnumeric, string, and fi le conditions.\nThe double parentheses command provides advanced mathematical evaluations using \nadditional operators. The double square bracket command allows you to perform advanced \nstring pattern-matching evaluations.\nFinally, the chapter discussed the \ncase command, which is a shorthand way of performing \nmultiple \nif-then-else commands, checking the value of a single variable against a list of \nvalues.\nThe next chapter continues the discussion of structured commands by examining the shell \nlooping commands. The \nfor and while  commands let you create loops that iterate through \ncommands for a given period of time.\n\nc12.indd  12/23/2014  Page  330\n\n331\nc13.indd  12/16/2014  Page  331\n CHAPTER \n13\nMore Structured Commands\nIN THIS CHAPTER\nLooping with the for statement\nIterating with the until statement\nUsing the while statement\nCombining loops\nRedirecting loop output\nI\nn the previous chapter, you saw how to manipulate the fl ow of a shell script program by check-\ning the output of commands and the values of variables. In this chapter, we continue to look at \nstructured commands that control the fl ow of your shell scripts. You’ll see how you can perform \nrepeating processes, commands that can loop through a set of commands until an indicated condi-\ntion has been met. This chapter discusses and demonstrates the \nfor, while, and until bash shell \nlooping commands.\nThe for Command\nIterating through a series of commands is a common programming practice. Often, you need to \nrepeat a set of commands until a specifi c condition has been met, such as processing all the fi les in \na directory, all the users on a system, or all the lines in a text fi le.\nThe bash shell provides the \nfor command to allow you to create a loop that iterates through a \nseries of values. Each iteration performs a defi ned set of commands using one of the values in the \nseries. Here’s the basic format of the bash shell \nfor command: \n for var in list\n do\n    \ncommands\n done\nYou supply the series of values used in the iterations in the list parameter. You can specify the val-\nues in the list in several ways.\n\n332\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  332\nIn each iteration, the variable var contains the current value in the list. The fi rst iteration \nuses the fi rst item in the list, the second iteration the second item, and so on until all the \nitems in the list have been used.\nThe commands entered between the \ndo and done statements can be one or more standard \nbash shell commands. Within the commands, the \n$var variable contains the current list \nitem value for the iteration. \nIf you prefer, you can include the do statement on the same line as the for statement, but you must separate it \nfrom the list items using a semicolon: \nfor var in list; do.\nWe mentioned that there are several different ways to specify the values in the list. The \nfollowing sections show the various ways to do that.\nReading values in a list\nThe most basic use of the for command is to iterate through a list of values defi ned within \nthe \nfor command itself: \n $ cat test1\n #!/bin/bash\n # basic for command\n \n for test in Alabama Alaska Arizona Arkansas California Colorado\n do\n    echo The next state is $test\n done\n $ ./test1\n The next state is Alabama\n The next state is Alaska\n The next state is Arizona\n The next state is Arkansas\n The next state is California\n The next state is Colorado\n $\nEach time the for command iterates through the list of values provided, it assigns the \n$test variable the next value in the list. The $test variable can be used just like any \nother script variable within the \nfor command statements. After the last iteration, the \n$test variable remains valid throughout the remainder of the shell script. It retains the \nlast iteration value (unless you change its value): \n $ cat test1b\n #!/bin/bash\n\n333\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  333\n13\n13\n # testing the for variable after the looping\n for test in Alabama Alaska Arizona Arkansas California Colorado\n do\n    echo \"The next state is $test\"\n done\n echo \"The last state we visited was $test\"\n test=Connecticut\n echo \"Wait, now we're visiting $test\"\n $ ./test1b\n The next state is Alabama\n The next state is Alaska\n The next state is Arizona\n The next state is Arkansas\n The next state is California\n The next state is Colorado\n The last state we visited was Colorado\n Wait, now we're visiting Connecticut\n $\nThe $test variable retained its value and allowed us to change the value and use it outside \nof the \nfor command loop, as any other variable would.\nReading complex values in a list\nThings aren’t always as easy as they seem with the for loop. There are times when you \nrun into data that causes problems. Here’s a classic example of what can cause problems for \nshell script programmers: \n $ cat badtest1\n #!/bin/bash\n # another example of how not to use the for command\n \n for test in I don't know if this'll work\n do\n    echo \"word:$test\"\n done\n $ ./badtest1\n word:I\n word:dont know if thisll\n word:work\n $\nOuch, that hurts. The shell saw the single quotation marks within the list values and \nattempted to use them to defi ne a single data value, and it really messed things up in the \nprocess.\n\n334\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  334\nYou have two ways to solve this problem: \n ■\nUse the escape character (the backslash) to escape the single quotation mark.\n ■\nUse double quotation marks to defi ne the values that use single quotation marks.\nNeither solution is all that fantastic, but each one helps solve the problem: \n $ cat test2\n #!/bin/bash\n # another example of how not to use the for command\n \n for test in I don\\'t know if \"this'll\" work\n do\n    echo \"word:$test\"\n done\n $ ./test2\n word:I\n word:don't\n word:know\n word:if\n word:this'll\n word:work\n $\nIn the fi rst problem value, you added the backslash character to escape the single quotation \nmark in the \ndon't value. In the second problem value, you enclosed the this'll value in \ndouble quotation marks. Both methods worked fi ne to distinguish the value.\nAnother problem you may run into is multi-word values. Remember that the \nfor loop \nassumes that each value is separated with a space. If you have data values that contain \nspaces, you run into yet another problem: \n $ cat badtest2\n #!/bin/bash\n # another example of how not to use the for command\n \n for test in Nevada New Hampshire New Mexico New York North Carolina\n do\n    echo \"Now going to $test\"\n done\n $ ./badtest1\n Now going to Nevada\n Now going to New\n Now going to Hampshire\n Now going to New\n Now going to Mexico\n Now going to New\n Now going to York\n\n335\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  335\n13\n Now going to North\n Now going to Carolina\n $\nOops, that’s not exactly what we wanted. The for command separates each value in the list \nwith a space. If there are spaces in the individual data values, you must accommodate them \nusing double quotation marks: \n $ cat test3\n #!/bin/bash\n # an example of how to properly define values\n \n for test in Nevada \"New Hampshire\" \"New Mexico\" \"New York\"\n do\n    echo \"Now going to $test\"\n done\n $ ./test3\n Now going to Nevada\n Now going to New Hampshire\n Now going to New Mexico\n Now going to New York\n $\nNow the for command can properly distinguish between the different values. Also, notice \nthat when you use double quotation marks around a value, the shell doesn’t include the \nquotation marks as part of the value.\nReading a list from a variable\nOften what happens in a shell script is that you accumulate a list of values stored in a vari-\nable and then need to iterate through the list. You can do this using the \nfor command as \nwell: \n $ cat test4\n #!/bin/bash\n # using a variable to hold the list\n \n list=\"Alabama Alaska Arizona Arkansas Colorado\"\n list=$list\" Connecticut\"\n \n for state in $list\n do\n    echo \"Have you ever visited $state?\"\n done\n $ ./test4\n Have you ever visited Alabama?\n Have you ever visited Alaska?\n Have you ever visited Arizona?\n\n336\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  336\n Have you ever visited Arkansas?\n Have you ever visited Colorado?\n Have you ever visited Connecticut?\n $\nThe $list variable contains the standard text list of values to use for the iterations. \nNotice that the code also uses another assignment statement to add (or concatenate) an \nitem to the existing list contained in the \n$list variable. This is a common method for \nadding text to the end of an existing text string stored in a variable.\nReading values from a command\nAnother way to generate values for use in the list is to use the output of a command. You \nuse command substitution to execute any command that produces output and then use the \noutput of the command in the \nfor command: \n $ cat test5\n #!/bin/bash\n # reading values from a file\n \n file=\"states\"\n \n for state in $(cat $file)\n do\n    echo \"Visit beautiful $state\"\n done\n $ cat states\n Alabama\n Alaska\n Arizona\n Arkansas\n Colorado\n Connecticut\n Delaware\n Florida\n Georgia\n $ ./test5\n Visit beautiful Alabama\n Visit beautiful Alaska\n Visit beautiful Arizona\n Visit beautiful Arkansas\n Visit beautiful Colorado\n Visit beautiful Connecticut\n Visit beautiful Delaware\n Visit beautiful Florida\n Visit beautiful Georgia\n $\n\n337\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  337\n13\nThis example uses the cat command in the command substitution to display the contents \nof the fi le states. Notice that the states fi le includes each state on a separate line, not sepa-\nrated by spaces. The \nfor command still iterates through the output of the cat command \none line at a time, assuming that each state is on a separate line. However, this doesn’t \nsolve the problem of having spaces in data. If you list a state with a space in it, the \nfor \ncommand still takes each word as a separate value. There’s a reason for this, which we look \nat in the next section. \nThe test5 code example assigned the fi lename to the variable using just the fi lename without a path. This requires \nthat the fi le be in the same directory as the script. If this isn’t the case, you need to use a full pathname (either abso-\nlute or relative) to reference the fi le location.\nChanging the fi eld separator\nThe cause of this problem is the special environment variable IFS, called the internal fi eld \nseparator. The IFS environment variable defi nes a list of characters the bash shell uses \nas fi eld separators. By default, the bash shell considers the following characters as fi eld \nseparators: \n ■\nA space\n ■\nA tab\n ■\nA newline\nIf the bash shell sees any of these characters in the data, it assumes that you’re starting \na new data fi eld in the list. When working with data that can contain spaces (such as fi le-\nnames), this can be annoying, as you saw in the previous script example.\nTo solve this problem, you can temporarily change the IFS environment variable values in \nyour shell script to restrict the characters the bash shell recognizes as fi eld separators. For \nexample, if you want to change the IFS value to recognize only the newline character, you \nneed to do this: \nIFS=$'\\n'\nAdding this statement to your script tells the bash shell to ignore spaces and tabs in data \nvalues. Applying this technique to the previous script yields the following: \n$ cat test5b\n #!/bin/bash\n # reading values from a file\n \n file=\"states\"\n \n IFS=$'\\n'\n\n338\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  338\n for state in $(cat $file)\n do\n    echo \"Visit beautiful $state\"\n done\n $ ./test5b\n Visit beautiful Alabama\n Visit beautiful Alaska\n Visit beautiful Arizona\n Visit beautiful Arkansas\n Visit beautiful Colorado\n Visit beautiful Connecticut\n Visit beautiful Delaware\n Visit beautiful Florida\n Visit beautiful Georgia\n Visit beautiful New York\n Visit beautiful New Hampshire\n Visit beautiful North Carolina\n $\nNow the shell script can use values in the list that contain spaces. \nWhen working on long scripts, it’s possible to change the IFS value in one place, and then forget about it and assume \nthe default value elsewhere in the script. A safe practice to get into is to save the original IFS value before changing \nit and then restore it when you’re fi nished.\nThis technique can be coded like this: \n IFS.OLD=$IFS\n IFS=$'\\n'\n <use the new IFS value in code>\n IFS=$IFS.OLD\nThis ensures that the IFS value is returned to the default value for future operations within the script.\nOther excellent applications of the IFS environment variable are possible. Suppose you \nwant to iterate through values in a fi le that are separated by a colon (such as in the \n/etc/\npasswd\n fi le). You just need to set the IFS value to a colon: \n IFS=:\nIf you want to specify more than one IFS character, just string them together on the \nassignment line: \n IFS=$'\\n':;\"\nThis assignment uses the newline, colon, semicolon, and double quotation mark characters \nas fi eld separators. There’s no limit to how you can parse your data using the IFS characters.\n\n339\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  339\n13\nReading a directory using wildcards\nFinally, you can use the for command to automatically iterate through a directory of fi les. \nTo do this, you must use a wildcard character in the fi le or pathname. This forces the shell \nto use fi le globbing. File globbing is the process of producing fi lenames or pathnames that \nmatch a specifi ed wildcard character.\nThis feature is great for processing fi les in a directory when you don’t know all the \nfi lenames: \n $ cat test6\n #!/bin/bash\n # iterate through all the files in a directory\n \n for file in /home/rich/test/*\n do\n \n    if [ -d \"$file\" ]\n    then\n       echo \"$file is a directory\"\n    elif [ -f \"$file\" ]\n    then\n       echo \"$file is a file\"\n    fi\n done\n $ ./test6\n /home/rich/test/dir1 is a directory\n /home/rich/test/myprog.c is a file\n /home/rich/test/myprog is a file\n /home/rich/test/myscript is a file\n /home/rich/test/newdir is a directory\n /home/rich/test/newfile is a file\n /home/rich/test/newfile2 is a file\n /home/rich/test/testdir is a directory\n /home/rich/test/testing is a file\n /home/rich/test/testprog is a file\n /home/rich/test/testprog.c is a file\n $\nThe for command iterates through the results of the /home/rich/test/* listing. \nThe code tests each entry using the \ntest command (using the square bracket method) \nto see if it’s a directory, using the \n-d parameter, or a fi le, using the -f parameter (See \nChapter 12).\nNotice in this example that we did something different in the \nif statement tests: \n if [ -d \"$file\" ]\n\n340\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  340\nIn Linux, it’s perfectly legal to have directory and fi lenames that contain spaces. To accom-\nmodate that, you should enclose the \n$file variable in double quotation marks. If you \ndon’t, you’ll get an error if you run into a directory or fi lename that contains spaces: \n ./test6: line 6: [: too many arguments\n ./test6: line 9: [: too many arguments\nThe bash shell interprets the additional words as arguments within the test command, \ncausing an error.\nYou can also combine both the directory search method and the list method in the same \nfor statement by listing a series of directory wildcards in the for command: \n $ cat test7\n #!/bin/bash\n # iterating through multiple directories\n \n for file in /home/rich/.b* /home/rich/badtest\n do\n    if [ -d \"$file\" ]\n    then\n       echo \"$file is a directory\"\n    elif [ -f \"$file\" ]\n    then\n       echo \"$file is a file\"\n    else\n      echo \"$file doesn't exist\"\n    fi\n done\n $ ./test7\n /home/rich/.backup.timestamp is a file\n /home/rich/.bash_history is a file\n /home/rich/.bash_logout is a file\n /home/rich/.bash_profile is a file\n /home/rich/.bashrc is a file\n /home/rich/badtest doesn't exist\n $\nThe for statement fi rst uses fi le globbing to iterate through the list of fi les that result from \nthe wildcard character; then it iterates through the next fi le in the list. You can combine \nany number of wildcard entries in the list to iterate through. \nNotice that you can enter anything in the list data. Even if the fi le or directory doesn’t exist, the for statement \nattempts to process whatever you place in the list. This can be a problem when working with fi les and directories. You \nhave no way of knowing if you’re trying to iterate through a nonexistent directory: It’s always a good idea to test each \nfi le or directory before trying to process it.\n\n341\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  341\n13\nThe C-Style for Command\nIf you’ve done any programming using the C programming language, you’re probably sur-\nprised by the way the bash shell uses the \nfor command. In the C language, a for loop nor-\nmally defi nes a variable, which it then alters automatically during each iteration. Typically, \nprogrammers use this variable as a counter and either increment or decrement the counter \nby one in each iteration. The bash \nfor command can also provide this functionality. This \nsection shows you how to use a C-style \nfor command in a bash shell script.\nThe C language for command\nThe C language for command has a specifi c method for specifying a variable, a condition \nthat must remain true for the iterations to continue, and a method for altering the variable \nfor each iteration. When the specifi ed condition becomes false, the \nfor loop stops. The con-\ndition equation is defi ned using standard mathematical symbols. For example, consider the \nfollowing C language code: \n for (i = 0; i < 10; i++)\n {\n    printf(\"The next number is %d\\n\", i);\n }\nThis code produces a simple iteration loop, where the variable i is used as a counter. The \nfi rst section assigns a default value to the variable. The middle section defi nes the condi-\ntion under which the loop will iterate. When the defi ned condition becomes false, the \nfor \nloop stops iterations. The last section defi nes the iteration process. After each iteration, \nthe expression defi ned in the last section is executed. In this example, the \ni variable is \nincremented by one after each iteration.\nThe bash shell also supports a version of the \nfor loop that looks similar to the C-style for \nloop, although it does have some subtle differences, including a couple of things that will \nconfuse shell script programmers. Here’s the basic format of the C-style bash \nfor loop: \n for (( variable assignment ; condition ; iteration process ))\nThe format of the C-style for loop can be confusing for bash shell script programmers, \nbecause it uses C-style variable references instead of the shell-style variable references. \nHere’s what a C-style \nfor command looks like: \n for (( a = 1; a < 10; a++ ))\nNotice that there are a couple of things that don’t follow the standard bash shell for \nmethod: \n ■\nThe assignment of the variable value can contain spaces.\n ■\nThe variable in the condition isn’t preceded with a dollar sign.\n ■\nThe equation for the iteration process doesn’t use the expr command format.\n\n342\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  342\nThe shell developers created this format to more closely resemble the C-style for command. \nAlthough this is great for C programmers, it can throw even expert shell programmers into a \ntizzy. Be careful when using the C-style \nfor loop in your scripts.\nHere’s an example of using the C-style \nfor command in a bash shell program: \n $ cat test8\n #!/bin/bash\n # testing the C-style for loop\n \n for (( i=1; i <= 10; i++ ))\n do\n    echo \"The next number is $i\"\n done\n $ ./test8\n The next number is 1\n The next number is 2\n The next number is 3\n The next number is 4\n The next number is 5\n The next number is 6\n The next number is 7\n The next number is 8\n The next number is 9\n The next number is 10\n $\nThe for loop iterates through the commands using the variable defi ned in the for loop \n(the letter i in this example). In each iteration, the \n$i variable contains the value assigned \nin the \nfor loop. After each iteration, the loop iteration process is applied to the variable, \nwhich in this example, increments the variable by one.\nUsing multiple variables\nThe C-style for command also allows you to use multiple variables for the iteration. The \nloop handles each variable separately, allowing you to defi ne a different iteration process \nfor each variable. Although you can have multiple variables, you can defi ne only one condi-\ntion in the \nfor loop: \n $ cat test9\n #!/bin/bash\n # multiple variables\n \n for (( a=1, b=10; a <= 10; a++, b-- ))\n do\n    echo \"$a - $b\"\n done\n $ ./test9\n\n343\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  343\n13\n 1 - 10\n 2 - 9\n 3 - 8\n 4 - 7\n 5 - 6\n 6 - 5\n 7 - 4\n 8 - 3\n 9 - 2\n 10 - 1\n $\nThe a and b variables are each initialized with different values, and different iteration pro-\ncesses are defi ned. While the loop increases the \na variable, it decreases the b variable for \neach iteration.\nThe while Command\nThe while command is somewhat of a cross between the if-then statement and the for \nloop. The \nwhile command allows you to defi ne a command to test and then loop through a \nset of commands for as long as the defi ned test command returns a zero exit status. It tests \nthe \ntest command at the start of each iteration. When the test command returns a non-\nzero exit status, the \nwhile command stops executing the set of commands.\nBasic while format\nHere’s fhe format of the while command: \n while test command\n do\n  \nother commands\n done\nThe test command defi ned in the while command is the exact same format as in if-then \nstatements (see Chapter 12). As in the \nif-then statement, you can use any normal bash shell \ncommand, or you can use the \ntest command to test for conditions, such as variable values.\nThe key to the \nwhile command is that the exit status of the test command specifi ed \nmust change, based on the commands run during the loop. If the exit status never changes, \nthe \nwhile loop will get stuck in an infi nite loop.\nThe most common use of the \ntest command is to use brackets to check a value of a shell \nvariable that’s used in the loop commands: \n $ cat test10\n #!/bin/bash\n\n344\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  344\n # while command test\n \n var1=10\n while [ $var1 -gt 0 ]\n do\n    echo $var1\n    var1=$[ $var1 - 1 ]\n done\n $ ./test10\n 10\n 9\n 8\n 7\n 6\n 5\n 4\n 3\n 2\n 1\n $\nThe while command defi nes the test condition to check for each iteration: \n while [ $var1 -gt 0 ]\n \nAs long as the test condition is true, the while command continues to loop through the \ncommands defi ned. Within the commands, the variable used in the test condition must \nbe modifi ed, or you’ll have an infi nite loop. In this example, we use shell arithmetic to \ndecrease the variable value by one: \n var1=$[ $var1 - 1 ]\nThe while loop stops when the test condition is no longer true.\nUsing multiple test commands\nThe while command allows you to defi ne multiple test commands on the while statement \nline. Only the exit status of the last test command is used to determine when the loop \nstops. This can cause some interesting results if you’re not careful. Here’s an example of \nwhat we mean: \n $ cat test11\n #!/bin/bash\n # testing a multicommand while loop\n \n var1=10\n \n while echo $var1\n\n345\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  345\n13\n       [ $var1 -ge 0 ]\n do\n    echo \"This is inside the loop\"\n    var1=$[ $var1 - 1 ]\n done\n $ ./test11\n 10\n This is inside the loop\n 9\n This is inside the loop\n 8\n This is inside the loop\n 7\n This is inside the loop\n 6\n This is inside the loop\n 5\n This is inside the loop\n 4\n This is inside the loop\n 3\n This is inside the loop\n 2\n This is inside the loop\n 1\n This is inside the loop\n 0\n This is inside the loop\n -1\n $\nPay close attention to what happened in this example. Two test commands were defi ned in \nthe \nwhile statement: \n while echo $var1\n       [ $var1 -ge 0 ]\nThe fi rst test simply displays the current value of the var1 variable. The second test uses \nbrackets to determine the value of the \nvar1 variable. Inside the loop, an echo statement \ndisplays a simple message, indicating that the loop was processed. Notice when you run the \nexample how the output ends: \n This is inside the loop\n -1\n $\n \nThe while loop executed the echo statement when the var1 variable was equal to zero \nand then decreased the \nvar1 variable value. Next, the test commands were executed for \n\n346\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  346\nthe next iteration. The echo test command was executed, displaying the value of the var1 \nvariable, which is now less than zero. It’s not until the shell executes the \ntest test com-\nmand that the \nwhile loop terminates.\nThis demonstrates that in a multi-command \nwhile statement, all the test commands are \nexecuted in each iteration, including the last iteration when the last test command fails. \nBe careful of this. Another thing to be careful of is how you specify the multiple test com-\nmands. Note that each test command is on a separate line!\nThe until Command\nThe until command works in exactly the opposite way from the while command. The \nuntil command requires that you specify a test command that normally produces a non-\nzero exit status. As long as the exit status of the test command is non-zero, the bash shell \nexecutes the commands listed in the loop. When the test command returns a zero exit sta-\ntus, the loop stops.\nAs you would expect, the format of the \nuntil command is: \n until test commands\n do\n    \nother commands\n done\nSimilar to the while command, you can have more than one test command in the until \ncommand statement. Only the exit status of the last command determines if the bash shell \nexecutes the other commands defi ned.\nThe following is an example of using the \nuntil command: \n $ cat test12\n #!/bin/bash\n # using the until command\n \n var1=100\n \n until [ $var1 -eq 0 ]\n do\n    echo $var1\n    var1=$[ $var1 - 25 ]\n done\n $ ./test12\n 100\n 75\n 50\n 25\n $\n\n347\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  347\n13\nThis example tests the var1 variable to determine when the until loop should stop. As \nsoon as the value of the variable is equal to zero, the \nuntil command stops the loop. The \nsame caution as for the \nwhile command applies when you use multiple test commands \nwith the \nuntil command: \n $ cat test13\n #!/bin/bash\n # using the until command\n \n var1=100\n \n until echo $var1\n       [ $var1 -eq 0 ]\n do\n    echo Inside the loop: $var1\n    var1=$[ $var1 - 25 ]\n done\n $ ./test13\n 100\n Inside the loop: 100\n 75\n Inside the loop: 75\n 50\n Inside the loop: 50\n 25\n Inside the loop: 25\n 0\n $\nThe shell executes the test commands specifi ed and stops only when the last command is \ntrue.\nNesting Loops\nA loop statement can use any other type of command within the loop, including other \nloop commands. This is called a nested loop. Care should be taken when using nested loops, \nbecause you’re performing an iteration within an iteration, which multiplies the number of \ntimes commands are being run. If you don’t pay close attention to this, it can cause prob-\nlems in your scripts.\nHere’s a simple example of nesting a \nfor loop inside another for loop: \n $ cat test14\n #!/bin/bash\n\n348\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  348\n # nesting for loops\n \n for (( a = 1; a <= 3; a++ ))\n do\n    echo \"Starting loop $a:\"\n    for (( b = 1; b <= 3; b++ ))\n    do\n       echo \"   Inside loop: $b\"\n    done\n done\n $ ./test14\n Starting loop 1:\n    Inside loop: 1\n    Inside loop: 2\n    Inside loop: 3\n Starting loop 2:\n    Inside loop: 1\n    Inside loop: 2\n    Inside loop: 3\n Starting loop 3:\n    Inside loop: 1\n    Inside loop: 2\n    Inside loop: 3\n $\nThe nested loop (also called the inner loop) iterates through its values for each iteration of \nthe outer loop. Notice that there’s no difference between the \ndo and done commands for \nthe two loops. The bash shell knows when the fi rst \ndone command is executed that it refers \nto the inner loop and not the outer loop.\nThe same applies when you mix loop commands, such as placing a \nfor loop inside a while \nloop: \n $ cat test15\n #!/bin/bash\n # placing a for loop inside a while loop\n \n var1=5\n \n while [ $var1 -ge 0 ]\n do\n    echo \"Outer loop: $var1\"\n    for (( var2 = 1; $var2 < 3; var2++ ))\n    do\n       var3=$[ $var1 * $var2 ]\n       echo \"  Inner loop: $var1 * $var2 = $var3\"\n    done\n    var1=$[ $var1 - 1 ]\n done\n $ ./test15\n\n349\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  349\n13\n Outer loop: 5\n   Inner loop: 5 * 1 = 5\n   Inner loop: 5 * 2 = 10\n Outer loop: 4\n   Inner loop: 4 * 1 = 4\n   Inner loop: 4 * 2 = 8\n Outer loop: 3\n   Inner loop: 3 * 1 = 3\n   Inner loop: 3 * 2 = 6\n Outer loop: 2\n   Inner loop: 2 * 1 = 2\n   Inner loop: 2 * 2 = 4\n Outer loop: 1\n   Inner loop: 1 * 1 = 1\n   Inner loop: 1 * 2 = 2\n Outer loop: 0\n   Inner loop: 0 * 1 = 0\n   Inner loop: 0 * 2 = 0\n $\nAgain, the shell distinguished between the do and done commands of the inner for loop \nfrom the same commands in the outer \nwhile loop.\nIf you really want to test your brain, you can even combine \nuntil and while loops: \n $ cat test16\n #!/bin/bash\n # using until and while loops\n \n var1=3\n \n until [ $var1 -eq 0 ]\n do\n    echo \"Outer loop: $var1\"\n    var2=1\n    while [ $var2 -lt 5 ]\n    do\n       var3=$(echo \"scale=4; $var1 / $var2\" | bc)\n       echo \"   Inner loop: $var1 / $var2 = $var3\"\n       var2=$[ $var2 + 1 ]\n    done\n    var1=$[ $var1 - 1 ]\n done\n $ ./test16\n Outer loop: 3\n    Inner loop: 3 / 1 = 3.0000\n    Inner loop: 3 / 2 = 1.5000\n    Inner loop: 3 / 3 = 1.0000\n    Inner loop: 3 / 4 = .7500\n\n350\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  350\n Outer loop: 2\n    Inner loop: 2 / 1 = 2.0000\n    Inner loop: 2 / 2 = 1.0000\n    Inner loop: 2 / 3 = .6666\n    Inner loop: 2 / 4 = .5000\n Outer loop: 1\n    Inner loop: 1 / 1 = 1.0000\n    Inner loop: 1 / 2 = .5000\n    Inner loop: 1 / 3 = .3333\n    Inner loop: 1 / 4 = .2500\n $\nThe outer until loop starts with a value of 3 and continues until the value equals 0. The \ninner \nwhile loop starts with a value of 1 and continues as long as the value is less than \n5. Each loop must change the value used in the test condition, or the loop will get stuck \ninfi nitely.\nLooping on File Data\nOften, you must iterate through items stored inside a fi le. This requires combining two of \nthe techniques covered: \n ■\nUsing nested loops\n ■\nChanging the IFS environment variable\nBy changing the \nIFS environment variable, you can force the for command to handle each \nline in the fi le as a separate item for processing, even if the data contains spaces. After \nyou’ve extracted an individual line in the fi le, you may have to loop again to extract data \ncontained within it.\nThe classic example of this is processing data in the \n/etc/passwd fi le. This requires that \nyou iterate through the \n/etc/passwd fi le line by line and then change the IFS variable \nvalue to a colon so you can separate the individual components in each line.\nThe following is an example of doing just that: \n #!/bin/bash\n # changing the IFS value\n \n IFS.OLD=$IFS\n IFS=$'\\n'\n for entry in $(cat /etc/passwd)\n do\n    echo \"Values in $entry –\"\n    IFS=:\n    for value in $entry\n\n351\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  351\n13\n    do\n       echo \"   $value\"\n    done\n done\n $\nThis script uses two different IFS values to parse the data. The fi rst IFS value parses the \nindividual lines in the \n/etc/passwd fi le. The inner for loop next changes the IFS value \nto the colon, which allows you to parse the individual values within the \n/etc/passwd \nlines.\nWhen you run this script, you get output something like this: \n Values in rich:x:501:501:Rich Blum:/home/rich:/bin/bash -\n    rich\n    x\n    501\n    501\n    Rich Blum\n    /home/rich\n    /bin/bash\n Values in katie:x:502:502:Katie Blum:/home/katie:/bin/bash -\n    katie\n    x\n    506\n    509\n    Katie Blum\n    /home/katie\n    /bin/bash\nThe inner loop parses each individual value in the /etc/passwd entry. This is also a great \nway to process comma-separated data, a common way to import spreadsheet data.\nControlling the Loop\nYou might be tempted to think that after you start a loop, you’re stuck until the loop fi n-\nishes all its iterations. This is not true. A couple of commands help us control what happens \ninside of a loop: \n ■\nThe break command\n ■\nThe continue command\nEach command has a different use in how to control the operation of a loop. The following \nsections describe how you can use these commands to control the operation of your loops.\n\n352\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  352\nThe break command\nThe break command is a simple way to escape a loop in progress. You can use the break \ncommand to exit any type of loop, including \nwhile and until loops.\nYou can use the \nbreak command in several situations. This section shows each of these \nmethods.\nBreaking out of a single loop\nWhen the shell executes a break command, it attempts to break out of the loop that’s cur-\nrently processing: \n $ cat test17\n #!/bin/bash\n # breaking out of a for loop\n \n for var1 in 1 2 3 4 5 6 7 8 9 10\n do\n    if [ $var1 -eq 5 ]\n    then\n       break\n    fi\n    echo \"Iteration number: $var1\"\n done\n echo \"The for loop is completed\"\n $ ./test17\n Iteration number: 1\n Iteration number: 2\n Iteration number: 3\n Iteration number: 4\n The for loop is completed\n $\nThe for loop should normally have iterated through all the values specifi ed in the list. \nHowever, when the \nif-then condition was satisfi ed, the shell executed the break com-\nmand, which stopped the \nfor loop.\nThis technique also works for \nwhile and until loops: \n $ cat test18\n #!/bin/bash\n # breaking out of a while loop\n \n var1=1\n \n while [ $var1 -lt 10 ]\n do\n    if [ $var1 -eq 5 ]\n\n353\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  353\n13\n    then\n       break\n    fi\n    echo \"Iteration: $var1\"\n    var1=$[ $var1 + 1 ]\n done\n echo \"The while loop is completed\"\n $ ./test18\n Iteration: 1\n Iteration: 2\n Iteration: 3\n Iteration: 4\n The while loop is completed\n $\nThe while loop terminated when the if-then condition was met, executing the break \ncommand.\nBreaking out of an inner loop\nWhen you’re working with multiple loops, the break command automatically terminates \nthe innermost loop you’re in: \n $ cat test19\n #!/bin/bash\n # breaking out of an inner loop\n \n for (( a = 1; a < 4; a++ ))\n do\n    echo \"Outer loop: $a\"\n    for (( b = 1; b < 100; b++ ))\n    do\n       if [ $b -eq 5 ]\n       then\n          break\n       fi\n       echo \"   Inner loop: $b\"\n    done\n done\n $ ./test19\n Outer loop: 1\n    Inner loop: 1\n    Inner loop: 2\n    Inner loop: 3\n    Inner loop: 4\n Outer loop: 2\n    Inner loop: 1\n    Inner loop: 2\n    Inner loop: 3\n\n354\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  354\n    Inner loop: 4\n Outer loop: 3\n    Inner loop: 1\n    Inner loop: 2\n    Inner loop: 3\n    Inner loop: 4\n $\nThe for statement in the inner loop specifi es to iterate until the b variable is equal to 100. \nHowever, the \nif-then statement in the inner loop specifi es that when the b variable value \nis equal to 5, the \nbreak command is executed. Notice that even though the inner loop is \nterminated with the \nbreak command, the outer loop continues working as specifi ed.\nBreaking out of an outer loop\nThere may be times when you’re in an inner loop but need to stop the outer loop. The \nbreak command includes a single command line parameter value: \n break n\nwhere n indicates the level of the loop to break out of. By default, n is 1, indicating to \nbreak out of the current loop. If you set n to a value of 2, the \nbreak command stops the \nnext level of the outer loop: \n $ cat test20\n #!/bin/bash\n # breaking out of an outer loop\n \n for (( a = 1; a < 4; a++ ))\n do\n    echo \"Outer loop: $a\"\n    for (( b = 1; b < 100; b++ ))\n    do\n       if [ $b -gt 4 ]\n       then\n          break 2\n       fi\n       echo \"   Inner loop: $b\"\n    done\n done\n $ ./test20\n Outer loop: 1\n    Inner loop: 1\n    Inner loop: 2\n    Inner loop: 3\n    Inner loop: 4\n $\nNow when the shell executes the break command, the outer loop stops.\n\n355\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  355\n13\nThe continue command\nThe continue command is a way to prematurely stop processing commands inside of a \nloop but not terminate the loop completely. This allows you to set conditions within a loop \nwhere the shell won’t execute commands. Here’s a simple example of using the \ncontinue \ncommand in a \nfor loop: \n $ cat test21\n #!/bin/bash\n # using the continue command\n \n for (( var1 = 1; var1 < 15; var1++ ))\n do\n    if [ $var1 -gt 5 ] && [ $var1 -lt 10 ]\n    then\n       continue\n    fi\n    echo \"Iteration number: $var1\"\n done\n $ ./test21\n Iteration number: 1\n Iteration number: 2\n Iteration number: 3\n Iteration number: 4\n Iteration number: 5\n Iteration number: 10\n Iteration number: 11\n Iteration number: 12\n Iteration number: 13\n Iteration number: 14\n $\nWhen the conditions of the if-then statement are met (the value is greater than 5 and \nless than 10), the shell executes the \ncontinue command, which skips the rest of the com-\nmands in the loop, but keeps the loop going. When the \nif-then condition is no longer \nmet, things return to normal.\nYou can use the \ncontinue command in while and until loops, but be extremely careful \nwith what you’re doing. Remember that when the shell executes the \ncontinue command, \nit skips the remaining commands. If you’re incrementing your test condition variable in \none of those conditions, bad things happen: \n $ cat badtest3\n #!/bin/bash\n # improperly using the continue command in a while loop\n \n var1=0\n \n while echo \"while iteration: $var1\"\n\n356\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  356\n       [ $var1 -lt 15 ]\n do\n    if [ $var1 -gt 5 ] && [ $var1 -lt 10 ]\n    then\n       continue\n    fi\n    echo \"   Inside iteration number: $var1\"\n    var1=$[ $var1 + 1 ]\n done\n $ ./badtest3 | more\n while iteration: 0\n    Inside iteration number: 0\n while iteration: 1\n    Inside iteration number: 1\n while iteration: 2\n    Inside iteration number: 2\n while iteration: 3\n    Inside iteration number: 3\n while iteration: 4\n    Inside iteration number: 4\n while iteration: 5\n    Inside iteration number: 5\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n while iteration: 6\n $\nYou’ll want to make sure you redirect the output of this script to the more command so you \ncan stop things. Everything seems to be going just fi ne until the \nif-then condition is met, \nand the shell executes the \ncontinue command. When the shell executes the continue \ncommand, it skips the remaining commands in the \nwhile loop. Unfortunately, that’s where \nthe \n$var1 counter variable that is tested in the while test command is incremented. That \nmeans that the variable isn’t incremented, as you can see from the continually displaying \noutput.\nAs with the \nbreak command, the continue command allows you to specify what level of \nloop to continue with a command line parameter: \n continue n\n\n357\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  357\n13\nwhere n defi nes the loop level to continue. Here’s an example of continuing an outer for \nloop: \n $ cat test22\n #!/bin/bash\n # continuing an outer loop\n \n for (( a = 1; a <= 5; a++ ))\n do\n    echo \"Iteration $a:\"\n    for (( b = 1; b < 3; b++ ))\n    do\n       if [ $a -gt 2 ] && [ $a -lt 4 ]\n       then\n          continue 2\n       fi\n       var3=$[ $a * $b ]\n       echo \"   The result of $a * $b is $var3\"\n    done\n done\n $ ./test22\n Iteration 1:\n    The result of 1 * 1 is 1\n    The result of 1 * 2 is 2\n Iteration 2:\n    The result of 2 * 1 is 2\n    The result of 2 * 2 is 4\n Iteration 3:\n Iteration 4:\n    The result of 4 * 1 is 4\n    The result of 4 * 2 is 8\n Iteration 5:\n    The result of 5 * 1 is 5\n    The result of 5 * 2 is 10\n $\nThe if-then statement: \n if [ $a -gt 2 ] && [ $a -lt 4 ]\n       then\n          continue 2\n       fi\nuses the continue command to stop processing the commands inside the loop but con-\ntinue the outer loop. Notice in the script output that the iteration for the value 3 doesn’t \nprocess any inner loop statements, because the \ncontinue command stopped the process-\ning, but it continues with the outer loop processing.\n\n358\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  358\nProcessing the Output of a Loop\nFinally, you can either pipe or redirect the output of a loop within your shell script. You do \nthis by adding the processing command to the end of the \ndone command: \n for file in /home/rich/*\n  do\n    if [ -d \"$file\" ]\n    then\n       echo \"$file is a directory\"\n    elif\n       echo \"$file is a file\"\n    fi\n done > output.txt\nInstead of displaying the results on the monitor, the shell redirects the results of the for \ncommand to the fi le \noutput.txt.\nConsider the following example of redirecting the output of a \nfor command to a fi le: \n $ cat test23\n #!/bin/bash\n # redirecting the for output to a file\n \n for (( a = 1; a < 10; a++ ))\n do\n    echo \"The number is $a\"\n done > test23.txt\n echo \"The command is finished.\"\n $ ./test23\n The command is finished.\n $ cat test23.txt\n The number is 1\n The number is 2\n The number is 3\n The number is 4\n The number is 5\n The number is 6\n The number is 7\n The number is 8\n The number is 9\n $\nThe shell creates the fi le test23.txt and redirects the output of the for command only \nto the fi le. The shell displays the \necho statement after the for command just as normal.\nThis same technique also works for piping the output of a loop to another command: \n $ cat test24\n #!/bin/bash\n\n359\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  359\n13\n # piping a loop to another command\n \n for state in \"North Dakota\" Connecticut Illinois Alabama Tennessee\n do\n    echo \"$state is the next place to go\"\n done | sort\n echo \"This completes our travels\"\n $ ./test24\n Alabama is the next place to go\n Connecticut is the next place to go\n Illinois is the next place to go\n North Dakota is the next place to go\n Tennessee is the next place to go\n This completes our travels\n $\nThe state values aren’t listed in any particular order in the for command list. The output \nof the \nfor command is piped to the sort command, which changes the order of the for \ncommand output. Running the script indeed shows that the output was properly sorted \nwithin the script.\nPractical Examples\nNow that you’ve seen how to use the different ways to create loops in shell scripts, let’s \nlook at some practical examples of how to use them. Looping is a common way to iterate \nthrough data on the system, whether it’s fi les in folders or data contained in a fi le. Here are \na couple of examples that demonstrate using simple loops to work with data.\nFinding executable fi les\nWhen you run a program from the command line, the Linux system searches a series of \nfolders looking for that fi le. Those folders are defi ned in the \nPATH environment variable. If \nyou want to fi nd out just what executable fi les are available on your system for you to use, \njust scan all the folders in the \nPATH environment variable. That may take some time to do \nmanually, but it’s a breeze working out a small shell script to do that.\nThe fi rst step is to create a \nfor loop to iterate through the folders stored in the PATH envi-\nronment variable. When you do that, don’t forget to set the \nIFS separator character:\nIFS=:\nfor folder in $PATH\ndo\nNow that you have the individual folders in the $folder variable, you can use another \nfor loop to iterate through all the fi les inside that particular folder:\nfor file in $folder/*\ndo\n\n360\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  360\nThe last step is to check whether the individual fi les have the executable permission set, \nwhich you can do using the \nif-then test feature:\nif [ -x $file ]\nthen\n   echo \"   $file\"\nfi\nAnd there you have it! Putting all the pieces together into a script looks like this:\n$ cat test25\n#!/bin/bash\n# finding files in the PATH\nIFS=:\nfor folder in $PATH\ndo\n   echo \"$folder:\"\n   for file in $folder/*\n   do\n      if [ -x $file ]\n      then\n         echo \"   $file\"\n      fi\n   done\ndone\n$\nWhen you run the code, you get a listing of the executable fi les that you can use from the \ncommand line:\n$ ./test25 | more\n/usr/local/bin:\n/usr/bin:\n   /usr/bin/Mail\n   /usr/bin/Thunar\n   /usr/bin/X\n   /usr/bin/Xorg\n   /usr/bin/[\n   /usr/bin/a2p\n   /usr/bin/abiword\n   /usr/bin/ac\n   /usr/bin/activation-client\n   /usr/bin/addr2line\n...\nThe output shows all the executable fi les found in all the folders defi ned in the PATH envi-\nronment variable, which is quite a few!\n\n361\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  361\n13\nCreating multiple user accounts\nThe goal of shell scripts is to make life easier for the system administrator. If you happen to \nwork in an environment with lots of users, one of the most boring tasks can be creating new \nuser accounts. Fortunately, you can use the \nwhile loop to make your job a little easier!\nInstead of having to manually enter \nuseradd commands for every new user account you \nneed to create, you can place the new user accounts in a text fi le and create a simple shell \nscript to do that work for you. The format of the text fi le that we’ll use looks like this:\nuserid,user name\nThe fi rst entry is the userid you want to use for the new user account. The second entry is \nthe full name of the user. The two values are separated by a comma, making this a comma-\nseparated fi le format, or .csv. This is a very common fi le format used in spreadsheets, so \nyou can easily create the user account list in a spreadsheet program and save it in .csv for-\nmat for your shell script to read and process.\nTo read the fi le data, we’re going to use a little shell scripting trick. We’ll actually set the \nIFS separator character to a comma as the test part of the while statement. Then to read \nthe individual lines, we’ll use the \nread command. That looks like this:\nwhile IFS=',' read –r userid name \nThe read command does the work of moving onto the next line of text in the .csv text fi le, \nso we don’t need another loop to do that. The \nwhile command exits when the read command \nreturns a \nFALSE value, which happens when it runs out of lines to read in the fi le. Tricky!\nTo feed the data from the fi le into the \nwhile command, you just use a redirection symbol \nat the end of the \nwhile command.\nPutting everything together results in this script:\n$ cat test26\n#!/bin/bash\n# process new user accounts\ninput=\"users.csv\"\nwhile IFS=',' read -r userid name\ndo\n  echo \"adding $userid\"\n  useradd -c \"$name\" -m $userid\ndone < \"$input\"\n$\nThe $input variable points to the data fi le and is used as the redirect data for the while \ncommand. The users.csv fi le looks like this:\n$ cat users.csv\nrich,Richard Blum\n\n362\nPart II: Shell Scripting Basics\nc13.indd  12/16/2014  Page  362\nchristine,Christine Bresnahan\nbarbara,Barbara Blum\ntim,Timothy Bresnahan\n$\nTo run the problem, you must be the root user account, because the useradd command \nrequires root privileges:\n# ./test26\nadding rich\nadding christine\nadding barbara\nadding tim\n# \nThen by taking a quick look at the /etc/passwd fi le, you can see that the accounts have \nbeen created:\n# tail /etc/passwd\nrich:x:1001:1001:Richard Blum:/home/rich:/bin/bash\nchristine:x:1002:1002:Christine Bresnahan:/home/christine:/bin/bash\nbarbara:x:1003:1003:Barbara Blum:/home/barbara:/bin/bash\ntim:x:1004:1004:Timothy Bresnahan:/home/tim:/bin/bash\n#\nCongratulations, you’ve saved yourself lots of time in adding user accounts!\nSummary\nLooping is an integral part of programming. The bash shell provides three looping com-\nmands that you can use in your scripts. \nThe \nfor command allows you to iterate through a list of values, either supplied within the \ncommand line, contained in a variable, or obtained by using fi le globbing, to extract fi le \nand directory names from a wildcard character.\nThe \nwhile command provides a method to loop based on the condition of a command, \nusing either ordinary commands or the test command, which allows you to test conditions \nof variables. As long as the command (or condition) produces a zero exit status, the \nwhile \nloop continues to iterate through the specifi ed set of commands.\nThe \nuntil command also provides a method to iterate through commands, but it bases \nits iterations on a command (or condition) producing a non-zero exit status. This feature \nallows you to set a condition that must be met before the iteration stops.\n\n363\nChapter 13: More Structured Commands\nc13.indd  12/16/2014  Page  363\n13\nYou can combine loops in shell scripts, producing multiple layers of loops. The bash shell \nprovides the \ncontinue and break  commands, which allow you to alter the fl ow of the nor-\nmal loop process based on different values within the loop.\nThe bash shell also allows you to use standard command redirection and piping to alter the \noutput of a loop. You can use redirection to redirect the output of a loop to a fi le or piping \nto redirect the output of a loop to another command. This provides a wealth of features \nwith which you can control your shell script execution.\nThe next chapter discusses how to interact with your shell script user. Often, shell scripts \naren’t completely self-contained. They require some sort of external data that must be sup-\nplied at the time you run them. The next chapter discusses different methods with which \nyou can provide real-time data to your shell scripts for processing. \n\nc13.indd  12/16/2014  Page  364\n\n365\nc14.indd  12/12/2014  Page  365\nCHAPTER \n14\nHandling User Input\nIN THIS CHAPTER\nPassing parameters\nTracking parameters\nBeing shifty\nWorking with options\nStandardizing options\nGetting user input\nS\no far you’ve seen how to write scripts that interact with data, variables, and fi les on the \nLinux system. Sometimes, you need to write a script that has to interact with the person \nrunning the script. The bash shell provides a few different methods for retrieving data from \npeople, including command line parameters (data values added after the command), command line \noptions (single-letter values that modify the behavior of the command), and the capability to read \ninput directly from the keyboard. This chapter discusses how to incorporate these different meth-\nods into your bash shell scripts to obtain data from the person running your script.\nPassing Parameters\nThe most basic method of passing data to your shell script is to use command line parameters. \nCommand line parameters allow you to add data values to the command line when you execute \nthe script: \n$ ./addem 10 30\nThis example passes two command line parameters (10 and 30) to the script addem. The script \nhandles the command line parameters using special variables. The following sections describe how \nto use command line parameters in your bash shell scripts.\n\n366\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  366\nReading parameters\nThe bash shell assigns special variables, called positional parameters, to all of the command \nline parameters entered. This includes the name of the script the shell is executing. The \npositional parameter variables are standard numbers, with \n$0 being the script’s name, $1 \nbeing the fi rst parameter, \n$2 being the second parameter, and so on, up to $9 for the ninth \nparameter.\nHere’s a simple example of using one command line parameter in a shell script: \n$ cat test1.sh\n#!/bin/bash\n# using one command line parameter\n#\nfactorial=1\nfor (( number = 1; number <= $1 ; number++ ))\ndo\n   factorial=$[ $factorial * $number ]\ndone\necho The factorial of $1 is $factorial\n$ \n$ ./test1.sh 5\nThe factorial of 5 is 120\n$\nYou can use the $1 variable just like any other variable in the shell script. The shell script \nautomatically assigns the value from the command line parameter to the variable; you don’t \nneed to do anything with it.\nIf you need to enter more command line parameters, each parameter must be separated by a \nspace on the command line: \n$ cat test2.sh\n#!/bin/bash\n# testing two command line parameters\n#\ntotal=$[ $1 * $2 ]\necho The first parameter is $1.\necho The second parameter is $2.\necho The total value is $total.\n$ \n$ ./test2.sh 2 5\nThe first parameter is 2.\nThe second parameter is 5.\nThe total value is 10.\n$\nThe shell assigns each parameter to the appropriate variable.\n\n367\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  367\n14\n14\nIn the preceding example, the command line parameters used were both numerical values. \nYou can also use text strings in the command line: \n$ cat test3.sh\n#!/bin/bash\n# testing string parameters\n#\necho Hello $1, glad to meet you.\n$ \n$ ./test3.sh Rich\nHello Rich, glad to meet you.\n$\nThe shell passes the string value entered into the command line to the script. However, \nyou’ll have a problem if you try to do this with a text string that contains spaces: \n$ ./test3.sh Rich Blum\nHello Rich, glad to meet you.\n$\nRemember that each of the parameters is separated by a space, so the shell interpreted the \nspace as just separating the two values. To include a space as a parameter value, you must \nuse quotation marks (either single or double quotation marks): \n$ ./test3.sh 'Rich Blum'\nHello Rich Blum, glad to meet you.\n$ \n$ ./test3.sh \"Rich Blum\"\nHello Rich Blum, glad to meet you.\n$\nThe quotation marks used when you pass text strings as parameters are not part of the data. They just delineate the \nbeginning and the end of the data.\nIf your script needs more than nine command line parameters, you can continue, but the \nvariable names change slightly. After the ninth variable, you must use braces around the \nvariable number, such as \n${10}. Here’s an example of doing that: \n$ cat test4.sh\n#!/bin/bash\n# handling lots of parameters\n#\ntotal=$[ ${10} * ${11} ]\necho The tenth parameter is ${10}\necho The eleventh parameter is ${11}\necho The total is $total\n\n368\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  368\n$ \n$ ./test4.sh 1 2 3 4 5 6 7 8 9 10 11 12\nThe tenth parameter is 10\nThe eleventh parameter is 11\nThe total is 110\n$\nThis technique allows you to add as many command line parameters to your scripts as you \ncould possibly need.\nReading the script name\nYou can use the $0 parameter to determine the script name the shell started from the \ncommand line. This can come in handy if you’re writing a utility that can have multiple \nfunctions. \n$ cat test5.sh\n#!/bin/bash\n# Testing the $0 parameter\n#\necho The zero parameter is set to: $0\n#\n$\n$ bash test5.sh\nThe zero parameter is set to: test5.sh\n$\nHowever, there is a potential problem. When using a different command to run the shell \nscript, the command becomes entangled with the script name in the \n$0 parameter:\n$ ./test5.sh\nThe zero parameter is set to: ./test5.sh\n$\nThere is another potential problem. When the actual string passed is the full script path, \nand not just the script’s name, the \n$0 variable gets set to the full script path and name: \n$ bash /home/Christine/test5.sh\nThe zero parameter is set to: /home/Christine/test5.sh\n$\nIf you want to write a script that performs different functions based on just the script’s \nname, you’ll have to do a little work. You need to be able to strip off whatever path is used to \nrun the script. Also, you need to be able to remove any entangled commands from the script. \nFortunately, there’s a handy little command available that does just that. The \nbasename \ncommand returns just the script’s name without the path: \n$ cat test5b.sh\n#!/bin/bash\n\n369\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  369\n14\n# Using basename with the $0 parameter\n#\nname=$(basename $0)\necho\necho The script name is: $name\n#\n$ bash /home/Christine/test5b.sh\nThe script name is: test5b.sh\n$\n$ ./test5b.sh\nThe script name is: test5b.sh\n$\nNow that’s much better. You can use this technique to write scripts that perform different \nfunctions based on the script name used. Here’s a simple example: \n$ cat test6.sh\n#!/bin/bash\n# Testing a Multi-function script\n#\nname=$(basename $0)\n#\nif [ $name = \"addem\" ]\nthen\n   total=$[ $1 + $2 ]\n#\nelif [ $name = \"multem\" ]\nthen\n   total=$[ $1 * $2 ]\nfi\n#\necho\necho The calculated value is $total\n#\n$\n$ cp test6.sh addem\n$ chmod u+x addem\n$\n$ ln -s test6.sh multem\n$\n$ ls -l *em\n-rwxrw-r--. 1 Christine Christine 224 Jun 30 23:50 addem\nlrwxrwxrwx. 1 Christine Christine   8 Jun 30 23:50 multem -> test6.sh\n$\n$ ./addem 2 5\nThe calculated value is 7\n\n370\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  370\n$\n$ ./multem 2 5\nThe calculated value is 10\n$\nThe example creates two separate fi lenames from the test6.sh script, one by just copying \nthe fi le to a new script (\naddem) and the other by using a symbolic link (see Chapter 3) to \ncreate the new script (\nmultem). In both cases, the script determines the script’s base name \nand performs the appropriate function based on that value.\nTesting parameters\nBe careful when using command line parameters in your shell scripts. If the script is run \nwithout the parameters, bad things can happen: \n$ ./addem 2\n./addem: line 8: 2 +  : syntax error: operand expected (error\n token is \" \")\nThe calculated value is\n$\nWhen the script assumes there is data in a parameter variable, and no data is present, \nmost likely you’ll get an error message from your script. This is a poor way to write scripts. \nAlways check your parameters to make sure the data is there before using it:\n$ cat test7.sh\n#!/bin/bash\n# testing parameters before use\n#\nif [ -n \"$1\" ]\nthen\n   echo Hello $1, glad to meet you.\nelse\n   echo \"Sorry, you did not identify yourself. \"\nfi\n$ \n$ ./test7.sh Rich\nHello Rich, glad to meet you.\n$ \n$ ./test7.sh\nSorry, you did not identify yourself.\n$\nIn this example, the -n test evaluation was used to check for data in the $1 command \nline parameter. In the next section, you’ll learn another way to check command line \nparameters.\n\n371\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  371\n14\nUsing Special Parameter Variables\nA few special bash shell variables track command line parameters. This section describes \nwhat they are and how to use them.\nCounting parameters\nAs you saw in the last section, you should verify command line parameters before using \nthem in your script. For scripts that use multiple command line parameters, this checking \ncan get tedious.\nInstead of testing each parameter, you can count how many parameters were entered on \nthe command line. The bash shell provides a special variable for this purpose.\nThe special \n$# variable contains the number of command line parameters included when \nthe script was run. You can use this special variable anywhere in the script, just like a nor-\nmal variable: \n$ cat test8.sh\n#!/bin/bash\n# getting the number of parameters\n#\necho There were $# parameters supplied.\n$ \n$ ./test8.sh\nThere were 0 parameters supplied.\n$ \n$ ./test8.sh 1 2 3 4 5\nThere were 5 parameters supplied.\n$ \n$ ./test8.sh 1 2 3 4 5 6 7 8 9 10\nThere were 10 parameters supplied.\n$ \n$ ./test8.sh \"Rich Blum\"\nThere were 1 parameters supplied.\n$\nNow you have the ability to test the number of parameters present before trying to \nuse them: \n$ cat test9.sh\n#!/bin/bash\n# Testing parameters\n#\nif [ $# -ne 2 ]\nthen\n   echo\n\n372\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  372\n   echo Usage: test9.sh a b\n   echo\nelse\n   total=$[ $1 + $2 ]\n   echo\n   echo The total is $total\n   echo\nfi\n#\n$ \n$ bash test9.sh\nUsage: test9.sh a b\n$ bash test9.sh 10\nUsage: test9.sh a b\n$ bash test9.sh 10 15\nThe total is 25\n$\nThe if-then statement uses the -ne evaluation to perform a numeric test of the command \nline parameters supplied. If the correct number of parameters isn’t present, an error mes-\nsage displays showing the correct usage of the script.\nThis variable also provides a cool way of grabbing the last parameter on the command line \nwithout having to know how many parameters were used. However, you need to use a little \ntrick to get there.\nIf you think this through, you might think that because the \n$# variable contains the value \nof the number of parameters, using the variable \n${$#} would represent the last command \nline parameter variable. Try that and see what happens: \n$ cat badtest1.sh\n#!/bin/bash\n# testing grabbing last parameter\n#\necho The last parameter was ${$#}\n$ \n$ ./badtest1.sh 10\nThe last parameter was 15354\n$\nWow, what happened? Obviously, something went wrong. It turns out that you can’t use the \ndollar sign within the braces. Instead, you must replace the dollar sign with an exclamation \nmark. Odd, but it works:\n\n373\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  373\n14\n$ cat test10.sh\n#!/bin/bash\n# Grabbing the last parameter\n#\nparams=$#\necho\necho The last parameter is $params\necho The last parameter is ${!#}\necho\n#\n$\n$ bash test10.sh 1 2 3 4 5\nThe last parameter is 5\nThe last parameter is 5\n$\n$ bash test10.sh\nThe last parameter is 0\nThe last parameter is test10.sh\n$\nPerfect. This script also assigned the $# variable value to the variable params and then \nused that variable within the special command line parameter variable format as well. Both \nversions worked. It’s also important to notice that, when there weren’t any parameters on \nthe command line, the \n$# value was zero, which is what appears in the params variable, \nbut the \n${!#} variable returns the script name used on the command line.\nGrabbing all the data\nIn some situations you want to grab all the parameters provided on the command line. \nInstead of having to mess with using the \n$# variable to determine how many parameters \nare on the command line and having to loop through all of them, you can use a couple of \nother special variables.\nThe \n$* and $@ variables provide easy access to all your parameters. Both of these variables \ninclude all the command line parameters within a single variable.\nThe \n$* variable takes all the parameters supplied on the command line as a single word. \nThe word contains each of the values as they appear on the command line. Basically, \ninstead of treating the parameters as multiple objects, the \n$* variable treats them all as \none parameter.\nThe \n$@ variable, on the other hand, takes all the parameters supplied on the command line \nas separate words in the same string. It allows you to iterate through the values, separating \nout each parameter supplied. This is most often accomplished using the \nfor command.\n\n374\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  374\nIt can easily get confusing to fi gure out how these two variables operate. Let’s look at the \ndifference between the two: \n$ cat test11.sh\n#!/bin/bash\n# testing $* and $@\n#\necho\necho \"Using the \\$* method: $*\"\necho\necho \"Using the \\$@ method: $@\"\n$ \n$ ./test11.sh rich barbara katie jessica\nUsing the $* method: rich barbara katie jessica\nUsing the $@ method: rich barbara katie jessica\n$\nNotice that on the surface, both variables produce the same output, showing all the com-\nmand line parameters provided at once.\nThe following example demonstrates where the differences are: \n$ cat test12.sh\n#!/bin/bash\n# testing $* and $@\n#\necho\ncount=1\n#\nfor param in \"$*\"\ndo\n   echo \"\\$* Parameter #$count = $param\"\n   count=$[ $count + 1 ]\ndone\n#\necho\ncount=1\n#\nfor param in \"$@\"\ndo\n   echo \"\\$@ Parameter #$count = $param\"\n   count=$[ $count + 1 ]\ndone\n$ \n$ ./test12.sh rich barbara katie jessica\n$* Parameter #1 = rich barbara katie jessica\n$@ Parameter #1 = rich\n\n375\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  375\n14\n$@ Parameter #2 = barbara\n$@ Parameter #3 = katie\n$@ Parameter #4 = jessica\n$\nNow we’re getting somewhere. By using the for command to iterate through the special \nvariables, you can see how they each treat the command line parameters differently. The \n$* variable treated all the parameters as a single parameter, while the $@ variable treated \neach parameter separately. This is a great way to iterate through command line parameters.\nBeing Shifty\nAnother tool you have in your bash shell tool belt is the shift command. The bash shell \nprovides the \nshift command to help you manipulate command line parameters. The \nshift command literally shifts the command line parameters in their relative positions.\nWhen you use the \nshift command, it moves each parameter variable one position to the \nleft by default. Thus, the value for variable \n$3 is moved to $2, the value for variable $2 is \nmoved to \n$1, and the value for variable $1 is discarded (note that the value for variable \n$0, the program name, remains unchanged).\nThis is another great way to iterate through command line parameters, especially if you \ndon’t know how many parameters are available. You can just operate on the fi rst parameter, \nshift the parameters over, and then operate on the fi rst parameter again.\nHere’s a short demonstration of how this works: \n$ cat test13.sh\n#!/bin/bash\n# demonstrating the shift command\necho\ncount=1\nwhile [ -n \"$1\" ]\ndo\n   echo \"Parameter #$count = $1\"\n   count=$[ $count + 1 ]\n   shift\ndone\n$ \n$ ./test13.sh rich barbara katie jessica\nParameter #1 = rich\nParameter #2 = barbara\nParameter #3 = katie\nParameter #4 = jessica\n$\n\n376\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  376\nThe script performs a while loop, testing the length of the fi rst parameter’s value. When \nthe fi rst parameter’s length is zero, the loop ends. After testing the fi rst parameter, the \nshift command is used to shift all the parameters one position.\nBe careful when working with the shift command. When a parameter is shifted out, its value is lost and can’t be \nrecovered.\nAlternatively, you can perform a multiple location shift by providing a parameter to the \nshift command. Just provide the number of places you want to shift:\n$ cat test14.sh\n#!/bin/bash\n# demonstrating a multi-position shift\n#\necho\necho \"The original parameters: $*\"\nshift 2\necho \"Here's the new first parameter: $1\"\n$ \n$ ./test14.sh 1 2 3 4 5\nThe original parameters: 1 2 3 4 5\nHere's the new first parameter: 3\n$\nBy using values in the shift command, you can easily skip over parameters you \ndon’t need. \nWorking with Options\nIf you’ve been following along in the book, you’ve seen several bash commands that provide \nboth parameters and options. Options are single letters preceded by a dash that alter the \nbehavior of a command. This section shows three methods for working with options in your \nshell scripts.\nFinding your options\nOn the surface, there’s nothing all that special about command line options. They appear \non the command line immediately after the script name, just the same as command line \nparameters. In fact, if you want, you can process command line options the same way you \nprocess command line parameters.\n\n377\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  377\n14\nProcessing simple options\nIn the test13.sh script earlier, you saw how to use the shift command to work your \nway down the command line parameters provided with the script program. You can use this \nsame technique to process command line options.\nAs you extract each individual parameter, use the \ncase statement (see Chapter 12) to \ndetermine when a parameter is formatted as an option: \n$ cat test15.sh\n#!/bin/bash\n# extracting command line options as parameters\n#\necho\nwhile [ -n \"$1\" ]\ndo\n   case \"$1\" in\n     -a) echo \"Found the -a option\" ;;\n     -b) echo \"Found the -b option\" ;;\n     -c) echo \"Found the -c option\" ;;\n      *) echo \"$1 is not an option\" ;;\n   esac\n   shift\ndone\n$ \n$ ./test15.sh -a -b -c -d\nFound the -a option\nFound the -b option\nFound the -c option\n-d is not an option\n$\nThe case statement checks each parameter for valid options. When one is found, the \nappropriate commands are run in the \ncase statement.\nThis method works, no matter in what order the options are presented on the command \nline: \n$ ./test15.sh -d -c -a\n-d is not an option\nFound the -c option\nFound the -a option\n$\nThe case statement processes each option as it fi nds it in the command line parameters. If \nany other parameters are included on the command line, you can include commands in the \ncatch-all part of the \ncase statement to process them.\n\n378\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  378\nSeparating options from parameters\nOften you’ll run into situations where you’ll want to use both options and parameters for a shell \nscript. The standard way to do this in Linux is to separate the two with a special character code \nthat tells the script when the options are fi nished and when the normal parameters start.\nFor Linux, this special character is the double dash (\n--). The shell uses the double dash to \nindicate the end of the option list. After seeing the double dash, your script can safely pro-\ncess the remaining command line parameters as parameters and not options.\nTo check for the double dash, simply add another entry in the \ncase statement: \n$ cat test16.sh\n#!/bin/bash\n# extracting options and parameters\necho\nwhile [ -n \"$1\" ]\ndo\n   case \"$1\" in\n      -a) echo \"Found the -a option\" ;;\n      -b) echo \"Found the -b option\";;\n      -c) echo \"Found the -c option\" ;;\n      --) shift\n          break ;;\n       *) echo \"$1 is not an option\";;\n   esac\n   shift\ndone\n#\ncount=1\nfor param in $@\ndo\n   echo \"Parameter #$count: $param\"\n   count=$[ $count + 1 ]\ndone\n$\nThis script uses the break command to break out of the while loop when it encounters \nthe double dash. Because we’re breaking out prematurely, we need to ensure that we stick \nin another \nshift command to get the double dash out of the parameter variables.\nFor the fi rst test, try running the script using a normal set of options and parameters: \n$ ./test16.sh -c -a -b test1 test2 test3\nFound the -c option\nFound the -a option\nFound the -b option\ntest1 is not an option\n\n379\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  379\n14\ntest2 is not an option\ntest3 is not an option\n$\nThe results show that the script assumed that all the command line parameters were \noptions when it processed them. Next, try the same thing, only this time using the double \ndash to separate the options from the parameters on the command line: \n$ ./test16.sh -c -a -b -- test1 test2 test3\nFound the -c option\nFound the -a option\nFound the -b option\nParameter #1: test1\nParameter #2: test2\nParameter #3: test3\n$\nWhen the script reaches the double dash, it stops processing options and assumes that any \nremaining parameters are command line parameters.\nProcessing options with values\nSome options require an additional parameter value. In these situations, the command line \nlooks something like this: \n$ ./testing.sh -a test1 -b -c -d test2\nYour script must be able to detect when your command line option requires an additional \nparameter and be able to process it appropriately. Here’s an example of how to do that: \n$ cat test17.sh\n#!/bin/bash\n# extracting command line options and values\necho\nwhile [ -n \"$1\" ]\ndo\n   case \"$1\" in\n      -a) echo \"Found the -a option\";;\n      -b) param=\"$2\"\n          echo \"Found the -b option, with parameter value $param\"\n          shift ;;\n      -c) echo \"Found the -c option\";;\n      --) shift\n          break ;;\n       *) echo \"$1 is not an option\";;\n   esac\n   shift\ndone\n\n380\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  380\n#\ncount=1\nfor param in \"$@\"\ndo\n   echo \"Parameter #$count: $param\"\n   count=$[ $count + 1 ]\ndone\n$ \n$ ./test17.sh -a -b test1 -d\nFound the -a option\nFound the -b option, with parameter value test1\n-d is not an option\n$\nIn this example, the case statement defi nes three options that it processes. The -b option \nalso requires an additional parameter value. Because the parameter being processed is \n$1, \nyou know that the additional parameter value is located in \n$2 (because all the parameters \nare shifted after they are processed). Just extract the parameter value from the \n$2 vari-\nable. Of course, because we used two parameter spots for this option, you also need to set \nthe \nshift command to shift one additional position.\nJust as with the basic feature, this process works no matter what order you place the options \nin (just remember to include the appropriate option parameter with the each option): \n$ ./test17.sh -b test1 -a -d\nFound the -b option, with parameter value test1\nFound the -a option\n-d is not an option\n$\nNow you have the basic ability to process command line options in your shell scripts, but \nthere are limitations. For example, this doesn’t work if you try to combine multiple options \nin one parameter: \n$ ./test17.sh -ac\n-ac is not an option\n$\nIt is a common practice in Linux to combine options, and if your script is going to be user-\nfriendly, you’ll want to offer this feature for your users as well. Fortunately, there’s another \nmethod for processing options that can help you.\nUsing the getopt command\nThe getopt command is a great tool to have handy when processing command line options \nand parameters. It reorganizes the command line parameters to make parsing them in your \nscript easier.\n\n381\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  381\n14\nLooking at the command format\nThe getopt command can take a list of command line options and parameters, in any form, \nand automatically turn them into the proper format. It uses the following command format: \ngetopt optstring parameters\nThe optstring is the key to the process. It defi nes the valid option letters that can be \nused in the command line. It also defi nes which option letters require a parameter value.\nFirst, list each command line option letter you’re going to use in your script in the \noptstring. Then place a colon after each option letter that requires a parameter value. \nThe \ngetopt command parses the supplied parameters based on the optstring you defi ne.\nA more advanced version of the getopt command, called getopts (notice it is plural), is available. The getopts \ncommand is covered later in this chapter. Because of their nearly identical spelling, it’s easy to get these two com-\nmands confused. Be careful!\nHere’s a simple example of how getopt works: \n$ getopt ab:cd -a -b test1 -cd test2 test3\n -a -b test1 -c -d -- test2 test3\n$\nThe optstring defi nes four valid option letters, a, b, c, and d. A colon (:) is placed behind \nthe letter \nb in order to require option b to have a parameter value. When the getopt com-\nmand runs, it examines the provided parameter list (\n-a -b test1 -cd test2 test3) \nand parses it based on the supplied \noptstring. Notice that it automatically separated the \n-cd options into two separate options and inserted the double dash to separate the addi-\ntional parameters on the line.\nIf you specify a parameter option not in the \noptstring, by default the getopt command \nproduces an error message: \n$ getopt ab:cd -a -b test1 -cde test2 test3\ngetopt: invalid option -- e\n -a -b test1 -c -d -- test2 test3\n$\nIf you prefer to just ignore the error messages, use getopt with the -q option:\n$ getopt -q ab:cd -a -b test1 -cde test2 test3\n -a -b 'test1' -c -d -- 'test2' 'test3'\n$\nNote that the getopt command options must be listed before the optstring. Now you \nshould be ready to use this command in your scripts to process command line options.\n\n382\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  382\nUsing getopt in your scripts\nYou can use the getopt command in your scripts to format any command line options or \nparameters entered for your script. It’s a little tricky, however, to use.\nThe trick is to replace the existing command line options and parameters with the for-\nmatted version produced by the \ngetopt command. The way to do that is to use the set \ncommand.\nYou saw the \nset command back in Chapter 6. The set command works with the different \nvariables in the shell. \nOne of the \nset command options is the double dash (--). The double dash instructs set to \nreplace the command line parameter variables with the values on the \nset command’s com-\nmand line.\nThe trick then is to feed the original script command line parameters to the \ngetopt com-\nmand and then feed the output of the \ngetopt command to the set command to replace \nthe original command line parameters with the nicely formatted ones from \ngetopt. This \nlooks something like this: \nset -- $(getopt -q ab:cd \"$@\")\nNow the values of the original command line parameter variables are replaced with the out-\nput from the \ngetopt command, which formats the command line parameters for us.\nUsing this technique, we can now write scripts that handle our command line parameters \nfor us: \n$ cat test18.sh\n#!/bin/bash\n# Extract command line options & values with getopt\n#\nset -- $(getopt -q ab:cd \"$@\")\n#\necho\nwhile [ -n \"$1\" ]\ndo\n   case \"$1\" in\n   -a) echo \"Found the -a option\" ;;\n   -b) param=\"$2\"\n       echo \"Found the -b option, with parameter value $param\"\n       shift ;;\n   -c) echo \"Found the -c option\" ;;\n   --) shift\n       break ;;\n    *) echo \"$1 is not an option\";;\n   esac\n   shift\n\n383\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  383\n14\ndone\n#\ncount=1\nfor param in \"$@\"\ndo\n   echo \"Parameter #$count: $param\"\n   count=$[ $count + 1 ]\ndone\n#\n$ \nYou’ll notice this is basically the same script as in test17.sh. The only thing that changed \nis the addition of the \ngetopt command to help format our command line parameters.\nNow when you run the script with complex options, things work much better: \n$ ./test18.sh -ac\nFound the -a option\nFound the -c option\n$\nAnd of course, all the original features work just fi ne as well: \n$ ./test18.sh -a -b test1 -cd test2 test3 test4\nFound the -a option\nFound the -b option, with parameter value 'test1'\nFound the -c option\nParameter #1: 'test2'\nParameter #2: 'test3'\nParameter #3: 'test4'\n$\nNow things are looking pretty fancy. However, there’s still one small bug that lurks in the \ngetopt command. Check out this example: \n$ ./test18.sh -a -b test1 -cd \"test2 test3\" test4\nFound the -a option\nFound the -b option, with parameter value 'test1'\nFound the -c option\nParameter #1: 'test2\nParameter #2: test3'\nParameter #3: 'test4'\n$\nThe getopt command isn’t good at dealing with parameter values with spaces and quota-\ntion marks. It interpreted the space as the parameter separator, instead of following the \n\n384\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  384\ndouble quotation marks and combining the two values into one parameter. Fortunately, this \nproblem has another solution.\nAdvancing to getopts\nThe getopts command (notice that it is plural) is built into the bash shell. It looks much \nlike its \ngetopt cousin, but has some expanded features.\nUnlike \ngetopt, which produces one output for all the processed options and parameters \nfound in the command line, the \ngetopts command works on the existing shell parameter \nvariables sequentially.\nIt processes the parameters it detects in the command line one at a time each time it’s \ncalled. When it runs out of parameters, it exits with an exit status greater than zero. This \nmakes it great for using in loops to parse all the parameters on the command line.\nHere’s the format of the \ngetopts command: \ngetopts optstring variable\nThe optstring value is similar to the one used in the getopt command. Valid option let-\nters are listed in the \noptstring, along with a colon if the option letter requires a param-\neter value. To suppress error messages, start the \noptstring with a colon. The getopts \ncommand places the current parameter in the \nvariable defi ned in the command line.\nThe \ngetopts command uses two environment variables. The OPTARG environment variable \ncontains the value to be used if an option requires a parameter value. The \nOPTIND environ-\nment variable contains the value of the current location within the parameter list where \ngetopts left off. This allows you to continue processing other command line parameters \nafter fi nishing the options.\nLet’s look at a simple example that uses the \ngetopts command: \n$ cat test19.sh\n#!/bin/bash\n# simple demonstration of the getopts command\n#\necho\nwhile getopts :ab:c opt\ndo\n   case \"$opt\" in\n      a) echo \"Found the -a option\" ;;\n      b) echo \"Found the -b option, with value $OPTARG\";;\n      c) echo \"Found the -c option\" ;;\n      *) echo \"Unknown option: $opt\";;\n   esac\ndone\n$ \n\n385\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  385\n14\n$ ./test19.sh -ab test1 -c\nFound the -a option\nFound the -b option, with value test1\nFound the -c option\n$\nThe while statement defi nes the getopts command, specifying what command line \noptions to look for, along with the variable name (\nopt) to store them in for each iteration.\nYou’ll notice something different about the \ncase statement in this example. When the \ngetopts command parses the command line options, it strips off the leading dash, so you \ndon’t need leading dashes in the \ncase defi nitions.\nThe \ngetopts command offers several nice features. For starters, you can include spaces in \nyour parameter values: \n$ ./test19.sh -b \"test1 test2\" -a\nFound the -b option, with value test1 test2\nFound the -a option\n$\nAnother nice feature is that you can run the option letter and the parameter value together \nwithout a space: \n$ ./test19.sh -abtest1\nFound the -a option\nFound the -b option, with value test1\n$\nThe getopts command correctly parsed the test1 value from the -b option. In addition, \nthe \ngetopts command bundles any undefi ned option it fi nds in the command line into a \nsingle output, the question mark: \n$ ./test19.sh -d\nUnknown option: ?\n$ \n$ ./test19.sh -acde\nFound the -a option\nFound the -c option\nUnknown option: ?\nUnknown option: ?\n$\nAny option letter not defi ned in the optstring value is sent to your code as a \nquestion mark.\n\n386\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  386\nThe getopts command knows when to stop processing options and leave the parameters \nfor you to process. As \ngetopts processes each option, it increments the OPTIND environ-\nment variable by one. When you’ve reached the end of the \ngetopts processing, you can \nuse the \nOPTIND value with the shift command to move to the parameters: \n$ cat test20.sh\n#!/bin/bash\n# Processing options & parameters with getopts\n#\necho\nwhile getopts :ab:cd opt\ndo\n   case \"$opt\" in\n   a) echo \"Found the -a option\"  ;;\n   b) echo \"Found the -b option, with value $OPTARG\" ;;\n   c) echo \"Found the -c option\"  ;;\n   d) echo \"Found the -d option\"  ;;\n   *) echo \"Unknown option: $opt\" ;;\n   esac\ndone\n#\nshift $[ $OPTIND - 1 ]\n#\necho\ncount=1\nfor param in \"$@\"\ndo\n   echo \"Parameter $count: $param\"\n   count=$[ $count + 1 ]\ndone\n#\n$ \n$ ./test20.sh -a -b test1 -d test2 test3 test4\nFound the -a option\nFound the -b option, with value test1\nFound the -d option\nParameter 1: test2\nParameter 2: test3\nParameter 3: test4\n$\nNow you have a full-featured command line option and parameter processing utility you \ncan use in all your shell scripts!\n\n387\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  387\n14\nStandardizing Options\nWhen you create your shell script, obviously you’re in control of what happens. It’s \n completely up to you as to which letter options you select to use and how you select to \nuse them.\nHowever, a few letter options have achieved a somewhat standard meaning in the \nLinux world. If you leverage these options in your shell script, your scripts will be more \nuser-friendly.\nTable 14-1 shows some of the common meanings for command line options used in Linux.\nTABLE 14 -1    Common Linux Command Line Options\nOptionDescription\n-a\nShows all objects\n-c\nProduces a count\n-d\nSpecifi es a directory\n-e\nExpands an object\n-f\nSpecifi es a fi le to read data from\n-h\nDisplays a help message for the command\n-i\nIgnores text case\n-l\nProduces a long format version of the output\n-n\nUses a non-interactive (batch) mode\n-o\nSpecifi es an output fi le to redirect all output to\n-q\nRuns in quiet mode\n-r\nProcesses directories and fi les recursively\n-s\nRuns in silent mode\n-v\nProduces verbose output\n-x\nExcludes an object\n-y\nAnswers yes to all questions\nYou’ll probably recognize most of these option meanings just from working with the various \nbash commands throughout the book. Using the same meaning for your options helps users \ninteract with your script without having to worry about manuals.\n\n388\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  388\nGetting User Input\nAlthough providing command line options and parameters is a great way to get data from \nyour script users, sometimes your script needs to be more interactive. Sometimes you need \nto ask a question while the script is running and wait for a response from the person run-\nning your script. The bash shell provides the \nread command just for this purpose.\nReading basics\nThe read command accepts input either from standard input (such as from the keyboard) \nor from another fi le descriptor. After receiving the input, the \nread command places the \ndata into a variable. Here’s the \nread command at its simplest: \n$ cat test21.sh\n#!/bin/bash\n# testing the read command\n#\necho -n \"Enter your name: \"\nread name\necho \"Hello $name, welcome to my program. \"\n#\n$ \n$ ./test21.sh\nEnter your name: Rich Blum\nHello Rich Blum, welcome to my program.\n$\nThat’s pretty simple. Notice that the echo command that produced the prompt uses the -n \noption. This suppresses the newline character at the end of the string, allowing the script \nuser to enter data immediately after the string, instead of on the next line. This gives your \nscripts a more form-like appearance.\nIn fact, the \nread command includes the -p option, which allows you to specify a prompt \ndirectly in the \nread command line: \n$ cat test22.sh\n#!/bin/bash\n# testing the read -p option\n#\nread -p \"Please enter your age: \" age\ndays=$[ $age * 365 ]\necho \"That makes you over $days days old! \"\n#\n$ \n$ ./test22.sh\nPlease enter your age: 10\nThat makes you over 3650 days old!\n$\n\n389\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  389\n14\nYou’ll notice in the fi rst example that when a name was entered, the read command \nassigned both the fi rst name and last name to the same variable. The \nread command \nassigns all data entered at the prompt to a single variable, or you can specify multiple vari-\nables. Each data value entered is assigned to the next variable in the list. If the list of vari-\nables runs out before the data does, the remaining data is assigned to the last variable: \n$ cat test23.sh\n#!/bin/bash\n# entering multiple variables\n#\nread -p \"Enter your name: \" first last\necho \"Checking data for $last, $first...\"\n$ \n$ ./test23.sh\nEnter your name: Rich Blum\nChecking data for Blum, Rich...\n$\nYou can also specify no variables on the read command line. If you do that, the read com-\nmand places any data it receives in the special environment variable \nREPLY:\n$ cat test24.sh\n#!/bin/bash\n# Testing the REPLY Environment variable\n#\nread -p \"Enter your name: \"\necho\necho Hello $REPLY, welcome to my program.\n#\n$ \n$ ./test24.sh\nEnter your name: Christine\nHello Christine, welcome to my program.\n$\nThe REPLY environment variable contains all the data entered in the input, and it can be \nused in the shell script as any other variable.\nTiming out\nBe careful when using the read command. Your script may get stuck waiting for the script \nuser to enter data. If the script must go on regardless of whether any data was entered, you \ncan use the \n-t option to specify a timer. The -t option specifi es the number of seconds for \nthe \nread command to wait for input. When the timer expires, the read command returns a \nnon-zero exit status: \n$ cat test25.sh\n#!/bin/bash\n\n390\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  390\n# timing the data entry\n#\nif read -t 5 -p \"Please enter your name: \" name\nthen\n   echo \"Hello $name, welcome to my script\"\nelse\n   echo\n   echo \"Sorry, too slow! \"\nfi\n$ \n$ ./test25.sh\nPlease enter your name: Rich\nHello Rich, welcome to my script\n$ \n$ ./test25.sh\nPlease enter your name:\nSorry, too slow!\n$\nBecause the read command exits with a non-zero exit status if the timer expires, it’s easy \nto use the standard structured statements, such as an \nif-then statement or a while loop \nto track what happened. In this example, when the timer expires, the \nif statement fails, \nand the shell executes the commands in the \nelse section.\nInstead of timing the input, you can also set the \nread command to count the input charac-\nters. When a preset number of characters has been entered, it automatically exits, assigning \nthe entered data to the variable: \n$ cat test26.sh\n#!/bin/bash\n# getting just one character of input\n#\nread -n1 -p \"Do you want to continue [Y/N]? \" answer\ncase $answer in\nY | y) echo\n       echo \"fine, continue on...\";;\nN | n) echo\n       echo OK, goodbye\n       exit;;\nesac\necho \"This is the end of the script\"\n$ \n$ ./test26.sh\nDo you want to continue [Y/N]? Y\nfine, continue on...\nThis is the end of the script\n$ \n$ ./test26.sh\n\n391\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  391\n14\nDo you want to continue [Y/N]? n\nOK, goodbye\n$\nThis example uses the -n option with the value of 1, instructing the read command to \naccept only a single character before exiting. As soon as you press the single character to \nanswer, the \nread command accepts the input and passes it to the variable. You don’t need \nto press the Enter key.\nReading with no display\nSometimes you need input from the script user, but you don’t want that input to display on \nthe monitor. The classic example is when entering passwords, but there are plenty of other \ntypes of data that you need to hide.\nThe \n-s option prevents the data entered in the read command from being displayed on the \nmonitor; actually, the data is displayed, but the \nread command sets the text color to the \nsame as the background color. Here’s an example of using the \n-s option in a script: \n$ cat test27.sh\n#!/bin/bash\n# hiding input data from the monitor\n#\nread -s -p \"Enter your password: \" pass\necho\necho \"Is your password really $pass? \"\n$ \n$ ./test27.sh\nEnter your password:\nIs your password really T3st1ng?\n$\nThe data typed at the input prompt doesn’t appear on the monitor but is assigned to the \nvariable for use in the script.\nReading from a fi le\nFinally, you can also use the read command to read data stored in a fi le on the Linux sys-\ntem. Each call to the \nread command reads a single line of text from the fi le. When no more \nlines are left in the fi le, the \nread command exits with a non-zero exit status.\nThe tricky part is getting the data from the fi le to the \nread command. The most common \nmethod is to \npipe the result of the cat command of the fi le directly to a while command \nthat contains the \nread command. Here’s an example: \n$ cat test28.sh \n#!/bin/bash\n\n392\nPart II: Shell Scripting Basics\nc14.indd  12/12/2014  Page  392\n# reading data from a file\n#\ncount=1\ncat test | while read line\ndo\n   echo \"Line $count: $line\"\n   count=$[ $count + 1]\ndone\necho \"Finished processing the file\"\n$ \n$ cat test\nThe quick brown dog jumps over the lazy fox.\nThis is a test, this is only a test.\nO Romeo, Romeo! Wherefore art thou Romeo?\n$ \n$ ./test28.sh\nLine 1: The quick brown dog jumps over the lazy fox.\nLine 2: This is a test, this is only a test.\nLine 3: O Romeo, Romeo! Wherefore art thou Romeo?\nFinished processing the file\n$\nThe while command loop continues processing lines of the fi le with the read command, \nuntil the \nread command exits with a non-zero exit status.\nSummary\nThis chapter showed three methods for retrieving data from the script user. Command \nline parameters allow users to enter data directly on the command line when they run the \nscript. The script uses positional parameters to retrieve the command line parameters and \nassign them to variables.\nThe \nshift command allows you to manipulate the command line parameters by rotating \nthem within the positional parameters. This command allows you to easily iterate through \nthe parameters without knowing how many parameters are available.\nYou can use three special variables when working with command line parameters. The shell \nsets the \n$# variable to the number of parameters entered on the command line. The $* \nvariable contains all the parameters as a single string, and the \n$@ variable contains all the \nparameters as separate words. These variables come in handy when you’re trying to process \nlong parameter lists.\nBesides parameters, your script users can use command line options to pass information to \nyour script. Command line options are single letters preceded by a dash. Different options \ncan be assigned to alter the behavior of your script.\n\n393\nChapter 14: Handling User Input\nc14.indd  12/12/2014  Page  393\n14\nThe bash shell provides three ways to handle command line options.\nThe fi rst way is to handle them just like command line parameters. You can iterate through \nthe options using the positional parameter variables, processing each option as it appears \non the command line.\nAnother way to handle command line options is with the \ngetopt command. This command \nconverts command line options and parameters into a standard format that you can process \nin your script. The \ngetopt command allows you to specify which letters it recognizes as \noptions and which options require an additional parameter value. The \ngetopt command \nprocesses the standard command line parameters and outputs the options and parameters \nin the proper order.\nThe fi nal method for handling command line options is via the \ngetopts command (note \nthat it’s plural). The \ngetopts command provides more advanced processing of the com-\nmand line parameters. It allows for multi-value parameters, along with identifying options \nnot defi ned by the script.\nAn interactive method to obtain data from your script users is the \nread command. The \nread command allows your scripts to query users for information and wait. The read com-\nmand places any data entered by the script user into one or more variables, which you can \nuse within the script.\nSeveral options are available for the \nread  command that allow you to customize the data \ninput into your script, such as using hidden data entry, applying timed data entry, and \nrequesting a specifi c number of input characters.\nIn the next chapter, we look further into how bash shell scripts output data. So far, you’ve \nseen how to display data on the monitor and redirect it to a fi le. Next, we explore a few \nother options that you have available not only to direct data to specifi c locations but also \nto direct specifi c types of data to specifi c locations. This will help make your shell scripts \nlook professional! \n \n\nc14.indd  12/12/2014  Page  394\n\n395\nc15.indd  12/08/2014  Page  395\nCHAPTER \n15\nPresenting Data\nIN THIS CHAPTER\nRevisiting redirection\nStandard input and output\nReporting errors\nThrowing away data\nCreating log fi les\nS\no far the scripts shown in this book display information either by echoing data to the monitor \nor by redirecting data to a fi le. Chapter 11 demonstrated how to redirect the output of a com-\nmand to a fi le. This chapter expands on that topic by showing you how you can redirect the \noutput of your script to different locations on your Linux system.\nUnderstanding Input and Output\nSo far, you’ve seen two methods for displaying the output from your scripts:\n ■\nDisplaying output on the monitor screen\n ■\nRedirecting output to a fi le\nBoth methods produced an all-or-nothing approach to data output. There are times, however, when \nit would be nice to display some data on the monitor and other data in a fi le. For these instances, it \ncomes in handy to know how Linux handles input and output so you can get your script output to \nthe right place.\nThe following sections describe how to use the standard Linux input and output system to your \nadvantage, to help direct script output to specifi c locations.\nStandard fi le descriptors\nThe Linux system handles every object as a fi le. This includes the input and output process. Linux \nidentifi es each fi le object using a file descriptor. The fi le descriptor is a non-negative integer that \n\n396\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  396\nuniquely identifi es open fi les in a session. Each process is allowed to have up to nine open \nfi le descriptors at a time. The bash shell reserves the fi rst three fi le descriptors (0, 1, and 2) \nfor special purposes. These are shown in Table 15-1.\nTABLE 15 -1    Linux Standard File Descriptors\nFile DescriptorAbbreviationDescription\n0\nSTDIN\nStandard input\n1\nSTDOUT\nStandard output\n2\nSTDERR\nStandard error\nThese three special fi le descriptors handle the input and output from your script. The shell \nuses them to direct the default input and output in the shell to the appropriate location, \nwhich by default is usually your monitor. The following sections describe each of these \nstandard fi le descriptors in greater detail.\nSTDIN\nThe STDIN fi le descriptor references the standard input to the shell. For a terminal inter-\nface, the standard input is the keyboard. The shell receives input from the keyboard on the \nSTDIN fi le descriptor and processes each character as you type it.\nWhen you use the input redirect symbol (<), Linux replaces the standard input fi le descrip-\ntor with the fi le referenced by the redirection. It reads the fi le and retrieves data just as if \nit were typed on the keyboard.\nMany bash commands accept input from \nSTDIN, especially if no fi les are specifi ed on \nthe command line. Here’s an example of using the \ncat command with data entered from \nSTDIN:\n $ cat\n this is a test\n this is a test\n this is a second test.\n this is a second test.\nWhen you enter the cat command on the command line by itself, it accepts input from \nSTDIN. As you enter each line, the cat command echoes the line to the display.\nHowever, you can also use the \nSTDIN redirect symbol to force the cat command to accept \ninput from another fi le other than \nSTDIN:\n $ cat < testfile\n This is the first line.\n This is the second line.\n\n397\nChapter 15: Presenting Data\nc15.indd  12/08/2014  Page  397\n15\n This is the third line.\n $\nNow the cat command uses the lines that are contained in the testfile fi le as the input. \nYou can use this technique to input data to any shell command that accepts data from \nSTDIN.\nSTDOUT\nThe STDOUT fi le descriptor references the standard output for the shell. On a terminal \ninterface, the standard output is the terminal monitor. All output from the shell (including \nprograms and scripts you run in the shell) is directed to the standard output, which is the \nmonitor.\nMost bash commands direct their output to the \nSTDOUT fi le descriptor by default. As shown \nin Chapter 11, you can change that using output redirection:\n $ ls -l > test2\n $ cat test2\n total 20\n -rw-rw-r-- 1 rich rich 53 2014-10-16 11:30 test\n -rw-rw-r-- 1 rich rich  0 2014-10-16 11:32 test2\n -rw-rw-r-- 1 rich rich 73 2014-10-16 11:23 testfile\n $\nWith the output redirection symbol, all the output that normally would go to the monitor is \ninstead redirected to the designated redirection fi le by the shell.\nYou can also append data to a fi le. You do this using the \n>> symbol:\n $ who >> test2\n $ cat test2\n total 20\n -rw-rw-r-- 1 rich rich 53 2014-10-16 11:30 test\n -rw-rw-r-- 1 rich rich  0 2014-10-16 11:32 test2\n -rw-rw-r-- 1 rich rich 73 2014-10-16 11:23 testfile\n rich     pts/0        2014-10-17 15:34 (192.168.1.2)\n $\nThe output generated by the who command is appended to the data already in the test2 \nfi le.\nHowever, if you use the standard output redirection for your scripts, you can run into a \nproblem. Here’s an example of what can happen in your script:\n $ ls -al badfile > test3\n ls: cannot access badfile: No such file or directory\n $ cat test3\n $\n\n398\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  398\nWhen a command produces an error message, the shell doesn’t redirect the error message to \nthe output redirection fi le. The shell created the output redirection fi le, but the error mes-\nsage appeared on the monitor screen. Notice that there isn’t an error when trying to display \nthe contents of the \ntest3 fi le. The test3 fi le was created just fi ne, but it’s empty.\nThe shell handles error messages separately from the normal output. If you’re creating a \nshell script that runs in background mode, often you must rely on the output messages \nbeing sent to a log fi le. Using this technique, if any error messages occur, they don’t appear \nin the log fi le. You need to do something different.\nSTDERR\nThe shell handles error messages using the special STDERR fi le descriptor. The STDERR fi le \ndescriptor references the standard error output for the shell. This is the location where the \nshell sends error messages generated by the shell or programs and scripts running in the shell.\nBy default, the \nSTDERR fi le descriptor points to the same place as the STDOUT fi le descrip-\ntor (even though they are assigned different fi le descriptor values). This means that, by \ndefault, all error messages go to the monitor display.\nHowever, as you saw in the example, when you redirect \nSTDOUT, this doesn’t automatically \nredirect \nSTDERR. When working with scripts, you’ll often want to change that behavior, \nespecially if you’re interested in logging error messages to a log fi le.\nRedirecting errors\nYou’ve already seen how to redirect the STDOUT data by using the redirection symbol. \nRedirecting the \nSTDERR data isn’t much different; you just need to defi ne the STDERR fi le \ndescriptor when you use the redirection symbol. You can do this in a couple of ways.\nRedirecting errors only\nAs you saw in Table 15-1, the STDERR fi le descriptor is set to the value 2. You can select \nto redirect only error messages by placing this fi le descriptor value immediately before the \nredirection symbol. The value must appear immediately before the redirection symbol or it \ndoesn’t work:\n $ ls -al badfile 2> test4\n $ cat test4\n ls: cannot access badfile: No such file or directory\n $\nNow when you run the command, the error message doesn’t appear on the monitor. Instead, \nthe output fi le contains any error messages that are generated by the command. Using this \nmethod, the shell redirects the error messages only, not the normal data. Here’s another \nexample of mixing \nSTDOUT and STDERR messages in the same output:\n $ ls -al test badtest test2 2> test5\n -rw-rw-r-- 1 rich rich 158 2014-10-16 11:32 test2\n\n399\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  399\n $ cat test5\n ls: cannot access test: No such file or directory\n ls: cannot access badtest: No such file or directory\n $\nThe normal STDOUT output from the ls command still goes to the default STDOUT fi le \ndescriptor, which is the monitor. Because the command redirects fi le descriptor 2 output \n(\nSTDERR) to an output fi le, the shell sends any error messages generated directly to the \nspecifi ed redirection fi le.\nRedirecting errors and data\nIf you want to redirect both errors and the normal output, you need to use two redirection \nsymbols. You need to precede each with the appropriate fi le descriptor for the data you \nwant to redirect and then have them point to the appropriate output fi le for holding the \ndata:\n $ ls -al test test2 test3 badtest 2> test6 1> test7\n $ cat test6\n ls: cannot access test: No such file or directory\n ls: cannot access badtest: No such file or directory\n $ cat test7\n -rw-rw-r-- 1 rich rich 158 2014-10-16 11:32 test2\n -rw-rw-r-- 1 rich rich   0 2014-10-16 11:33 test3\n $\nThe shell redirects the normal output of the ls command that would have gone to STDOUT \nto the \ntest7 fi le using the 1> symbol. Any error messages that would have gone to \nSTDERR were redirected to the test6 fi le using the 2> symbol.\nYou can use this technique to separate normal script output from any error messages \nthat occur in the script. This allows you to easily identify errors without having to wade \nthrough thousands of lines of normal output data.\nAlternatively, if you want, you can redirect both \nSTDERR and STDOUT output to the same \noutput fi le. The bash shell provides a special redirection symbol just for this purpose, the \n&> symbol:\n $ ls -al test test2 test3 badtest &> test7\n $ cat test7\n ls: cannot access test: No such file or directory\n ls: cannot access badtest: No such file or directory\n -rw-rw-r-- 1 rich rich 158 2014-10-16 11:32 test2\n -rw-rw-r-- 1 rich rich   0 2014-10-16 11:33 test3\n $\nWhen you use the &> symbol, all the output generated by the command is sent to the \nsame location, both data and errors. Notice that one of the error messages is out of order \nfrom what you’d expect. The error message for the \nbadtest fi le (the last fi le to be listed) \n\n400\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  400\nappears second in the output fi le. The bash shell automatically gives error messages a \nhigher priority than the standard output. This allows you to view the error messages \ntogether, rather than scattered throughout the output fi le.\nRedirecting Output in Scripts\nYou can use the STDOUT and STDERR fi le descriptors in your scripts to produce output \nin multiple locations simply by redirecting the appropriate fi le descriptors. There are two \nmethods for redirecting output in the script:\n ■\nTemporarily redirecting each line\n ■\nPermanently redirecting all commands in the script\nThe following sections describe how each of these methods works.\nTemporary redirections\nIf you want to purposely generate error messages in your script, you can redirect an indi-\nvidual output line to \nSTDERR. You just need to use the output redirection symbol to redi-\nrect the output to the \nSTDERR fi le descriptor. When you redirect to a fi le descriptor, you \nmust precede the fi le descriptor number with an ampersand (\n&):\n echo \"This is an error message\" >&2\nThis line displays the text wherever the STDERR fi le descriptor for the script is pointing, \ninstead of the normal \nSTDOUT. The following is an example of a script that uses this feature:\n $ cat test8\n #!/bin/bash\n # testing STDERR messages\n \n echo \"This is an error\" >&2\n echo \"This is normal output\"\n $\nIf you run the script as normal, you don’t notice any difference:\n $ ./test8\n This is an error\n This is normal output\n $\nRemember that, by default, Linux directs the STDERR output to STDOUT. However, if you redi-\nrect \nSTDERR when running the script, any text directed to STDERR in the script is redirected:\n $ ./test8 2> test9\n This is normal output\n\n401\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  401\n $ cat test9\n This is an error\n $\nPerfect! The text displayed using STDOUT appears on the monitor, while the echo state-\nment text sent to \nSTDERR is redirected to the output fi le.\nThis method is great for generating error messages in your scripts. If someone uses your scripts, \nthey can easily redirect the error messages using the \nSTDERR fi le descriptor, as shown.\nPermanent redirections\nIf you have lots of data that you’re redirecting in your script, it can get tedious having \nto redirect every \necho statement. Instead, you can tell the shell to redirect a specifi c fi le \ndescriptor for the duration of the script by using the \nexec command:\n $ cat test10\n #!/bin/bash\n # redirecting all output to a file\n exec 1>testout\n \n echo \"This is a test of redirecting all output\"\n echo \"from a script to another file.\"\n echo \"without having to redirect every individual line\"\n $ ./test10\n $ cat testout\n This is a test of redirecting all output\n from a script to another file.\n without having to redirect every individual line\n $\nThe exec command starts a new shell and redirects the STDOUT fi le descriptor to a fi le. All \noutput in the script that goes to \nSTDOUT is instead redirected to the fi le.\nYou can also redirect the \nSTDOUT in the middle of a script:\n $ cat test11\n #!/bin/bash\n # redirecting output to different locations\n \n exec 2>testerror\n \n echo \"This is the start of the script\"\n echo \"now redirecting all output to another location\"\n \n exec 1>testout\n \n echo \"This output should go to the testout file\"\n\n402\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  402\n echo \"but this should go to the testerror file\" >&2\n $\n $ ./test11\n This is the start of the script\n now redirecting all output to another location\n $ cat testout\n This output should go to the testout file\n $ cat testerror\n but this should go to the testerror file\n $\nThe script uses the exec command to redirect any output going to STDERR to the fi le \ntesterror. Next, the script uses the echo statement to display a few lines to STDOUT. \nAfter that, the \nexec command is used again to redirect STDOUT to the testout fi le. \nNotice that even when \nSTDOUT is redirected, you can still specify the output from an echo \nstatement to go to \nSTDERR, which in this case is still redirected to the testerror fi le.\nThis feature can come in handy when you want to redirect the output of just parts of a \nscript to an alternative location, such as an error log. There’s just one problem you run into \nwhen using this.\nAfter you redirect \nSTDOUT or STDERR, you can’t easily redirect them back to their original \nlocation. If you need to switch back and forth with your redirection, you need to learn a \ntrick. The “Creating Your Own Redirection” section later in this chapter discusses this trick \nand how to use it in your shell scripts.\nRedirecting Input in Scripts\nYou can use the same technique used to redirect STDOUT and STDERR in your scripts to \nredirect \nSTDIN from the keyboard. The exec command allows you to redirect STDIN from a \nfi le on the Linux system:\n exec 0< testfile\nThis command informs the shell that it should retrieve input from the fi le testfile \ninstead of \nSTDIN. This redirection applies anytime the script requests input. Here’s an \nexample of this in action:\n $ cat test12\n #!/bin/bash\n # redirecting file input\n \n exec 0< testfile\n count=1\n \n while read line\n\n403\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  403\n do\n    echo \"Line #$count: $line\"\n    count=$[ $count + 1 ]\n done\n $ ./test12\n Line #1: This is the first line.\n Line #2: This is the second line.\n Line #3: This is the third line.\n $\nChapter 14 showed you how to use the read command to read data entered from the key-\nboard by a user. By redirecting \nSTDIN from a fi le, when the read command attempts to \nread from \nSTDIN, it retrieves data from the fi le instead of the keyboard.\nThis is an excellent technique to read data in fi les for processing in your scripts. A common \ntask for Linux system administrators is to read data from log fi les for processing. This is the \neasiest way to accomplish that task.\nCreating Your Own Redirection\nWhen you redirect input and output in your script, you’re not limited to the three default \nfi le descriptors. I mentioned that you could have up to nine open fi le descriptors in the \nshell. The other six fi le descriptors are numbered from 3 through 8 and are available for you \nto use as either input or output redirection. You can assign any of these fi le descriptors to a \nfi le and then use them in your scripts as well. This section shows you how to use the other \nfi le descriptors in your scripts.\nCreating output fi le descriptors\nYou assign a fi le descriptor for output by using the exec command. As with the standard \nfi le descriptors, after you assign an alternative fi le descriptor to a fi le location, that redi-\nrection stays permanent until you reassign it. Here’s a simple example of using an alterna-\ntive fi le descriptor in a script:\n $ cat test13\n #!/bin/bash\n # using an alternative file descriptor\n \n exec 3>test13out\n \n echo \"This should display on the monitor\"\n echo \"and this should be stored in the file\" >&3\n echo \"Then this should be back on the monitor\"\n $ ./test13\n This should display on the monitor\n\n404\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  404\n Then this should be back on the monitor\n $ cat test13out\n and this should be stored in the file\n $\nThe script uses the exec command to redirect fi le descriptor 3 to an alternative fi le loca-\ntion. When the script executes the \necho statements, they display on STDOUT as you would \nexpect. However, the \necho statements that you redirect to fi le descriptor 3 go to the \nalternative fi le. This allows you to keep normal output for the monitor and redirect special \ninformation to fi les, such as log fi les.\nYou can also use the \nexec command to append data to an existing fi le instead of creating a \nnew fi le:\nexec 3>>test13out\nNow the output is appended to the test13out fi le instead of creating a new fi le.\nRedirecting fi le descriptors\nHere’s the trick to help you bring back a redirected fi le descriptor. You can assign an alter-\nnative fi le descriptor to a standard fi le descriptor, and vice versa. This means that you can \nredirect the original location of \nSTDOUT to an alternative fi le descriptor and then redirect \nthat fi le descriptor back to \nSTDOUT. This might sound somewhat complicated, but in prac-\ntice it’s fairly straightforward. This example will clear things up for you:\n $ cat test14\n #!/bin/bash\n # storing STDOUT, then coming back to it\n \n exec 3>&1\n exec 1>test14out\n \n echo \"This should store in the output file\"\n echo \"along with this line.\"\n \n exec 1>&3\n \n echo \"Now things should be back to normal\"\n $\n $ ./test14\n Now things should be back to normal\n $ cat test14out\n This should store in the output file\n along with this line.\n $\n\n405\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  405\nThis example is a little crazy so let’s walk through it piece by piece. First, the script redi-\nrects fi le descriptor 3 to the current location of fi le descriptor 1, which is \nSTDOUT. This \nmeans that any output sent to fi le descriptor 3 goes to the monitor.\nThe second \nexec command redirects STDOUT to a fi le. The shell now redirects any output \nsent to \nSTDOUT directly to the output fi le. However, fi le descriptor 3 still points to the \noriginal location of \nSTDOUT, which is the monitor. If you send output data to fi le descriptor \n3 at this point, it still goes to the monitor, even though \nSTDOUT is redirected.\nAfter sending some output to \nSTDOUT, which points to a fi le, the script then redirects \nSTDOUT to the current location of fi le descriptor 3, which is still set to the monitor. This \nmeans that now \nSTDOUT points to its original location, the monitor.\nThis method can get confusing, but it’s a common way to temporarily redirect output in \nscript fi les and then set the output back to the normal settings.\nCreating input fi le descriptors\nYou can redirect input fi le descriptors exactly the same way as output fi le descriptors. Save \nthe \nSTDIN fi le descriptor location to another fi le descriptor before redirecting it to a fi le; \nwhen you’re fi nished reading the fi le, you can restore \nSTDIN to its original location:\n $ cat test15\n #!/bin/bash\n # redirecting input file descriptors\n \n exec 6<&0\n \n exec 0< testfile\n \n count=1\n while read line\n do\n    echo \"Line #$count: $line\"\n    count=$[ $count + 1 ]\n done\n exec 0<&6\n read -p \"Are you done now? \" answer\n case $answer in\n Y|y) echo \"Goodbye\";;\n N|n) echo \"Sorry, this is the end.\";;\n esac\n $ ./test15\n Line #1: This is the first line.\n Line #2: This is the second line.\n Line #3: This is the third line.\n Are you done now? y\n Goodbye\n $\n\n406\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  406\nIn this example, fi le descriptor 6 is used to hold the location for STDIN. The script then \nredirects \nSTDIN to a fi le. All the input for the read command comes from the redirected \nSTDIN, which is now the input fi le.\nWhen all the lines have been read, the script returns \nSTDIN to its original location by redi-\nrecting it to fi le descriptor 6. The script tests to make sure that \nSTDIN is back to normal by \nusing another read command, which this time waits for input from the keyboard.\nCreating a read/write fi le descriptor\nAs odd as it may seem, you can also open a single fi le descriptor for both input and output. \nYou can then use the same fi le descriptor to both read data from a fi le and write data to the \nsame fi le.\nYou need to be especially careful with this method, however. As you read and write data to \nand from a fi le, the shell maintains an internal pointer, indicating where it is in the fi le. \nAny reading or writing occurs where the fi le pointer last left off. This can produce some \ninteresting results if you’re not careful. Look at this example:\n $ cat test16\n #!/bin/bash\n # testing input/output file descriptor\n \n exec 3<> testfile\n read line <&3\n echo \"Read: $line\"\n echo \"This is a test line\" >&3\n $ cat testfile\n This is the first line.\n This is the second line.\n This is the third line.\n $ ./test16\n Read: This is the first line.\n $ cat testfile\n This is the first line.\n This is a test line\n ine.\n This is the third line.\n $\nThis example uses the exec command to assign fi le descriptor 3 for both input and output \nsent to and from the fi le \ntestfile. Next, it uses the read command to read the fi rst line \nin the fi le, using the assigned fi le descriptor, and then it displays the read line of data in \nSTDOUT. After that, it uses the echo statement to write a line of data to the fi le opened \nwith the same fi le descriptor.\n\n407\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  407\nWhen you run the script, at fi rst things look just fi ne. The output shows that the script \nread the fi rst line in the \ntestfile fi le. However, if you display the contents of the \ntestfile fi le after running the script, you see that the data written to the fi le overwrote \nthe existing data.\nWhen the script writes data to the fi le, it starts where the fi le pointer is located. The \nread \ncommand reads the fi rst line of data, so it left the fi le pointer pointing to the fi rst charac-\nter in the second line of data. When the \necho statement outputs data to the fi le, it places \nthe data at the current location of the fi le pointer, overwriting whatever data was there.\nClosing fi le descriptors\nIf you create new input or output fi le descriptors, the shell automatically closes them when \nthe script exits. There are situations, however, when you need to manually close a fi le \ndescriptor before the end of the script.\nTo close a fi le descriptor, redirect it to the special symbol \n&-. This is how this looks in the \nscript:\n exec 3>&-\nThis statement closes fi le descriptor 3, preventing it from being used any more in the \nscript. Here’s an example of what happens when you try to use a closed fi le descriptor:\n $ cat badtest\n #!/bin/bash\n # testing closing file descriptors\n \n exec 3> test17file\n \n echo \"This is a test line of data\" >&3\n \n exec 3>&-\n \n echo \"This won't work\" >&3\n $ ./badtest\n ./badtest: 3: Bad file descriptor\n $\nAfter you close the fi le descriptor, you can’t write any data to it in your script or the shell \nproduces an error message.\nThere’s yet another thing to be careful of when closing fi le descriptors. If you open the \nsame output fi le later on in your script, the shell replaces the existing fi le with a new fi le. \nThis means that if you output any data, it overwrites the existing fi le. Consider the follow-\ning example of this problem:\n\n408\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  408\n $ cat test17\n #!/bin/bash\n # testing closing file descriptors\n \n exec 3> test17file\n echo \"This is a test line of data\" >&3\n exec 3>&-\n \n cat test17file\n \n exec 3> test17file\n echo \"This'll be bad\" >&3\n $ ./test17\n This is a test line of data\n $ cat test17file\n This'll be bad\n $\nAfter sending a data string to the test17file fi le and closing the fi le descriptor, the \nscript uses the \ncat command to display the contents of the fi le. So far, so good. Next, the \nscript reopens the output fi le and sends another data string to it. When you display the \ncontents of the output fi le, all you see is the second data string. The shell overwrote the \noriginal output fi le.\nListing Open File Descriptors\nWith only nine fi le descriptors available to you, you’d think that it wouldn’t be hard to \nkeep things straight. Sometimes, however, it’s easy to get lost when trying to keep track of \nwhich fi le descriptor is redirected where. To help you keep your sanity, the bash shell pro-\nvides the \nlsof command.\nThe \nlsof command lists all the open fi le descriptors on the entire Linux system. This is \nsomewhat of a controversial feature, because it can provide information about the Linux \nsystem to non-system-administrators. That’s why many Linux systems hide this command \nso users don’t accidentally stumble across it.\nOn many Linux systems (such as Fedora) the \nlsof command is located in the /usr/sbin \ndirectory. To run it with a normal user account, I have to reference it by its full pathname:\n $ /usr/sbin/lsof\nThis produces an amazing amount of output. It displays information about every fi le cur-\nrently open on the Linux system. This includes all the processes running on background, as \nwell as any user accounts logged in to the system.\n\n409\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  409\nPlenty of command line parameters and options are available to help fi lter out the lsof \noutput. The most commonly used are \n-p, which allows you to specify a process ID (PID), \nand \n-d, which allows you to specify the fi le descriptor numbers to display.\nTo easily determine the current PID of the process, you can use the special environment \nvariable \n$$, which the shell sets to the current PID. The -a option is used to perform a \nBoolean \nAND of the results of the other two options, to produce the following:\n $ /usr/sbin/lsof -a -p $$ -d 0,1,2\n COMMAND  PID USER   FD   TYPE DEVICE SIZE NODE NAME\n bash    3344 rich    0u   CHR  136,0         2 /dev/pts/0\n bash    3344 rich    1u   CHR  136,0         2 /dev/pts/0\n bash    3344 rich    2u   CHR  136,0         2 /dev/pts/0\n $\nThis shows the default fi le descriptors (0, 1, and 2) for the current process (the bash shell). \nThe default output of \nlsof contains several columns of information, described in Table 15-2.\nTABLE 15 -2    Default  lsof  Output\nColumnDescription\nCOMMANDThe fi rst nine characters of the name of the command in the process\nPIDThe process ID of the process\nUSERThe login name of the user who owns the process\nFDThe fi le descriptor number and access type [r—(read), w—(write), u—(read/\nwrite)]\nTYPEThe type of fi le [CHR—(character), BLK— (block), DIR— (directory), REG—\n(regular fi le)]\nDEVICEThe device numbers (major and minor) of the device\nSIZEIf available, the size of the fi le\nNODEThe node number of the local fi le\nNAMEThe name of the fi le\nThe fi le type associated with STDIN, STDOUT, and STDERR is character mode. Because the \nSTDIN, STDOUT, and STDERR fi le descriptors all point to the terminal, the name of the \noutput fi le is the device name of the terminal. All three standard fi les are available for both \nreading and writing (although it does seem odd to be able to write to \nSTDIN and read from \nSTDOUT).\nNow, let’s look at the results of the \nlsof command from inside a script that’s opened a \ncouple of alternative fi le descriptors:\n\n410\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  410\n $ cat test18\n #!/bin/bash\n # testing lsof with file descriptors\n \n exec 3> test18file1\n exec 6> test18file2\n exec 7< testfile\n \n /usr/sbin/lsof -a -p $$ -d0,1,2,3,6,7\n $ ./test18\n COMMAND  PID USER   FD   TYPE DEVICE SIZE   NODE NAME\n test18  3594 rich    0u   CHR  136,0           2 /dev/pts/0\n test18  3594 rich    1u   CHR  136,0           2 /dev/pts/0\n test18  3594 rich    2u   CHR  136,0           2 /dev/pts/0\n 18  3594 rich    3w   REG  253,0    0 360712 /home/rich/test18file1\n 18  3594 rich    6w   REG  253,0    0 360715 /home/rich/test18file2\n 18  3594 rich    7r   REG  253,0   73 360717 /home/rich/testfile\n $\nThe script creates three alternative fi le descriptors, two for output (3 and 6) and one for \ninput (7). When the script runs the \nlsof command, you can see the new fi le descriptors \nin the output. We truncated the fi rst part of the output so you could see the results of the \nfi lename. The fi lename shows the complete pathname for the fi les used in the fi le descrip-\ntors. It shows each of the fi les as type \nREG, which indicates that they are regular fi les on \nthe fi lesystem.\nSuppressing Command Output\nSometimes, you may not want to display any output from your script. This often occurs \nif you’re running a script as a background process (see Chapter 16). If any error messages \noccur from the script while it’s running in the background, the shell e-mails them to the \nowner of the process. This can get tedious, especially if you run scripts that generate minor \nnuisance errors.\nTo solve that problem, you can redirect \nSTDERR to a special fi le called the null file. The null \nfi le is pretty much what it says it is — a fi le that contains nothing. Any data that the shell \noutputs to the null fi le is not saved, thus the data are lost.\nThe standard location for the null fi le on Linux systems is \n/dev/null. Any data you redi-\nrect to that location is thrown away and doesn’t appear:\n $ ls -al > /dev/null\n $ cat /dev/null\n $\nThis is a common way to suppress any error messages without actually saving them:\n\n411\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  411\n $ ls -al badfile test16 2> /dev/null\n -rwxr--r--    1 rich     rich          135 Oct 29 19:57 test16*\n $\nYou can also use the /dev/null fi le for input redirection as an input fi le. Because the /\ndev/null\n fi le contains nothing, it is often used by programmers to quickly remove data \nfrom an existing fi le without having to remove the fi le and re-create it:\n $ cat testfile\n This is the first line.\n This is the second line.\n This is the third line.\n $ cat /dev/null > testfile\n $ cat testfile\n $\nThe fi le testfile still exists on the system, but now it is empty. This is a common method \nused to clear out log fi les that must remain in place for applications to operate.\nUsing Temporary Files\nThe Linux system contains a special directory location reserved for temporary fi les. Linux \nuses the \n/tmp directory for fi les that don’t need to be kept indefi nitely. Most Linux distribu-\ntions confi gure the system to automatically remove any fi les in the \n/tmp directory at bootup.\nAny user account on the system has privileges to read and write fi les in the \n/tmp directory. \nThis feature provides an easy way for you to create temporary fi les that you don’t necessar-\nily have to worry about cleaning up.\nThere’s even a specifi c command to use for creating a temporary fi le. The \nmktemp command \nallows you to easily create a unique temporary fi le in the \n/tmp folder. The shell creates the \nfi le but doesn’t use your default umask value (see Chapter 7). Instead, it only assigns read \nand write permissions to the fi le’s owner and makes you the owner of the fi le. After you \ncreate the fi le, you have full access to read and write to and from it from your script, but no \none else can access it (other than the root user, of course).\nCreating a local temporary fi le\nBy default, mktemp creates a fi le in the local directory. To create a temporary fi le in a local \ndirectory with the \nmktemp command, you just need to specify a fi lename template. The \ntemplate consists of any text fi lename, plus six X’s appended to the end of the fi lename:\n $ mktemp testing.XXXXXX\n $ ls -al testing*\n -rw-------   1 rich     rich      0 Oct 17 21:30 testing.UfIi13\n $\n\n412\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  412\nThe mktemp command replaces the six X’s with a six-character code to ensure the fi lename \nis unique in the directory. You can create multiple temporary fi les and be assured that each \none is unique:\n $ mktemp testing.XXXXXX\n testing.1DRLuV\n $ mktemp testing.XXXXXX\n testing.lVBtkW\n $ mktemp testing.XXXXXX\n testing.PgqNKG\n $ ls -l testing*\n -rw-------    1 rich     rich     0 Oct 17 21:57 testing.1DRLuV\n -rw-------    1 rich     rich     0 Oct 17 21:57 testing.PgqNKG\n -rw-------    1 rich     rich     0 Oct 17 21:30 testing.UfIi13\n -rw-------    1 rich     rich     0 Oct 17 21:57 testing.lVBtkW\n $\nAs you can see, the output of the mktemp command is the name of the fi le that it creates. \nWhen you use the \nmktemp command in a script, you’ll want to save that fi lename in a vari-\nable, so you can refer to it later on in the script:\n $ cat test19\n #!/bin/bash\n # creating and using a temp file\n \n tempfile=$(mktemp test19.XXXXXX)\n \n exec 3>$tempfile\n \n echo \"This script writes to temp file $tempfile\"\n \n echo \"This is the first line\" >&3\n echo \"This is the second line.\" >&3\n echo \"This is the last line.\" >&3\n exec 3>&-\n \n echo \"Done creating temp file. The contents are:\"\n cat $tempfile\n rm -f $tempfile 2> /dev/null\n $ ./test19\n This script writes to temp file test19.vCHoya\n Done creating temp file. The contents are:\n This is the first line\n This is the second line.\n This is the last line.\n $ ls -al test19*\n -rwxr--r--    1 rich     rich          356 Oct 29 22:03 test19*\n $\n\n413\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  413\nThe script uses the mktemp command to create a temporary fi le and assigns the fi lename to \nthe \n$tempfile variable. It then uses the temporary fi le as the output redirection fi le for \nfi le descriptor 3. After displaying the temporary fi lename on \nSTDOUT, it writes a few lines \nto the temporary fi le, and then it closes the fi le descriptor. Finally, it displays the contents \nof the temporary fi le and then uses the \nrm command to remove it.\nCreating a temporary fi le in /tmp\nThe -t option forces mktemp to create the fi le in the temporary directory of the system. \nWhen you use this feature, the \nmktemp command returns the full pathname used to create \nthe temporary fi le, not just the fi lename:\n $ mktemp -t test.XXXXXX\n /tmp/test.xG3374\n $ ls -al /tmp/test*\n -rw------- 1 rich rich 0 2014-10-29 18:41 /tmp/test.xG3374\n $\nBecause the mktemp command returns the full pathname, you can then reference the tem-\nporary fi le from any directory on the Linux system, no matter where it places the temporary \ndirectory:\n $ cat test20\n #!/bin/bash\n # creating a temp file in /tmp\n \n tempfile=$(mktemp -t tmp.XXXXXX)\n \n echo \"This is a test file.\" > $tempfile\n echo \"This is the second line of the test.\" >> $tempfile\n \n echo \"The temp file is located at: $tempfile\"\n cat $tempfile\n rm -f $tempfile\n $ ./test20\n The temp file is located at: /tmp/tmp.Ma3390\n This is a test file.\n This is the second line of the test.\n $\nWhen mktemp creates the temporary fi le, it returns the full pathname to the environment \nvariable. You can then use that value in any command to reference the temporary fi le.\nCreating a temporary directory\nThe -d option tells the mktemp command to create a temporary directory instead of a fi le. \nYou can then use that directory for whatever purposes you need, such as creating addi-\ntional temporary fi les:\n\n414\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  414\n $ cat test21\n #!/bin/bash\n # using a temporary directory\n \n tempdir=$(mktemp -d dir.XXXXXX)\n cd $tempdir\n tempfile1=$(mktemp temp.XXXXXX)\n tempfile2=$(mktemp temp.XXXXXX)\n exec 7> $tempfile1\n exec 8> $tempfile2\n \n echo \"Sending data to directory $tempdir\"\n echo \"This is a test line of data for $tempfile1\" >&7\n echo \"This is a test line of data for $tempfile2\" >&8\n $ ./test21\n Sending data to directory dir.ouT8S8\n $ ls -al\n total 72\n drwxr-xr-x    3 rich     rich         4096 Oct 17 22:20 ./\n drwxr-xr-x    9 rich     rich         4096 Oct 17 09:44 ../\n drwx------    2 rich     rich         4096 Oct 17 22:20 dir.ouT8S8/\n -rwxr--r--    1 rich     rich          338 Oct 17 22:20 test21*\n $ cd dir.ouT8S8\n [dir.ouT8S8]$ ls -al\n total 16\n drwx------    2 rich     rich         4096 Oct 17 22:20 ./\n drwxr-xr-x    3 rich     rich         4096 Oct 17 22:20 ../\n -rw-------    1 rich     rich           44 Oct 17 22:20 temp.N5F3O6\n -rw-------    1 rich     rich           44 Oct 17 22:20 temp.SQslb7\n [dir.ouT8S8]$ cat temp.N5F3O6\n This is a test line of data for temp.N5F3O6\n [dir.ouT8S8]$ cat temp.SQslb7\n This is a test line of data for temp.SQslb7\n [dir.ouT8S8]$\nThe script creates a directory in the current directory and uses the cd command to change \nto that directory before creating two temporary fi les. The two temporary fi les are then \nassigned to fi le descriptors and used to store output from the script.\nLogging Messages\nSometimes, it’s benefi cial to send output both to the monitor and to a fi le for logging. \nInstead of having to redirect output twice, you can use the special \ntee command.\nThe \ntee command is like a T-connector for pipes. It sends data from STDIN to two desti-\nnations at the same time. One destination is \nSTDOUT. The other destination is a fi lename \nspecifi ed on the \ntee command line:\n tee filename\n\n415\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  415\nBecause tee redirects data from STDIN, you can use it with the pipe command to redirect \noutput from any command:\n $ date | tee testfile\n Sun Oct 19 18:56:21 EDT 2014\n $ cat testfile\n Sun Oct 19 18:56:21 EDT 2014\n $\nThe output appears in STDOUT and is written to the fi le specifi ed. Be careful: By default, \nthe \ntee command overwrites the output fi le on each use:\n $ who | tee testfile\n rich     pts/0        2014-10-17 18:41 (192.168.1.2)\n $ cat testfile\n rich     pts/0        2014-10-17 18:41 (192.168.1.2)\n $\nIf you want to append data to the fi le, you must use the -a option:\n $ date | tee -a testfile\n Sun Oct 19 18:58:05 EDT 2014\n $ cat testfile\n rich     pts/0        2014-10-17 18:41 (192.168.1.2)\n Sun Oct 19 18:58:05 EDT 2014\n $\nUsing this technique, you can both save data in fi les and display the data on the monitor \nfor your users:\n $ cat test22\n #!/bin/bash\n # using the tee command for logging\n \n tempfile=test22file\n \n echo \"This is the start of the test\" | tee $tempfile\n echo \"This is the second line of the test\" | tee -a $tempfile\n echo \"This is the end of the test\" | tee -a $tempfile\n $ ./test22\n This is the start of the test\n This is the second line of the test\n This is the end of the test\n $ cat test22file\n This is the start of the test\n This is the second line of the test\n This is the end of the test\n $\nNow you can save a permanent copy of your output at the same time as you’re displaying it \nto your users.\n\n416\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  416\nPractical Example\nFile redirection is very common both when reading fi les into scripts and when outputting \ndata from a script into a fi le. This example script does both of those things. It reads a .csv-\nformatted data fi le and outputs \nSQL INSERT statements to insert the data into a database \n(see Chapter 25).\nThe shell script uses a command line parameter to defi ne the name of the \n.csv fi le from \nwhich to read the data. The \n.csv format is used to export data from spreadsheets, so you \ncan place the database data into a spreadsheet, save the spreadsheet in \n.csv format, read \nthe fi le, and create \nINSERT statements to insert the data into a MySQL database.\nHere’s what the script looks like:\n$cat test23\n#!/bin/bash\n# read file and create INSERT statements for MySQL\noutfile='members.sql'\nIFS=','\nwhile read lname fname address city state zip\ndo\n   cat >> $outfile << EOF\n   INSERT INTO members (lname,fname,address,city,state,zip) VALUES \n('$lname', '$fname', '$address', '$city', '$state', '$zip');\nEOF\ndone < ${1}\n$\nThat’s a pretty short script, thanks to the fi le redirection that goes on! There are three \nredirection operations happening in the script. The \nwhile loop uses the read statement \n(discussed in Chapter 14) to read text from the data fi le. Notice in the \ndone statement the \nredirection symbol:\ndone < ${1}\nThe $1 represents the fi rst command line parameter when you run the test23 program. \nThat specifi es the data fi le from which to read the data. The \nread statement parses the \ntext using the \nIFS character, which we specify as a comma.\nThe other two redirection operations in the script both appear in the same statement:\ncat >> $outfile << EOF\n\n417\nChapter 15: Presenting Data\n15\nc15.indd  12/08/2014  Page  417\nThis one statement has one output append redirection (the double greater-than symbol) and \none input append redirection (the double less-than symbol). The output redirection appends \nthe \ncat command output to the fi le specifi ed by the $outfile variable. The input to the \ncat command is redirected from the standard input to use the data stored inside the script. \nThe EOF symbol marks the start and end delimiter of the data that’s appended to the fi le:\nINSERT INTO members (lname,fname,address,city,state,zip) VALUES \n('$lname', '$fname', '$address', '$city', '$state', '$zip');\nThe text creates a standard SQL INSERT statement. Notice that the data values are \nreplaced with the variables for the data read from the \nread statement.\nSo basically the \nwhile loop reads on the data one line at a time, plugs those data values \ninto the \nINSERT statement template, then outputs the result to the output fi le.\nFor this experiment, I used this as the input data fi le:\n$ cat members.csv\nBlum,Richard,123 Main St.,Chicago,IL,60601\nBlum,Barbara,123 Main St.,Chicago,IL,60601\nBresnahan,Christine,456 Oak Ave.,Columbus,OH,43201\nBresnahan,Timothy,456 Oak Ave.,Columbus,OH,43201\n$\nWhen you run the script, nothing appears in the output on the monitor:\n$ ./test23 < members.csv\n$\nBut when you look at the members.sql output fi le, you should see the output data:\n$ cat members.sql\n   INSERT INTO members (lname,fname,address,city,state,zip) VALUES ('Blum',\n 'Richard', '123 Main St.', 'Chicago', 'IL', '60601');\n   INSERT INTO members (lname,fname,address,city,state,zip) VALUES ('Blum',\n 'Barbara', '123 Main St.', 'Chicago', 'IL', '60601');\n   INSERT INTO members (lname,fname,address,city,state,zip) VALUES ('Bresnahan',\n 'Christine', '456 Oak Ave.', 'Columbus', 'OH', '43201');\n   INSERT INTO members (lname,fname,address,city,state,zip) VALUES ('Bresnahan',\n 'Timothy', '456 Oak Ave.', 'Columbus', 'OH', '43201');\n$\nThe script worked exactly as expected! Now you can easily import the members.sql fi le \ninto a MySQL database table (see Chapter 25).\n\n418\nPart II: Shell Scripting Basics\nc15.indd  12/08/2014  Page  418\nSummary\nUnderstanding how the bash shell handles input and output can come in handy when creat-\ning your scripts. You can manipulate both how the script receives data and how it displays \ndata, to customize your script for any environment. You can redirect the input of a script \nfrom the standard input (\nSTDIN) to any fi le on the system. You can also redirect the output \nof the script from the standard output (\nSTDOUT) to any fi le on the system.\nBesides the \nSTDOUT, you can redirect any error messages your script generates by redirect-\ning the \nSTDERR output. This is accomplished by redirecting the fi le descriptor associated \nwith the \nSTDERR output, which is fi le descriptor 2. You can redirect STDERR output to the \nsame fi le as the \nSTDOUT output or to a completely separate fi le. This enables you to sepa-\nrate normal script messages from any error messages generated by the script.\nThe bash shell allows you to create your own fi le descriptors for use in your scripts. You can \ncreate fi le descriptors 3 through 8 and assign them to any output fi le you desire. After you \ncreate a fi le descriptor, you can redirect the output of any command to it, using the stan-\ndard redirection symbols.\nThe bash shell also allows you to redirect input to a fi le descriptor, providing an easy way \nto read data contained in a fi le into your script. You can use the \nlsof command to display \nthe active fi le descriptors in your shell.\nLinux systems provide a special fi le, called \n/dev/null, to allow you to redirect output that \nyou don’t want. The Linux system discards anything redirected to the \n/dev/null fi le. You \ncan also use this fi le to produce an empty fi le by redirecting the contents of the \n/dev/\nnull\n fi le to the fi le.\nThe \nmktemp command is a handy feature of the bash shell that allows you to easily create \ntemporary fi les and directories. Simply specify a template for the \nmktemp command, and it \ncreates a unique fi le each time you call it, based on the fi le template format. You can also \ncreate temporary fi les and directories in the \n/tmp directory on the Linux system, which is \na special location that isn’t preserved between system boots.\nThe \ntee  command is a handy way to send output both to the standard output and to a log \nfi le. This enables you to display messages from your script on the monitor and store them \nin a log fi le at the same time.\nIn Chapter 16, you’ll see how to control and run your scripts. Linux provides several dif-\nferent methods for running scripts other than directly from the command line interface \nprompt. You’ll see how to schedule your scripts to run at a specifi c time, as well as learn \nhow to pause them while they’re running. \n\n419\nc16.indd  12/16/2014  Page  419\nCHAPTER \n16\nScript Control\nIN THIS CHAPTER\nHandling signals\nRunning scripts in the background\nForbidding hang-ups\nControlling a Job\nModifying script priority\nAutomating script execution\nA\ns you start building advanced scripts, you’ll probably wonder how to run and control them \non your Linux system. So far in this book, the only way we’ve run scripts is directly from the \ncommand line interface in real-time mode. This isn’t the only way to run scripts in Linux. \nQuite a few options are available for running your shell scripts. There are also options for control-\nling your scripts. Various control methods include sending signals to your script, modifying a \nscript’s priority, and switching the run mode while a script is running. This chapter examines the \ndifferent ways you can control your shell scripts.\nHandling Signals\nLinux uses signals to communicate with processes running on the system. Chapter 4 described the \ndifferent Linux signals and how the Linux system uses these signals to stop, start, and kill pro-\ncesses. You can control the operation of your shell script by programming the script to perform cer-\ntain commands when it receives specifi c signals.\nSignaling the bash shell\nThere are more than 30 Linux signals that can be generated by the system and applications. Table 16-1 \nlists the most common Linux system signals that you’ll run across in your shell script writing.\n\n420\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  420\nTABLE 16 -1    Linux  Signals\nSignalValueDescription\n 1            SIGHUP\nHangs up the process\n 2            SIGINT\nInterrupts the process\n 3            SIGQUIT\nStops the process\n 9            SIGKILL\nUnconditionally terminates the process\n15            SIGTERM\nTerminates the process if possible\n17            SIGSTOP\nUnconditionally stops, but doesn’t terminate, the process\n18            SIGTSTP\nStops or pauses the process, but doesn’t terminate\n19            SIGCONT\nContinues a stopped process\nBy default, the bash shell ignores any SIGQUIT (3) and SIGTERM (15) signals it receives \n(so an interactive shell cannot be accidentally terminated). However, the bash shell does \nnot ignore any \nSIGHUP (1) and SIGINT (2) signals it receives.\nIf the bash shell receives a \nSIGHUP signal, such as when you leave an interactive shell, it \nexits. Before it exits, however, it passes the \nSIGHUP signal to any processes started by the \nshell, including any running shell scripts.\nWith a \nSIGINT signal, the shell is just interrupted. The Linux kernel stops giving the shell \nprocessing time on the CPU. When this happens, the shell passes the \nSIGINT signal to any \nprocesses started by the shell to notify them of the situation.\nAs you probably have noticed, the shell passes these signals on to your shell script program \nfor processing. However, a shell script’s default behavior does not govern these signals, \nwhich may have an adverse effect on the script’s operation. To avoid this situation, you can \nprogram your script to recognize signals and perform commands to prepare the script for \nthe consequences of the signal.\nGenerating signals\nThe bash shell allows you to generate two basic Linux signals using key combinations on \nthe keyboard. This feature comes in handy if you need to stop or pause a runaway script.\nInterrupting a process\nThe Ctrl+C key combination generates a SIGINT signal and sends it to any processes cur-\nrently running in the shell. You can test this by running a command that normally takes a \nlong time to fi nish and pressing the Ctrl+C key combination: \n$ sleep 100\n^C\n$\n\n421\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  421\n16\n16\nThe Ctrl+C key combination sends a SIGINT signal, which simply stops the current process \nrunning in the shell. The \nsleep command pauses the shell’s operation for the specifi ed \nnumber of seconds and returns the shell prompt. By pressing the Ctrl+C key combination \nbefore the time passed, the \nsleep command terminated prematurely.\nPausing a process\nInstead of terminating a process, you can pause it in the middle of whatever it’s doing. \nSometimes, this can be a dangerous thing (for example, if a script has a fi le lock open on \na crucial system fi le), but often it allows you to peek inside what a script is doing without \nactually terminating the process.\nThe Ctrl+Z key combination generates a \nSIGTSTP signal, stopping any processes running in \nthe shell. Stopping a process is different than terminating the process. Stopping the process \nleaves the program in memory and able to continue running from where it left off. In the \n“Controlling the Job” section later in this chapter, you learn how to restart a process that’s \nbeen stopped.\nWhen you use the Ctrl+Z key combination, the shell informs you that the process has been \nstopped: \n$ sleep 100\n^Z\n[1]+  Stopped                 sleep 100\n$\nThe number in the square brackets is the job number assigned by the shell. The shell refers \nto each process running in the shell as a job and assigns each job a unique job number \nwithin the current shell. It assigns the fi rst started process job number 1, the second job \nnumber 2, and so on.\nIf you have a stopped job assigned to your shell session, bash warns you if you try to exit \nthe shell:\n$ sleep 100\n^Z\n[1]+  Stopped                 sleep 100\n$ exit\nexit\nThere are stopped jobs.\n$\nYou can view the stopped jobs using the ps command: \n$ sleep 100\n^Z\n[1]+  Stopped                 sleep 100\n$\n$ ps -l\n\n422\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  422\nF S UID   PID  PPID  C PRI NI ADDR SZ WCHAN  TTY       TIME CMD\n0 S 501  2431  2430  0  80  0 - 27118 wait   pts/0 00:00:00 bash\n0 T 501  2456  2431  0  80  0 - 25227 signal pts/0 00:00:00 sleep\n0 R 501  2458  2431  0  80  0 - 27034 -      pts/0 00:00:00 ps\n$\nIn the S column (process state), the ps command shows the stopped job’s state as T. This \nindicates the command is either being traced or is stopped.\nIf you really want to exit the shell with a stopped job still active, just type the \nexit com-\nmand again. The shell exits, terminating the stopped job. Alternately, now that you know \nthe PID of the stopped job, you can use the \nkill command to send a SIGKILL signal to \nterminate it:\n$ kill -9 2456\n$\n[1]+  Killed                  sleep 100\n$\nWhen you kill the job, initially you don’t get any response. However, the next time you do \nsomething that produces a shell prompt (such as pressing the Enter key), you’ll see a message \nindicating that the job was killed. Each time the shell produces a prompt, it also displays the \nstatus of any jobs that have changed states in the shell. After you kill a job, the next time \nyou force the shell to produce a prompt, it displays a message showing that the job was killed \nwhile running.\nTrapping signals\nInstead of allowing your script to leave signals ungoverned, you can trap them when they \nappear and perform other commands. The \ntrap command allows you to specify which \nLinux signals your shell script can watch for and intercept from the shell. If the script \nreceives a signal listed in the \ntrap command, it prevents it from being processed by the \nshell and instead handles it locally.\nThe format of the \ntrap command is:\ntrap commands signals\nOn the trap command line, you just list the commands you want the shell to execute, \nalong with a space-separated list of signals you want to trap. You can specify the signals \neither by their numeric value or by their Linux signal name.\nHere’s a simple example of using the \ntrap command to capture the SIGINT signal and gov-\nern the script’s behavior when the signal is sent:\n$ cat test1.sh\n#!/bin/bash\n# Testing signal trapping\n#\n\n423\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  423\n16\ntrap \"echo ' Sorry! I have trapped Ctrl-C'\" SIGINT\n#\necho This is a test script\n#\ncount=1\nwhile [ $count -le 10 ]\ndo\n   echo \"Loop #$count\"\n   sleep 1\n   count=$[ $count + 1 ]\ndone\n#\necho \"This is the end of the test script\"\n#\nThe trap command used in this example displays a simple text message each time it \ndetects the \nSIGINT signal. Trapping this signal makes this script impervious to the user \nattempting to stop the program by using the bash shell keyboard Ctrl+C command:\n$ ./test1.sh\nThis is a test script\nLoop #1\nLoop #2\nLoop #3\nLoop #4\nLoop #5\n^C Sorry! I have trapped Ctrl-C\nLoop #6\nLoop #7\nLoop #8\n^C Sorry! I have trapped Ctrl-C\nLoop #9\nLoop #10\nThis is the end of the test script\n$\nEach time the Ctrl+C key combination was used, the script executed the echo statement \nspecifi ed in the \ntrap command instead of not managing the signal and allowing the shell \nto stop the script.\nTrapping a script exit\nBesides trapping signals in your shell script, you can trap them when the shell script exits. \nThis is a convenient way to perform commands just as the shell fi nishes its job.\nTo trap the shell script exiting, just add the \nEXIT signal to the trap command:\n$ cat test2.sh\n#!/bin/bash\n\n424\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  424\n# Trapping the script exit\n#\ntrap \"echo Goodbye...\" EXIT\n#\ncount=1\nwhile [ $count -le 5 ]\ndo\n   echo \"Loop #$count\"\n   sleep 1\n   count=$[ $count + 1 ]\ndone\n#\n$\n$ ./test2.sh\nLoop #1\nLoop #2\nLoop #3\nLoop #4\nLoop #5\nGoodbye...\n$\nWhen the script gets to the normal exit point, the trap is triggered, and the shell executes \nthe command you specify on the \ntrap command line. The EXIT trap also works if you pre-\nmaturely exit the script:\n$ ./test2.sh\nLoop #1\nLoop #2\nLoop #3\n^CGoodbye...\n$\nBecause the SIGINT signal isn’t listed in the trap command list, when the Ctrl+C key \ncombination is used to send that signal, the script exits. However, before the script exits, \nbecause the \nEXIT is trapped, the shell executes the trap command.\nModifying or removing a trap\nTo handle traps differently in various sections of your shell script, you simply reissue the \ntrap command with new options:\n$ cat test3.sh\n#!/bin/bash\n# Modifying a set trap\n#\ntrap \"echo ' Sorry... Ctrl-C is trapped.'\" SIGINT\n\n425\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  425\n16\n#\ncount=1\nwhile [ $count -le 5 ]\ndo\n   echo \"Loop #$count\"\n   sleep 1\n   count=$[ $count + 1 ]\ndone\n#\ntrap \"echo ' I modified the trap!'\" SIGINT\n#\ncount=1\nwhile [ $count -le 5 ]\ndo\n   echo \"Second Loop #$count\"\n   sleep 1\n   count=$[ $count + 1 ]\ndone\n#\n$\nAfter the signal trap is modifi ed, the script manages the signal or signals differently. \nHowever, if a signal is received before the trap is modifi ed, the script processes it per the \noriginal \ntrap command:\n$ ./test3.sh\nLoop #1\nLoop #2\nLoop #3\n^C Sorry... Ctrl-C is trapped.\nLoop #4\nLoop #5\nSecond Loop #1\nSecond Loop #2\n^C I modified the trap!\nSecond Loop #3\nSecond Loop #4\nSecond Loop #5\n$\nYou can also remove a set trap. Simply add two dashes after the trap command and a list \nof the signals you want to return to default behavior:\n$ cat test3b.sh\n#!/bin/bash\n# Removing a set trap\n#\ntrap \"echo ' Sorry... Ctrl-C is trapped.'\" SIGINT\n#\n\n426\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  426\ncount=1\nwhile [ $count -le 5 ]\ndo\n   echo \"Loop #$count\"\n   sleep 1\n   count=$[ $count + 1 ]\ndone\n#\n# Remove the trap\ntrap -- SIGINT\necho \"I just removed the trap\"\n#\ncount=1\nwhile [ $count -le 5 ]\ndo\n   echo \"Second Loop #$count\"\n   sleep 1\n   count=$[ $count + 1 ]\ndone\n#\n$ ./test3b.sh\nLoop #1\nLoop #2\nLoop #3\nLoop #4\nLoop #5\nI just removed the trap\nSecond Loop #1\nSecond Loop #2\nSecond Loop #3\n^C\n$\nYou can use a single dash instead of a double dash after the trap command to return signals to their default behav-\nior. Both the single and double dash work properly.\nAfter the signal trap is removed, the script handles the SIGINT signal in its default man-\nner, terminating the script. However, if a signal is received before the trap is removed, the \nscript processes it per the original \ntrap command:\n$ ./test3b.sh\nLoop #1\nLoop #2\nLoop #3\n^C Sorry... Ctrl-C is trapped.\nLoop #4\n\n427\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  427\n16\nLoop #5\nI just removed the trap\nSecond Loop #1\nSecond Loop #2\n^C\n$\nIn this example, the fi rst Ctrl+C key combination was used to attempt to terminate the \nscript prematurely. Because the signal was received before the trap was removed, the script \nexecuted the command specifi ed in the trap. After the script executed the trap removal, \nthen Ctrl+C could prematurely terminate the script.\nRunning Scripts in Background Mode\nSometimes, running a shell script directly from the command line interface is inconve-\nnient. Some scripts can take a long time to process, and you may not want to tie up the \ncommand line interface waiting. While the script is running, you can’t do anything else in \nyour terminal session. Fortunately, there’s a simple solution to that problem.\nWhen you use the \nps command, you see a whole bunch of different processes running on \nthe Linux system. Obviously, all these processes are not running on your terminal moni-\ntor. This is called running processes in the background. In background mode, a process runs \nwithout being associated with a \nSTDIN, STDOUT, and STDERR on a terminal session (see \nChapter 15).\nYou can exploit this feature with your shell scripts as well, allowing them to run behind \nthe scenes and not lock up your terminal session. The following sections describe how to \nrun your scripts in background mode on your Linux system.\nRunning in the background\nRunning a shell script in background mode is a fairly easy thing to do. To run a shell script \nin background mode from the command line interface, just place an ampersand symbol (&) \nafter the command:\n$ cat test4.sh\n#!/bin/bash\n# Test running in the background\n#\ncount=1\nwhile [ $count -le 10 ]\ndo\n   sleep 1\n   count=$[ $count + 1 ]\ndone\n\n428\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  428\n#\n$\n$ ./test4.sh &\n[1] 3231\n$\nWhen you place the ampersand symbol after a command, it separates the command from \nthe bash shell and runs it as a separate background process on the system. The fi rst thing \nthat displays is the line:\n[1] 3231\nThe number in the square brackets is the job number assigned by the shell to the back-\nground process. The next number is the Process ID (PID) the Linux system assigns to the \nprocess. Every process running on the Linux system must have a unique PID.\nAs soon as the system displays these items, a new command line interface prompt appears. \nYou are returned to the shell, and the command you executed runs safely in background \nmode. At this point, you can enter new commands at the prompt.\nWhen the background process fi nishes, it displays a message on the terminal:\n[1]   Done                    ./test4.sh\nThis shows the job number and the status of the job (Done), along with the command used \nto start the job.\nBe aware that while the background process is running, it still uses your terminal monitor \nfor \nSTDOUT and STDERR messages:\n$ cat test5.sh\n#!/bin/bash\n# Test running in the background with output\n#\necho \"Start the test script\"\ncount=1\nwhile [ $count -le 5 ]\ndo\n   echo \"Loop #$count\"\n   sleep 5\n   count=$[ $count + 1 ]\ndone\n#\necho \"Test script is complete\"\n#\n$\n$ ./test5.sh &\n[1] 3275\n\n429\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  429\n16\n$ Start the test script\nLoop #1\nLoop #2\nLoop #3\nLoop #4\nLoop #5\nTest script is complete\n[1]   Done                    ./test5.sh\n$\nYou’ll notice from the example that the output from the test5.sh script displays. The \noutput intermixes with the shell prompt, which is why \nStart the test script appears \nnext to the \n$ prompt.\nYou can still issue commands while this output is occurring:\n$ ./test5.sh &\n[1] 3319\n$ Start the test script\nLoop #1\nLoop #2\nLoop #3\nls myprog*\nmyprog  myprog.c\n$ Loop #4\nLoop #5\nTest script is complete\n[1]+  Done                    ./test5.sh\n$$\nWhile the test5.sh script is running in the background, the command ls  myprog* \nwas entered. The script’s output, the typed command, and the command’s output all inter-\nmixed with each other’s output display. This can be confusing! It is a good idea to redirect \nSTDOUT and STDERR for scripts you will be running in the background (Chapter 15) to \navoid this messy output.\nRunning multiple background jobs\nYou can start any number of background jobs at the same time from the command line \nprompt:\n$ ./test6.sh &\n[1] 3568\n$ This is Test Script #1\n$ ./test7.sh &\n\n430\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  430\n[2] 3570\n$ This is Test Script #2\n$ ./test8.sh &\n[3] 3573\n$ And...another Test script\n$ ./test9.sh &\n[4] 3576\n$ Then...there was one more test script\n$\nEach time you start a new job, the Linux system assigns it a new job number and PID. You \ncan see that all the scripts are running using the \nps command:\n$ ps\n  PID TTY          TIME CMD\n 2431 pts/0    00:00:00 bash\n 3568 pts/0    00:00:00 test6.sh\n 3570 pts/0    00:00:00 test7.sh\n 3573 pts/0    00:00:00 test8.sh\n 3574 pts/0    00:00:00 sleep\n 3575 pts/0    00:00:00 sleep\n 3576 pts/0    00:00:00 test9.sh\n 3577 pts/0    00:00:00 sleep\n 3578 pts/0    00:00:00 sleep\n 3579 pts/0    00:00:00 ps\n$\nYou must be careful when using background processes from a terminal session. Notice in the \noutput from the \nps command that each of the background processes is tied to the terminal \nsession (pts/0) terminal. If the terminal session exits, the background process also exits.\nEarlier in this chapter we mentioned that when you attempt to exit a terminal session, a warning is issued if there \nare stopped processes. However, with background processes, only some terminal emulators remind you that a back-\nground job is running, before you attempt to exit the terminal session.\nIf you want your script to continue running in background mode after you have logged off \nthe console, there’s something else you need to do. The next section discusses that process.\nRunning Scripts without a Hang-Up\nSometimes, you may want to start a shell script from a terminal session and let the script \nrun in background mode until it fi nishes, even if you exit the terminal session. You can do \nthis by using the \nnohup command.\n\n431\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  431\n16\nThe nohup command runs another command blocking any SIGHUP signals that are sent to \nthe process. This prevents the process from exiting when you exit your terminal session.\nThe format used for the \nnohup command is as follows:\n$ nohup ./test1.sh &\n[1] 3856\n$ nohup: ignoring input and appending output to 'nohup.out'\n$\nAs with a normal background process, the shell assigns the command a job number, and the \nLinux system assigns a PID number. The difference is that when you use the \nnohup com-\nmand, the script ignores any \nSIGHUP signals sent by the terminal session if you close the \nsession.\nBecause the \nnohup command disassociates the process from the terminal, the process loses \nthe \nSTDOUT and STDERR output links. To accommodate any output generated by the com-\nmand, the \nnohup command automatically redirects STDOUT and STDERR messages to a fi le, \ncalled \nnohup.out.\nIf you run another command using nohup, the output is appended to the existing nohup.out fi le. Be careful when \nrunning multiple commands from the same directory, because all the output is sent to the same \nnohup.out fi le, \nwhich can get confusing.\nThe nohup.out fi le contains all the output that would normally be sent to the terminal \nmonitor. After the process fi nishes running, you can view the \nnohup.out fi le for the out-\nput results:\n$ cat nohup.out\nThis is a test script\nLoop 1\nLoop 2\nLoop 3\nLoop 4\nLoop 5\nLoop 6\nLoop 7\nLoop 8\nLoop 9\nLoop 10\nThis is the end of the test script\n$\nThe output appears in the nohup.out fi le just as if the process ran on the command line.\n\n432\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  432\nControlling the Job\nEarlier in this chapter, you saw how to use the Ctrl+C key combination to stop a job run-\nning in the shell. After you stop a job, the Linux system lets you either kill or restart it. \nYou can kill the process by using the \nkill command. Restarting a stopped process requires \nthat you send it a \nSIGCONT signal.\nThe function of starting, stopping, killing, and resuming jobs is called job control. With job \ncontrol, you have full control over how processes run in your shell environment. This sec-\ntion describes the commands used to view and control jobs running in your shell.\nViewing jobs\nThe key command for job control is the jobs command. The jobs command allows you to \nview the current jobs being handled by the shell:\n$ cat test10.sh\n#!/bin/bash\n# Test job control\n#\necho \"Script Process ID: $$\"\n#\ncount=1\nwhile [ $count -le 10 ]\ndo\n   echo \"Loop #$count\"\n   sleep 10\n   count=$[ $count + 1 ]\ndone\n#\necho \"End of script...\"\n#\n$ \nThe script uses the $$ variable to display the PID that the Linux system assigns to the \nscript; then it goes into a loop, sleeping for 10 seconds at a time for each iteration.\nYou can start the script from the command line interface and then stop it using the Ctrl+Z \nkey combination:\n$ ./test10.sh\nScript Process ID: 1897\nLoop #1\nLoop #2\n^Z\n[1]+  Stopped                 ./test10.sh\n$\n\n433\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  433\n16\nUsing the same script, another job is started as a background process, using the ampersand \nsymbol. To make life a little easier, the output of that script is redirected to a fi le so it \ndoesn’t appear on the screen:\n$ ./test10.sh > test10.out &\n[2] 1917\n$\nThe jobs command enables you to view the jobs assigned to the shell. The jobs command \nshows both the stopped and the running jobs, along with their job numbers and the com-\nmands used in the jobs:\n$ jobs\n[1]+  Stopped                 ./test10.sh\n[2]-  Running                 ./test10.sh > test10.out &\n$\nYou can view the various jobs’ PIDs by adding the -l parameter (lowercase L) on the jobs \ncommand:\n$ jobs -l\n[1]+  1897 Stopped                 ./test10.sh\n[2]-  1917 Running                 ./test10.sh > test10.out &\n$\nThe jobs command uses a few different command line parameters, as shown in Table 16-2.\nTABLE 16 -2    The jobs Command Parameters\nParameterDescription\n-l\nLists the PID of the process along with the job number\n-n\nLists only jobs that have changed their status since the last notifi cation from the \nshell\n-p\nLists only the PIDs of the jobs\n-r\nLists only the running jobs\n-s\nLists only stopped jobs\nYou probably noticed the plus and minus signs in the jobs command output. The job with \nthe plus sign is considered the default job. It would be the job referenced by any job control \ncommands if a job number wasn’t specifi ed in the command line.\nThe job with the minus sign is the job that would become the default job when the current \ndefault job fi nishes processing. There will be only one job with the plus sign and one job \nwith the minus sign at any time, no matter how many jobs are running in the shell.\n\n434\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  434\nThe following is an example showing how the next job in line takes over the default status, \nwhen the default job is removed. Three separate processes are started in the background. \nThe \njobs command listing shows the three processes, their PID, and their status. Note that \nthe default process (the one listed with the plus sign) is the last process started, job #3.\n$ ./test10.sh > test10a.out &\n[1] 1950\n$ ./test10.sh > test10b.out &\n[2] 1952\n$ ./test10.sh > test10c.out &\n[3] 1955\n$\n$ jobs -l\n[1]   1950 Running                 ./test10.sh > test10a.out &\n[2]-  1952 Running                 ./test10.sh > test10b.out &\n[3]+  1955 Running                 ./test10.sh > test10c.out &\n$\nUsing the kill command to send a SIGHUP signal to the default process causes the job to \nterminate. In the next \njobs listing, the job that previously had the minus sign now has \nthe plus sign and is the default job:\n$ kill 1955\n$\n[3]+  Terminated              ./test10.sh > test10c.out\n$\n$ jobs -l\n[1]-  1950 Running                 ./test10.sh > test10a.out &\n[2]+  1952 Running                 ./test10.sh > test10b.out &\n$\n$ kill 1952\n$\n[2]+  Terminated              ./test10.sh > test10b.out\n$\n$ jobs -l\n[1]+  1950 Running                 ./test10.sh > test10a.out &\n$\nAlthough changing a background job to the default process is interesting, it doesn’t seem \nvery useful. In the next section, you learn how to use commands to interact with the \ndefault process using no PID or job number.\nRestarting stopped jobs\nUnder bash job control, you can restart any stopped job as either a background process or a \nforeground process. A foreground process takes over control of the terminal you’re working \non, so be careful about using that feature.\n\n435\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  435\n16\nTo restart a job in background mode, use the bg command:\n$ ./test11.sh\n^Z\n[1]+  Stopped                 ./test11.sh\n$\n$ bg\n[1]+ ./test11.sh &\n$\n$ jobs\n[1]+  Running                 ./test11.sh &\n$ \nBecause the job was the default job, indicated by the plus sign, only the bg command was \nneeded to restart it in background mode. Notice that no PID is listed when the job is moved \ninto background mode.\nIf you have additional jobs, you need to use the job number along with the \nbg command:\n$ ./test11.sh\n^Z\n[1]+  Stopped                 ./test11.sh\n$\n$ ./test12.sh\n^Z\n[2]+  Stopped                 ./test12.sh\n$\n$ bg 2\n[2]+ ./test12.sh &\n$\n$ jobs\n[1]+  Stopped                 ./test11.sh\n[2]-  Running                 ./test12.sh &\n$\nThe command bg 2 was used to send the second job into background mode. Notice that \nwhen the \njobs command was used, it listed both jobs with their status, even though the \ndefault job is not currently in background mode.\nTo restart a job in foreground mode, use the \nfg command, along with the job number:\n$ fg 2\n./test12.sh\nThis is the script's end...\n$\nBecause the job is running in foreground mode, the command line interface prompt does \nnot appear until the job fi nishes.\n\n436\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  436\nBeing Nice\nIn a multitasking operating system (which Linux is), the kernel is responsible for assigning \nCPU time for each process running on the system. The scheduling priority is the amount of \nCPU time the kernel assigns to the process relative to the other processes. By default, all \nprocesses started from the shell have the same scheduling priority on the Linux system.\nThe scheduling priority is an integer value, from -20 (the highest priority) to +19 (the low-\nest priority). By default, the bash shell starts all processes with a scheduling priority of 0.\nIt’s confusing to remember that -20, the lowest value, is the highest priority and 19, the highest value, is the low-\nest priority. Just remember the phrase, “Nice guys fi nish last.” The “nicer” or higher you are in value, the lower your \nchance of getting the CPU.\nSometimes, you want to change the priority of a shell script, either lowering its priority so \nit doesn’t take as much processing power away from other processes or giving it a higher \npriority so it gets more processing time. You can do this by using the \nnice command.\nUsing the nice command\nThe nice command allows you to set the scheduling priority of a command as you start it. \nTo make a command run with less priority, just use the \n-n command line option for nice \nto specify a new priority level:\n$ nice -n 10 ./test4.sh > test4.out &\n[1] 4973\n$\n$ ps -p 4973 -o pid,ppid,ni,cmd\n  PID  PPID  NI CMD\n 4973  4721  10 /bin/bash ./test4.sh\n$\nNotice that you must use the nice command on the same line as the command you are \nstarting. The output from the \nps command confi rms that the nice value (column NI) has \nbeen set to \n10.\nThe \nnice command causes the script to run at a lower priority. However, if you try to \nincrease the priority of one of your commands, you might be in for a surprise:\n$ nice -n -10 ./test4.sh > test4.out &\n[1] 4985\n$ nice: cannot set niceness: Permission denied\n [1]+  Done                    nice -n -10 ./test4.sh > test4.out\n$\n\n437\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  437\n16\nThe nice command prevents normal system users from increasing the priority of their \ncommands. Notice that the job does run, even though the attempt to raise its priority with \nthe \nnice command failed.\nYou don’t have to use the \n-n option with the nice command. You can simply type the pri-\nority preceded by a dash:\n$ nice -10 ./test4.sh > test4.out &\n[1] 4993\n$\n$ ps -p 4993 -o pid,ppid,ni,cmd\n  PID  PPID  NI CMD\n 4993  4721  10 /bin/bash ./test4.sh\n$\nHowever, this can get confusing when the priority is a negative number, because you must \nhave a double-dash. It’s best just to use the \n-n option to avoid confusion.\nUsing the renice command\nSometimes, you’d like to change the priority of a command that’s already running on the \nsystem. That’s what the \nrenice command is for. It allows you to specify the PID of a run-\nning process to change its priority:\n$ ./test11.sh &\n[1] 5055\n$\n$ ps -p 5055 -o pid,ppid,ni,cmd\n  PID  PPID  NI CMD\n 5055  4721   0 /bin/bash ./test11.sh\n$\n$ renice -n 10 -p 5055\n5055: old priority 0, new priority 10\n$\n$ ps -p 5055 -o pid,ppid,ni,cmd\n  PID  PPID  NI CMD\n 5055  4721  10 /bin/bash ./test11.sh\n$\nThe renice command automatically updates the scheduling priority of the running \nprocess. As with the \nnice command, the renice command has some limitations:\n ■\nYou can only renice processes that you own.\n ■\nYou can only renice your processes to a lower priority.\n ■\nThe root user can renice any process to any priority.\nIf you want to fully control running processes, you must be logged in as the root account or \nuse the \nsudo command.\n\n438\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  438\nRunning Like Clockwork\nWhen you start working with scripts, you may want to run a script at a preset time, usu-\nally at a time when you’re not there. The Linux system provides a couple of ways to run a \nscript at a preselected time: the \nat command and the cron table. Each method uses a dif-\nferent technique for scheduling when and how often to run scripts. The following sections \ndescribe each of these methods.\nScheduling a job using the at command\nThe at command allows you to specify a time when the Linux system will run a script. The \nat command submits a job to a queue with directions on when the shell should run the \njob. The \nat daemon, atd, runs in the background and checks the job queue for jobs to run. \nMost Linux distributions start this daemon automatically at boot time.\nThe \natd daemon checks a special directory on the system (usually /var/spool/at) for \njobs submitted using the \nat command. By default, the atd daemon checks this directory \nevery 60 seconds. When a job is present, the \natd daemon checks the time the job is set to \nbe run. If the time matches the current time, the \natd daemon runs the job.\nThe following sections describe how to use the \nat command to submit jobs to run and how \nto manage these jobs.\nUnderstanding the at command format\nThe basic at command format is pretty simple: \nat [-f filename] time\nBy default, the at command submits input from STDIN to the queue. You can specify a fi le-\nname used to read commands (your script fi le) using the \n-f parameter.\nThe \ntime parameter specifi es when you want the Linux system to run the job. If you spec-\nify a time that has already passed, the \nat command runs the job at that time on the next \nday.\nYou can get pretty creative with how you specify the time. The \nat command recognizes \nlots of different time formats:\n ■\nA standard hour and minute, such as 10:15\n ■\nAn AM/PM indicator, such as 10:15PM\n ■\nA specifi c named time, such as now, noon, midnight, or teatime (4PM)\nIn addition to specifying the time to run the job, you can also include a specifi c date, using \na few different date formats:\n\n439\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  439\n16\n ■\nA standard date format, such as MMDDYY, MM/DD/YY, or DD.MM.YY\n ■\nA text date, such as Jul 4 or Dec 25, with or without the year\n ■\nA time increment:\n ■\nNow + 25 minutes\n ■\n10:15PM tomorrow\n ■\n10:15 + 7 days\nWhen you use the \nat command, the job is submitted into a job queue. The job queue holds \nthe jobs submitted by the \nat command for processing. There are 26 different job queues \navailable for different priority levels. Job queues are referenced using lowercase letters, a \nthrough z, and uppercase letters A through Z.\nA few years ago, the batch command was another method that allowed a script to be run at a later time. The \nbatch command was unique because you could schedule a script to run when the system was at a lower usage \nlevel. However, nowadays, the \nbatch command is just simply a script, /usr/bin/batch, that calls the at com-\nmand and submits your job to the \nb queue.\nThe higher alphabetically the job queue, the lower the priority (higher nice value) the job \nwill run under. By default, \nat jobs are submitted to the at job a queue. If you want to run \na job at a lower priority, you can specify a different queue letter using the \n-q parameter.\nRetrieving job output\nWhen the job runs on the Linux system, there’s no monitor associated with the job. \nInstead, the Linux system uses the e-mail address of the user who submitted the job as \nSTDOUT and STDERR. Any output destined to STDOUT or STDERR is mailed to the user via \nthe mail system.\nHere’s a simple example using the \nat command to schedule a job to run on a CentOS \ndistribution:\n$ cat test13.sh\n#!/bin/bash\n# Test using at command\n#\necho \"This script ran at $(date +%B%d,%T)\"\necho\nsleep 5\necho \"This is the script's end...\"\n#\n\n440\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  440\n$ at -f test13.sh now\njob 7 at 2015-07-14 12:38\n$\nThe at command displays the job number assigned to the job along with the time the job is \nscheduled to run. The \n-f option tells what script fi le to use and the now time designation \ndirects \nat to run the script immediately.\nUsing e-mail for the \nat command’s output is inconvenient at best. The at command sends \ne-mail via the sendmail application. If your system does not use sendmail, you won’t get \nany output! Therefore, it’s best to redirect \nSTDOUT and STDERR in your scripts (see Chapter \n15) when using the \nat command, as the following example shows:\n$ cat test13b.sh\n#!/bin/bash\n# Test using at command\n#\necho \"This script ran at $(date +%B%d,%T)\" > test13b.out\necho >> test13b.out\nsleep 5\necho \"This is the script's end...\" >> test13b.out\n#\n$\n$ at -M -f test13b.sh now\njob 8 at 2015-07-14 12:48\n$\n$ cat test13b.out\nThis script ran at July14,12:48:18\nThis is the script's end...\n$\nIf you don’t want to use e-mail or redirection with at, it is best to add the -M option to \nsuppress any output generated by jobs using the \nat command.\nListing pending jobs\nThe atq command allows you to view what jobs are pending on the system:\n$ at -M -f test13b.sh teatime\njob 17 at 2015-07-14 16:00\n$\n$ at -M -f test13b.sh tomorrow\njob 18 at 2015-07-15 13:03\n$\n$ at -M -f test13b.sh 13:30\njob 19 at 2015-07-14 13:30\n$\n$ at -M -f test13b.sh now\n\n441\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  441\n16\njob 20 at 2015-07-14 13:03\n$\n$ atq\n20      2015-07-14 13:03 = Christine\n18      2015-07-15 13:03 a Christine\n17      2015-07-14 16:00 a Christine\n19      2015-07-14 13:30 a Christine\n$\nThe job listing shows the job number, the date and time the system will run the job, and \nthe job queue the job is stored in.\nRemoving jobs\nAfter you know the information about what jobs are pending in the job queues, you can use \nthe \natrm command to remove a pending job:\n$ atq\n18      2015-07-15 13:03 a Christine\n17      2015-07-14 16:00 a Christine\n19      2015-07-14 13:30 a Christine\n$\n$ atrm 18\n$\n$ atq\n17      2015-07-14 16:00 a Christine\n19      2015-07-14 13:30 a Christine\n$\nJust specify the job number you want to remove. You can only remove jobs that you submit \nfor execution. You can’t remove jobs submitted by others.\nScheduling regular scripts\nUsing the at command to schedule a script to run at a preset time is great, but what if you \nneed that script to run at the same time every day or once a week or once a month? Instead \nof having to continually submit \nat jobs, you can use another feature of the Linux system.\nThe Linux system uses the \ncron program to allow you to schedule jobs that need to run on \na regular basis. The \ncron program runs in the background and checks special tables, called \ncron tables, for jobs that are scheduled to run.\nLooking at the cron table\nThe cron table uses a special format for allowing you to specify when a job should be run. \nThe format for the \ncron table is:\nmin hour dayofmonth month dayofweek command\n\n442\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  442\nThe cron table allows you to specify entries as specifi c values, ranges of values (such as \n1–5), or as a wildcard character (the asterisk). For example, if you want to run a command \nat 10:15 on every day, you would use this \ncron table entry:\n15 10 * * * command\nThe wildcard character used in the dayofmonth, month, and dayofweek fi elds indicates \nthat \ncron will execute the command every day of every month at 10:15. To specify a com-\nmand to run at 4:15 PM every Monday, you would use the following:\n15 16 * * 1 command\nYou can specify the dayofweek entry as either a three-character text value (mon, tue, \nwed, thu, fri, sat, sun) or as a numeric value, with 0 being Sunday and 6 being Saturday.\nHere’s another example: to execute a command at 12 noon on the fi rst day of every month, \nyou would use the following format:\n00 12 1 * * command\nThe dayofmonth entry specifi es a date value (1–31) for the month.\nThe astute reader might be wondering just how you would be able to set a command to execute on the last day of \nevery month because you can’t set the \ndayofmonth value to cover every month. This problem has plagued Linux \nand Unix programmers, and has spawned quite a few different solutions. A common method is to add an \nif-then \nstatement that uses the date command to check if tomorrow’s date is 01:\n00 12 * * * if [`date +%d -d tomorrow` = 01 ] ;  then ; command\nThis checks every day at 12 noon to see if it’s the last day of the month, and if so, cron runs the command.\nThe command list must specify the full command pathname or shell script to run. You \ncan add any command line parameters or redirection symbols you like, as a regular \ncommand line:\n15 10 * * * /home/rich/test4.sh > test4out\nThe cron program runs the script using the user account that submitted the job. Thus, you \nmust have the proper permissions to access the command and output fi les specifi ed in the \ncommand listing.\nBuilding the cron table\nEach system user can have their own cron table (including the root user) for running \nscheduled jobs. Linux provides the \ncrontab command for handling the cron table. To list \nan existing \ncron table, use the -l parameter:\n\n443\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  443\n16\n$ crontab -l\nno crontab for rich\n$\nBy default, each user’s cron table fi le doesn’t exist. To add entries to your cron table, \nuse the \n-e parameter. When you do that, the crontab command starts a text editor (see \nChapter 10) with the existing \ncron table (or an empty fi le if it doesn’t yet exist).\nViewing cron directories\nWhen you create a script that has less precise execution time needs, it is easier to use one \nof the pre-confi gured \ncron script directories. There are four basic directories: hourly, daily, \nmonthly, and weekly.\n$ ls /etc/cron.*ly\n/etc/cron.daily:\ncups       makewhatis.cron  prelink         tmpwatch\nlogrotate  mlocate.cron     readahead.cron\n/etc/cron.hourly:\n0anacron\n/etc/cron.monthly:\nreadahead-monthly.cron\n/etc/cron.weekly:\n$\nThus, if you have a script that needs to be run one time per day, just copy the script to the \ndaily directory and \ncron executes it each day.\nLooking at the anacron program\nThe only problem with the cron program is that it assumes that your Linux system is oper-\national 24 hours a day, 7 days a week. Unless you’re running Linux in a server environment, \nthis may not necessarily be true.\nIf the Linux system is turned off at the time a job is scheduled to run in the \ncron table, \nthe job doesn’t run. The \ncron program doesn’t retroactively run missed jobs when the \nsystem is turned back on. To resolve this issue, many Linux distributions also include the \nanacron program.\nIf \nanacron determines that a job has missed a scheduled running, it runs the job as soon \nas possible. This means that if your Linux system is turned off for a few days, when it \nstarts back up, any jobs scheduled to run during the time it was off are automatically run.\nThis feature is often used for scripts that perform routine log maintenance. If the system \nis always off when the script should run, the log fi les would never get trimmed and could \n\n444\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  444\ngrow to undesirable sizes. With anacron, you’re guaranteed that the log fi les are trimmed \nat least each time the system is started.\nThe \nanacron program deals only with programs located in the cron directories, such as \n/etc/cron.monthly. It uses timestamps to determine if the jobs have been run at the \nproper scheduled interval. A timestamp fi le exists for each \ncron directory and is located in \n/var/spool/anacron:\n$ sudo cat /var/spool/anacron/cron.monthly\n20150626\n$\nThe anacron program has its own table (usually located at /etc/anacrontab) to check \nthe job directories:\n$ sudo cat /etc/anacrontab\n# /etc/anacrontab: configuration file for anacron\n# See anacron(8) and anacrontab(5) for details.\nSHELL=/bin/sh\nPATH=/sbin:/bin:/usr/sbin:/usr/bin\nMAILTO=root\n# the maximal random delay added to the base delay of the jobs\nRANDOM_DELAY=45\n# the jobs will be started during the following hours only\nSTART_HOURS_RANGE=3-22\n#period in days   delay in minutes   job-identifier   command\n1       5       cron.daily              nice run-parts /etc/cron.daily\n7       25      cron.weekly             nice run-parts /etc/cron.weekly\n@monthly 45     cron.monthly            nice run-parts /etc/cron.monthly\n$\nThe basic format of the anacron table is slightly different from that of the cron table:\nperiod delay identifier command\nThe period entry defi nes how often the jobs should be run, specifi ed in days. The anacron \nprogram uses this entry to check against the jobs’ timestamp fi le. The delay entry specifi es \nhow many minutes after the system starts the \nanacron program should run missed scripts. \nThe command entry contains the \nrun-parts program and a cron script directory name. \nThe \nrun-parts program is responsible for running any script in the directory passed to it.\nNotice that \nanacron does not run the scripts located in /etc/cron.hourly. This is \nbecause the \nanacron program does not deal with scripts that have execution time needs of \nless than daily.\n\n445\nChapter 16: Script Control\nc16.indd  12/16/2014  Page  445\n16\nThe identifi er entry is a unique non-blank character string — for example, cron-weekly. \nIt is used to uniquely identify the job in log messages and error e-mails.\nStarting scripts with a new shell\nThe ability to run a script every time a user starts a new bash shell (even just when a spe-\ncifi c user starts a bash shell) can come in handy. Sometimes, you want to set shell features \nfor a shell session or just ensure that a specifi c fi le has been set.\nRecall the startup fi les run when a user logs into the bash shell (covered in detail in \nChapter6). Also, remember that not every distribution has all the startup fi les. Essentially, \nthe fi rst fi le found in the following ordered list is run and the rest are ignored:\n ■\n$HOME/.bash_profile\n ■\n$HOME/.bash_login\n ■\n$HOME/.profile\nTherefore, you should place any scripts you want run at login time in the fi rst fi le listed. \nThe bash shell runs the \n.bashrc fi le any time a new shell is started. You can test this by \nadding a simple echo statement to the \n.bashrc fi le in your home directory and starting a \nnew shell:\n$ cat .bashrc\n# .bashrc\n# Source global definitions\nif [ -f /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n# User specific aliases and functions\necho \"I'm in a new shell!\"\n$\n$ bash\nI'm in a new shell!\n$\n$ exit\nexit\n$\nThe .bashrc fi le is also typically run from one of the bash startup fi les. Because the \n.bashrc fi le runs both when you log into the bash shell and when you start a bash shell, if \nyou need a script to run in both instances, place your shell script inside this fi le.\n\n446\nPart II: Shell Scripting Basics\nc16.indd  12/16/2014  Page  446\nSummary\nThe Linux system allows you to control your shell scripts by using signals. The bash shell \naccepts signals and passes them on to any process running under the shell process. Linux sig-\nnals allow you to easily kill a runaway process or temporarily pause a long-running process.\nYou can use the \ntrap statement in your scripts to catch signals and perform commands. This \nfeature provides a simple way to control whether a user can interrupt your script while it’s \nrunning.\nBy default, when you run a script in a terminal session shell, the interactive shell is sus-\npended until the script completes. You can cause a script or command to run in background \nmode by adding an ampersand sign (&) after the command name. When you run a script \nor command in background mode, the interactive shell returns, allowing you to continue \nentering more commands. Any background processes run using this method are still tied to \nthe terminal session. If you exit the terminal session, the background processes also exit.\nTo prevent this from happening, use the \nnohup command. This command intercepts any \nsignals intended for the command that would stop it — for example, when you exit the ter-\nminal session. This allows scripts to continue running in background mode even if you exit \nthe terminal session.\nWhen you move a process to background mode, you can still control what happens to it. The \njobs command allows you to view processes started from the shell session. After you know \nthe job ID of a background process, you can use the \nkill command to send Linux signals \nto the process or use the \nfg command to bring the process back to the foreground in the \nshell session. You can suspend a running foreground process by using the Ctrl+Z key combi-\nnation and place it back in background mode, using the \nbg command.\nThe \nnice and renice commands allow you to change the priority level of a process. By \ngiving a process a lower priority, you allow the CPU to allocate less time to it. This comes in \nhandy when running long processes that can take lots of CPU time.\nIn addition to controlling processes while they’re running, you can also determine when a pro-\ncess starts on the system. Instead of running a script directly from the command line interface \nprompt, you can schedule the process to run at an alternative time. You can accomplish this in \nseveral different ways. The \nat command enables you to run a script once at a preset time. The \ncron program provides an interface that can run scripts at a regularly scheduled interval.\nFinally, the Linux system provides script fi les for you to use for scheduling your scripts to \nrun whenever a user starts a new bash shell. Similarly, the startup fi les, such as \n.bashrc , \nare located in every user’s home directory to provide a location to place scripts and com-\nmands that run with a new shell.\nIn the next chapter, we look at how to write script functions. Script functions allow you to \nwrite code blocks once and then use them in multiple locations throughout your script. \n\nc17.indd  12/08/2014  Page  447\nPart III\nAdvanced Shell Scripting\nIN THIS PART\nChapter 17\nCreating Functions\nChapter 18\nWriting Scripts for Graphical Desktops\nChapter 19\nIntroducing sed and gawk\nChapter 20\nRegular Expressions\nChapter 21\nAdvanced sed\nChapter 22\nAdvanced gawk\nChapter 23\nWorking with Alternative Shells\n\n\n\n449\nc17.indd  12/08/2014  Page  449\nCHAPTER \n17\nCreating Functions\nIN THIS CHAPTER\nBasic script functions\nReturning a value\nUsing variables in functions\nArray and variable functions\nFunction recursion\nCreating a library\nUsing functions on the command line\nO\nften while writing shell scripts, you’ll fi nd yourself using the same code in multiple loca-\ntions. If it’s just a small code snippet, it’s usually not that big of a deal. However, rewriting \nlarge chunks of code multiple times in your shell script can get tiring. The bash shell pro-\nvides a way to help you out by supporting user-defi ned functions. You can encapsulate your shell \nscript code into a function and use it as many times as you want anywhere in your script. This \nchapter walks you through the process of creating your own shell script functions and demon-\nstrates how to use them in other shell script applications.\nBasic Script Functions\nAs you start writing more complex shell scripts, you’ll fi nd yourself reusing parts of code that \nperform specifi c tasks. Sometimes, it’s something simple, such as displaying a text message and \nretrieving an answer from the script users. Other times, it’s a complicated calculation that’s used \nmultiple times in your script as part of a larger process.\nIn each of these situations, it can get tiresome writing the same blocks of code over and over in \nyour script. It would be nice to just write the block of code once and be able to refer to that block \nof code anywhere in your script without having to rewrite it.\nThe bash shell provides a feature allowing you to do just that. Functions are blocks of script code \nthat you assign a name to and reuse anywhere in your code. Anytime you need to use that block of \n\n450\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  450\ncode in your script, you simply use the function name you assigned it (referred to as calling \nthe function). This section describes how to create and use functions in your shell scripts.\nCreating a function\nThere are two formats you can use to create functions in bash shell scripts. The fi rst format \nuses the keyword \nfunction, along with the function name you assign to the block of code:\nfunction name {\n    commands\n}\nThe name attribute defi nes a unique name assigned to the function. Each function you \ndefi ne in your script must be assigned a unique name.\nThe \ncommands are one or more bash shell commands that make up your function. When \nyou call the function, the bash shell executes each of the commands in the order they \nappear in the function, just as in a normal script.\nThe second format for defi ning a function in a bash shell script more closely follows how \nfunctions are defi ned in other programming languages:\nname() {\ncommands\n}\nThe empty parentheses after the function name indicate that you’re defi ning a function. \nThe same naming rules apply in this format as in the original shell script function format.\nUsing functions\nTo use a function in your script, specify the function name on a line, just as you would any \nother shell command:\n$ cat test1\n#!/bin/bash\n# using a function in a script\nfunction func1 {\n   echo \"This is an example of a function\"\n}\ncount=1\nwhile [ $count -le 5 ]\ndo\n   func1\n   count=$[ $count + 1 ]\n\n451\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  451\n17\ndone\necho \"This is the end of the loop\"\nfunc1\necho \"Now this is the end of the script\"\n$ \n$ ./test1\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is the end of the loop\nThis is an example of a function\nNow this is the end of the script\n$\nEach time you reference the func1 function name, the bash shell returns to the \nfunc1 function defi nition and executes any commands you defi ned there.\nThe function defi nition doesn’t have to be the fi rst thing in your shell script, but be care-\nful. If you attempt to use a function before it’s defi ned, you’ll get an error message:\n$ cat test2\n#!/bin/bash\n# using a function located in the middle of a script\ncount=1\necho \"This line comes before the function definition\"\nfunction func1 {\n   echo \"This is an example of a function\"\n}\nwhile [ $count -le 5 ]\ndo\n   func1\n   count=$[ $count + 1 ]\ndone\necho \"This is the end of the loop\"\nfunc2\necho \"Now this is the end of the script\"\nfunction func2 {\n   echo \"This is an example of a function\"\n}\n$ \n$ ./test2\nThis line comes before the function definition\nThis is an example of a function\n\n452\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  452\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is the end of the loop\n./test2: func2: command not found\nNow this is the end of the script\n$\nThe fi rst function, func1, was defi ned after a couple of statements in the script, which is \nperfectly fi ne. When the \nfunc1 function was used in the script, the shell knew where to \nfi nd it.\nHowever, the script attempted to use the \nfunc2 function before it was defi ned. Because \nthe \nfunc2 function wasn’t defi ned, when the script reached the place where we used it, it \nproduced an error message.\nYou also need to be careful about your function names. Remember, each function name \nmust be unique, or you’ll have a problem. If you redefi ne a function, the new defi nition \noverrides the original function defi nition, without producing any error messages: \n$ cat test3\n#!/bin/bash\n# testing using a duplicate function name\nfunction func1 {\necho \"This is the first definition of the function name\"\n}\nfunc1\nfunction func1 {\n   echo \"This is a repeat of the same function name\"\n}\nfunc1\necho \"This is the end of the script\"\n$ \n$ ./test3\nThis is the first definition of the function name\nThis is a repeat of the same function name\nThis is the end of the script\n$\nThe original defi nition of the func1 function works fi ne, but after the second defi nition of \nthe \nfunc1 function, any subsequent uses of the function use the second defi nition.\n\n453\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  453\n17\nReturning a Value\nThe bash shell treats functions like mini-scripts, complete with an exit status (see \nChapter11). There are three different ways you can generate an exit status for your \nfunctions.\nThe default exit status\nBy default, the exit status of a function is the exit status returned by the last command in \nthe function. After the function executes, you use the standard \n$? variable to determine \nthe exit status of the function:\n$ cat test4\n#!/bin/bash\n# testing the exit status of a function\nfunc1() {\n   echo \"trying to display a non-existent file\"\n   ls -l badfile\n}\necho \"testing the function: \"\nfunc1\necho \"The exit status is: $?\"\n$ \n$ ./test4\ntesting the function:\ntrying to display a non-existent file\nls: badfile: No such file or directory\nThe exit status is: 1\n$\nThe exit status of the function is 1 because the last command in the function failed. \nHowever, you have no way of knowing if any of the other commands in the function com-\npleted successfully or not. Look at this example: \n$ cat test4b\n#!/bin/bash\n# testing the exit status of a function\nfunc1() {\n   ls -l badfile\n   echo \"This was a test of a bad command\"\n\n454\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  454\n}\necho \"testing the function:\"\nfunc1\necho \"The exit status is: $?\"\n$ \n$ ./test4b\ntesting the function:\nls: badfile: No such file or directory\nThis was a test of a bad command\nThe exit status is: 0\n$\nThis time, because the function ended with an echo statement that completed successfully, \nthe exit status of the function is 0, even though one of the commands in the function \nfailed. Using the default exit status of a function can be a dangerous practice. Fortunately, \nwe have a couple of other solutions.\nUsing the return command\nThe bash shell uses the return command to exit a function with a specifi c exit status. \nThe \nreturn command allows you to specify a single integer value to defi ne the function \nexit status, providing an easy way for you to programmatically set the exit status of your \nfunction:\n$ cat test5\n#!/bin/bash\n# using the return command in a function\nfunction dbl {\n   read -p \"Enter a value: \" value\n   echo \"doubling the value\"\n   return $[ $value * 2 ]\n}\ndbl\necho \"The new value is $?\"\n$\nThe dbl function doubles the integer value contained in the $value variable provided by \nthe user input. It then returns the result using the \nreturn command, which the script dis-\nplays using the \n$? variable.\nYou must be careful, however, when using this technique to return a value from a function. \nKeep the following two tips in mind to avoid problems:\n ■\nRemember to retrieve the return value as soon as the function completes.\n ■\nRemember that an exit status must be in the range of 0 to 255.\n\n455\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  455\n17\nIf you execute any other commands before retrieving the value of the function, using the \n$? variable, the return value from the function is lost. Remember that the $? variable \nreturns the exit status of the last executed command.\nThe second problem defi nes a limitation for using this return value technique. Because an \nexit status must be less than 256, the result of your function must produce an integer value \nless than 256. Any value over that returns an error value: \n$ ./test5\nEnter a value: 200\ndoubling the value\nThe new value is 1\n$\nYou cannot use this return value technique if you need to return either larger integer val-\nues or a string value. Instead, you need to use another method, demonstrated in the next \nsection.\nUsing function output\nJust as you can capture the output of a command to a shell variable, you can also capture \nthe output of a function to a shell variable. You can use this technique to retrieve any type \nof output from a function to assign to a variable: \n result='dbl'\nThis command assigns the output of the dbl function to the $result shell variable. Here’s \nan example of using this method in a script: \n$ cat test5b\n#!/bin/bash\n# using the echo to return a value\nfunction dbl {\n   read -p \"Enter a value: \" value\n   echo $[ $value * 2 ]\n}\nresult=$(dbl)\necho \"The new value is $result\"\n$ \n$ ./test5b\nEnter a value: 200\nThe new value is 400\n$ \n$ ./test5b\n\n456\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  456\nEnter a value: 1000\nThe new value is 2000\n$\nThe new function now uses an echo statement to display the result of the calculation. The \nscript just captures the output of the \ndbl function instead of looking at the exit status for \nthe answer.\nThere’s a subtle trick that this example demonstrates. You’ll notice that the \ndb1 function \nreally outputs two messages. The \nread command outputs a short message querying the \nuser for the value. The bash shell script is smart enough to not consider this as part of the \nSTDOUT output and ignores it. If you had used an echo statement to produce this query \nmessage to the user, it would have been captured by the shell variable as well as the output \nvalue. \nUsing this technique, you can also return fl oating point and string values, making this an extremely versatile method \nfor returning values from functions.\nUsing Variables in Functions\nYou might have noticed in the test5 example in the previous section that we used a vari-\nable called \n$value within the function to hold the value that it processed. When you use \nvariables in your functions, you need to be somewhat careful about how you defi ne and \nhandle them. This is a common cause of problems in shell scripts. This section goes over a \nfew techniques for handling variables both inside and outside your shell script functions.\nPassing parameters to a function\nAs mentioned earlier in the “Returning a Value” section, the bash shell treats functions \njust like mini-scripts. This means that you can pass parameters to a function just like a \nregular script (see Chapter 14).\nFunctions can use the standard parameter environment variables to represent any param-\neters passed to the function on the command line. For example, the name of the function is \ndefi ned in the \n$0 variable, and any parameters on the function command line are defi ned \nusing the variables \n$1, $2, and so on. You can also use the special variable $# to determine \nthe number of parameters passed to the function.\nWhen specifying the function in your script, you must provide the parameters on the same \ncommand line as the function, like this:\nfunc1 $value1 10\n\n457\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  457\n17\nThe function can then retrieve the parameter values using the parameter environment \nvariables. Here’s an example of using this method to pass values to a function:\n$ cat test6\n#!/bin/bash\n# passing parameters to a function\nfunction addem {\n   if [ $# -eq 0 ] || [ $# -gt 2 ]\n   then\n      echo -1\n   elif [ $# -eq 1 ]\n   then\n      echo $[ $1 + $1 ]\n   else\n      echo $[ $1 + $2 ]\n   fi\n}\necho -n \"Adding 10 and 15: \"\nvalue=$(addem 10 15)\necho $value\necho -n \"Let's try adding just one number: \"\nvalue=$(addem 10)\necho $value\necho -n \"Now trying adding no numbers: \"\nvalue=$(addem)\necho $value\necho -n \"Finally, try adding three numbers: \"\nvalue=$(addem 10 15 20)\necho $value\n$ \n$ ./test6\nAdding 10 and 15: 25\nLet's try adding just one number: 20\nNow trying adding no numbers: -1\nFinally, try adding three numbers: -1\n$\nThe addem function in the text6 script fi rst checks the number of parameters passed to \nit by the script. If there aren’t any parameters, or if there are more than two parameters, \naddem returns a value of -1. If there’s just one parameter, addem adds the parameter to \nitself for the result. If there are two parameters, \naddem adds them together for the result.\nBecause the function uses the special parameter environment variables for its own param-\neter values, it can’t directly access the script parameter values from the command line of \nthe script. The following example fails:\n$ cat badtest1\n#!/bin/bash\n\n458\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  458\n# trying to access script parameters inside a function\nfunction badfunc1 {\n   echo $[ $1 * $2 ]\n}\nif [ $# -eq 2 ]\nthen\n   value=$(badfunc1)\n   echo \"The result is $value\"\nelse\n   echo \"Usage: badtest1 a b\"\nfi\n$ \n$ ./badtest1\nUsage: badtest1 a b\n$ ./badtest1 10 15\n./badtest1: *  : syntax error: operand expected (error token is \"* \n\")\nThe result is\n$\nEven though the function uses the $1 and $2 variables, they aren’t the same $1 and $2 \nvariables available in the main part of the script. Instead, if you want to use those values \nin your function, you have to manually pass them when you call the function: \n$ cat test7\n#!/bin/bash\n# trying to access script parameters inside a function\nfunction func7 {\n   echo $[ $1 * $2 ]\n}\nif [ $# -eq 2 ]\nthen\n   value=$(func7 $1 $2)\n   echo \"The result is $value\"\nelse\n   echo \"Usage: badtest1 a b\"\nfi\n$ \n$ ./test7\nUsage: badtest1 a b\n$ ./test7 10 15\nThe result is 150\n$\n\n459\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  459\n17\nBy passing the $1 and $2 variables to the function, they become available for the function \nto use, just like any other parameter.\nHandling variables in a function\nOne thing that causes problems for shell script programmers is the scope of a variable. The \nscope is where the variable is visible. Variables defi ned in functions can have a different \nscope than regular variables. That is, they can be hidden from the rest of the script.\nFunctions use two types of variables:\n ■\nGlobal\n ■\nLocal\nThe following sections describe how to use both types of variables in your functions.\nGlobal variables\nGlobal variables are variables that are valid anywhere within the shell script. If you defi ne a \nglobal variable in the main section of a script, you can retrieve its value inside a function. \nLikewise, if you defi ne a global variable inside a function, you can retrieve its value in the \nmain section of the script.\nBy default, any variables you defi ne in the script are global variables. Variables defi ned \noutside of a function can be accessed within the function just fi ne:\n$ cat test8\n#!/bin/bash\n# using a global variable to pass a value\nfunction dbl {\n   value=$[ $value * 2 ]\n}\nread -p \"Enter a value: \" value\ndbl\necho \"The new value is: $value\"\n$ \n$ ./test8\nEnter a value: 450\nThe new value is: 900\n$\nThe $value variable is defi ned outside of the function and assigned a value outside of the \nfunction. When the \ndbl function is called, the variable and its value are still valid inside \n\n460\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  460\nthe function. When the variable is assigned a new value inside the function, that new \nvalue is still valid when the script references the variable.\nThis can be a dangerous practice, however, especially if you intend to use your functions \nin different shell scripts. It requires that you know exactly what variables are used in the \nfunction, including any variables used to calculate values not returned to the script. Here’s \nan example of how things can go bad:\n$ cat badtest2\n#!/bin/bash\n# demonstrating a bad use of variables\nfunction func1 {\n   temp=$[ $value + 5 ]\n   result=$[ $temp * 2 ]\n}\ntemp=4\nvalue=6\nfunc1\necho \"The result is $result\"\nif [ $temp -gt $value ]\nthen\n   echo \"temp is larger\"\nelse\n   echo \"temp is smaller\"\nfi\n$ \n$ ./badtest2\nThe result is 22\ntemp is larger\n$\nBecause the $temp variable was used in the function, its value is compromised in the \nscript, producing a result that you may not have intended. There’s an easy way to solve this \nproblem in your functions, as shown in the next section.\nLocal variables\nInstead of using global variables in functions, any variables that the function uses inter-\nnally can be declared as local variables. To do that, just use the \nlocal keyword in front of \nthe variable declaration:\nlocal temp\nYou can also use the local keyword in an assignment statement while assigning a value to \nthe variable:\nlocal temp=$[ $value + 5 ]\n\n461\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  461\n17\nThe local keyword ensures that the variable is limited to only within the function. If a \nvariable with the same name appears outside the function in the script, the shell keeps \nthe two variable values separate. Now you can easily keep your function variables separate \nfrom your script variables and share only the ones you want to share:\n$ cat test9\n#!/bin/bash\n# demonstrating the local keyword\nfunction func1 {\n   local temp=$[ $value + 5 ]\n   result=$[ $temp * 2 ]\n}\ntemp=4\nvalue=6\nfunc1\necho \"The result is $result\"\nif [ $temp -gt $value ]\nthen\n   echo \"temp is larger\"\nelse\n   echo \"temp is smaller\"\nfi\n$ \n$ ./test9\nThe result is 22\ntemp is smaller\n$\nNow when you use the $temp variable within the func1 function, it doesn’t affect the \nvalue assigned to the \n$temp variable in the main script.\nArray Variables and Functions\nChapter 6 discussed an advanced way of allowing a single variable to hold multiple values \nby using arrays. Using array variable values with functions is a little tricky, and there are \nsome special considerations. This section describes a technique that allows you to do that.\nPassing arrays to functions\nThe art of passing an array variable to a script function can be confusing. If you try to pass \nthe array variable as a single parameter, it doesn’t work:\n$ cat badtest3\n#!/bin/bash\n\n462\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  462\n# trying to pass an array variable\nfunction testit {\n   echo \"The parameters are: $@\"\n   thisarray=$1\n   echo \"The received array is ${thisarray[*]}\"\n}\n \nmyarray=(1 2 3 4 5)\necho \"The original array is: ${myarray[*]}\"\ntestit $myarray\n$ \n$ ./badtest3\nThe original array is: 1 2 3 4 5\nThe parameters are: 1\nThe received array is 1\n$\nIf you try using the array variable as a function parameter, the function only picks up the \nfi rst value of the array variable.\nTo solve this problem, you must disassemble the array variable into its individual values \nand use the values as function parameters. Inside the function, you can reassemble all the \nparameters into a new array variable. Here’s an example of doing this: \n$ cat test10\n#!/bin/bash\n# array variable to function test\nfunction testit {\n   local newarray\n   newarray=(;'echo \"$@\"')\n   echo \"The new array value is: ${newarray[*]}\"\n}\nmyarray=(1 2 3 4 5)\necho \"The original array is ${myarray[*]}\"\ntestit ${myarray[*]}\n$ \n$ ./test10\nThe original array is 1 2 3 4 5\nThe new array value is: 1 2 3 4 5\n$\nThe script uses the $myarray variable to hold all the individual array values to place them \nall on the command line for the function. The function then rebuilds the array variable \nfrom the command line parameters. Once inside the function, the array can be used just \nlike any other array:\n$ cat test11\n#!/bin/bash\n\n463\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  463\n17\n# adding values in an array\nfunction addarray {\n   local sum=0\n   local newarray\n   newarray=($(echo \"$@\"))\n   for value in ${newarray[*]}\n   do\n      sum=$[ $sum + $value ]\n   done\n   echo $sum\n}\nmyarray=(1 2 3 4 5)\necho \"The original array is: ${myarray[*]}\"\narg1=$(echo ${myarray[*]})\nresult=$(addarray $arg1)\necho \"The result is $result\"\n$ \n$ ./test11\nThe original array is: 1 2 3 4 5\nThe result is 15\n$\nThe addarray function iterates through the array values, adding them together. You can \nput any number of values in the \nmyarray array variable, and the addarray function adds \nthem.\nReturning arrays from functions\nPassing an array variable from a function back to the shell script uses a similar technique. \nThe function uses an \necho statement to output the individual array values in the proper \norder, and the script must reassemble them into a new array variable:\n$ cat test12\n#!/bin/bash\n# returning an array value\nfunction arraydblr {\n   local origarray\n   local newarray\n   local elements\n   local i\n   origarray=($(echo \"$@\"))\n   newarray=($(echo \"$@\"))\n   elements=$[ $# - 1 ]\n   for (( i = 0; i <= $elements; i++ ))\n   {\n      newarray[$i]=$[ ${origarray[$i]} * 2 ]\n   }\n\n464\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  464\n   echo ${newarray[*]}\n}\nmyarray=(1 2 3 4 5)\necho \"The original array is: ${myarray[*]}\"\narg1=$(echo ${myarray[*]})\nresult=($(arraydblr $arg1))\necho \"The new array is: ${result[*]}\"\n$ \n$ ./test12\nThe original array is: 1 2 3 4 5\nThe new array is: 2 4 6 8 10\nThe script passes the array value, using the $arg1 variable to the arraydblr function. \nThe \narraydblr function reassembles the array into a new array variable, and it makes a \ncopy for the output array variable. It then iterates through the individual array variable \nvalues, doubles each value, and places it into the copy of the array variable in the function.\nThe \narraydblr function then uses the echo statement to output the individual values of \nthe array variable values. The script uses the output of the \narraydblr function to reas-\nsemble a new array variable with the values.\nFunction Recursion\nOne feature that local function variables provide is self-containment. A self-contained func-\ntion doesn’t use any resources outside of the function, other than whatever variables the \nscript passes to it in the command line.\nThis feature enables the function to be called recursively, which means that the function \ncalls itself to reach an answer. Usually, a recursive function has a base value that it eventu-\nally iterates down to. Many advanced mathematical algorithms use recursion to reduce a \ncomplex equation down one level repeatedly, until they get to the level defi ned by the base \nvalue.\nThe classic example of a recursive algorithm is calculating factorials. A factorial of a num-\nber is the value of the preceding numbers multiplied with the number. Thus, to fi nd the \nfactorial of 5, you’d perform the following equation:\n5! = 1 * 2 * 3 * 4 * 5 = 120\nUsing recursion, the equation is reduced down to the following format:\nx! = x * (x-1)!\nor in English, the factorial of x is equal to x times the factorial of x-1. This can be \nexpressed in a simple recursive script:\nfunction factorial {\n   if [ $1 -eq 1 ]\n\n465\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  465\n17\n   then\n      echo 1\n   else\n      local temp=$[ $1 - 1 ]\n      local result='factorial $temp'\n      echo $[ $result * $1 ]\n   fi\n}\nThe factorial function uses itself to calculate the value for the factorial:\n$ cat test13\n#!/bin/bash\n# using recursion\nfunction factorial {\n   if [ $1 -eq 1 ]\n   then\n      echo 1\n   else\n      local temp=$[ $1 - 1 ]\n      local result=$(factorial $temp)\n      echo $[ $result * $1 ]\n   fi\n}\n \nread -p \"Enter value: \" value\nresult=$(factorial $value)\necho \"The factorial of $value is: $result\"\n$ \n$ ./test13\nEnter value: 5\nThe factorial of 5 is: 120\n$\nUsing the factorial function is easy. Having created a function like this, you may want to \nuse it in other scripts. Next, we look at how to do that effi ciently.\nCreating a Library\nIt’s easy to see how functions can help save typing in a single script, but what if you just \nhappen to use the same single code block between scripts? It’s obviously challenging if you \nhave to defi ne the same function in each script, only to use it one time in each script.\nThere’s a solution for that problem! The bash shell allows you to create a library file for your \nfunctions and then reference that single library fi le in as many scripts as you need to.\n\n466\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  466\nThe fi rst step in the process is to create a common library fi le that contains the functions \nyou need in your scripts. Here’s a simple library fi le called \nmyfuncs that defi nes three \nsimple functions:\n$ cat myfuncs\n# my script functions\nfunction addem {\n   echo $[ $1 + $2 ]\n}\nfunction multem {\n   echo $[ $1 * $2 ]\n}\nfunction divem {\n   if [ $2 -ne 0 ]\n   then\n      echo $[ $1 / $2 ]\n   else\n      echo -1\n   fi\n}\n$\nThe next step is to include the myfuncs library fi le in your script fi les that want to use \nany of the functions. This is where things get tricky.\nThe problem is with the scope of shell functions. As with environment variables, shell func-\ntions are valid only for the shell session in which you defi ne them. If you run the \nmyfuncs \nshell script from your shell command line interface prompt, the shell creates a new shell \nand runs the script in that new shell. This defi nes the three functions for that shell, but \nwhen you try to run another script that uses those functions, they aren’t available.\nThis applies to scripts as well. If you try to just run the library fi le as a regular script fi le, \nthe functions don’t appear in your script:\n$ cat badtest4\n#!/bin/bash\n# using a library file the wrong way\n./myfuncs\nresult=$(addem 10 15)\necho \"The result is $result\"\n$ \n$ ./badtest4\n./badtest4: addem: command not found\nThe result is\n$\n\n467\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  467\n17\nThe key to using function libraries is the source command. The source command exe-\ncutes commands within the current shell context instead of creating a new shell to execute \nthem. You use the \nsource command to run the library fi le script inside of your shell script. \nThis makes the functions available to the script.\nThe \nsource command has a shortcut alias, called the dot operator. To source the myfuncs \nlibrary fi le in a shell script, you just need to add the following line:\n. ./myfuncs\nThis example assumes that the myfuncs library fi le is located in the same directory as the \nshell script. If not, you need to use the appropriate path to access the fi le. Here’s an exam-\nple of creating a script that uses the \nmyfuncs library fi le:\n$ cat test14\n#!/bin/bash\n# using functions defined in a library file\n. ./myfuncs\nvalue1=10\nvalue2=5\nresult1=$(addem $value1 $value2)\nresult2=$(multem $value1 $value2)\nresult3=$(divem $value1 $value2)\necho \"The result of adding them is: $result1\"\necho \"The result of multiplying them is: $result2\"\necho \"The result of dividing them is: $result3\"\n$ \n$ ./test14\nThe result of adding them is: 15\nThe result of multiplying them is: 50\nThe result of dividing them is: 2\n$\nThe script successfully uses the functions defi ned in the myfuncs library fi le.\nUsing Functions on the Command Line\nYou can use script functions to create some pretty complex operations. Sometimes, it would \nbe nice to be able to use these functions directly on the command line interface prompt.\nJust as you can use a script function as a command in a shell script, you can also use a \nscript function as a command in the command line interface. This is a nice feature because \nafter you defi ne the function in the shell, you can use it from any directory on the system; \nyou don’t have to worry about a script being in your \nPATH environment variable. The trick \nis to get the shell to recognize the function. You can do that in a couple of ways.\n\n468\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  468\nCreating functions on the command line\nBecause the shell interprets commands as you type them, you can defi ne a function \ndirectly on the command line. You can do that in two ways.\nThe fi rst method defi nes the function all on one line:\n$ function divem { echo $[ $1 / $2 ];  }\n$ divem 100 5\n20\n$\nWhen you defi ne the function on the command line, you must remember to include a semi-\ncolon at the end of each command, so the shell knows where to separate commands:\n$ function doubleit { read -p \"Enter value: \" value; echo $[\n $value * 2 ]; }\n$ \n$ doubleit\nEnter value: 20\n40\n$\nThe other method is to use multiple lines to defi ne the function. When you do that, \nthe bash shell uses the secondary prompt to prompt you for more commands. Using this \nmethod, you don’t need to place a semicolon at the end of each command; just press the \nEnter key:\n$ function multem {\n> echo $[ $1 * $2 ]\n> }\n$ multem 2 5\n10\n$\nWhen you use the brace at the end of the function, the shell knows that you’re fi nished \ndefi ning the function. \nBe extremely careful when creating functions on the command line. If you use a function with the same name as a \nbuilt-in command or another command, the function overrides the original command.\nDefi ning functions in the .bashrc fi le\nThe obvious downside to defi ning shell functions directly on the command line is that \nwhen you exit the shell, your function disappears. For complex functions, this can become \na problem.\n\n469\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  469\n17\nA much simpler method is to defi ne the function in a place where it is reloaded by the shell \neach time you start a new shell.\nThe best place to do that is the \n.bashrc fi le. The bash shell looks for this fi le in your home \ndirectory each time it starts, whether interactively or as the result of starting a new shell \nfrom within an existing shell.\nDirectly defining functions\nYou can defi ne the functions directly in the .bashrc fi le in your home directory. Most \nLinux distributions already defi ne some things in the \n.bashrc fi le, so be careful not to \nremove those items. Just add your functions to the bottom of the existing fi le. Here’s an \nexample of doing that:\n$ cat .bashrc\n# .bashrc\n# Source global definitions\nif [ -r /etc/bashrc ]; then\n        . /etc/bashrc\nfi\nfunction addem {\n   echo $[ $1 + $2 ]\n}\n$\nThe function doesn’t take effect until the next time you start a new bash shell. After you \ndo that, you can use the function anywhere on the system.\nSourcing function files\nJust as in a shell script, you can use the source command (or its alias the dot operator) to \nadd functions from an existing library fi le to your \n.bashrc script:\n$ cat .bashrc\n# .bashrc\n# Source global definitions\nif [ -r /etc/bashrc ]; then\n        . /etc/bashrc\nfi\n. /home/rich/libraries/myfuncs\n$\nMake sure that you include the proper pathname to reference the library fi le for the bash \nshell to fi nd. The next time you start a shell, all the functions in your library are available \nat the command line interface:\n\n470\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  470\n$ addem 10 5\n15\n$ multem 10 5\n50\n$ divem 10 5\n2\n$\nEven better, the shell also passes any defi ned functions to child shell processes so your \nfunctions are automatically available for any shell scripts you run from your shell session. \nYou can test this by writing a script that uses the functions without defi ning or sourcing \nthem:\n$ cat test15\n#!/bin/bash\n# using a function defined in the .bashrc file\nvalue1=10\nvalue2=5\nresult1=$(addem $value1 $value2)\nresult2=$(multem $value1 $value2)\nresult3=$(divem $value1 $value2)\necho \"The result of adding them is: $result1\"\necho \"The result of multiplying them is: $result2\"\necho \"The result of dividing them is: $result3\"\n$ \n$ ./test15\nThe result of adding them is: 15\nThe result of multiplying them is: 50\nThe result of dividing them is: 2\n$\nEven without sourcing the library fi le, the functions worked perfectly in the shell script.\nFollowing a Practical Example\nThere’s much more to using functions than just creating your own functions to work with. \nIn the open source world, code sharing is key, and that also applies to shell script func-\ntions. Quite a few different shell script functions are available for you to download and use \nin your own applications.\nThis section walks through downloading, installing, and using the GNU shtool shell script \nfunction library. The shtool library provides some simple shell script functions for perform-\ning everyday shell functions, such as working with temporary fi les and folders or format-\nting output to display.\n\n471\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  471\n17\nDownloading and installing\nThe fi rst step in the process is to download and install the GNU shtool library to your sys-\ntem so you can use the library functions in your own shell scripts. To do that, you need to \nuse an FTP client program or a browser in a graphical desktop. Use this URL to download \nthe shtool package:\nftp://ftp.gnu.org/gnu/shtool/shtool-2.0.8.tar.gz\nThis downloads the fi le shtool-2.0.8.tar.gz to the download folder. From there, you \ncan use the \ncp command line tool or the graphical fi le manager tool in your Linux distribu-\ntion (such as Nautilus in Ubuntu) to copy the fi le to your Home folder.\nAfter you copy the fi le to your Home folder, you can extract it using the \ntar command:\ntar -zxvf shtool-2.0.8.tar.gz\nThis extracts the package fi les into a folder named shtool-2.0.8. Now you’re ready to \nbuild the shell script library fi le.\nBuilding the library\nThe shtool distribution fi le must be confi gured for your specifi c Linux environment. To do \nthat, it uses standard \nconfigure and make commands, commonly used in the C program-\nming environment. To build the library fi le, you just need to run two commands:\n$ ./confifgure\n$ make\nThe configure command checks the software necessary to build the shtool library fi le. \nAs it fi nds the tools it needs, it modifi es the confi guration fi le with the proper paths to the \ntools.\nThe \nmake command runs through the steps to build the shtool library fi le. The resulting fi le \n(\nshtool) is the full library package fi le. You can test the library fi le using the make com-\nmand as well:\n$ make test\nRunning test suite:\necho...........ok\nmdate..........ok\ntable..........ok\nprop...........ok\nmove...........ok\ninstall........ok\nmkdir..........ok\nmkln...........ok\n\n472\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  472\nmkshadow.......ok\nfixperm........ok\nrotate.........ok\ntarball........ok\nsubst..........ok\nplatform.......ok\narx............ok\nslo............ok\nscpp...........ok\nversion........ok\npath...........ok\nOK: passed: 19/19\n$\nThe test mode tests all the functions available in the shtool library. If all pass, then you’re \nready to install the library into a common location on your Linux system so all your scripts \ncan use it. To do that, you can use the \ninstall option of the make command. However, \nyou need to be logged in as the root user account to run it:\n$ su\nPassword:\n# make install\n./shtool mkdir -f -p -m 755 /usr/local\n./shtool mkdir -f -p -m 755 /usr/local/bin\n./shtool mkdir -f -p -m 755 /usr/local/share/man/man1\n./shtool mkdir -f -p -m 755 /usr/local/share/aclocal\n./shtool mkdir -f -p -m 755 /usr/local/share/shtool\n...\n./shtool install -c -m 644 sh.version /usr/local/share/shtool/sh.version\n./shtool install -c -m 644 sh.path /usr/local/share/shtool/sh.path\n#\nNow you’re ready to start using the functions in your own shell scripts!\nThe shtool library functions\nThe shtool library provides quite a few functions that can come in handy when working \nwith shell scripts. Table 17.1 shows the functions available in the library.\nTABLE 17.1    The shtool Library Functions\nFunctionDescription\nArxCreates an archive with extended features\nEchoDisplays the string value with construct expansion\nfi  x p e r mC h a n g e s   fi le permissions inside a folder tree\n\n473\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  473\n17\ninstallInstalls a script or fi le\nmdateDisplays modifi cation time of a fi le or directory\nmkdirCreates one or more directories\nMklnCreates a link using relative paths\nmkshadowCreates a shadow tree\nmoveMoves fi les with substitution\nPathWorks with program paths\nplatformDisplays the platform identity\nPropDisplays an animated progress propeller\nrotateRotates logfi les\nScppThe sharing C pre-processor\nSloSeparates linker options by library class\nSubstUses sed substitution operations\nTableDisplays fi eld-separated data in a table format\ntarballCreates tar fi les from fi les and folders\nversionCreates a version information fi le\nEach of the shtool functions has lots of options and arguments that you can use to modify \nhow it works. Here’s the format to use a shtool function:\nshtool [options] [function [options] [args]]\nUsing the library\nYou can use the shtool functions directly from the command line or from within your \nshell scripts. Here’s an example of using the \nplatform function inside a shell script:\n$ cat test16\n#!/bin/bash\nshtool platform\n$ ./test16\nUbuntu 14.04 (iX86)\n$\nThe platform function returns the Linux distribution and the CPU hardware that the host \nsystem is using. One of my favorites is the \nprop function. It creates a spinning propeller \nfrom alternating the \\, |, /, and – characters while something is processing. That’s a great \ntool to help show your shell script users that something is happening in the background \nwhile the script is running.\n\n474\nPart III: Advanced Shell Scripting\nc17.indd  12/08/2014  Page  474\nTo use the prop function, you just pipe the output of the function you want to monitor to \nthe \nshtool script:\n$ ls –al /usr/bin | shtool prop –p \"waiting...\"\nwaiting...\n$\nThe prop function alternates between the propeller characters to indicate that something \nis happening. In this case, it’s the output from the \nls command. How much of that you see \ndepends on how fast your CPU can list out all the fi les in the \n/usr/bin folder! The –p \noption allows you to customize the output text that appears before the propeller charac-\nters. Now that’s getting fancy!\nSummary\nShell script functions allow you to place script code that’s repeated throughout the script \nin a single place. Instead of having to rewrite blocks of code, you can create a function \ncontaining the code block and then just reference the function name in your script. The \nbash shell jumps to the function code block whenever it sees the function name used in the \nscript.\nYou can even create script functions that return values. This allows you to create functions \nthat interact with the script, returning both numeric and character data. Script functions \ncan return numeric data by using the exit status of the last command in the function or \nusing the \nreturn command. The return command allows you to programmatically set the \nexit status of your function to a specifi c value based on the results of the function.\nFunctions can also return values using the standard \necho statement. You can capture the \noutput data using the backtick character as you would any other shell command. This \nenables you to return any type of data from a function, including strings and fl oating-\npoint numbers.\nYou can use shell variables within your functions, assigning values to variables and retriev-\ning values from existing variables. This allows you to pass any type of data both into and \nout of a script function from the main script program. Functions also allow you to defi ne \nlocal variables, which are accessible only from within the function code block. Local vari-\nables allow you to create self-contained functions, which don’t interfere with any variables \nor processes used in the main shell script.\nFunctions can also call other functions, including themselves. When a function calls itself, \nit is called recursion. A recursive function often has a base value that is the terminal value \nof the function. The function continues to call itself with a decreasing parameter value \nuntil the base value is reached.\n\n475\nChapter 17: Creating Functions\nc17.indd  12/08/2014  Page  475\n17\nIf you use lots of functions in your shell scripts, you can create library fi les of script func-\ntions. The library fi les can be included in any shell script fi le by using the source command, \nor its alias, the dot operator. This is called sourcing the library fi le. The shell doesn’t run \nthe library fi le but makes the functions available within the shell that runs the script. You \ncan use this same technique to create functions that you can use on the normal shell com-\nmand line. You can either defi ne functions directly on the command line or you can add \nthem to your \n.bashrc fi le so they are available for each new shell session you start. This \nis a handy way to create utilities that can be used no matter what your \nPATH  environment \nvariable is set to.\nThe next chapter discusses the use of text graphics in your scripts. In this day of modern \ngraphical interfaces, sometimes a plain text interface just doesn’t cut it. The bash shell pro-\nvides some easy ways for you to incorporate simple graphics features in your scripts to help \nspice things up. \n\n\n\n477\nc18.indd  12/16/2014  Page  477\nCHAPTER \n18\nWriting Scripts for Graphical \nDesktops\nIN THIS CHAPTER\nCreating text menus\nBuilding text window widgets\nAdding X Window graphics\nO\nver the years, shell scripts have acquired a reputation for being dull and boring. This doesn’t \nhave to be the case, however, if you plan on running your scripts in a graphical environ-\nment. There are plenty of ways to interact with your script user that don’t rely on the \nread \nand \necho statements. This chapter dives into a few different methods you can use to help add life \nto your interactive scripts so they don’t look so old-fashioned.\nCreating Text Menus\nThe most common way to create an interactive shell script is to utilize a menu. Offering your \ncustomers a choice of various options helps guide them through exactly what the script can and \ncan’t do.\nMenu scripts usually clear the display area and then show a list of options available. The customer \ncan select an option by pressing an associated letter or number assigned to each option. Figure 18-1 \nshows the layout of a sample menu.\nThe core of a shell script menu is the \ncase command (see Chapter 12). The case command performs \nspecifi c commands, depending on what character your customer selects from the menu.\nThe following sections walk you through the steps you should follow to create a menu-based \nshell script.\n\n478\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  478\nFIGURE 18-1\nDisplaying a menu from a shell script\nCreate the menu layout\nThe fi rst step in creating a menu is, obviously, to determine what elements you want to \nappear in the menu and lay them out the way that you want them to appear.\nBefore creating the menu, it’s usually a good idea to clear the monitor display. This enables \nyou to display your menu in a clean environment without distracting text.\nThe \nclear command uses the terminfo data of your terminal session (see Chapter 2) to \nclear any text that appears on the monitor. After the \nclear command, you can use the \necho command to display your menu elements.\nBy default, the \necho command can only display printable text characters. When creating \nmenu items, it’s often helpful to use nonprintable items, such as the tab and newline char-\nacters. To include these characters in your \necho command, you must use the -e option. \nThus, the command:\n echo -e \"1.\\tDisplay disk space\"\nresults in the output line:\n 1.        Display disk space\n\n479\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  479\n18\nThis greatly helps in formatting the layout of the menu items. With just a few echo com-\nmands, you can create a reasonable-looking menu:\n    clear\n    echo\n    echo -e \"\\t\\t\\tSys Admin Menu\\n\"\n    echo -e \"\\t1. Display disk space\"\n    echo -e \"\\t2. Display logged on users\"\n    echo -e \"\\t3. Display memory usage\"\n    echo -e \"\\t0. Exit menu\\n\\n\"\n    echo –en \"\\t\\tEnter option: \"\nThe -en option on the last line displays the line without adding the newline character at \nthe end. This gives the menu a more professional look, because the cursor stays at the end \nof the line waiting for the customer’s input.\nThe last part of creating the menu is to retrieve the input from the customer. This is done \nusing the \nread command (see Chapter 14). Because we expect only single-character input, \nthe nice thing to do is to use the \n-n option in the read command to retrieve only one char-\nacter. This allows the customer to enter a number without having to press the Enter key:\n read -n 1 option\nNext, you need to create your menu functions.\nCreate the menu functions\nShell script menu options are easier to create as a group of separate functions. This enables \nyou to create a simple, concise \ncase command that is easy to follow.\nTo do that, you need to create separate shell functions for each of your menu options. The \nfi rst step in creating a menu shell script is to determine what functions you want your \nscript to perform and lay them out as separate functions in your code.\nIt is common practice to create stub functions for functions that aren’t implemented yet. A \nstub function is a function that doesn’t contain any commands yet or possibly just an \necho \nstatement indicating what should be there eventually:\n function diskspace {\n    clear\n    echo \"This is where the diskspace commands will go\"\n }\nThis enables your menu to operate smoothly while you work on the individual functions. \nYou don’t have to code all the functions for your menu to work. You’ll notice that the \n\n480\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  480\nfunction starts out with the clear command. This enables you to start the function on a \nclean monitor screen, without the menu showing.\nOne thing that helps out in the shell script menu is to create the menu layout itself as a \nfunction:\n function menu {\n    clear\n    echo\n    echo -e \"\\t\\t\\tSys Admin Menu\\n\"\n    echo -e \"\\t1. Display disk space\"\n    echo -e \"\\t2. Display logged on users\"\n    echo -e \"\\t3. Display memory usage\"\n    echo -e \"\\t0. Exit program\\n\\n\"\n    echo -en \"\\t\\tEnter option: \"\n    read -n 1 option\n }\nThis enables you to easily redisplay the menu at any time just by calling the menu function.\nAdd the menu logic\nNow that you have your menu layout and your functions, you just need to create the \nprogramming logic to put the two together. As mentioned, this requires the \ncase command.\nThe \ncase command should call the appropriate function according to the character selec-\ntion expected from the menu. It’s always a good idea to use the default \ncase command \ncharacter (the asterisk) to catch any incorrect menu entries.\nThe following code illustrates the use of the \ncase command in a typical menu:\n menu\n case $option in\n 0)\n    break ;;\n 1)\n    diskspace ;;\n 2)\n    whoseon ;;\n 3)\n    memusage ;;\n *)\n    clear\n    echo \"Sorry, wrong selection\";;\n esac\nThis code fi rst uses the menu function to clear the monitor screen and display the menu. The \nread command in the menu function pauses until the customer hits a character on the keyboard. \n\n481\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  481\n18\nAfter that’s been done, the case command takes over. The case command calls the appropriate \nfunction based on the returned character. After the function completes, the \ncase command exits.\nPutting it all together\nNow that you’ve seen all the parts that make up a shell script menu, let’s put them together \nand see how they all interoperate. Here’s an example of a full menu script:\n $ cat menu1\n #!/bin/bash\n # simple script menu\n function diskspace {\n    clear\n    df -k\n }\n function whoseon {\n    clear\n    who\n }\n function memusage {\n    clear\n    cat /proc/meminfo\n }\n function menu {\n    clear\n    echo\n    echo -e \"\\t\\t\\tSys Admin Menu\\n\"\n    echo -e \"\\t1. Display disk space\"\n    echo -e \"\\t2. Display logged on users\"\n    echo -e \"\\t3. Display memory usage\"\n    echo -e \"\\t0. Exit program\\n\\n\"\n    echo -en \"\\t\\tEnter option: \"\n    read -n 1 option\n }\n while [ 1 ]\n do\n    menu\n    case $option in\n    0)\n       break ;;\n    1)\n       diskspace ;;\n\n482\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  482\n    2)\n       whoseon ;;\n    3)\n       memusage ;;\n    *)\n       clear\n       echo \"Sorry, wrong selection\";;\n    esac\n    echo -en \"\\n\\n\\t\\t\\tHit any key to continue\"\n    read -n 1 line\n done\n clear\n $\nThis menu creates three functions to retrieve administrative information about the Linux \nsystem using common commands. It uses a \nwhile loop to continually loop through the \nmenu until the customer selects option 0, which uses the \nbreak command to break out of \nthe \nwhile loop.\nYou can use this same template to create any shell script menu interface. It provides a \nsimple way to interact with your customers.\nUsing the select command\nYou may have noticed that half the challenge of creating a text menu is just creating the \nmenu layout and retrieving the answer that you enter. The bash shell provides a handy \nlittle utility for you that does all this work automatically.\nThe \nselect command allows you to create a menu from a single command line and then \nretrieve the entered answer and automatically process it. The format of the \nselect com-\nmand is as follows:\n select variable in list\n do\n     commands\n done\nThe list parameter is a space-separated list of text items that build the menu. The \nselect command displays each item in the list as a numbered option and then displays a \nspecial prompt, defi ned by the \nPS3 environment variable, for the selection.\nHere’s a simple example of the \nselect command in action:\n $ cat smenu1\n #!/bin/bash\n # using select in the menu\n function diskspace {\n\n483\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  483\n18\n    clear\n    df -k\n }\n function whoseon {\n    clear\n    who\n }\n function memusage {\n    clear\n    cat /proc/meminfo\n }\n PS3=\"Enter option: \"\n select option in \"Display disk space\" \"Display logged on users\"¬\n \"Display memory usage\" \"Exit program\"\n do\n    case $option in\n    \"Exit program\")\n          break ;;\n    \"Display disk space\")\n          diskspace ;;\n    \"Display logged on users\")\n          whoseon ;;\n    \"Display memory usage\")\n          memusage ;;\n    *)\n          clear\n          echo \"Sorry, wrong selection\";;\n    esac\n done\n clear\n $\nThe select statement must all be on one line in the code fi le. That’s indicated by the con-\ntinuation character in the listing. When you run the program, it automatically produces the \nfollowing menu:\n $ ./smenu1\n 1) Display disk space       3) Display memory usage\n 2) Display logged on users  4) Exit program\n Enter option:\nWhen you use the select command, remember that the result value stored in the variable \nis the entire text string and not the number associated with the menu item. The text string \nvalues are what you need to compare in your \ncase statements.\n\n484\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  484\nDoing Windows\nUsing text menus is a step in the right direction, but there’s still so much missing in \nour interactive scripts, especially if we try to compare them to the graphical Windows \nworld. Fortunately for us, some very resourceful people out in the open source world have \nhelped us out.\nThe dialog package is a nifty little tool originally created by Savio Lam and currently main-\ntained by Thomas E. Dickey. This package recreates standard Windows dialog boxes in a \ntext environment using ANSI escape control codes. You can easily incorporate these dialog \nboxes in your shell scripts to interact with your script users. This section describes the dia-\nlog package and demonstrates how to use it in shell scripts.\nThe dialog package isn’t installed in all Linux distributions by default. If it’s not installed by default, because of its \npopularity it’s almost always included in the software repository. Check your specifi c Linux distribution documenta-\ntion for how to load the dialog package. For the Ubuntu Linux distribution, the following is the command line com-\nmand to install it:\nsudo apt-get install dialog\nThat package installs the dialog package plus the required libraries for your system.\nThe dialog package\nThe dialog command uses command line parameters to determine what type of Windows \nwidget to produce. A widget is the dialog package term for a type of Windows element. The \ndialog package currently supports the types of widgets shown in Table 18-1.\nTABLE 18-1    The  dialog  Widgets\nWidgetDescription\ncalendar\nProvides a calendar from which to select a date \nchecklist\nDisplays multiple entries where each entry can be turned on or off\nform\nAllows you to build a form with labels and text fi elds to be fi lled out\nfselect\nProvides a fi le selection window to browse for a fi le\ngauge\nDisplays a meter showing a percentage of completion\ninfobox\nDisplays a message without waiting for a response\ninputbox\nDisplays a single text form box for text entry\ninputmenu\nProvides an editable menu\nmenu\nDisplays a list of selections from which to choose \n\n485\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  485\n18\nmsgbox\nDisplays a message and requires the user to select an OK button\npause\nDisplays a meter showing the status of a specifi ed pause period\npasswordbox\nDisplays a single textbox that hides entered text\npasswordform\nDisplays a form with labels and hidden text fi elds\nradiolist\nProvides a group of menu items where only one item can be selected\ntailbox\nDisplays text from a fi le in a scroll window using the tail command\ntailboxbg\nSame as tailbox, but operates in background mode\ntextbox\nDisplays the contents of a fi le in a scroll window\ntimebox\nProvides a window to select an hour, minute, and second\nyesno\nProvides a simple message with Yes and No buttons\nAs you can see from Table 18-1, you can choose from lots of different widgets. This can give \nyour scripts a more professional look with very little effort.\nTo specify a specifi c widget on the command line, you need to use the double dash format:\n dialog --widget parameters\nwhere widget is the widget name as seen in Table 18-1, and parameters defi nes the size \nof the widget window and any text required for the widget.\nEach dialog widget provides output in two forms:\n ■\nUsing STDERR\n ■\nUsing the exit code status\nThe exit code status of the \ndialog command determines the button selected by the \nuser. If an OK or Yes button is selected, the \ndialog command returns a 0 exit status. \nIf a Cancel or No button is selected, the \ndialog command returns a 1 exit status. You \ncan use the standard \n$? variable to determine which button was selected in the dialog \nwidget.\nIf a widget returns any data, such as a menu selection, the \ndialog command sends the \ndata to \nSTDERR. You can use the standard bash shell technique of redirecting the STDERR \noutput to another fi le or fi le descriptor:\n dialog --inputbox \"Enter your age:\" 10 20 2>age.txt\nThis command redirects the text entered in the textbox to the age.txt fi le.\nThe following sections look at some examples of the more common dialog widgets you’ll use \nin your shell scripts.\n\n486\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  486\nThe msgbox widget\nThe msgbox widget is the most common type of dialog box. It displays a simple message in \na window and waits for the user to click an OK button before disappearing. The following \nformat is required to use a \nmsgbox widget:\n dialog --msgbox text height width\nThe text parameter is any string you want to place in the window. The dialog command \nautomatically wraps the text to fi t the size of the window you create, using the \nheight \nand \nwidth parameters. If you want to place a title at the top of the window, you can also \nuse the \n--title parameter, along with the text of the title. Here’s an example of using the \nmsgbox widget:\n $ dialog --title Testing --msgbox \"This is a test\" 10 20\nAfter entering this command, the message box appears on the screen of the terminal \nemulator session you’re using. Figure 18-2 shows what this looks like.\nFIGURE 18-2\nUsing the msgbox widget in the dialog command\nIf your terminal emulator supports the mouse, you can click the OK button to close the \ndialog box. You can also use keyboard commands to simulate a click — just press the \nEnter key.\n\n487\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  487\n18\nThe yesno widget\nThe yesno widget takes the msgbox widget one step further, allowing the user to answer \na yes/no question displayed in the window. It produces two buttons at the bottom of the \nwindow — one for Yes and another for No. The user can switch between buttons by using \nthe mouse, the tab key, or the keyboard arrow keys. To select the button, the user can \neither press the spacebar or the Enter key.\nHere’s an example of using the \nyesno widget:\n $ dialog --title \"Please answer\" --yesno \"Is this thing on?\" 10 20\n $ echo $?\n 1\n $\nThis produces the widget shown in Figure 18-3.\nFIGURE 18-3\nUsing the yesno widget in the dialog command\nThe exit status of the dialog command is set depending on which button the user selects. \nIf the No button is selected, the exit status is 1, and if the Yes button is selected, the exit \nstatus is 0.\nThe inputbox widget\nThe inputbox widget provides a simple textbox area for the user to enter a text string. \nThe dialog command sends the value of the text string to \nSTDERR. You must redirect that \nto retrieve the answer. Figure 18-4 demonstrates what the \ninputbox widget looks like.\n\n488\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  488\nFIGURE 18-4\nThe inputbox widget\nAs you can see in Figure 18-4, the inputbox provides two buttons — OK and Cancel. If \nthe Cancel button is selected, the exit status of the command is 1; otherwise, the exit \nstatus is 0:\n $ dialog --inputbox \"Enter your age:\" 10 20 2>age.txt\n $ echo $?\n 0\n $ cat age.txt\n 12$\nYou’ll notice when you use the cat command to display the contents of the text fi le that \nthere’s no newline character after the value. This enables you to easily redirect the fi le con-\ntents to a variable in a shell script to extract the string entered by the user.\nThe textbox widget\nThe textbox widget is a great way to display lots of information in a window. It produces \na scrollable window containing the text from a fi le specifi ed in the parameters:\n $ dialog --textbox /etc/passwd 15 45\nThe contents of the /etc/passwd fi le are shown within the scrollable text window, as \nillustrated in Figure 18-5.\n\n489\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  489\n18\nFIGURE 18-5\nThe textbox widget\nYou can use the arrow keys to scroll left and right, as well as up and down in the text \nfi le. The bottom line in the window shows the percent location within the fi le that \nyou’re viewing. The textbox contains only a single Exit button, which should be selected \nto exit the widget.\nThe menu widget\nThe menu widget allows you to create a window version of the text menu we created earlier \nin this chapter. You simply provide a selection tag and the text for each item:\n $ dialog --menu \"Sys Admin Menu\" 20 30 10 1 \"Display disk space\"\n 2 \"Display users\" 3 \"Display memory usage\" 4 \"Exit\" 2> test.txt\nThe fi rst parameter defi nes a title for the menu. The next two parameters defi ne the height \nand width of the menu window, while the third parameter defi nes the number of menu \nitems that appear in the window at one time. If there are more menu items, you can scroll \nthrough them using the arrow keys.\nFollowing those parameters, you must add menu item pairs. The fi rst element is the tag \nused to select the menu item. Each tag should be unique for each menu item and can be \nselected by pressing the appropriate key on the keyboard. The second element is the text \nused in the menu. Figure 18-6 demonstrates the menu produced by the example command.\n\n490\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  490\nFIGURE 18-6\nThe menu widget with menu items\nIf the user selects a menu item by pressing the appropriate key for the tag, that menu item \nis highlighted but not selected. A selection isn’t made until the OK button is selected by \nusing either the mouse or the Enter key. The \ndialog command sends the selected menu \nitem text to \nSTDERR, which you can redirect as needed.\nThe fselect widget\nThere are several fancy built-in widgets provided by the dialog command. The fselect \nwidget is extremely handy when working with fi lenames. Instead of forcing the user to \ntype a fi lename, you can use the \nfselect widget to browse to the fi le location and select \nthe fi le, as shown in Figure 18-7.\nThe \nfselect widget format looks like:\n $ dialog --title \"Select a file\" --fselect $HOME/ 10 50 2>file.txt\nThe fi rst parameter after the fselect option is the starting folder location used in the \nwindow. The \nfselect widget window consists of a directory listing on the left side, a fi le \nlisting on the right side that shows all the fi les in the selected directory, and a simple text-\nbox that contains the currently selected fi le or directory. You can manually type a fi lename \nin the textbox, or you can use the directory and fi le listings to select one (use the spacebar \nto select a fi le to add to the textbox).\n\n491\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  491\n18\nFIGURE 18-7\nThe fselect widget\nThe dialog options\nIn addition to the standard widgets, you can customize lots of different options in the dia-\nlog command. You’ve already seen the \n--title parameter in action. This allows you to set \na title for the widget that appears at the top of the window.\nLots of other options allow you to completely customize both the appearance and the \nbehavior of your windows. Table 18-2 shows the options available for the \ndialog \ncommand.\nTABLE 18-2  The dialog Command Options\nOptionDescription\n--add-widget\nProceeds to the next dialog unless Esc or the Cancel button \nhas been pressed\n--aspect ratioSpecifi es the width/height aspect ratio of the window\n--backtitle titleSpecifi es a title to display on the background, at the top of the \nscreen\n--begin x ySpecifi es the starting location of the top-left corner of the \nwindow\n--cancel-label labelSpecifi es an alternative label for the Cancel button\nContinues\n\n492\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  492\nOptionDescription\n--clear\nClears the display using the default dialog background color\n--colors\nEmbeds ANSI color codes in dialog text\n--cr-wrap\nAllows newline characters in dialog text and forces a line wrap\n--create-rc fileDumps a sample confi guration fi le to the specifi ed fi le\n--defaultno\nMakes the default of a yes/no dialog No\n--default-item stringSets the default item in a checklist, form, or menu dialog\n--exit-label labelSpecifi es an alternative label for the Exit button\n--extra-button\nDisplays an extra button between the OK and Cancel buttons\n--extra-label labelSpecifi es an alternative label for the Extra button\n--help\nDisplays the dialog command help message\n--help-button\nDisplays a Help button after the OK and Cancel buttons\n--help-label labelSpecifi es an alternative label for the Help button\n--help-status\nWrites the checklist, radiolist, or form information after the \nhelp information in the Help button was selected\n--ignore\nIgnores options that dialog does not recognize\n--input-fd fdSpecifi es an alternative fi le descriptor, other than STDIN\n--insecure\nChanges the password widget to display asterisks when \ntyping\n--item-help\nAdds a help column at the bottom of the screen for each tag \nin a checklist, radiolist, or menu for the tag item\n--keep-window\nDoesn’t clear old widgets from the screen\n--max-input sizeSpecifi es a maximum string size for the input; default is 2048\n--nocancel\nSuppresses the Cancel button\n--no-collapse\nDoesn’t convert tabs to spaces in dialog text\n--no-kill\nPlaces the tailboxbg dialog in background and disables \nSIGHUP for the process\n--no-label labelSpecifi es an alternative label for the No button\n--no-shadow\nDoesn’t display shadows for dialog windows\n--ok-label labelSpecifi es an alternative label for the OK button\n--output-fd fdSpecifi es an alternative output fi le descriptor other than \nSTDERR\n--print-maxsize\nPrints the maximum size of dialog windows allowed to the \noutput\nTABLE 18-2   (continued)\n\n493\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  493\n18\n--print-size\nPrints the size of each dialog window to the output\n--print-version\nPrints the dialog version to output\n--separate-output\nOutputs the result of a checklist widget one line at a time with \nno quoting\n--separator stringSpecifi es a string that separates the output for each widget\n--separate-widget \nstring\nSpecifi es a string that separates the output for each widget\n--shadow\nDraws a shadow to the right and bottom of each window\n--single-quoted\nUses single quoting if needed for the checklist output\n--sleep sec\nDelays for the specifi ed number of seconds after processing \nthe dialog window\n--stderr\nSends output to STDERR — the default behavior\n--stdout\nSends output to STDOUT\n--tab-correct\nConverts tabs to spaces\n--tab-len nSpecifi es the number of spaces a tab character uses; default is 8\n--timeout secSpecifi es the number of seconds before exiting with an error \ncode if no user input\n--title titleSpecifi es the title of the dialog window\n--trim\nRemoves leading spaces and newline characters from dialog text\n--visit-items\nModifi es the tab stops in the dialog window to include the list \nof items\n--yes-label labelSpecifi es an alternative label for the Yes button\nThe --backtitle option is a handy way to create a common title for your menu through \nthe script. If you specify it for each dialog window, it persists throughout your application, \ncreating a professional look to your script.\nAs you can tell from Table 18-2, you can overwrite any of the button labels in your dialog \nwindow. This feature allows you to create just about any window situation you need.\nUsing the dialog command in a script\nUsing the dialog command in your scripts is a snap. There are just two things you must \nremember:\n ■\nCheck the exit status of the dialog command if there’s a Cancel or No button \navailable.\n ■\nRedirect STDERR to retrieve the output value.\n\n494\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  494\nIf you follow these two rules, you’ll have a professional-looking interactive script in no \ntime. Here’s an example using dialog widgets to reproduce the system admin menu created \nearlier in the chapter:\n $ cat menu3\n #!/bin/bash\n # using dialog to create a menu\n temp=$(mktemp -t test.XXXXXX)\n temp2=$(mktemp -t test2.XXXXXX)\n function diskspace {\n    df -k > $temp\n    dialog --textbox $temp 20 60\n }\n function whoseon {\n    who > $temp\n    dialog --textbox $temp 20 50\n }\n function memusage {\n    cat /proc/meminfo > $temp\n    dialog --textbox $temp 20 50\n }\n while [ 1 ]\n do\n dialog --menu \"Sys Admin Menu\" 20 30 10 1 \"Display disk space\" 2\n \"Display users\" 3 \"Display memory usage\" 0 \"Exit\" 2> $temp2\n if [ $? -eq 1 ]\n then\n    break\n fi\n selection=$(cat $temp2)\n case $selection in\n 1)\n    diskspace ;;\n 2)\n    whoseon ;;\n 3)\n    memusage ;;\n 0)\n    break ;;\n *)\n    dialog --msgbox \"Sorry, invalid selection\" 10 30\n\n495\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  495\n18\n esac\n done\n rm -f $temp 2> /dev/null\n rm -f $temp2 2> /dev/null\n $\nThe script uses the while loop with a constant true value to create an endless loop dis-\nplaying the menu dialog. This means that, after every function, the script returns to dis-\nplaying the menu.\nThe \nmenu dialog includes a Cancel button, so the script checks the exit status of the dialog \ncommand in case the user presses the Cancel button to exit. Because it’s in a \nwhile loop, \nexiting is as easy as using the \nbreak command to jump out of the while loop.\nThe script uses the \nmktemp command to create two temporary fi les for holding data for \nthe \ndialog commands. The fi rst one, $temp, is used to hold the output of the df, whoe-\nson\n, and meminfo commands so they can be displayed in the textbox dialog (see Figure \n18-8). The second temporary fi le, \n$temp2, is used to hold the selection value from the \nmain menu dialog.\nFIGURE 18-8\nThe meminfo command output displayed using the textbox dialog option\nNow this is starting to look like a real application that you can show off to people!\n\n496\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  496\nGetting Graphic\nIf you’re looking for even more graphics for your interactive scripts, you can go one step \nfurther. Both the KDE and GNOME desktop environments (see Chapter 1) have expanded on \nthe \ndialog command idea and include commands that produce X Window graphical widgets \nfor their respective environments.\nThis section describes the kdialog and zenity packages, which provide graphical window \nwidgets for the KDE and GNOME desktops, respectively.\nThe KDE environment\nThe KDE graphical environment includes the kdialog package by default. The kdialog pack-\nage uses the \nkdialog command to generate standard windows, similar to the dialog-style \nwidgets, within your KDE desktop. However, instead of having the clunky feel to them, \nthese windows blend right in with the rest of your KDE application windows! This allows \nyou to produce Windows-quality user interfaces directly from your shell scripts!\nJust because your Linux distribution uses the KDE desktop doesn’t necessarily mean it has the kdialog package \ninstalled by default. You may need to manually install it from the distribution repository.\nkdialog widgets\nJust like the dialog command, the kdialog command uses command line options to \nspecify what type of window widget to use. The following is the format of the \nkdialog \ncommand:\n kdialog display-options window-options arguments\nThe window-options options allow you to specify what type of window widget to use. \nThe available options are shown in Table 18-3.\nTABLE 18-3    kdialog  Window  Options\nOptionDescription\n--checklist title [tag \nitem status]\nA checklist menu, with status specifying if the item is checked \nor not\n--error textError message box\n--inputbox text [init]Input textbox where you can specify the default value using \nthe \ninit value\n--menu title [tag item]Menu selection box title and a list of items identifi ed by a tag\n\n497\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  497\n18\n--msgbox textSimple message box with specifi ed text\n--password textPassword input textbox that hides user input\n--radiolist title [tag \nitem status]\nA radiolist menu, with status specifying if the item is selected \nor not\n--separate-output\nReturns items on separate lines for checklist and radiolist \nmenus\n--sorry textSorry message box\n--textbox file [width] \n[height]\nTextbox displaying the contents of file, alternatively speci-\nfi ed by \nwidth and height\n--title\n titleSpecifi es a title for the TitleBar area of the dialog window\n--warningyesno textWarning message box with Yes and No buttons\n--warningcontinuecancel \ntext\nWarning message box with Continue and Cancel buttons\n--warningyesnocancel \ntext\nWarning message box with Yes, No, and Cancel buttons\n--yesno textQuestion box with Yes and No buttons\n--yesnocancel text\nQuestion box with Yes, No, and Cancel buttons\nAs you can see from Table 18-3, all the standard window dialog box types are represented. \nHowever, when you use a kdialog window widget, it appears as a separate window in the \nKDE desktop, not inside the terminal emulator session!\nThe \nchecklist and radiolist widgets allow you to defi ne individual items in the lists \nand whether they are selected by default:\n$kdialog --checklist \"Items I need\" 1 \"Toothbrush\" on 2 \"Toothpaste\"\n  off 3 \"Hair brush\" on 4 \"Deodorant\" off 5 \"Slippers\" off\nThe resulting checklist window is shown in Figure 18-9.\nThe items specifi ed as “on” are highlighted in the checklist. To select or deselect an item in \nthe checklist, just click it. If you select the OK button, the \nkdialog sends the tag values \nto \nSTDOUT:\n\"1\" \"3\"\n$\nWhen you press the Enter key, the kdialog box appears with the selections. When you click \nthe OK or Cancel buttons, the \nkdialog command returns each tag as a string value to \nSTDOUT (these are the \"1\", and \"3\" values you see in the output). Your script must be able \nto parse the resulting values and match them with the original values.\n\n498\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  498\nFIGURE 18-9\nA kdialog checklist dialog window\nUsing kdialog\nYou can use the kdialog window widgets in your shell scripts similarly to how you use \nthe \ndialog widgets. The big difference is that the kdialog window widgets output values \nusing \nSTDOUT instead of STDERR.\nHere’s a script that converts the sys admin menu created earlier into a KDE application:\n $ cat menu4\n #!/bin/bash\n # using kdialog to create a menu\n temp=$(mktemp -t temp.XXXXXX)\n temp2=$(mktemp -t temp2.XXXXXX)\n function diskspace {\n    df -k > $temp\n    kdialog --textbox $temp 1000 10\n }\n function whoseon {\n    who > $temp\n    kdialog --textbox $temp 500 10\n }\n function memusage {\n    cat /proc/meminfo > $temp\n    kdialog --textbox $temp 300 500\n }\n while [ 1 ]\n do\n\n499\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  499\n18\nkdialog --menu \"Sys Admin Menu\" \"1\" \"Display diskspace\" \"2\" \"Display\n users\" \"3\" \"Display memory usage\" \"0\" \"Exit\" > $temp2\n if [ $? -eq 1 ]\n then\n    break\n fi\n selection=$(cat $temp2)\n case $selection in\n 1)\n    diskspace ;;\n 2)\n    whoseon ;;\n 3)\n    memusage ;;\n 0)\n    break ;;\n *)\n    kdialog --msgbox \"Sorry, invalid selection\"\n esac\n done\n $\nThere isn’t much difference in the script from using the kdialog command and the \ndialog command. The resulting main menu generated is shown in Figure 18-10.\nFIGURE 18-10\nThe sys admin menu script using kdialog\nNow your simple shell script looks just like a real KDE application! There’s no limit to what \nyou can do with your interactive scripts now.\n\n500\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  500\nThe GNOME environment\nThe GNOME graphical environment supports two popular packages that can generate stan-\ndard windows:\n ■\ngdialog\n ■\nzenity\nBy far, zenity is the most commonly available package found in most GNOME desktop Linux \ndistributions (it’s installed by default in both Ubuntu and Fedora). This section describes \nthe features of zenity and demonstrates how to use it in your shell scripts.\nzenity Widgets\nAs you would expect, zenity allows you to create different windows widgets by using com-\nmand line options. Table 18-4 shows the different widgets that zenity can produce.\nTABLE 18- 4    The zenity Windows Widgets\nOptionDescription\n--calendar\nDisplays a full month calendar\n--entry\nDisplays a text entry dialog window\n--error\nDisplays an error message dialog window\n--file-selection\nDisplays a full pathname and fi lename dialog window\n--info\nDisplays an informational dialog window\n--list\nDisplays a checklist or radiolist dialog window\n--notification\nDisplays a notifi cation icon\n--progress\nDisplays a progress bar dialog window\n--question\nDisplays a yes/no question dialog window\n--scale\nDisplays a scale dialog window\n--text-info\nDisplays a textbox containing text\n--warning\nDisplays a warning dialog window\nThe zenity command line program works somewhat differently than the kdialog and \ndialog programs. Many of the widget types are defi ned using additional options on the \ncommand line, instead of including them as arguments to an option.\nThe \nzenity command does offer some pretty cool advanced dialog windows. The \ncalendar option produces a full month calendar, as shown in Figure 18-11.\n\n501\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  501\n18\nFIGURE 18-11\nThe zenity calendar dialog window\nWhen you select a date from the calendar, the zenity command returns the value to \nSTDOUT, just like kdialog:\n $ zenity --calendar\n 12/25/2011\n $\nAnother pretty cool window in zenity is the fi le selection option, shown in Figure 18-12.\nYou can use the dialog window to browse to any directory location on the system (as long \nas you have the privileges to view the directory) and select a fi le. When you select a fi le, \nthe \nzenity command returns the full fi le and pathname:\n $ zenity --file-selection\n /home/ubuntu/menu5\n $\nWith tools like that at your disposal, the sky’s the limit with your shell script creations!\nUsing zenity in scripts\nAs you would expect, zenity performs well in shell scripts. Unfortunately, zenity chose not \nto follow the option convention used in \ndialog and kdialog, so converting any existing \ninteractive scripts to zenity may prove challenging.\n\n502\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  502\nFIGURE 18-12\nThe zenity fi le selection dialog window\nIn converting the sys admin menu from kdialog to zenity, we had to do quite a bit of \nmanipulation of the widget defi nitions:\n $cat menu5\n #!/bin/bash\n # using zenity to create a menu\n temp=$(mktemp -t temp.XXXXXX)\n temp2=$(mktemp -t temp2.XXXXXX)\n function diskspace {\n    df -k > $temp\n    zenity --text-info --title \"Disk space\" --filename=$temp\n --width 750 --height 10\n }\n function whoseon {\n    who > $temp\n    zenity --text-info --title \"Logged in users\" --filename=$temp\n\n503\nChapter 18: Writing Scripts for Graphical Desktops\nc18.indd  12/16/2014  Page  503\n18\n --width 500 --height 10\n }\n function memusage {\n    cat /proc/meminfo > $temp\n    zenity --text-info --title \"Memory usage\" --filename=$temp\n --width 300 --height 500\n }\n while [ 1 ]\n do\n zenity --list --radiolist --title \"Sys Admin Menu\" --column \"Select\"\n--column \"Menu Item\" FALSE \"Display diskspace\" FALSE \"Display users\"\n FALSE \"Display memory usage\" FALSE \"Exit\" > $temp2\n if [ $? -eq 1 ]\n then\n    break\n fi\n selection=$(cat $temp2)\n case $selection in\n \"Display disk space\")\n    diskspace ;;\n \"Display users\")\n    whoseon ;;\n \"Display memory usage\")\n    memusage ;;\n Exit)\n    break ;;\n *)\n    zenity --info \"Sorry, invalid selection\"\n esac\n done\n $\nBecause zenity doesn’t support the menu dialog window, we used a radiolist type window \nfor the main menu, as shown in Figure 18-13.\nThe radiolist uses two columns, each with a column heading. The fi rst column includes \nthe radio buttons to select. The second column is the item text. The radiolist also doesn’t \nuse tags for the items. When you select an item, the full text of the item is returned to \nSTDOUT. This makes life a little more interesting for the case command. You must use the \nfull text from the items in the case options. If there are any spaces in the text, you need to \nuse quotation marks around the text.\nUsing the zenity package, you can add a Windows feel to your interactive shell scripts in \nthe GNOME desktop.\n\n504\nPart III: Advanced Shell Scripting\nc18.indd  12/16/2014  Page  504\nFIGURE 18-13\nThe sys admin menu using zenity\nSummary\nInteractive shell scripts have a reputation for being dull and boring. You can change that by \nusing a few different techniques and tools available on most Linux systems. First, you can create \nmenu systems for your interactive scripts by using the \ncase command and shell script functions.\nThe \nmenu command allows you to paint a menu, using the standard echo command, and \nread a response from the user, using the \nread command. The case command then selects \nthe appropriate shell script function based on the value entered.\nThe \ndialog program provides several prebuilt text widgets for creating Windows-like \nobjects on a text-based terminal emulator. You can create dialog boxes for displaying text, \nentering text, and choosing fi les and dates by using the dialog program. This helps bring \neven more life to your shell script.\nIf you’re running your shell scripts in a graphical X Window environment, you can utilize \neven more tools in your interactive scripts. For the KDE desktop, there’s the \nkdialog pro-\ngram. This program provides simple commands to create windows widgets for all the basic \nwindows functions. For the GNOME desktop, there are the \ngdialog and zenity programs. \nEach of these programs provides window widgets that blend into the GNOME desktop just \nlike a real Windows application.\nThe next chapter dives into the subject of editing and manipulating text data fi les. Often \nthe biggest use of shell scripts revolves around parsing and displaying data in text fi les \nsuch as log and error fi les. The Linux environment includes two very useful tools, \nsed and \ngawk , for working with text data in your shell scripts. The next chapter introduces you to \nthese tools, and shows the basics of how to use them. \n\n505\nc19.indd  12/16/2014  Page  505\nCHAPTER \n19\nIntroducing sed and gawk\nIN THIS CHAPTER\nLearning about the sed Editor\nGetting introduced to the gawk Editor\nExploring sed Editor basics\nB\ny far, one of the most common functions that people use shell scripts for is to work with text \nfi les. Between examining log fi les, reading confi guration fi les, and handling data elements, \nshell scripts can help automate the mundane tasks of manipulating any type of data con-\ntained in text fi les. However, trying to manipulate the contents of text fi les using just shell script \ncommands can be somewhat awkward. If you perform any type of data manipulation in your shell \nscripts, you want to become familiar with the \nsed and gawk tools available in Linux. These tools \ncan greatly simplify any data-handling tasks you need to perform.\nManipulating Text\nChapter 10 showed you how to edit text fi les using different editor programs available in the Linux \nenvironment. These editors enable you to easily manipulate text contained in a text fi le by using \nsimple commands or mouse clicks.\nThere are times, however, when you’ll fi nd yourself wanting to manipulate text in a text fi le on the \nfl y, without having to pull out a full-fl edged interactive text editor. In these situations, it would be \nuseful to have a simple command line editor that could easily format, insert, modify, or delete text \nelements automatically.\nThe Linux system provides two common tools for doing just that. This section describes the two \nmost popular command line editors used in the Linux world, \nsed and gawk.\nGetting to know the sed editor\nThe sed editor is called a stream editor, as opposed to a normal interactive text editor. In an inter-\nactive text editor, such as \nvim, you interactively use keyboard commands to insert, delete, or \nreplace text in the data. A stream editor edits a stream of data based on a set of rules you supply \nahead of time, before the editor processes the data.\n\n506\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  506\nThe sed editor can manipulate data in a data stream based on commands you either enter \ninto the command line or store in a command text fi le. The \nsed editor does these things:\n \n1.  Reads one data line at a time from the input\n \n2.  Matches that data with the supplied editor commands \n \n3.  Changes data in the stream as specifi ed in the commands\n \n4.  Outputs the new data to STDOUT\nAfter the stream editor matches all the commands against a line of data, it reads the next \nline of data and repeats the process. After the stream editor processes all the lines of data \nin the stream, it terminates.\nBecause the commands are applied sequentially line by line, the \nsed editor makes only one \npass through the data stream to make the edits. This makes the \nsed editor much faster than \nan interactive editor and allows you to quickly make changes to data in a fi le on the fl y.\nHere’s the format for using the \nsed command:\nsed options script file\nThe options parameters allow you to customize the behavior of the sed command and \ninclude the options shown in Table 19-1.\nTABLE 19-1    The sed Command Options\nOptionDescription\n-e script\nAdds commands specifi ed in the script to the commands run while process-\ning the input\n-f file\nAdds the commands specifi ed in the fi le to the commands run while process-\ning the input\n-n\nDoesn’t produce output for each command, but waits for the print \ncommand\nThe script parameter specifi es a single command to apply against the stream data. If more \nthan one command is required, you must use either the \n-e option to specify them in the \ncommand line or the \n-f option to specify them in a separate fi le. Numerous commands are \navailable for manipulating data. We examine some of the basic commands used by the \nsed \neditor in this chapter and then look at some of the more advanced commands in Chapter 21.\nDefining an editor command in the command line\nBy default, the sed editor applies the specifi ed commands to the STDIN input stream. This \nallows you to pipe data directly to the \nsed editor for processing. Here’s a quick example \ndemonstrating how to do this:\n\n507\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  507\n19\n$ echo \"This is a test\" | sed 's/test/big test/'\nThis is a big test\n$\nThis example uses the s command in the sed editor. The s command substitutes a second \ntext string for the fi rst text string pattern specifi ed between the forward slashes. In this \nexample, the words \nbig test were substituted for the word test.\nWhen you run this example, it should display the results almost instantaneously. That’s the \npower of using the \nsed editor. You can make multiple edits to data in about the same time \nit takes for some of the interactive editors just to start up.\nOf course, this simple test just edited one line of data. You should get the same speedy \nresults when editing complete fi les of data:\n$ cat data1.txt\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\n$\n$ sed 's/dog/cat/' data1.txt\nThe quick brown fox jumps over the lazy cat.\nThe quick brown fox jumps over the lazy cat.\nThe quick brown fox jumps over the lazy cat.\nThe quick brown fox jumps over the lazy cat.\n$\nThe sed command executes and returns the data almost instantaneously. As it processes \neach line of data, the results are displayed. You’ll start seeing results before the \nsed editor \ncompletes processing the entire fi le.\nIt’s important to note that the \nsed editor doesn’t modify the data in the text fi le itself. It \nonly sends the modifi ed text to \nSTDOUT. If you look at the text fi le, it still contains the \noriginal data:\n$ cat data1.txt\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\n$\nUsing multiple editor commands in the command line\nTo execute more than one command from the sed command line, just use the -e option:\n$ sed -e 's/brown/green/; s/dog/cat/' data1.txt\nThe quick green fox jumps over the lazy cat.\n\n508\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  508\nThe quick green fox jumps over the lazy cat.\nThe quick green fox jumps over the lazy cat.\nThe quick green fox jumps over the lazy cat.\n$\nBoth commands are applied to each line of data in the fi le. The commands must be sepa-\nrated with a semicolon, and there shouldn’t be any spaces between the end of the command \nand the semicolon.\nInstead of using a semicolon to separate the commands, you can use the secondary prompt \nin the bash shell. Just enter the fi rst single quotation mark to open the \nsed program script \n(\nsed editor command list), and bash continues to prompt you for more commands until you \nenter the closing quotation mark:\n$ sed -e '\n> s/brown/green/\n> s/fox/elephant/\n> s/dog/cat/' data1.txt\nThe quick green elephant jumps over the lazy cat.\nThe quick green elephant jumps over the lazy cat.\nThe quick green elephant jumps over the lazy cat.\nThe quick green elephant jumps over the lazy cat.\n$\nYou must remember to fi nish the command on the same line where the closing single quota-\ntion mark appears. After the bash shell detects the closing quotation mark, it processes the \ncommand. After it starts, the \nsed command applies each command you specifi ed to each \nline of data in the text fi le.\nReading editor commands from a file\nFinally, if you have lots of sed commands you want to process, it is often easier to just \nstore them in a separate fi le. Use the \n-f option to specify the fi le in the sed command:\n$ cat script1.sed\ns/brown/green/\ns/fox/elephant/\ns/dog/cat/\n$\n$ sed -f script1.sed data1.txt\nThe quick green elephant jumps over the lazy cat.\nThe quick green elephant jumps over the lazy cat.\nThe quick green elephant jumps over the lazy cat.\nThe quick green elephant jumps over the lazy cat.\n$\nIn this case, you don’t put a semicolon after each command. The sed editor knows that \neach line contains a separate command. As with entering commands on the command line, \n\n509\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  509\n19\nthe sed editor reads the commands from the specifi ed fi le and applies them to each line in \nthe data fi le.\nIt can be easy to confuse your sed editor script fi les with your bash shell script fi les. To eliminate confusion, use a .sed \nfi le extension on your \nsed script fi les.\nWe’ll look at some other sed editor commands that come in handy for manipulating data \nin the “Commanding at the \nsed Editor Basics” section. Before that, let’s quickly look at the \nother Linux data editor.\nGetting to know the gawk program\nAlthough the sed editor is a handy tool for modifying text fi les on the fl y, it has its limita-\ntions. Often, you need a more advanced tool for manipulating data in a fi le, one that pro-\nvides a more programming-like environment allowing you to modify and reorganize data in \na fi le. This is where \ngawk comes in.\nThe gawk program is not installed by default on all distributions. If your Linux distribution does not have the gawk \nprogram, install the \ngawk package using Chapter 9 as a guide.\nThe gawk program is the GNU version of the original awk program in Unix. The gawk \nprogram takes stream editing one step further than the \nsed editor by providing a program-\nming language instead of just editor commands. Within the \ngawk programming language, \nyou can do the following:\n ■\nDefi ne variables to store data.\n ■\nUse arithmetic and string operators to operate on data.\n ■\nUse structured programming concepts, such as if-then statements and loops, to \nadd logic to your data processing.\n ■\nGenerate formatted reports by extracting data elements within the data fi le and \nrepositioning them in another order or format.\nThe \ngawk program’s report-generating capabilities are often used for extracting data \nelements from large bulky text fi les and formatting them into a readable report. The per-\nfect example of this is formatting log fi les. Trying to pore through lines of errors in a log \nfi le can be diffi cult. The \ngawk program allows you to fi lter just the data elements you want \nto view from the log fi le, and then you can format them in a manner that makes reading \nthe important data easier.\n\n510\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  510\nVisiting the gawk command format\nHere’s the basic format of the gawk program:\ngawk options program file\nTable 19-2 shows the options available with the gawk program.\nTABLE 19-2    The  gawk  Options\nOptionDescription\n-F fs\nSpecifi es a fi le separator for delineating data fi elds in a line\n-f file\nSpecifi es a fi le name to read the program from\n-v var=value\nDefi nes a variable and default value used in the gawk program\n-mf N\nSpecifi es the maximum number of fi elds to process in the data fi le\n-mr N\nSpecifi es the maximum record size in the data fi le\n-W keyword\nSpecifi es the compatibility mode or warning level for gawk\nThe command line options provide an easy way to customize features in the gawk program. \nWe’ll look more closely at these as we explore \ngawk.\nThe power of \ngawk is in the program script. You can write scripts to read the data within a \ntext line and then manipulate and display the data to create any type of output report.\nReading the program script from the command line\nA gawk program script is defi ned by opening and closing braces. You must place script com-\nmands between the two braces (\n{}). If you incorrectly use a parenthesis instead of a brace \nto enclose your gawk script, you get error messages, similar to the following:\n$ gawk '(print \"Hello World!\"}'\ngawk: (print \"Hello World!\"}\ngawk:  ^ syntax error\nBecause the gawk command line assumes that the script is a single text string, you must \nalso enclose your script in single quotation marks. Here’s an example of a simple \ngawk \nprogram script specifi ed on the command line:\n$ gawk '{print \"Hello World!\"}'\nThe program script defi nes a single command, the print command. The print command \ndoes what it says: It prints text to \nSTDOUT. If you try running this command, you’ll be \nsomewhat disappointed, because nothing happens right away. Because no fi lename was \ndefi ned in the command line, the \ngawk program retrieves data from STDIN. When you run \nthe program, it just waits for text to come in via \nSTDIN.\n\n511\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  511\n19\nIf you type a line of text and press the Enter key, gawk runs the text through the program \nscript. Just like the \nsed editor, the gawk program executes the program script on each line \nof text available in the data stream. Because the program script is set to display a fi xed text \nstring, no matter what text you enter in the data stream, you get the same text output:\n$ gawk '{print \"Hello World!\"}'\nThis is a test\nHello World!\nhello\nHello World!\nThis is another test\nHello World!\nTo terminate the gawk program, you must signal that the data stream has ended. The bash \nshell provides a key combination to generate an End-of-File (EOF) character. The Ctrl+D key \ncombination generates an EOF character in bash. Using that key combination terminates \nthe \ngawk program and returns you to a command line interface prompt.\nUsing data field variables\nOne of the primary features of gawk is its ability to manipulate data in the text fi le. It does \nthis by automatically assigning a variable to each data element in a line. By default, \ngawk \nassigns the following variables to each data fi eld it detects in the line of text:\n ■\n$0 represents the entire line of text.\n ■\n$1 represents the fi rst data fi eld in the line of text.\n ■\n$2 represents the second data fi eld in the line of text.\n ■\n$n represents the nth data fi eld in the line of text.\nEach data fi eld is determined in a text line by a fi eld separation character. When \ngawk reads \na line of text, it delineates each data fi eld using the defi ned fi eld separation character. The \ndefault fi eld separation character in \ngawk is any whitespace character (such as the tab or \nspace characters).\nHere’s an example \ngawk program that reads a text fi le and displays only the fi rst data fi eld \nvalue:\n$ cat data2.txt\nOne line of test text.\nTwo lines of test text.\nThree lines of test text.\n$\n$ gawk '{print $1}' data2.txt\nOne\nTwo\nThree\n$\n\n512\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  512\nThis program uses the $1 fi eld variable to display only the fi rst data fi eld for each line \nof text.\nIf you’re reading a fi le that uses a different fi eld separation character, you can specify it by \nusing the \n-F option:\n$ gawk -F: '{print $1}' /etc/passwd\nroot\nbin\ndaemon\nadm\nlp\nsync\nshutdown\nhalt\nmail\n[...]\nThis short program displays the fi rst data fi eld in the password fi le on the system. Because \nthe \n/etc/passwd fi le uses a colon to separate the data fi elds, if you want to separate each \ndata element, you must specify it as the fi eld separation character in the \ngawk options.\nUsing multiple commands in the program script\nA programming language wouldn’t be very useful if you could only execute one command. \nThe \ngawk programming language allows you to combine commands into a normal program. \nTo use multiple commands in the program script specifi ed on the command line, just place a \nsemicolon between each command:\n$ echo \"My name is Rich\" | gawk '{$4=\"Christine\"; print $0}'\nMy name is Christine\n$\nThe fi rst command assigns a value to the $4 fi eld variable. The second command then \nprints the entire data fi eld. Notice from the output that the \ngawk program replaced the \nfourth data fi eld in the original text with the new value.\nYou can also use the secondary prompt to enter your program script commands one line at \na time:\n$ gawk '{\n> $4=\"Christine\"\n> print $0}'\nMy name is Rich\nMy name is Christine\n$\nAfter you open the single quotation mark, the bash shell provides the secondary prompt to \nprompt you for more data. You can add your commands one at a time on each line until you \n\n513\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  513\n19\nenter the closing single quotation mark. Because no fi lename was defi ned in the command \nline, the \ngawk program retrieves data from STDIN. When you run the program, it waits for \ntext to come in via \nSTDIN. To exit the program, just press the Ctrl+D key combination to \nsignal the end of the data.\nReading the program from a file\nAs with the sed editor, the gawk editor allows you to store your programs in a fi le and \nrefer to them in the command line:\n$ cat script2.gawk\n{print $1 \"'s home directory is \" $6}\n$\n$ gawk -F: -f script2.gawk /etc/passwd\nroot's home directory is /root\nbin's home directory is /bin\ndaemon's home directory is /sbin\nadm's home directory is /var/adm\nlp's home directory is /var/spool/lpd\n[...]\nChristine's home directory is /home/Christine\nSamantha's home directory is /home/Samantha\nTimothy's home directory is /home/Timothy\n$\nThe script2.gawk program script uses the print command again to print the /etc/\npasswd\n fi le’s home directory data fi eld (fi eld variable $6) and the userid data fi eld (fi eld \nvariable \n$1).\nYou can specify multiple commands in the program fi le. To do so, just place each command \non a separate line. You don’t need to use semicolons:\n$ cat script3.gawk\n{\ntext = \"'s home directory is \"\nprint $1 text $6\n}\n$\n$ gawk -F: -f script3.gawk /etc/passwd\nroot's home directory is /root\nbin's home directory is /bin\ndaemon's home directory is /sbin\nadm's home directory is /var/adm\nlp's home directory is /var/spool/lpd\n[...]\nChristine's home directory is /home/Christine\nSamantha's home directory is /home/Samantha\nTimothy's home directory is /home/Timothy\n$\n\n514\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  514\nThe script3.gawk program script defi nes a variable to hold a text string used in the \nprint command. Notice that gawk programs don’t use a dollar sign when referencing a \nvariable’s value, as a shell script does.\nRunning scripts before processing data\nThe gawk program also allows you to specify when the program script is run. By default, \ngawk reads a line of text from the input and then executes the program script on the data \nin the line of text. Sometimes, you may need to run a script before processing data, such \nas to create a header section for a report. The \nBEGIN keyword is used to accomplish this. It \nforces \ngawk to execute the program script specifi ed after the BEGIN keyword, before gawk \nreads the data:\n$ gawk 'BEGIN {print \"Hello World!\"}'\nHello World!\n$\nThis time the print command displays the text before reading any data. However, after it \ndisplays the text, it quickly exits, without waiting for any data.\nThe reason for this is that the \nBEGIN keyword only applies the specifi ed script before it \nprocesses any data. If you want to process data with a normal program script, you must \ndefi ne the program using another script section:\n$ cat data3.txt\nLine 1\nLine 2\nLine 3\n$\n$ gawk 'BEGIN {print \"The data3 File Contents:\"}\n> {print $0}' data3.txt\nThe data3 File Contents:\nLine 1\nLine 2\nLine 3\n$\nNow after gawk executes the BEGIN script, it uses the second script to process any fi le \ndata. Be careful when doing this; both of the scripts are still considered one text string on \nthe \ngawk command line. You need to place your single quotation marks accordingly.\nRunning scripts after processing data\nLike the BEGIN keyword, the END keyword allows you to specify a program script that \ngawk executes after reading the data:\n$ gawk 'BEGIN {print \"The data3 File Contents:\"}\n> {print $0}\n\n515\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  515\n19\n> END {print \"End of File\"}' data3.txt\nThe data3 File Contents:\nLine 1\nLine 2\nLine 3\nEnd of File\n$\nWhen the gawk program is fi nished printing the fi le contents, it executes the commands \nin the \nEND script. This is a great technique to use to add footer data to reports after all the \nnormal data has been processed.\nYou can put all these elements together into a nice little program script fi le to create a full \nreport from a simple data fi le:\n$ cat script4.gawk\nBEGIN {\nprint \"The latest list of users and shells\"\nprint \" UserID \\t Shell\"\nprint \"-------- \\t -------\"\nFS=\":\"\n}\n{\nprint $1 \"     \\t \"  $7\n}\nEND {\nprint \"This concludes the listing\"\n}\n$\nThis script uses the BEGIN script to create a header section for the report. It also defi nes a \nspecial variable called \nFS. This is yet another way to defi ne the fi eld separation character. \nThis way, you don’t have to depend on the script’s user to defi ne the fi eld separation char-\nacter in the command line options.\nHere’s a somewhat truncated output from running this \ngawk program script:\n$ gawk -f script4.gawk /etc/passwd\nThe latest list of users and shells\n UserID          Shell\n--------         -------\nroot             /bin/bash\nbin              /sbin/nologin\ndaemon           /sbin/nologin\n[...]\nChristine        /bin/bash\n\n516\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  516\nmysql            /bin/bash\nSamantha         /bin/bash\nTimothy          /bin/bash\nThis concludes the listing\n$\nAs expected, the BEGIN script created the header text, the program script processed the \ninformation from the specifi ed data fi le (the \n/etc/passwd fi le), and the END script pro-\nduced the footer text. The \n\\t within the print command produces some nicely formatted \ntabbed output.\nThis gives you a small taste of the power available when you use simple \ngawk scripts. \nChapter 22 describes some more basic programming principles available for your \ngawk \nscripts, along with some even more advanced programming concepts you can use in your \ngawk program scripts to create professional looking reports from even the most cryptic \ndata fi les.\nCommanding at the sed Editor Basics\nThe key to successfully using the sed editor is to know its myriad of commands and for-\nmats, which help you to customize your text editing. This section describes some of the \nbasic commands and features you can incorporate into your script to start using the \nsed \neditor.\nIntroducing more substitution options\nYou’ve already seen how to use the s command to substitute new text for the text in a line. \nHowever, a few additional options are available for the \nsubstitute command that can \nhelp make your life easier.\nSubstituting flags\nThere’s a caveat to how the substitute command replaces matching patterns in the text \nstring. Watch what happens in this example:\n$ cat data4.txt\nThis is a test of the test script.\nThis is the second test of the test script.\n$\n$ sed 's/test/trial/' data4.txt\nThis is a trial of the test script.\nThis is the second trial of the test script.\n$\n\n517\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  517\n19\nThe substitute command works fi ne in replacing text in multiple lines, but by default, it \nreplaces only the fi rst occurrence in each line. To get the \nsubstitute command to work \non different occurrences of the text, you must use a substitution fl ag. The substitution fl ag \nis set after the substitution command strings:\ns/pattern/replacement/flags\nFour types of substitution fl ags are available:\n ■\nA number, indicating the pattern occurrence for which new text should be \nsubstituted\n ■\ng, indicating that new text should be substituted for all occurrences of the \nexisting text\n ■\np, indicating that the contents of the original line should be printed\n ■\nw file, which means to write the results of the substitution to a fi le\nIn the fi rst type of substitution, you can specify which occurrence of the matching pattern \nthe \nsed editor should substitute new text for:\n$ sed 's/test/trial/2' data4.txt\nThis is a test of the trial script.\nThis is the second test of the trial script.\n$\nAs a result of specifying a 2 as the substitution fl ag, the sed editor replaces the pattern \nonly in the second occurrence in each line. The \ng substitution fl ag enables you to replace \nevery occurrence of the pattern in the text:\n$ sed 's/test/trial/g' data4.txt\nThis is a trial of the trial script.\nThis is the second trial of the trial script.\n$\nThe p substitution fl ag prints a line that contains a matching pattern in the substitute \ncommand. This is most often used in conjunction with the \n-n sed option:\n$ cat data5.txt\nThis is a test line.\nThis is a different line.\n$\n$ sed -n 's/test/trial/p' data5.txt\nThis is a trial line.\n$\nThe -n option suppresses output from the sed editor. However, the p substitution fl ag out-\nputs any line that has been modifi ed. Using the two in combination produces output only \nfor lines that have been modifi ed by the substitute command.\n\n518\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  518\nThe w substitution fl ag produces the same output but stores the output in the specifi ed fi le:\n$ sed 's/test/trial/w test.txt' data5.txt\nThis is a trial line.\nThis is a different line.\n$\n$ cat test.txt\nThis is a trial line.\n$\nThe normal output of the sed editor appears in STDOUT, but only the lines that include the \nmatching pattern are stored in the specifi ed output fi le.\nReplacing characters\nSometimes, you run across characters in text strings that aren’t easy to use in the substitu-\ntion pattern. One popular example in the Linux world is the forward slash (/).\nSubstituting pathnames in a fi le can get awkward. For example, if you wanted to substitute \nthe C shell for the bash shell in the \n/etc/passwd fi le, you’d have to do this:\n$ sed 's/\\/bin\\/bash/\\/bin\\/csh/' /etc/passwd\nBecause the forward slash is used as the string delimiter, you must use a backslash to \nescape it if it appears in the pattern text. This often leads to confusion and mistakes.\nTo solve this problem, the \nsed editor allows you to select a different character for the \nstring delimiter in the substitute command:\n$ sed 's!/bin/bash!/bin/csh!' /etc/passwd\nIn this example, the exclamation point is used for the string delimiter, making the path-\nnames much easier to read and understand.\nUsing addresses\nBy default, the commands you use in the sed editor apply to all lines of the text data. If \nyou want to apply a command only to a specifi c line or a group of lines, you must use line \naddressing.\nThere are two forms of line addressing in the \nsed editor:\n ■\nA numeric range of lines\n ■\nA text pattern that fi lters out a line\nBoth forms use the same format for specifying the address:\n[address]command\n\n519\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  519\n19\nYou can also group more than one command together for a specifi c address:\naddress {\n    command1\n    command2\n    command3\n}\nThe sed editor applies each of the commands you specify only to lines that match the \naddress specifi ed. This section demonstrates using both of these addressing techniques in \nyour \nsed editor scripts.\nAddressing the numeric line\nWhen using numeric line addressing, you reference lines using their line position in the \ntext stream. The \nsed editor assigns the fi rst line in the text stream as line number one and \ncontinues sequentially for each new line.\nThe address you specify in the command can be a single line number or a range of lines \nspecifi ed by a starting line number, a comma, and an ending line number. Here’s an exam-\nple of specifying a line number to which the \nsed command will be applied:\n$ sed '2s/dog/cat/' data1.txt\nThe quick brown fox jumps over the lazy dog\nThe quick brown fox jumps over the lazy cat\nThe quick brown fox jumps over the lazy dog\nThe quick brown fox jumps over the lazy dog\n$\nThe sed editor modifi ed the text only in line two per the address specifi ed. Here’s another \nexample, this time using a range of line addresses:\n$ sed '2,3s/dog/cat/' data1.txt\nThe quick brown fox jumps over the lazy dog\nThe quick brown fox jumps over the lazy cat\nThe quick brown fox jumps over the lazy cat\nThe quick brown fox jumps over the lazy dog\n$\nIf you want to apply a command to a group of lines starting at some point within the text, \nbut continuing to the end of the text, you can use the special address, the dollar sign:\n$ sed '2,$s/dog/cat/' data1.txt\nThe quick brown fox jumps over the lazy dog\nThe quick brown fox jumps over the lazy cat\nThe quick brown fox jumps over the lazy cat\nThe quick brown fox jumps over the lazy cat\n$\n\n520\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  520\nBecause you may not know how many lines of data are in the text, the dollar sign often \ncomes in handy.\nUsing text pattern filters\nThe other method of restricting which lines a command applies to is a bit more compli-\ncated. The \nsed editor allows you to specify a text pattern that it uses to fi lter lines for the \ncommand. This is the format:\n/pattern/command\nYou must encapsulate the pattern you specify in forward slashes. The sed editor applies the \ncommand only to lines that contain the text pattern you specify.\nFor example, if you want to change the default shell for only the user Samantha, you’d use \nthe \nsed command:\n$ grep Samantha /etc/passwd\nSamantha:x:502:502::/home/Samantha:/bin/bash\n$\n$ sed '/Samantha/s/bash/csh/' /etc/passwd\nroot:x:0:0:root:/root:/bin/bash\nbin:x:1:1:bin:/bin:/sbin/nologin\n[...]\nChristine:x:501:501:Christine B:/home/Christine:/bin/bash\nSamantha:x:502:502::/home/Samantha:/bin/csh\nTimothy:x:503:503::/home/Timothy:/bin/bash\n$\nThe command was applied only to the line with the matching text pattern. Although using \na fi xed text pattern may be useful for fi ltering specifi c values, as in the \nuserid example, \nit’s somewhat limited in what you can do with it. The \nsed editor uses a feature called \nregular expressions in text patterns to allow you to create patterns that get pretty involved.\nRegular expressions allow you to create advanced text pattern–matching formulas to match \nall sorts of data. These formulas combine a series of wildcard characters, special characters, \nand fi xed text characters to produce a concise pattern that can match just about any text \nsituation. Regular expressions are one of the scarier parts of shell script programming, and \nChapter 20 covers them in great detail.\nGrouping commands\nIf you need to perform more than one command on an individual line, group the commands \ntogether using braces. The \nsed editor processes each command listed on the address line(s):\n$ sed '2{\n> s/fox/elephant/\n> s/dog/cat/\n\n521\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  521\n19\n> }' data1.txt\nThe quick brown fox jumps over the lazy dog.\nThe quick brown elephant jumps over the lazy cat.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\n$\nBoth commands are processed against the address. And of course, you can specify an \naddress range before the grouped commands:\n$ sed '3,${\n> s/brown/green/\n> s/lazy/active/\n> }' data1.txt\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick green fox jumps over the active dog.\nThe quick green fox jumps over the active dog.\n$\nThe sed editor applies all the commands to all the lines in the address range.\nDeleting lines\nThe text substitution command isn’t the only command available in the sed editor. If you \nneed to delete specifi c lines of text in a text stream, you can use the \ndelete command.\nThe \ndelete command, d, pretty much does what it says. It deletes any text lines that \nmatch the addressing scheme supplied. Be careful with the \ndelete command, because if \nyou forget to include an addressing scheme, all the lines are deleted from the stream:\n$ cat data1.txt\nThe quick brown fox jumps over the lazy dog\nThe quick brown fox jumps over the lazy dog\nThe quick brown fox jumps over the lazy dog\nThe quick brown fox jumps over the lazy dog\n$\n$ sed 'd' data1.txt\n$\nThe delete command is obviously most useful when used in conjunction with a specifi ed \naddress. This allows you to delete specifi c lines of text from the data stream, either by line \nnumber:\n$ cat data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\n\n522\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  522\nThis is line number 4.\n$\n$ sed '3d' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 4.\n$\nor by a specifi c range of lines:\n$ sed '2,3d' data6.txt\nThis is line number 1.\nThis is line number 4.\n$\nor by using the special end-of-fi le character:\n$ sed '3,$d' data6.txt\nThis is line number 1.\nThis is line number 2.\n$\nThe pattern-matching feature of the sed editor also applies to the delete command:\n$ sed '/number 1/d' data6.txt\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\n$\nThe sed editor removes the line containing text that matches the pattern you specify.\nRemember that the sed editor doesn’t touch the original fi le. Any lines you delete are only gone from the output of \nthe \nsed editor. The original fi le still contains the “deleted” lines.\nYou can also delete a range of lines using two text patterns, but be careful if you do this. \nThe fi rst pattern you specify “turns on” the line deletion, and the second pattern “turns \noff” the line deletion. The \nsed editor deletes any lines between the two specifi ed lines \n(including the specifi ed lines):\n$ sed '/1/,/3/d' data6.txt\nThis is line number 4.\n$\nIn addition, you must be careful because the delete feature “turns on” whenever the sed \neditor detects the start pattern in the data stream. This may produce an unexpected result:\n\n523\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  523\n19\n$ cat data7.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\nThis is line number 1 again.\nThis is text you want to keep.\nThis is the last line in the file.\n$\n$ sed '/1/,/3/d' data7.txt\nThis is line number 4.\n$\nThe second occurrence of a line with the number 1 in it triggered the delete command \nagain, deleting the rest of the lines in the data stream, because the stop pattern wasn’t \nrecognized. Of course, the other obvious problem occurs if you specify a stop pattern that \nnever appears in the text:\n$ sed '/1/,/5/d' data7.txt\n$\nBecause the delete features “turned on” at the fi rst pattern match, but never found the end \npattern match, the entire data stream was deleted.\nInserting and appending text\nAs you would expect, like any other editor, the sed editor allows you to insert and append \ntext lines to the data stream. The difference between the two actions can be confusing:\n ■\nThe insert command (i) adds a new line before the specifi ed line.\n ■\nThe append command (a) adds a new line after the specifi ed line.\nWhat is confusing about these two commands is their formats. You can’t use these com-\nmands on a single command line. You must specify the line to insert or append the line to \ninsert on a separate line by itself. Here’s the format for doing this:\nsed '[address]command\\\nnew line'\nThe text in new line appears in the sed editor output in the place you specify. Remember \nthat when you use the \ninsert command, the text appears before the data stream text:\n$ echo \"Test Line 2\" | sed 'i\\Test Line 1'\nTest Line 1\nTest Line 2\n$\n\n524\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  524\nAnd when you use the append command, the text appears after the data stream text:\n$ echo \"Test Line 2\" | sed 'a\\Test Line 1'\nTest Line 2\nTest Line 1\n$\nWhen you use the sed editor from the command line interface prompt, you get the second-\nary prompt to enter the new line of data. You must complete the \nsed editor command on \nthis line. After you enter the ending single quotation mark, the bash shell processes the \ncommand:\n$ echo \"Test Line 2\" | sed 'i\\\n> Test Line 1'\nTest Line 1\nTest Line 2\n$\nThis works well for adding text before or after the text in the data stream, but what about \nadding text inside the data stream?\nTo insert or append data inside the data stream lines, you must use addressing to tell the \nsed editor where you want the data to appear. You can specify only a single line address \nwhen using these commands. You can match either a numeric line number or a text pat-\ntern, but you cannot use a range of addresses. This is logical, because you can only insert \nor append before or after a single line, and not a range of lines.\nHere’s an example of inserting a new line before line 3 in the data stream:\n$ sed '3i\\\n> This is an inserted line.' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is an inserted line.\nThis is line number 3.\nThis is line number 4.\n$\nHere’s an example of appending a new line after line 3 in the data stream:\n$ sed '3a\\\n> This is an appended line.' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is an appended line.\nThis is line number 4.\n$\n\n525\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  525\n19\nThis uses the same process as the insert command; it just places the new text line after \nthe specifi ed line number. If you have a multiline data stream, and you want to append a \nnew line of text to the end of a data stream, just use the dollar sign, which represents the \nlast line of data:\n$ sed '$a\\\n> This is a new line of text.' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\nThis is a new line of text.\n$\nThe same idea applies if you want to add a new line at the beginning of the data stream. \nJust insert a new line before line number one.\nTo insert or append more than one line of text, you must use a backslash on each line of \nnew text until you reach the last text line where you want to insert or append text:\n$ sed '1i\\\n> This is one line of new text.\\\n> This is another line of new text.' data6.txt\nThis is one line of new text.\nThis is another line of new text.\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\n$\nBoth of the specifi ed lines are added to the data stream.\nChanging lines\nThe change command allows you to change the contents of an entire line of text in the \ndata stream. It works the same way as the \ninsert and append commands, in that you \nmust specify the new line separately from the rest of the \nsed command:\n$ sed '3c\\\n> This is a changed line of text.' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is a changed line of text.\nThis is line number 4.\n$\n\n526\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  526\nIn this example, the sed editor changes the text in line number 3. You can also use a text \npattern for the address:\n$ sed '/number 3/c\\\n> This is a changed line of text.' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is a changed line of text.\nThis is line number 4.\n$\nThe text pattern change command changes any line of text in the data stream that it \nmatches.\n$ cat data8.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\nThis is line number 1 again.\nThis is yet another line.\nThis is the last line in the file.\n$\n$ sed '/number 1/c\\\n> This is a changed line of text.' data8.txt\nThis is a changed line of text.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\nThis is a changed line of text.\nThis is yet another line.\nThis is the last line in the file.\n$\nYou can use an address range in the change command, but the results may not be what you \nexpect:\n$ sed '2,3c\\\n> This is a new line of text.' data6.txt\nThis is line number 1.\nThis is a new line of text.\nThis is line number 4.\n$\nInstead of changing both lines with the text, the sed editor uses the single line of text to \nreplace both lines.\n\n527\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  527\n19\nTransforming characters\nThe transform command (y) is the only sed editor command that operates on a single \ncharacter. The \ntransform command uses the format:\n[address]y/inchars/outchars/\nThe transform command performs a one-to-one mapping of the inchars and the \noutchars values. The fi rst character in inchars is converted to the fi rst character in \noutchars. The second character in inchars is converted to the second character in \noutchars. This mapping continues throughout the length of the specifi ed characters. If the \ninchars and outchars are not the same length, the sed editor produces an error message.\nHere’s a simple example of using the \ntransform command:\n$ sed 'y/123/789/' data8.txt\nThis is line number 7.\nThis is line number 8.\nThis is line number 9.\nThis is line number 4.\nThis is line number 7 again.\nThis is yet another line.\nThis is the last line in the file.\n$\nAs you can see from the output, each instance of the characters specifi ed in the inchars \npattern has been replaced by the character in the same position in the \noutchars pattern.\nThe \ntransform command is a global command; that is, it performs the transformation on \nany character found in the text line automatically, without regard to the occurrence:\n$ echo \"This 1 is a test of 1 try.\" | sed 'y/123/456/'\nThis 4 is a test of 4 try.\n$\nThe sed editor transformed both instances of the matching character 1 in the text line. \nYou can’t limit the transformation to a specifi c occurrence of the character.\nPrinting revisited\nThe “Introducing more substitution options” section showed you how to use the p fl ag with \nthe substitution command to display lines that the \nsed editor changed. In addition, three \ncommands that can be used to print information from the data stream:\n ■\nThe p command to print a text line\n ■\nThe equal sign (=) command to print line numbers\n ■\nThe l (lowercase L) command to list a line\n\n528\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  528\nThe following sections look at these three printing commands in the sed editor.\nPrinting lines\nLike the p fl ag in the substitution command, the p command prints a line in the sed \neditor output. On its own, this command doesn’t offer much excitement:\n$ echo \"this is a test\" | sed 'p'\nthis is a test\nthis is a test\n$\nAll it does is print the data text that you already know is there. The most common use for \nthe \nprint command is printing lines that contain matching text from a text pattern:\n$ cat data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\n$\n$ sed -n '/number 3/p' data6.txt\nThis is line number 3.\n$\nBy using the -n option on the command line, you can suppress all the other lines and print \nonly the line that contains the matching text pattern.\nYou can also use this as a quick way to print a subset of lines in a data stream:\n$ sed -n '2,3p' data6.txt\nThis is line number 2.\nThis is line number 3.\n$\nYou can also use the print command when you need to see a line before it gets altered, \nsuch as with the \nsubstitution or change command. You can create a script that displays \nthe line before it’s changed:\n$ sed -n '/3/{\n> p\n> s/line/test/p\n> }' data6.txt\nThis is line number 3.\nThis is test number 3.\n$\nThis sed editor command searches for lines that contain the number 3 and executes two \ncommands. First, the script uses the \np command to print the original version of the line; \n\n529\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  529\n19\nthen it uses the s command to substitute text, along with the p fl ag to print the resulting \ntext. The output shows both the original line text and the new line text.\nPrinting line numbers\nThe equal sign command prints the current line number for the line within the data \nstream. Line numbers are determined by using the newline character in the data stream. \nEach time a newline character appears in the data stream, the \nsed editor assumes that it \nterminates a line of text:\n$ cat data1.txt\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\nThe quick brown fox jumps over the lazy dog.\n$\n$ sed '=' data1.txt\n1\nThe quick brown fox jumps over the lazy dog.\n2\nThe quick brown fox jumps over the lazy dog.\n3\nThe quick brown fox jumps over the lazy dog.\n4\nThe quick brown fox jumps over the lazy dog.\n$\nThe sed editor prints the line number before the actual line of text. The equal sign com-\nmand comes in handy if you’re searching for a specifi c text pattern in the data stream:\n$ sed -n '/number 4/{\n> =\n> p\n> }' data6.txt\n4\nThis is line number 4.\n$\nBy using the -n option, you can have the sed editor display both the line number and text \nfor the line that contains the matching text pattern.\nListing lines\nThe list command (l) allows you to print both the text and nonprintable characters in a \ndata stream. Any nonprintable characters are shown using either their octal values, pre-\nceded by a backslash or the standard C-style nomenclature for common nonprintable char-\nacters, such as \n\\t for tab characters:\n$ cat data9.txt\nThis    line    contains        tabs.\n\n530\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  530\n$\n$ sed -n 'l' data9.txt\nThis\\tline\\tcontains\\ttabs.$\n$\nThe tab character locations are shown with the \\t nomenclature. The dollar sign at the \nend of the line indicates the newline character. If you have a data stream that contains an \nescape character, the \nlist command displays it using the octal code if necessary:\n$ cat data10.txt\nThis line contains an escape character.\n$\n$ sed -n 'l' data10.txt\nThis line contains an escape character. \\a$\n$\nThe data10.txt fi le contains an escape control code, which generates a bell sound. When \nyou use the \ncat command to display the text fi le, you don’t see the escape control code; \nyou just hear the sound (if your speakers are turned on). However, using the \nlist com-\nmand, you can display the escape control code used.\nUsing fi les with sed\nThe substitution command contains fl ags that allow you to work with fi les. There are \nalso regular \nsed editor commands that let you do that without having to substitute text.\nWriting to a file\nThe w command is used to write lines to a fi le. Here’s the format for the w command:\n[address]w filename\nThe filename can be specifi ed as either a relative or absolute pathname, but in either \ncase, the person running the \nsed editor must have write permissions for the fi le. The \naddress can be any type of addressing method used in \nsed, such as a single line number, a \ntext pattern, or a range of line numbers or text patterns.\nHere’s an example that prints only the fi rst two lines of a data stream to a text fi le:\n$ sed '1,2w test.txt' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\n$\n$ cat test.txt\nThis is line number 1.\nThis is line number 2.\n$\n\n531\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  531\n19\nOf course, if you don’t want the lines to display on STDOUT, you can use the -n option for \nthe \nsed command.\nThis is a great tool to use if you need to create a data fi le from a master fi le on the basis of \ncommon text values, such as those in a mailing list:\n$ cat data11.txt\nBlum, R       Browncoat\nMcGuiness, A  Alliance\nBresnahan, C  Browncoat\nHarken, C     Alliance\n$\n$ sed -n '/Browncoat/w Browncoats.txt' data11.txt\n$\n$ cat Browncoats.txt\nBlum, R       Browncoat\nBresnahan, C  Browncoat\n$\nThe sed editor writes to a destination fi le only the data lines that contain the text pattern.\nReading data from a file\nYou’ve already seen how to insert data into and append text to a data stream from the sed \ncommand line. The \nread command (r) allows you to insert data contained in a separate fi le.\nHere’s the format of the \nread command:\n[address]r filename\nThe filename parameter specifi es either an absolute or relative pathname for the fi le that \ncontains the data. You can’t use a range of addresses for the \nread command. You can only \nspecify a single line number or text pattern address. The \nsed editor inserts the text from \nthe fi le after the address.\n$ cat data12.txt\nThis is an added line.\nThis is the second added line.\n$\n$ sed '3r data12.txt' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is an added line.\nThis is the second added line.\nThis is line number 4.\n$\n\n532\nPart III: Advanced Shell Scripting\nc19.indd  12/16/2014  Page  532\nThe sed editor inserts into the data stream all the text lines in the data fi le. The same \ntechnique works when using a text pattern address:\n$ sed '/number 2/r data12.txt' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is an added line.\nThis is the second added line.\nThis is line number 3.\nThis is line number 4.\n$\nIf you want to add text to the end of a data stream, just use the dollar sign address symbol:\n$ sed '$r data12.txt' data6.txt\nThis is line number 1.\nThis is line number 2.\nThis is line number 3.\nThis is line number 4.\nThis is an added line.\nThis is the second added line.\n$\nA cool application of the read command is to use it in conjunction with a delete com-\nmand to replace a placeholder in a fi le with data from another fi le. For example, suppose \nthat you had a form stored in a text fi le that looked like this:\n$ cat notice.std\nWould the following people:\nLIST\nplease report to the ship's captain.\n$\nThe form letter uses the generic placeholder LIST in place of a list of people. To insert the \nlist of people after the placeholder, you just use the \nread command. However, this still \nleaves the placeholder text in the output. To remove that, just use the \ndelete command. \nThe result looks like this:\n$ sed '/LIST/{\n> r data11.txt\n> d\n> }' notice.std\nWould the following people:\nBlum, R       Browncoat\nMcGuiness, A  Alliance\nBresnahan, C  Browncoat\n\n533\nChapter 19: Introducing sed and gawk\nc19.indd  12/16/2014  Page  533\n19\nHarken, C     Alliance\nplease report to the ship's captain.\n$\nNow the placeholder text is replaced with the list of names from the data fi le.\nSummary\nShell scripts can do lots of work on their own, but it’s often diffi cult to manipulate data \nwith just a shell script. Linux provides two handy utilities to help with handling text data. \nThe \nsed editor is a stream editor that quickly processes data on the fl y as it reads it. You \nmust provide the \nsed editor with a list of editing commands, which it applies to the data.\nThe \ngawk program is a utility from the GNU organization that mimics and expands on the \nfunctionality of the Unix \nawk program. The gawk program contains a built-in programming \nlanguage that you can use to write scripts to handle and process data. You can use the \ngawk program to extract data elements from large data fi les and output them in just about \nany format you desire. This makes processing large log fi les a snap, as well as  creating \ncustom reports from data fi les.\nA crucial element of using both the \nsed and gawk  programs is knowing how to use regular \nexpressions. Regular expressions are key to creating customized fi lters for extracting and \nmanipulating data in text fi les. The next chapter dives into the often misunderstood world \nof regular expressions, showing you how to build regular expressions for manipulating all \ntypes of data. \n\nc19.indd  12/16/2014  Page  534\n\n535\nc20.indd  12/23/2014  Page  535\nCHAPTER \n20\nRegular Expressions\nIN THIS CHAPTER\nDefi ning regular expressions\nLooking at the basics\nExtending our patterns\nCreating expressions\nT\nhe key to successfully working with the sed editor and the gawk program in your shell script \nis your comfort using regular expressions. This is not always an easy thing to do, because \ntrying to fi lter specifi c data from a large batch of data can (and often does) get complicated. \nThis chapter describes how to create regular expressions in both the \nsed editor and the gawk \nprogram that can fi lter out just the data you need.\nWhat Are Regular Expressions?\nThe fi rst step to understanding regular expressions is to defi ne just exactly what they are. This \nsection explains what a regular expression is and describes how Linux uses regular expressions.\nA defi nition\nA regular expression is a pattern template you defi ne that a Linux utility uses to fi lter text. \nA Linux utility (such as the \nsed editor or the gawk program) matches the regular expression \npattern against data as that data fl ows into the utility. If the data matches the pattern, it’s \naccepted for processing. If the data doesn’t match the pattern, it’s rejected. This is illustrated \nin Figure 20-1.\nThe regular expression pattern makes use of wildcard characters to represent one or more charac-\nters in the data stream. There are plenty of instances in Linux where you can specify a wildcard \ncharacter to represent data you don’t know about. You’ve already seen an example of using wildcard \ncharacters with the Linux \nls command for listing fi les and directories (see Chapter 3).\n\n536\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  536\nFIGURE 20-1\nMatching data against a regular expression pattern\nregular\nexpression\nmatching data\ndata stream\nrejected data\nThe asterisk wildcard character allows you to list only fi les that match a certain criteria. \nFor example:\n$ ls -al da*\n-rw-r--r--    1 rich     rich           45 Nov 26 12:42 data\n-rw-r--r--    1 rich     rich           25 Dec  4 12:40 data.tst\n-rw-r--r--    1 rich     rich          180 Nov 26 12:42 data1\n-rw-r--r--    1 rich     rich           45 Nov 26 12:44 data2\n-rw-r--r--    1 rich     rich           73 Nov 27 12:31 data3\n-rw-r--r--    1 rich     rich           79 Nov 28 14:01 data4\n-rw-r--r--    1 rich     rich          187 Dec  4 09:45 datatest\n$\nThe da* parameter instructs the ls command to list only the fi les whose name starts with \nda. There can be any number of characters after the \nda in the fi lename (including none). \nThe \nls command reads the information regarding all the fi les in the directory but displays \nonly the ones that match the wildcard character.\nRegular expression wildcard patterns work in a similar way. The regular expression pattern \ncontains text and/or special characters that defi ne a template for the \nsed editor and the \ngawk program to follow when matching data. You can use different special characters in a \nregular expression to defi ne a specifi c pattern for fi ltering data.\nTypes of regular expressions\nThe biggest problem with using regular expressions is that there isn’t just one set of them. \nSeveral different applications use different types of regular expressions in the Linux envi-\nronment. These include such diverse applications as programming languages (Java, Perl, and \n\n537\nChapter 20: Regular Expressions\nc20.indd  12/23/2014  Page  537\n20\nPython), Linux utilities (such as the sed editor, the gawk program, and the grep utility), \nand mainstream applications (such as the MySQL and PostgreSQL database servers).\nA regular expression is implemented using a regular expression engine. A regular expres-\nsion engine is the underlying software that interprets regular expression patterns and uses \nthose patterns to match text.\nThe Linux world has two popular regular expression engines: \n ■\nThe POSIX Basic Regular Expression (BRE) engine\n ■\nThe POSIX Extended Regular Expression (ERE) engine\nMost Linux utilities at a minimum conform to the POSIX BRE engine specifi cations, \nrecognizing all the pattern symbols it defi nes. Unfortunately, some utilities (such as the \nsed editor) conform only to a subset of the BRE engine specifi cations. This is due to speed \nconstraints, because the \nsed editor attempts to process text in the data stream as quickly \nas possible.\nThe POSIX ERE engine is often found in programming languages that rely on regular expres-\nsions for text fi ltering. It provides advanced pattern symbols as well as special symbols for \ncommon patterns, such as matching digits, words, and alphanumeric characters. The \ngawk \nprogram uses the ERE engine to process its regular expression patterns.\nBecause there are so many different ways to implement regular expressions, it’s hard to \npresent a single, concise description of all the possible regular expressions. The following \nsections discuss the most commonly found regular expressions and demonstrate how to use \nthem in the \nsed editor and gawk program.\nDefi ning BRE Patterns\nThe most basic BRE pattern is matching text characters in a data stream. This section dem-\nonstrates how you can defi ne text in the regular expression pattern and what to expect \nfrom the results.\nPlain text\nChapter 18 demonstrated how to use standard text strings in the sed editor and the gawk \nprogram to fi lter data. Here’s an example to refresh your memory:\n$ echo \"This is a test\" | sed -n '/test/p'\nThis is a test\n$ echo \"This is a test\" | sed -n '/trial/p'\n$\n$ echo \"This is a test\" | gawk '/test/{print $0}'\n\n538\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  538\nThis is a test\n$ echo \"This is a test\" | gawk '/trial/{print $0}'\n$\nThe fi rst pattern defi nes a single word, test. The sed editor and gawk program scripts each \nuse their own version of the \nprint command to print any lines that match the regular \nexpression pattern. Because the \necho statement contains the word “test” in the text \nstring, the data stream text matches the defi ned regular expression pattern, and the \nsed \neditor displays the line.\nThe second pattern again defi nes just a single word, this time the word “trial.” Because \nthe \necho statement text string doesn’t contain that word, the regular expression pattern \ndoesn’t match, so neither the \nsed editor nor the gawk program prints the line.\nYou probably already noticed that the regular expression doesn’t care where in the data \nstream the pattern occurs. It also doesn’t matter how many times the pattern occurs. After \nthe regular expression can match the pattern anywhere in the text string, it passes the \nstring along to the Linux utility that’s using it.\nThe key is matching the regular expression pattern to the data stream text. It’s important \nto remember that regular expressions are extremely picky about matching patterns. The \nfi rst rule to remember is that regular expression patterns are case sensitive. This means \nthey’ll match only those patterns with the proper case of characters:\n$ echo \"This is a test\" | sed -n '/this/p'\n$\n$ echo \"This is a test\" | sed -n '/This/p'\nThis is a test\n$\nThe fi rst attempt failed to match because the word “this” doesn’t appear in all lowercase in \nthe text string, while the second attempt, which uses the uppercase letter in the pattern, \nworked just fi ne.\nYou don’t have to limit yourself to whole words in the regular expression. If the defi ned \ntext appears anywhere in the data stream, the regular expression matches the following:\n$ echo \"The books are expensive\" | sed -n '/book/p'\nThe books are expensive\n$\nEven though the text in the data stream is books, the data in the stream contains the \nregular expression book, so the regular expression pattern matches the data. Of course, if \nyou try the opposite, the regular expression fails:\n$ echo \"The book is expensive\" | sed -n '/books/p'\n$\n\n539\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  539\nThe complete regular expression text didn’t appear in the data stream, so the match failed \nand the \nsed editor didn’t display the text.\nYou also don’t have to limit yourself to single text words in the regular expression. You can \ninclude spaces and numbers in your text string as well:\n$ echo \"This is line number 1\" | sed -n '/ber 1/p'\nThis is line number 1\n$\nSpaces are treated just like any other character in the regular expression:\n$ echo \"This is line number1\" | sed -n '/ber 1/p'\n$\nIf you defi ne a space in the regular expression, it must appear in the data stream. You can \neven create a regular expression pattern that matches multiple contiguous spaces:\n$ cat data1\nThis is a normal line of text.\nThis is  a line with too many spaces.\n$ sed -n '/  /p' data1\nThis is  a line with too many spaces.\n$ \nThe line with two spaces between words matches the regular expression pattern. This is a \ngreat way to catch spacing problems in text fi les!\nSpecial characters\nAs you use text strings in your regular expression patterns, there’s something you need to \nbe aware of. There are a few exceptions when defi ning text characters in a regular expres-\nsion. Regular expression patterns assign a special meaning to a few characters. If you try to \nuse these characters in your text pattern, you won’t get the results you were expecting.\nThese special characters are recognized by regular expressions:\n.*[]^${}\\+?|()\nAs the chapter progresses, you’ll fi nd out just what these special characters do in a regular \nexpression. For now, however, just remember that you can’t use these characters by them-\nselves in your text pattern.\nIf you want to use one of the special characters as a text character, you need to escape it. \nWhen you escape the special characters, you add a special character in front of it to indi-\ncate to the regular expression engine that it should interpret the next character as a nor-\nmal text character. The special character that does this is the backslash character (\\).\n\n540\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  540\nFor example, if you want to search for a dollar sign in your text, just precede it with a \nbackslash character:\n$ cat data2\nThe cost is $4.00\n$ sed -n '/\\$/p' data2\nThe cost is $4.00\n$\nBecause the backslash is a special character, if you need to use it in a regular expression \npattern, you need to escape it as well, producing a double backslash:\n$ echo \"\\ is a special character\" | sed -n '/\\\\/p'\n\\ is a special character\n$ \nFinally, although the forward slash isn’t a regular expression special character, if you use it \nin your regular expression pattern in the \nsed editor or the gawk program, you get an error:\n$ echo \"3 / 2\" | sed -n '///p'\nsed: -e expression #1, char 2: No previous regular expression\n$ \nTo use a forward slash, you need to escape that as well:\n$ echo \"3 / 2\" | sed -n '/\\//p'\n3 / 2\n$\nNow the sed editor can properly interpret the regular expression pattern, and all is well.\nAnchor characters\nAs shown in the “Plain Text” section, by default, when you specify a regular expression \npattern, if the pattern appears anywhere in the data stream, it matches. You can use two \nspecial characters to anchor a pattern to either the beginning or the end of lines in the \ndata stream.\nStarting at the beginning\nThe caret character (^) defi nes a pattern that starts at the beginning of a line of text in \nthe data stream. If the pattern is located any place other than the start of the line of text, \nthe regular expression pattern fails.\nTo use the caret character, you must place it before the pattern specifi ed in the regular \nexpression:\n$ echo \"The book store\" | sed -n '/\n^\nbook/p'\n$\n\n541\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  541\n$ echo \"Books are great\" | sed -n '/\n^\nBook/p'\nBooks are great\n$\nThe caret anchor character checks for the pattern at the beginning of each new line of \ndata, as determined by the newline character:\n$ cat data3\nThis is a test line.\nthis is another test line.\nA line that tests this feature.\nYet more testing of this\n$ sed -n '/\n^\nthis/p' data3\nthis is another test line.\n$\nAs long as the pattern appears at the start of a new line, the caret anchor catches it.\nIf you position the caret character in any place other than at the beginning of the pattern, \nit acts like a normal character and not as a special character:\n$ echo \"This \n^\n is a test\" | sed -n '/s \n^\n/p'\nThis ^ is a test\n$ \nBecause the caret character is listed last in the regular expression pattern, the sed editor \nuses it as a normal character to match text.\nIf you need to specify a regular expression pattern using only the caret character, you don’t need to escape it with a \nbackslash. However, if you specify the caret character fi rst, followed by additional text in the pattern, you need to use \nthe escape character before the caret character.\nLooking for the ending\nThe opposite of looking for a pattern at the start of a line is looking for it at the end of a \nline. The dollar sign (\n$) special character defi nes the end anchor. Add this special character \nafter a text pattern to indicate that the line of data must end with the text pattern:\n$ echo \"This is a good book\" | sed -n '/book$/p'\nThis is a good book\n$ echo \"This book is good\" | sed -n '/book$/p'\n$\nThe problem with an ending text pattern is that you must be careful what you’re looking for:\n$ echo \"There are a lot of good books\" | sed -n '/book$/p'\n$\n\n542\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  542\nMaking the word “book” plural at the end of the line means that it no longer matches the \nregular expression pattern, even though book is in the data stream. The text pattern must \nbe the last thing on the line for the pattern to match.\nCombining anchors\nIn some common situations, you can combine both the start and end anchor on the same \nline. In the fi rst situation, suppose you want to look for a line of data containing only a \nspecifi c text pattern:\n$ cat data4\nthis is a test of using both anchors\nI said this is a test\nthis is a test\nI'm sure this is a test.\n$ sed -n '/\n^\nthis is a test$/p' data4\nthis is a test\n$\nThe sed editor ignores the lines that include other text besides the specifi ed text.\nThe second situation may seem a little odd at fi rst but is extremely useful. By combining \nboth anchors in a pattern with no text, you can fi lter blank lines from the data stream. \nConsider this example:\n$ cat data5\nThis is one test line.\n \nThis is another test line.\n$ sed '/\n^\n$/d' data5\nThis is one test line.\nThis is another test line.\n$\nThe regular expression pattern that is defi ned looks for lines that have nothing between \nthe start and end of the line. Because blank lines contain no text between the two newline \ncharacters, they match the regular expression pattern. The \nsed editor uses the d delete \ncommand to delete lines that match the regular expression pattern, thus removing all \nblank lines from the text. This is an effective way to remove blank lines from documents.\nThe dot character\nThe dot special character is used to match any single character except a newline character. The \ndot character must match a character, however; if there’s no character in the place of the dot, \nthen the pattern fails.\nLet’s look at a few examples of using the dot character in a regular expression pattern:\n\n543\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  543\n$ cat data6\nThis is a test of a line.\nThe cat is sleeping.\nThat is a very nice hat.\nThis test is at line four.\nat ten o'clock we'll go home.\n$ sed -n '/.at/p' data6\nThe cat is sleeping.\nThat is a very nice hat.\nThis test is at line four.\n$\nYou should be able to fi gure out why the fi rst line failed and why the second and third \nlines passed. The fourth line is a little tricky. Notice that we matched the \nat, but there’s \nno character in front of it to match the dot character. Ah, but there is! In regular expres-\nsions, spaces count as characters, so the space in front of the \nat matches the pattern. The \nfi fth line proves this, by putting the \nat in the front of the line, which fails to match the \npattern.\nCharacter classes\nThe dot special character is great for matching a character position against any character, \nbut what if you want to limit what characters to match? This is called a character class in \nregular expressions.\nYou can defi ne a class of characters that would match a position in a text pattern. If one of \nthe characters from the character class is in the data stream, it matches the pattern.\nTo defi ne a character class, you use square brackets. The brackets should contain any char-\nacter you want to include in the class. You then use the entire class within a pattern just \nlike any other wildcard character. This takes a little getting used to at fi rst, but after you \ncatch on, it can generate some pretty amazing results.\nThe following is an example of creating a character class:\n$ sed -n '/[ch]at/p' data6\nThe cat is sleeping.\nThat is a very nice hat.\n$\nUsing the same data fi le as in the dot special character example, we came up with a differ-\nent result. This time we managed to fi lter out the line that just contained the word \nat. The \nonly words that match this pattern are \ncat and hat. Also notice that the line that started \nwith \nat didn’t match as well. There must be a character in the character class that matches \nthe appropriate position.\n\n544\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  544\nCharacter classes come in handy if you’re not sure which case a character is in:\n$ echo \"Yes\" | sed -n '/[Yy]es/p'\nYes\n$ echo \"yes\" | sed -n '/[Yy]es/p'\nyes\n$\nYou can use more than one character class in a single expression:\n$ echo \"Yes\" | sed -n '/[Yy][Ee][Ss]/p'\nYes\n$ echo \"yEs\" | sed -n '/[Yy][Ee][Ss]/p'\nyEs\n$ echo \"yeS\" | sed -n '/[Yy][Ee][Ss]/p'\nyeS\n$\nThe regular expression used three character classes to cover both lower and upper cases for \nall three character positions.\nCharacter classes don’t have to contain just letters; you can use numbers in them as well:\n$ cat data7\nThis line doesn't contain a number.\nThis line has 1 number on it.\nThis line a number 2 on it.\nThis line has a number 4 on it.\n$ sed -n '/[0123]/p' data7\nThis line has 1 number on it.\nThis line a number 2 on it.\n$\nThe regular expression pattern matches any lines that contain the numbers 0, 1, 2, or 3. \nAny other numbers are ignored, as are lines without numbers in them.\nYou can combine character classes to check for properly formatted numbers, such as phone \nnumbers and ZIP codes. However, when you’re trying to match a specifi c format, you must \nbe careful. Here’s an example of a ZIP code match gone wrong:\n$ cat data8\n60633\n46201\n223001\n4353\n22203\n$ sed -n '\n>/[0123456789][0123456789][0123456789][0123456789][0123456789]/p\n>' data8\n\n545\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  545\n60633\n46201\n223001\n22203\n$\nThis might not have produced the result you were thinking of. It did a fi ne job of fi ltering \nout the number that was too short to be a ZIP code, because the last character class didn’t \nhave a character to match against. However, it still passed the six-digit number, even \nthough we only defi ned fi ve character classes.\nRemember that the regular expression pattern can be found anywhere in the text of the \ndata stream. You may always have additional characters besides the matching pattern char-\nacters. If you want to ensure that you match against only fi ve numbers, you need to delin-\neate them somehow, either with spaces, or as in this example, by showing that they’re at \nthe start and end of the line:\n$ sed -n '\n> /\n^\n[0123456789][0123456789][0123456789][0123456789][0123456789]$/p\n> ' data8\n60633\n46201\n22203\n$\nNow that’s much better! Later in this chapter, we look at how to simplify this even further.\nOne extremely popular use for character classes is parsing words that might be misspelled, \nsuch as data entered from a user form. You can easily create regular expressions that can \naccept common misspellings in data:\n$ cat data9\nI need to have some maintenence done on my car.\nI'll pay that in a seperate invoice.\nAfter I pay for the maintenance my car will be as good as new.\n$ sed -n '\n/maint[ea]n[ae]nce/p\n/sep[ea]r[ea]te/p\n' data9\nI need to have some maintenence done on my car.\nI'll pay that in a seperate invoice.\nAfter I pay for the maintenance my car will be as good as new.\n$\nThe two sed print commands in this example utilize regular expression character classes \nto help catch the misspelled words, maintenance and separate, in the text. The same regular \nexpression pattern also matches the properly spelled occurrence of “maintenance.”\n\n546\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  546\nNegating character classes\nIn regular expression patterns, you can also reverse the effect of a character class. Instead of \nlooking for a character contained in the class, you can look for any character that’s not in the \nclass. To do that, just place a caret character at the beginning of the character class range:\n$ sed -n '/[\n^\nch]at/p' data6\nThis test is at line four.\n$\nBy negating the character class, the regular expression pattern matches any character \nthat’s neither a c nor an h, along with the text pattern. Because the space character fi ts \nthis category, it passed the pattern match. However, even with the negation, the character \nclass must still match a character, so the line with the \nat in the start of the line still \ndoesn’t match the pattern.\nUsing ranges\nYou may have noticed when I showed the ZIP code example earlier that it was somewhat \nawkward having to list all the possible digits in each character class. Fortunately, you can \nuse a shortcut so you don’t have to do that.\nYou can use a range of characters within a character class by using the dash symbol. Just \nspecify the fi rst character in the range, a dash, and then the last character in the range. \nThe regular expression includes any character that’s within the specifi ed character range, \naccording to the character set used by the Linux system (see Chapter 2).\nNow you can simplify the ZIP code example by specifying a range of digits:\n$ sed -n '/^[0-9][0-9][0-9][0-9][0-9]$/p' data8\n60633\n46201\n45902\n$\nThat saved lots of typing! Each character class matches any digit from 0 to 9. The pattern \nfails if a letter is present anywhere in the data:\n$ echo \"a8392\" | sed -n '/^[0-9][0-9][0-9][0-9][0-9]$/p'\n$\n$ echo \"1839a\" | sed -n '/^[0-9][0-9][0-9][0-9][0-9]$/p'\n$\n$ echo \"18a92\" | sed -n '/^[0-9][0-9][0-9][0-9][0-9]$/p'\n$\nThe same technique works with letters:\n$ sed -n '/[c-h]at/p' data6\nThe cat is sleeping.\n\n547\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  547\nThat is a very nice hat.\n$\nThe new pattern [c-h]at matches words where the fi rst letter is between the letter c and \nthe letter h. In this case, the line with only the word \nat failed to match the pattern.\nYou can also specify multiple, non-continuous ranges in a single character class: \n$ sed -n '/[a-ch-m]at/p' data6\nThe cat is sleeping.\nThat is a very nice hat.\n$\nThe character class allows the ranges a through c, and h through m to appear before the at \ntext. This range would reject any letters between d and g:\n$ echo \"I'm getting too fat.\" | sed -n '/[a-ch-m]at/p'\n$\nThis pattern rejected the fat text, as it wasn’t in the specifi ed range.\nSpecial character classes\nIn addition to defi ning your own character classes, the BRE contains special character \nclasses you can use to match against specifi c types of characters. Table 20-1 describes the \nBRE special characters you can use.\nTABLE 20 -1    BRE Special Character Classes\nClassDescription\n[[:alpha:]]\nMatches any alphabetical character, either upper or lower case\n[[:alnum:]]\nMatches any alphanumeric character 0–9, A–Z, or a–z\n[[:blank:]]\nMatches a space or Tab character\n[[:digit:]]\nMatches a numerical digit from 0 through 9\n[[:lower:]]\nMatches any lowercase alphabetical character a–z\n[[:print:]]\nMatches any printable character\n[[:punct:]]\nMatches a punctuation character\n[[:space:]]\nMatches any whitespace character: space, Tab, NL, FF, VT, CR\n[[:upper:]]\nMatches any uppercase alphabetical character A–Z\nYou use the special character classes just as you would a normal character class in your \nregular expression patterns:\n$ echo \"abc\" | sed -n '/[[:digit:]]/p'\n$\n\n548\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  548\n$ echo \"abc\" | sed -n '/[[:alpha:]]/p'\nabc\n$ echo \"abc123\" | sed -n '/[[:digit:]]/p'\nabc123\n$ echo \"This is, a test\" | sed -n '/[[:punct:]]/p'\nThis is, a test\n$ echo \"This is a test\" | sed -n '/[[:punct:]]/p'\n$\nUsing the special character classes is an easy way to defi ne ranges. Instead of having to use \na range [0–9], you can just use \n[[:digit:]].\nThe asterisk\nPlacing an asterisk after a character signifi es that the character must appear zero or more \ntimes in the text to match the pattern:\n$ echo \"ik\" | sed -n '/ie*k/p'\nik\n$ echo \"iek\" | sed -n '/ie*k/p'\niek\n$ echo \"ieek\" | sed -n '/ie*k/p'\nieek\n$ echo \"ieeek\" | sed -n '/ie*k/p'\nieeek\n$ echo \"ieeeek\" | sed -n '/ie*k/p'\nieeeek\n$\nThis pattern symbol is commonly used for handling words that have a common misspelling \nor variations in language spellings. For example, if you need to write a script that may be \nused in either American or British English, you could write:\n$ echo \"I'm getting a color TV\" | sed -n '/colou*r/p'\nI'm getting a color TV\n$ echo \"I'm getting a colour TV\" | sed -n '/colou*r/p'\nI'm getting a colour TV\n$\nThe u* in the pattern indicates that the letter u may or may not appear in the text to \nmatch the pattern. Similarly, if you know of a word that is commonly misspelled, you can \naccommodate it by using the asterisk:\n$ echo \"I ate a potatoe with my lunch.\" | sed -n '/potatoe*/p'\nI ate a potatoe with my lunch.\n$ echo \"I ate a potato with my lunch.\" | sed -n '/potatoe*/p'\nI ate a potato with my lunch.\n$\n\n549\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  549\nPlacing an asterisk next to the possible extra letter allows you to accept the \nmisspelled word.\nAnother handy feature is combining the dot special character with the asterisk special \ncharacter. This combination provides a pattern to match any number of any characters. It’s \noften used between two text strings that may or may not appear next to each other in the \ndata stream:\n$ echo \"this is a regular pattern expression\" | sed -n '\n> /regular.*expression/p'\nthis is a regular pattern expression\n$\nUsing this pattern, you can easily search for multiple words that may appear anywhere in a \nline of text in the data stream.\nThe asterisk can also be applied to a character class. This allows you to specify a group or \nrange of characters that can appear more than once in the text:\n$ echo \"bt\" | sed -n '/b[ae]*t/p'\nbt\n$ echo \"bat\" | sed -n '/b[ae]*t/p'\nbat\n$ echo \"bet\" | sed -n '/b[ae]*t/p'\nbet\n$ echo \"btt\" | sed -n '/b[ae]*t/p'\nbtt\n$\n$ echo \"baat\" | sed -n '/b[ae]*t/p'\nbaat\n$ echo \"baaeeet\" | sed -n '/b[ae]*t/p'\nbaaeeet\n$ echo \"baeeaeeat\" | sed -n '/b[ae]*t/p'\nbaeeaeeat\n$ echo \"baakeeet\" | sed -n '/b[ae]*t/p'\n$\nAs long as the a and e characters appear in any combination between the b and t characters \n(including not appearing at all), the pattern matches. If any other character outside of the \ndefi ned character class appears, the pattern match fails.\nExtended Regular Expressions\nThe POSIX ERE patterns include a few additional symbols that are used by some Linux \napplications and utilities. The \ngawk program recognizes the ERE patterns, but the sed \neditor doesn’t.\n\n550\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  550\nRemember that the regular expression engines in the sed editor and the gawk program are different. The gawk \nprogram can use most of the extended regular expression pattern symbols, and it can provide some additional fi l-\ntering capabilities that the \nsed editor doesn’t have. However, because of this, it is often slower in processing data \nstreams.\nThis section describes the more commonly found ERE pattern symbols that you can use in \nyour \ngawk program scripts.\nThe question mark\nThe question mark is similar to the asterisk, but with a slight twist. The question mark \nindicates that the preceding character can appear zero or one time, but that’s all. It doesn’t \nmatch repeating occurrences of the character:\n$ echo \"bt\" | gawk '/be?t/{print $0}'\nbt\n$ echo \"bet\" | gawk '/be?t/{print $0}'\nbet\n$ echo \"beet\" | gawk '/be?t/{print $0}'\n$\n$ echo \"beeet\" | gawk '/be?t/{print $0}'\n$\nIf the e character doesn’t appear in the text, or as long as it appears only once in the text, \nthe pattern matches.\nAs with the asterisk, you can use the question mark symbol along with a character class:\n$ echo \"bt\" | gawk '/b[ae]?t/{print $0}'\nbt\n$ echo \"bat\" | gawk '/b[ae]?t/{print $0}'\nbat\n$ echo \"bot\" | gawk '/b[ae]?t/{print $0}'\n$\n$ echo \"bet\" | gawk '/b[ae]?t/{print $0}'\nbet\n$ echo \"baet\" | gawk '/b[ae]?t/{print $0}'\n$\n$ echo \"beat\" | gawk '/b[ae]?t/{print $0}'\n$\n$ echo \"beet\" | gawk '/b[ae]?t/{print $0}'\n$\nIf zero or one character from the character class appears, the pattern match passes. \nHowever, if both characters appear, or if one of the characters appears twice, the pattern \nmatch fails.\n\n551\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  551\nThe plus sign\nThe plus sign is another pattern symbol that’s similar to the asterisk, but with a different \ntwist than the question mark. The plus sign indicates that the preceding character can \nappear one or more times, but must be present at least once. The pattern doesn’t match if \nthe character is not present:\n$ echo \"beeet\" | gawk '/be+t/{print $0}'\nbeeet\n$ echo \"beet\" | gawk '/be+t/{print $0}'\nbeet\n$ echo \"bet\" | gawk '/be+t/{print $0}'\nbet\n$ echo \"bt\" | gawk '/be+t/{print $0}'\n$\nIf the e character is not present, the pattern match fails. The plus sign also works with \ncharacter classes, the same way as the asterisk and question mark do:\n$ echo \"bt\" | gawk '/b[ae]+t/{print $0}'\n$\n$ echo \"bat\" | gawk '/b[ae]+t/{print $0}'\nbat\n$ echo \"bet\" | gawk '/b[ae]+t/{print $0}'\nbet\n$ echo \"beat\" | gawk '/b[ae]+t/{print $0}'\nbeat\n$ echo \"beet\" | gawk '/b[ae]+t/{print $0}'\nbeet\n$ echo \"beeat\" | gawk '/b[ae]+t/{print $0}'\nbeeat\n$\nThis time if either character defi ned in the character class appears, the text matches the \nspecifi ed pattern.\nUsing braces\nCurly braces are available in ERE to allow you to specify a limit on a repeatable regular \nexpression. This is often referred to as an interval. You can express the interval in two \nformats:\n ■\nm: The regular expression appears exactly m times.\n ■\nm,n: The regular expression appears at least m times, but no more than n times.\nThis feature allows you to fi ne-tune exactly how many times you allow a character (or char-\nacter class) to appear in a pattern.\n\n552\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  552\nBy default, the gawk program doesn’t recognize regular expression intervals. You must specify the \n--re-interval command line option for the gawk program to recognize regular expression intervals.\nHere’s an example of using a simple interval of one value:\n$ echo \"bt\" | gawk --re-interval '/be{1}t/{print $0}'\n$\n$ echo \"bet\" | gawk --re-interval '/be{1}t/{print $0}'\nbet\n$ echo \"beet\" | gawk --re-interval '/be{1}t/{print $0}'\n$\nBy specifying an interval of one, you restrict the number of times the character can be \npresent for the string to match the pattern. If the character appears more times, the \npattern match fails.\nOften, specifying the lower and upper limit comes in handy:\n$ echo \"bt\" | gawk --re-interval '/be{1,2}t/{print $0}'\n$\n$ echo \"bet\" | gawk --re-interval '/be{1,2}t/{print $0}'\nbet\n$ echo \"beet\" | gawk --re-interval '/be{1,2}t/{print $0}'\nbeet\n$ echo \"beeet\" | gawk --re-interval '/be{1,2}t/{print $0}'\n$\nIn this example, the e character can appear once or twice for the pattern match to pass; \notherwise, the pattern match fails.\nThe interval pattern match also applies to character classes:\n$ echo \"bt\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\n$\n$ echo \"bat\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\nbat\n$ echo \"bet\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\nbet\n$ echo \"beat\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\nbeat\n$ echo \"beet\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\nbeet\n$ echo \"beeat\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\n$\n$ echo \"baeet\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\n$\n\n553\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  553\n$ echo \"baeaet\" | gawk --re-interval '/b[ae]{1,2}t/{print $0}'\n$\nThis regular expression pattern matches if there are exactly one or two instances of the \nletter a or e in the text pattern, but it fails if there are any more in any combination.\nThe pipe symbol\nThe pipe symbol allows to you to specify two or more patterns that the regular expression engine \nuses in a logical \nOR formula when examining the data stream. If any of the patterns match the \ndata stream text, the text passes. If none of the patterns match, the data stream text fails.\nHere’s the format for using the pipe symbol:\nexpr1|expr2|...\nHere’s an example of this: \n$ echo \"The cat is asleep\" | gawk '/cat|dog/{print $0}'\nThe cat is asleep\n$ echo \"The dog is asleep\" | gawk '/cat|dog/{print $0}'\nThe dog is asleep\n$ echo \"The sheep is asleep\" | gawk '/cat|dog/{print $0}'\n$\nThis example looks for the regular expression cat or dog in the data stream. You can’t \nplace any spaces within the regular expressions and the pipe symbol, or they’re added to \nthe regular expression pattern.\nThe regular expressions on either side of the pipe symbol can use any regular expression \npattern, including character classes, to defi ne the text:\n$ echo \"He has a hat.\" | gawk '/[ch]at|dog/{print $0}'\nHe has a hat.\n$\nThis example would match cat, hat, or dog in the data stream text.\nGrouping expressions\nRegular expression patterns can also be grouped by using parentheses. When you group a \nregular expression pattern, the group is treated like a standard character. You can apply a \nspecial character to the group just as you would to a regular character. For example:\n$ echo \"Sat\" | gawk '/Sat(urday)?/{print $0}'\nSat\n$ echo \"Saturday\" | gawk '/Sat(urday)?/{print $0}'\nSaturday\n$\n\n554\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  554\nThe grouping of the “urday” ending along with the question mark allows the pattern to \nmatch either the full day name Saturday or the abbreviated name Sat.\nIt’s common to use grouping along with the pipe symbol to create groups of possible pattern \nmatches:\n$ echo \"cat\" | gawk '/(c|b)a(b|t)/{print $0}'\ncat\n$ echo \"cab\" | gawk '/(c|b)a(b|t)/{print $0}'\ncab\n$ echo \"bat\" | gawk '/(c|b)a(b|t)/{print $0}'\nbat\n$ echo \"bab\" | gawk '/(c|b)a(b|t)/{print $0}'\nbab\n$ echo \"tab\" | gawk '/(c|b)a(b|t)/{print $0}'\n$\n$ echo \"tac\" | gawk '/(c|b)a(b|t)/{print $0}'\n$\nThe pattern (c|b)a(b|t) matches any combination of the letters in the fi rst group along \nwith any combination of the letters in the second group.\nRegular Expressions in Action\nNow that you’ve seen the rules and a few simple demonstrations of using regular expression \npatterns, it’s time to put that knowledge into action. The following sections demonstrate \nsome common regular expression examples within shell scripts.\nCounting directory fi les\nTo start things out, let’s look at a shell script that counts the executable fi les that are pres-\nent in the directories defi ned in your \nPATH environment variable. To do that, you need to \nparse out the \nPATH variable into separate directory names. Chapter 6 showed you how to \ndisplay the \nPATH environment variable:\n$ echo $PATH\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/\nlocal/games\n$\nYour PATH environment variable will differ, depending on where the applications are \nlocated on your Linux system. The key is to recognize that each directory in the \nPATH is \nseparated by a colon. To get a listing of directories that you can use in a script, you must \nreplace each colon with a space. You now recognize that the \nsed editor can do just that \nusing a simple regular expression:\n\n555\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  555\n$ echo $PATH | sed 's/:/ /g'\n/usr/local/sbin /usr/local/bin /usr/sbin /usr/bin /sbin /bin\n/usr/games /usr/local/games\n$\nAfter you have the directories separated out, you can use them in a standard for state-\nment (see Chapter 13) to iterate through each directory:\nmypath=$(echo $PATH | sed 's/:/ /g')\nfor directory in $mypath\ndo\n...\ndone\nAfter you have each directory, you can use the ls command to list each fi le in each direc-\ntory, and use another \nfor statement to iterate through each fi le, incrementing a counter \nfor each fi le.\nThe fi nal version of the script looks like this:\n$ cat countfiles\n#!/bin/bash\n# count number of files in your PATH\nmypath=$(echo $PATH | sed 's/:/ /g')\ncount=0\nfor directory in $mypath\ndo\n   check=$(ls $directory)\n   for item in $check\n   do\n         count=$[ $count + 1 ]\n  done\n  echo \"$directory - $count\"\n  count=0\ndone\n$ ./countfiles /usr/local/sbin - 0\n/usr/local/bin - 2\n/usr/sbin - 213\n/usr/bin - 1427\n/sbin - 186\n/bin - 152\n/usr/games - 5\n/usr/local/games – 0\n$\nNow we’re starting to see some of the power behind regular expressions!\n\n556\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  556\nValidating a phone number\nThe previous example showed how to incorporate the simple regular expression along with \nsed to replace characters in a data stream to process data. Often, regular expressions are \nused to validate data to ensure that data is in the correct format for a script.\nA common data validation application checks phone numbers. Often, data entry forms \nrequest phone numbers, and often customers fail to enter a properly formatted phone num-\nber. People in the United States use several common ways to display a phone number:\n(123)456-7890\n(123) 456-7890\n123-456-7890\n123.456.7890\nThis leaves four possibilities for how customers can enter their phone number in a form. \nThe regular expression must be robust enough to handle any of these situations.\nWhen building a regular expression, it’s best to start on the left side and build your pattern \nto match the possible characters you’ll run into. In this example, there may or may not be a \nleft parenthesis in the phone number. This can be matched by using the pattern:\n^\\(?\nThe caret is used to indicate the beginning of the data. Because the left parenthesis is a \nspecial character, you must escape it to use it as a normal character. The question mark \nindicates that the left parenthesis may or may not appear in the data to match.\nNext is the three-digit area code. In the United States, area codes start with the number 2 \n(no area codes start with the digits 0 or 1), and can go to 9. To match the area code, you’d \nuse the following pattern: \n[2-9][0-9]{2}\nThis requires that the fi rst character be a digit between 2 and 9, followed by any two digits. \nAfter the area code, the ending right parenthesis may or may not appear:\n\\)?\nAfter the area code, there can be a space, no space, a dash, or a dot. You can group those \nusing a character group along with the pipe symbol:\n(| |-|\\.)\nThe very fi rst pipe symbol appears immediately after the left parenthesis to match the no \nspace condition. You must use the escape character for the dot; otherwise, it is interpreted \nto match any character.\n\n557\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  557\nNext is the three-digit phone exchange number. Nothing special is required here:\n[0-9]{3}\nAfter the phone exchange number, you must match a space, a dash, or a dot (this time \nyou don’t have to worry about matching no space because there must be at least a space \nbetween the phone exchange number and the rest of the number):\n ( |-|\\.)\nThen to fi nish things off, you must match the four-digit local phone extension at the end \nof the string:\n[0-9]{4}$\nPutting the entire pattern together results in this:\n^\\(?[2-9][0-9]{2}\\)?(| |-|\\.)[0-9]{3}( |-|\\.)[0-9]{4}$\nYou can use this regular expression pattern in the gawk program to fi lter out bad phone \nnumbers. Now you just need to create a simple script using the regular expression in a \ngawk \nprogram and fi lter your phone list through the script. Remember that when you use regular \nexpression intervals in the \ngawk program, you must use the --re-interval command \nline option, or you won’t get the correct results.\nHere’s the script:\n$ cat isphone\n#!/bin/bash\n# script to filter out bad phone numbers\ngawk --re-interval '/^\\(?[2-9][0-9]{2}\\)?(| |-|\\¬\n[0-9]{3}( |-|\\.)[0-9]{4}/{print $0}'\n$\nAlthough you can’t tell from this listing, the gawk command is on a single line in the shell \nscript. You can then redirect phone numbers to the script for processing:\n$ echo \"317-555-1234\" | ./isphone\n317-555-1234\n$ echo \"000-555-1234\" | ./isphone\n$ echo \"312 555-1234\" | ./isphone\n312 555-1234\n$\nOr you can redirect an entire fi le of phone numbers to fi lter out the invalid ones:\n$ cat phonelist\n000-000-0000\n123-456-7890\n\n558\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  558\n212-555-1234\n(317)555-1234\n(202) 555-9876\n33523\n1234567890\n234.123.4567\n$ cat phonelist | ./isphone\n212-555-1234\n(317)555-1234\n(202) 555-9876\n234.123.4567\n$\nOnly the valid phone numbers that match the regular expression pattern appear.\nParsing an e-mail address\nThese days, e-mail has become a crucial form of communication. Trying to validate e-mail \naddresses has become quite a challenge for script builders because of the myriad ways to \ncreate an e-mail address. This is the basic form of an e-mail address:\nusername@hostname\nThe username value can use any alphanumeric character, along with several special \ncharacters:\n ■\nDot\n ■\nDash\n ■\nPlus sign\n ■\nUnderscore\nThese characters can appear in any combination in a valid e-mail userid. The \nhostname \nportion of the e-mail address consists of one or more domain names and a server name. The \nserver and domain names must also follow strict naming rules, allowing only alphanumeric \ncharacters, along with the special characters:\n ■\nDot\n ■\nUnderscore\nThe server and domain names are each separated by a dot, with the server name specifi ed \nfi rst, any subdomain names specifi ed next, and fi nally, the top-level domain name without \na trailing dot.\nAt one time, the top-level domains were fairly limited, and regular expression \npattern builders attempted to add them all in patterns for validation. Unfortunately, as \nthe Internet grew, so did the possible top-level domains. This technique is no longer a \nviable solution.\n\n559\nChapter 20: Regular Expressions\n20\nc20.indd  12/23/2014  Page  559\nLet’s start building the regular expression pattern from the left side. We know that there \ncan be multiple valid characters in the username. This should be fairly easy:\n^([a-zA-Z0-9_\\-\\.\\+]+)@\nThis grouping specifi es the allowable characters in the username and the plus sign to indi-\ncate that at least one character must be present. The next character obviously is the @ \nsymbol — no surprises there.\nThe hostname pattern uses the same technique to match the server name and the subdo-\nmain names:\n([a-zA-Z0-9_\\-\\.]+)\nThis pattern matches the text:\nserver\nserver.subdomain\nserver.subdomain.subdomain\nThere are special rules for the top-level domain. Top-level domains are only alphabetic characters, \nand they must be no fewer than two characters (used in country codes) and no more than fi ve \ncharacters in length. The following is the regular expression pattern for the top-level domain:\n\\.([a-zA-Z]{2,5})$\nPutting the entire pattern together results in the following:\n^([a-zA-Z0-9_\\-\\.\\+]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})$\nThis pattern fi lters out poorly formatted e-mail addresses from a data list. Now you can \ncreate your script to implement the regular expression:\n$ echo \"rich@here.now\" | ./isemail\nrich@here.now\n$ echo \"rich@here.now.\" | ./isemail\n$\n$ echo \"rich@here.n\" | ./isemail\n$\n$ echo \"rich@here-now\" | ./isemail\n$\n$ echo \"rich.blum@here.now\" | ./isemail\nrich.blum@here.now\n$ echo \"rich_blum@here.now\" | ./isemail\nrich_blum@here.now\n$ echo \"rich/blum@here.now\" | ./isemail\n$\n\n560\nPart III: Advanced Shell Scripting\nc20.indd  12/23/2014  Page  560\n$ echo \"rich#blum@here.now\" | ./isemail\n$\n$ echo \"rich*blum@here.now\" | ./isemail\n$\nSummary\n If you manipulate data fi les in shell scripts, you need to become familiar with regular \nexpressions. Regular expressions are implemented in Linux utilities, programming \nlanguages, and applications using regular expression engines. A host of different regular \nexpression engines is available in the Linux world. The two most popular are the POSIX \nBasic Regular Expression (BRE) engine and the POSIX Extended Regular Expression (ERE) \nengine. The \nsed editor conforms mainly to the BRE engine, while the gawk program uti-\nlizes most features found in the ERE engine.\nA regular expression defi nes a pattern template that’s used to fi lter text in a data stream. \nThe pattern consists of a combination of standard text characters and special characters. \nThe special characters are used by the regular expression engine to match a series of one or \nmore characters, similarly to how wildcard characters work in other applications.\nBy combining characters and special characters, you can defi ne a pattern to match almost \nany type of data. You can then use the \nsed editor or gawk program to fi lter specifi c data \nfrom a larger data stream, or for validating data received from data entry applications.\nThe next chapter digs deeper into using the \nsed editor to perform advanced text manipula-\ntion. Lots of advanced features are available in the \nsed editor that make it useful for han-\ndling large data streams and fi ltering out just what you need.\n\n561\nc21.indd  12/05/2014  Page  561\nCHAPTER \n21\nAdvanced sed\nIN THIS CHAPTER\nUsing multiline commands\nUnderstanding the hold space\nNegating a command\nChanging the fl ow\nReplacing via a pattern\nUsing sed in scripts\nCreating sed utilities\nC\nhapter 19 showed you how to use the basics of the sed editor to manipulate text in data \nstreams. The basic \nsed editor commands are capable of handling most of your everyday text-\nediting requirements. This chapter looks at the more advanced features that the \nsed editor \nhas to offer. These are features that you might not use as often. But when you need them, it’s nice \nto know that they’re available and how to use them.\nLooking at Multiline Commands\nWhen using the basic sed editor commands, you may have noticed a limitation. All the sed \neditor commands perform functions on a single line of data. As the \nsed editor reads a data stream, \nit divides the data into lines based on the presence of newline characters. The \nsed editor handles \neach data line one at a time, processing the defi ned script commands on the data line, and then \nmoving on to the next line and repeating the processing.\nSometimes, you need to perform actions on data that spans more than one line. This is especially \ntrue if you’re trying to fi nd or replace a phrase.\nFor example, if you’re looking for the phrase \nLinux System Administrators Group in your \ndata, it’s quite possible that the phrase’s words can be split onto two lines. If you processed the \ntext using a normal \nsed editor command, it would be impossible to detect the split phrase.\n\n562\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  562\nFortunately, the designers behind the sed editor thought of that situation and devised a \nsolution. The \nsed editor includes three special commands that you can use to process mul-\ntiline text: \n ■\nN adds the next line in the data stream to create a multiline group for processing.\n ■\nD deletes a single line in a multiline group.\n ■\nP prints a single line in a multiline group.\nThe following sections examine these multiline commands more closely and demonstrate \nhow you can use them in your scripts.\nNavigating the next command\nBefore you can examine the multiline next command, you fi rst need to look at how the \nsingle-line version of the \nnext command works. After you know what that command does, \nit’s much easier to understand how the multiline version of the \nnext command operates.\nUsing the single-line next command\nThe lowercase n command tells the sed editor to move to the next line of text in the data \nstream, without going back to the beginning of the commands. Remember that normally \nthe \nsed editor processes all the defi ned commands on a line before moving to the next line \nof text in the data stream. The single-line \nnext command alters this fl ow.\nThis may sound somewhat complicated, and sometimes it is. In this example, you have a \ndata fi le that contains fi ve lines, two of which are empty. The goal is to remove the blank \nline after the header line but leave the blank line before the last line intact. If you write a \nsed script to just remove blank lines, you remove both blank lines:\n$ cat data1.txt\nThis is the header line.\nThis is a data line.\nThis is the last line.\n$ \n$ sed '/\n^\n$/d' data1.txt\nThis is the header line.\nThis is a data line.\nThis is the last line.\n$\nBecause the line you want to remove is blank, you don’t have any text you can search for to \nuniquely identify the line. The solution is to use the \nn command. In this next example, the \n\n563\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  563\n21\nscript looks for a unique line that contains the word header. After the script identifi es that \nline, the \nn command moves the sed editor to the next line of text, which is the empty line.\n$ sed '/header/{n ; d}' data1.txt\nThis is the header line.\nThis is a data line.\nThis is the last line.\n$\nAt that point, the sed editor continues processing the command list, which uses the d \ncommand to delete the empty line. When the \nsed editor reaches the end of the command \nscript, it reads the next line of text from the data stream and starts processing commands \nfrom the top of the command script. The \nsed editor does not fi nd another line with the \nword \nheader; thus, no further lines are deleted.\nCombining lines of text\nNow that you’ve seen the single-line next command, you can look at the multiline version. \nThe single-line \nnext command moves the next line of text from the data stream into the \nprocessing space (called the \npattern space) of the sed editor. The multiline version of \nthe \nnext command (which uses a capital N) adds the next line of text to the text already \nin the pattern space.\nThis has the effect of combining two lines of text from the data stream into the same pat-\ntern space. The lines of text are still separated by a newline character, but the \nsed editor \ncan now treat both lines of text as one line.\nHere’s a demonstration of how the \nN command operates:\n$ cat data2.txt\nThis is the header line.\nThis is the first data line.\nThis is the second data line.\nThis is the last line.\n$ \n$ sed '/first/{ N ; s/\\n/ / }' data2.txt\nThis is the header line.\nThis is the first data line. This is the second data line.\nThis is the last line.\n$\nThe sed editor script searches for the line of text that contains the word “fi rst” in it. When \nit fi nds the line, it uses the \nN command to combine the next line with that line. It then \nuses the \nsubstitution command (s) to replace the newline character with a space. The \nresult is that the two lines in the text fi le appear as one line in the \nsed editor output.\n\n564\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  564\nThis has a practical application if you’re searching for a text phrase that may be split \nbetween two lines in the data fi le. Here’s an example:\n$ cat data3.txt\nOn Tuesday, the Linux System\nAdministrator's group meeting will be held.\nAll System Administrators should attend.\nThank you for your attendance.\n$\n$ sed 'N ; s/System Administrator/Desktop User/' data3.txt\nOn Tuesday, the Linux System\nAdministrator's group meeting will be held.\nAll Desktop Users should attend.\nThank you for your attendance.\n$\nThe substitution command is looking for the specifi c two-word phrase \nSystem Administrator in the text fi le. In the single line where the phrase appears, \neverything is fi ne; the \nsubstitution command can replace the text. But in the situation \nwhere the phrase is split between two lines, the \nsubstitution command doesn’t recog-\nnize the matching pattern.\nThe \nN command helps solve this problem:\n$ sed 'N ; s/System.Administrator/Desktop User/' data3.txt\nOn Tuesday, the Linux Desktop User's group meeting will be held.\nAll Desktop Users should attend.\nThank you for your attendance.\n$\nBy using the N command to combine the next line with the line where the fi rst word is \nfound, you can detect when a line split occurs in the phrase.\nNotice that the \nsubstitution command uses a wildcard pattern (.) between the word \nSystem and the word Administrator to match both the space and the newline situation. \nHowever, when it matched the newline character, it removed it from the string, causing the \ntwo lines to merge into one line. This may not be exactly what you want.\nTo solve this problem, you can use two \nsubstitution commands in the sed editor script, \none to match the multiline occurrence and one to match the single-line occurrence:\n$ sed 'N\n> s/System\\nAdministrator/Desktop\\nUser/\n> s/System Administrator/Desktop User/\n> ' data3.txt\nOn Tuesday, the Linux Desktop\n\n565\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  565\n21\nUser's group meeting will be held.\nAll Desktop Users should attend.\nThank you for your attendance.\n$\nThe fi rst substitution command specifi cally looks for the newline character between the \ntwo search words and includes it in the replacement string. This allows you to add the new-\nline character in the same place in the new text.\nThere’s still one subtle problem with this script, however. The script always reads the \nnext line of text into the pattern space before executing the \nsed editor commands. When \nit reaches the last line of text, there isn’t a next line of text to read, so the \nN command \ncauses the \nsed editor to stop. If the matching text is on the last line in the data stream, \nthe commands don’t catch the matching data:\n$ cat data4.txt\nOn Tuesday, the Linux System\nAdministrator's group meeting will be held.\nAll System Administrators should attend.\n$\n$ sed 'N\n> s/System\\nAdministrator/Desktop\\nUser/\n> s/System Administrator/Desktop User/\n> ' data4.txt\nOn Tuesday, the Linux Desktop\nUser's group meeting will be held.\nAll System Administrators should attend.\n$\nBecause the System Administrator text appears in the last line in the data stream, the \nN command misses it, as there isn’t another line to read into the pattern space to combine. \nYou can easily resolve this problem by moving your single-line commands before the \nN com-\nmand and having only the multiline commands appear after the \nN command, like this:\n$ sed '\n> s/System Administrator/Desktop User/\n> N\n> s/System\\nAdministrator/Desktop\\nUser/\n> ' data4.txt\nOn Tuesday, the Linux Desktop\nUser's group meeting will be held.\nAll Desktop Users should attend.\n$\nNow, the substitution command that looks for the phrase in a single line works just fi ne \non the last line in the data stream, and the multiline \nsubstitution command covers the \noccurrence in the middle of the data stream.\n\n566\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  566\nNavigating the multiline delete command\nChapter 19 introduced the single-line delete command (d). The sed editor uses it to \ndelete the current line in the pattern space. If you’re working with the \nN command, how-\never, you must be careful when using the single-line \ndelete command:\n$ sed 'N ; /System\\nAdministrator/d' data4.txt\nAll System Administrators should attend.\n$\nThe delete command looked for the words System and Administrator in separate lines \nand deleted both of the lines in the pattern space. This may or may not have been what you \nintended.\nThe \nsed editor provides the multiline delete command (D), which deletes only the \nfi rst line in the pattern space. It removes all characters up to and including the newline \ncharacter:\n$ sed 'N ; /System\\nAdministrator/D' data4.txt\nAdministrator's group meeting will be held.\nAll System Administrators should attend.\n$\nThe second line of text, added to the pattern space by the N command, remains intact. This \ncomes in handy if you need to remove a line of text that appears before a line that you fi nd \na data string in.\nHere’s an example of removing a blank line that appears before the fi rst line in a data \nstream:\n$ cat data5.txt\nThis is the header line.\nThis is a data line.\nThis is the last line.\n$ \n$ sed '/^$/{N ; /header/D}' data5.txt\nThis is the header line.\nThis is a data line.\nThis is the last line.\n$\nThis sed editor script looks for blank lines and then uses the N command to add the next \nline of text into the pattern space. If the new pattern space contents contain the word \n\n567\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  567\n21\nheader, the D command removes the fi rst line in the pattern space. Without the combina-\ntion of the \nN and D commands, it would be impossible to remove the fi rst blank line with-\nout removing all other blank lines.\nNavigating the multiline print command\nBy now, you’re probably catching on to the difference between the single-line and multiline \nversions of the commands. The multiline print command (\nP) follows along using the same \ntechnique. It prints only the fi rst line in a multiline pattern space. This includes all char-\nacters up to the newline character in the pattern space. It is used in much the same way as \nthe single-line \np command to display text when you use the -n option to suppress output \nfrom the script.\n$ sed -n 'N ; /System\\nAdministrator/P' data3.txt\nOn Tuesday, the Linux System\n$\nWhen the multiline match occurs, the P command prints only the fi rst line in the pattern \nspace. The power of the multiline \nP command comes into play when you combine it with \nthe \nN and D multiline commands.\nThe \nD command has a unique feature in that it forces the sed editor to return to the begin-\nning of the script and repeat the commands on the same pattern space (it doesn’t read \na new line of text from the data stream). By including the \nN command in the command \nscript, you can effectively single-step through the pattern space, matching multiple lines \ntogether.\nNext, by using the \nP command, you can print the fi rst line, and then using the D command, \nyou can delete the fi rst line and loop back to the beginning of the script. When you are \nback at the script’s beginning, the \nN command reads in the next line of text and starts the \nprocess all over again. This loop continues until you reach the end of the data stream.\nHolding Space\nThe pattern space is an active buffer area that holds the text examined by the sed editor \nwhile it processes commands. However, it isn’t the only space available in the \nsed editor for \nstoring text.\nThe \nsed editor utilizes another buffer area called the hold space. You can use the hold \nspace to temporarily hold lines of text while working on other lines in the pattern space. \nThe fi ve commands associated with operating with the hold space are shown in Table 21-1.\n\n568\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  568\nTABLE 21-1    The sed Editor Hold Space Commands\nCommandDescription\nh\nCopies pattern space to hold space\nH\nAppends pattern space to hold space\ng\nCopies hold space to pattern space\nG\nAppends hold space to pattern space\nx\nExchanges contents of pattern and hold spaces\nThese commands let you copy text from the pattern space to the hold space. This frees up \nthe pattern space to load another string for processing.\nUsually, after using the \nh or H commands to move a string to the hold space, eventually you \nwant to use the \ng, G, or x commands to move the stored string back into the pattern space \n(otherwise, you wouldn’t have cared about saving them in the fi rst place).\nWith two buffer areas, trying to determine what line of text is in which buffer area can \nsometimes get confusing. Here’s a short example that demonstrates how to use the \nh and g \ncommands to move data back and forth between the \nsed editor buffer spaces:\n$ cat data2.txt\nThis is the header line.\nThis is the first data line.\nThis is the second data line.\nThis is the last line.\n$ \n$ sed -n '/first/ {h ; p ; n ; p ; g ; p }' data2.txt\nThis is the first data line.\nThis is the second data line.\nThis is the first data line.\n$\nLook at the preceding code example step by step:\n \n1.  The  sed script uses a regular expression in the address to fi lter the line containing \nthe word \nfirst.\n \n2.  When the line containing the word first appears, the initial command in {}, the \nh command, places the line in the hold space.\n \n3.  The next command, the p command, prints the contents of the pattern space, \nwhich is still the fi rst data line.\n \n4.  The  n command retrieves the next line in the data stream \n(\nThis is the second data line) and places it in the pattern space.\n \n5.  The  p command prints the contents of the pattern space, which is now the second \ndata line.\n\n569\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  569\n21\n 6.  The  g command places the contents of the hold space \n(\nThis is the first data line) back into the pattern space, replacing the \ncurrent text.\n \n7.  The  p command prints the current contents of the pattern space, which is now back \nto the fi rst data line.\nBy shuffl ing the text lines around using the hold space, you can force the fi rst data line to \nappear after the second data line in the output. If you just drop the fi rst \np command, you \ncan output the two lines in reverse order:\n$ sed -n '/first/ {h ; n ; p ; g ; p }' data2.txt\nThis is the second data line.\nThis is the first data line.\n$\nThis is the start of something useful. You can use this technique to create a sed script that \nreverses an entire fi le of text lines! To do that, however, you need to see the negating fea-\nture of the \nsed editor, which is what the next section is all about.\nNegating a Command\nChapter 19 showed that the sed editor applies commands either to every text line in the \ndata stream or to lines specifi cally indicated by either a single address or an address range. \nYou can also confi gure a command to not apply to a specifi c address or address range in the \ndata stream.\nThe exclamation mark command (\n!) is used to negate a command. This means in situations \nwhere the command would normally have been activated, it isn’t. Here’s an example demon-\nstrating this feature:\n$ sed -n '/header/!p' data2.txt\nThis is the first data line.\nThis is the second data line.\nThis is the last line.\n$\nThe normal p command would have printed only the line in the data2 fi le that contained \nthe word \nheader. By adding the exclamation mark, the opposite happens — all lines in the \nfi le are printed except the one that contained the word \nheader.\nUsing the exclamation mark comes in handy in several applications. Recall that earlier in \nthe chapter, the “Navigating the next command” section showed a situation where a \nsed \neditor command wouldn’t operate on the last line of text in the data stream because there \nwasn’t a line after it. You can use the exclamation point to fi x that problem:\n$ sed 'N;\n> s/System\\nAdministrator/Desktop\\nUser/\n> s/System Administrator/Desktop User/\n\n570\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  570\n> ' data4.txt\nOn Tuesday, the Linux Desktop\nUser's group meeting will be held.\nAll System Administrators should attend.\n$\n$ sed '$!N;\n> s/System\\nAdministrator/Desktop\\nUser/\n> s/System Administrator/Desktop User/\n> ' data4.txt\nOn Tuesday, the Linux Desktop\nUser's group meeting will be held.\nAll Desktop Users should attend.\n$ \nThis example shows the exclamation mark used with the N command, along with the dollar \nsign (\n$) special address. The dollar sign represents the last line of text in the data stream, \nso when the \nsed editor reaches the last line, it doesn’t execute the N command. However, \nfor all other lines, it does execute the command.\nUsing this technique, you can reverse the order of text lines in a data stream. To reverse \nthe order of the lines as they appear in the text stream (display the last line fi rst and the \nfi rst line last), you need to do some fancy footwork using the hold space.\nThe pattern you need to work with goes like this:\n \n1.  Place a line in the pattern space.\n \n2.  Place the line from the pattern space to the hold space.\n \n3.  Put the next line of text in the pattern space.\n \n4.  Append the hold space to the pattern space.\n \n5.  Place everything in the pattern space into the hold space.\n \n6.  Repeat Steps 3 through 5 until you’ve put all the lines in reverse order in the \nhold space.\n \n7.   Retrieve the lines, and print them.\nFigure 21-1 diagrams what this looks like in more detail.\nWhen using this technique, you do not want to print lines as they are processed. This \nmeans using the \n-n command line option for sed. The next thing to determine is how to \nappend the hold space text to the pattern space text. This is done by using the \nG command. \nThe only problem is that you don’t want to append the hold space to the fi rst line of text \nprocessed. This is easily solved by using the exclamation mark command:\n1!G\nThe next step is to place the new pattern space (the text line with the appended reverse \nlines) into the hold space. This is simple enough; just use the \nh command.\n\n571\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  571\n21\nWhen you’ve got the entire data stream in the pattern space in reverse order, you just need \nto print the results. You know you have the entire data stream in the pattern space when \nyou’ve reached the last line in the data stream. To print the results, just use the following \ncommand:\n$p\nFIGURE 21-1\nReversing the order of a text file using the hold space\nLine 1\nLine 1\nLine 1\nLine 1\nLine 1\nLine 1\nLine 1\nLine 1\ndata filePattern SpaceHold Space\nLine 2\nLine 2\nLine 3\nLine 3\nLine 3\nLine 3\nLine 2\n1\n3\n7\n5\n4\n2\nLine 2\nLine 2\nLine 2\nLine 2\nLine 3\nLine 4\nLine 4\nLine 4\nThose are the pieces you need to create your line-reversing sed editor script. Now try it out \nin a test run:\n$ cat data2.txt\nThis is the header line.\nThis is the first data line.\nThis is the second data line.\nThis is the last line.\n$ \n$ sed -n '{1!G ; h ; $p }' data2.txt\nThis is the last line.\nThis is the second data line.\nThis is the first data line.\nThis is the header line.\n$\n\n572\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  572\nThe sed editor script performed as expected. The output from the script reverses the origi-\nnal lines in the text fi le. This demonstrates the power of using the hold space in your \nsed \nscripts. It provides an easy way to manipulate the order of lines in the script output.\nIn case you’re wondering, a bash shell command can perform the function of reversing a text fi le. The tac command \ndisplays a text fi le in reverse order. You probably noticed the clever name of the command because it performs the \nreverse function of the \ncat command.\nChanging the Flow\nNormally, the sed editor processes commands starting at the top and proceeding toward \nthe end of the script (the exception is the \nD command, which forces the sed editor to \nreturn to the top of the script without reading a new line of text). The \nsed editor provides \na method for altering the fl ow of the command script, producing a result similar to that of a \nstructured programming environment.\nBranching\nIn the previous section, you saw how the exclamation mark command is used to negate \nthe effect of a command on a line of text. The \nsed editor provides a way to negate an \nentire section of commands, based on an address, an address pattern, or an address range. \nThis allows you to perform a group of commands only on a specifi c subset within the data \nstream.\nHere’s the format of the \nbranch command:\n[address]b [label]\nThe address parameter determines which line or lines of data trigger the branch com-\nmand. The \nlabel parameter defi nes the location to branch to. If the label parameter is \nnot present, the \nbranch command proceeds to the end of the script.\n$ cat data2.txt\nThis is the header line.\nThis is the first data line.\nThis is the second data line.\nThis is the last line.\n$\n$ sed '{2,3b ; s/This is/Is this/ ; s/line./test?/}' data2.txt\nIs this the header test?\nThis is the first data line.\nThis is the second data line.\nIs this the last test?\n$\n\n573\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  573\n21\nThe branch command skips the two substitution commands for the second and third \nlines in the data stream.\nInstead of going to the end of the script, you can defi ne a label for the \nbranch command \nto jump to. Labels start with a colon and can be up to seven characters in length:\n:label2\nTo specify the label, just add it after the b command. Using labels allows you to skip com-\nmands that match the \nbranch address but still process other commands in the script:\n$ sed '{/first/b jump1 ; s/This is the/No jump on/ \n> :jump1 \n> s/This is the/Jump here on/}' data2.txt \nNo jump on header line \nJump here on first data line \nNo jump on second data line \nNo jump on last line \n$\nThe branch command specifi es that the program should jump to the script line labeled \njump1 if the matching text “fi rst” appears in the line. If the branch command pattern \ndoesn’t match, the \nsed editor continues processing commands in the script, including the \ncommand after the \nbranch label. (Thus, all three substitution commands are processed \non lines that don’t match the \nbranch pattern.)\nIf a line matches the \nbranch pattern, the sed editor branches to the branch label line. \nThus, only the last \nsubstitution command is executed.\nThe example shows branching to a label further down in the \nsed script. You can also \nbranch to a label that appears earlier in the script, thus creating a looping effect:\n$ echo \"This, is, a, test, to, remove, commas.\" | sed -n '{\n> :start\n> s/,//1p\n> b start\n> }'\nThis is, a, test, to, remove, commas.\nThis is a, test, to, remove, commas.\nThis is a test, to, remove, commas.\nThis is a test to, remove, commas.\nThis is a test to remove, commas.\nThis is a test to remove commas.\n^C\n$\nEach script iteration removes the fi rst occurrence of a comma from the text string and \nprints the string. There’s one catch to this script: It never ends. This situation creates an \nendless loop, searching for commas until you manually stop it by sending a signal with the \nCtrl+C key combination.\n\n574\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  574\nTo prevent this problem, you should specify an address pattern for the branch command to \nlook for. If the pattern isn’t present, the branching should stop:\n$ echo \"This, is, a, test, to, remove, commas.\" | sed -n '{\n> :start\n> s/,//1p\n> /,/b start\n> }'\nThis is, a, test, to, remove, commas.\nThis is a, test, to, remove, commas.\nThis is a test, to, remove, commas.\nThis is a test to, remove, commas.\nThis is a test to remove, commas.\nThis is a test to remove commas.\n$\nNow the branch command branches only if there’s a comma in the line. After the last \ncomma has been removed, the \nbranch command doesn’t execute, allowing the script to \nproperly fi nish.\nTesting\nSimilar to the branch command, the test command (t) is also used to modify the fl ow \nof the \nsed editor script. Instead of jumping to a label based on an address, the test com-\nmand jumps to a label based on the outcome of a \nsubstitution command.\nIf the \nsubstitution command successfully matches and substitutes a pattern, the test \ncommand branches to the specifi ed label. If the \nsubstitution command doesn’t match \nthe specifi ed pattern, the \ntest command doesn’t branch.\nThe \ntest command uses the same format as the branch command:\n[address]t [label]\nLike the branch command, if you don’t specify a label, sed branches to the end of the \nscript if the test succeeds.\nThe \ntest command provides a cheap way to perform a basic if-then statement on the \ntext in the data stream. For example, if you don’t need to make a substitution if another \nsubstitution was made, the \ntest command can help:\n$ sed '{ \n> s/first/matched/ \n> t \n> s/This is the/No match on/ \n> }' data2.txt \nNo match on header line \n\n575\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  575\n21\nThis is the matched data line \nNo match on second data line \nNo match on last line \n$\nThe fi rst substitution command looks for the pattern text first. If it matches the \npattern in the line, it replaces the text, and the \ntest command jumps over the second \nsubstitution command. If the fi rst substitution command doesn’t match the pattern, \nthe second \nsubstitution command is processed.\nUsing the \ntest command, you can clean up the loop you tried using the branch command:\n$ echo \"This, is, a, test, to, remove, commas. \" | sed -n '{\n> :start\n> s/,//1p\n> t start\n> }'\nThis is, a, test, to, remove, commas.\nThis is a, test, to, remove, commas.\nThis is a test, to, remove, commas.\nThis is a test to, remove, commas.\nThis is a test to remove, commas.\nThis is a test to remove commas.\n$\nWhen there are no more substitutions to make, the test command doesn’t branch and con-\ntinues with the rest of the script.\nReplacing via a Pattern\nYou’ve seen how to use patterns in the sed commands to replace text in the data stream. \nHowever, when using wildcard characters it’s not easy to know exactly what text will match \nthe pattern.\nFor example, say that you want to place double quotation marks around a word you match \nin a line. That’s simple enough if you’re just looking for one word in the pattern to match:\n$ echo \"The cat sleeps in his hat.\" | sed 's/cat/\"cat\"/'\nThe \"cat\" sleeps in his hat.\n$\nBut what if you use a wildcard character (.) in the pattern to match more than one word? \n$ echo \"The cat sleeps in his hat.\" | sed 's/.at/\".at\"/g'\nThe \".at\" sleeps in his \".at\".\n$\n\n576\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  576\nThe substitution string used the dot wildcard character to match any occurrence of a letter \nfollowed by “at”. Unfortunately, the replacement string doesn’t match the wildcard charac-\nter value of the matching word.\nUsing the ampersand\nThe sed editor has a solution for you. The ampersand symbol (&) is used to represent the \nmatching pattern in the \nsubstitution command. Whatever text matches the pattern \ndefi ned, you can use the ampersand symbol to recall it in the replacement pattern. This lets \nyou manipulate whatever word matches the pattern defi ned:\n$ echo \"The cat sleeps in his hat.\" | sed 's/.at/\"&\"/g'\nThe \"cat\" sleeps in his \"hat\".\n$\nWhen the pattern matches the word cat, “cat” appears in the substituted word. When it \nmatches the word hat, “hat” appears in the substituted word.\nReplacing individual words\nThe ampersand symbol retrieves the entire string that matches the pattern you specify \nin the \nsubstitution command. Sometimes, you’ll only want to retrieve a subset of the \nstring. You can do that, too, but it’s a little tricky.\nThe \nsed editor uses parentheses to defi ne a substring component within the substitution \npattern. You can then reference each substring component using a special character in the \nreplacement pattern. The replacement character consists of a backslash and a number. The \nnumber indicates the substring component’s position. The \nsed editor assigns the fi rst com-\nponent the character \n\\1, the second component the character \\2, and so on.\nWhen you use parentheses in the substitution command, you must use the escape character to identify them \nas grouping characters and not normal parentheses. This is the reverse of when you escape other special characters.\nLook at an example of using this feature in a sed editor script:\n$ echo \"The System Administrator manual\" | sed '\n> s/\\(System\\) Administrator/\\1 User/'\nThe System User manual\n$\nThis substitution command uses one set of parentheses around the word System identi-\nfying it as a substring component. It then uses the \n\\1 in the replacement pattern to recall \nthe fi rst identifi ed component. This isn’t too exciting, but it can really be useful when \nworking with wildcard patterns.\n\n577\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  577\n21\nIf you need to replace a phrase with just a single word, that’s a substring of the phrase, but \nthat substring just happens to be using a wildcard character; using substring components \nis a lifesaver:\n$ echo \"That furry cat is pretty\" | sed 's/furry \\(.at\\)/\\1/'\nThat cat is pretty\n$\n$ echo \"That furry hat is pretty\" | sed 's/furry \\(.at\\)/\\1/'\nThat hat is pretty\n$\nIn this situation, you can’t use the ampersand symbol, because it would replace the entire \nmatching pattern. The substring component provides the answer, allowing you to select \njust which part of the pattern to use as the replacement pattern.\nThis feature can be especially helpful when you need to insert text between two or more \nsubstring components. Here’s a script that uses substring components to insert a comma in \nlong numbers:\n$ echo \"1234567\" | sed '{\n> :start\n> s/\\(.*[0-9]\\)\\([0-9]\\{3\\}\\)/\\1,\\2/\n> t start\n> }'\n1,234,567\n$\nThe script divides the matching pattern into two components:\n.*[0-9]\n[0-9]{3}\nThis pattern looks for two substrings. The fi rst substring is any number of characters, \nending in a digit. The second substring is a series of three digits (see Chapter 20 for infor-\nmation about how to use braces in a regular expression). If this pattern is found in the \ntext, the replacement text puts a comma between the two components, each identifi ed by \nits component position. The script uses the \ntest command to iterate through the number \nuntil all commas have been placed.\nPlacing sed Commands in Scripts\nNow that you’ve seen the various parts of the sed editor, it’s time to put them together \nand use them in your shell scripts. This section demonstrates some of the features that you \nshould know about when using the \nsed editor in your bash shell scripts.\n\n578\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  578\nUsing wrappers\nYou may have noticed that trying to implement a sed editor script can be cumbersome, \nespecially if the script is long. Instead of having to retype the entire script each time you \nwant to use it, you can place the \nsed editor command in a shell script wrapper. The wrap-\nper acts as a go-between for the \nsed editor script and the command line.\nOnce inside the shell script, you can use normal shell variables and parameters with your \nsed editor scripts. Here’s an example of using the command line parameter variable as the \ninput to a \nsed script:\n$ cat reverse.sh\n#!/bin/bash\n# Shell wrapper for sed editor script.\n#               to reverse text file lines.\n#\nsed -n '{ 1!G ; h ; $p }' $1\n#\n$\nThe shell script called reverse uses the sed editor script to reverse text lines in a data \nstream. It uses the \n$1 shell parameter to retrieve the fi rst parameter from the command \nline, which should be the name of the fi le to reverse:\n$ ./reverse.sh data2.txt\nThis is the last line.\nThis is the second data line.\nThis is the first data line.\nThis is the header line.\n$\nNow you can easily use the sed editor script on any fi le, without having to constantly \nretype the entire command line.\nRedirecting sed output\nBy default, the sed editor outputs the results of the script to STDOUT. You can employ all \nthe standard methods of redirecting the output of the \nsed editor in your shell scripts.\nYou can use dollar sign/parenthesis, \n$(), to redirect the output of your sed editor com-\nmand to a variable for use later in the script. The following is an example of using the \nsed \nscript to add commas to the result of a numeric computation:\n$ cat fact.sh\n#!/bin/bash\n# Add commas to number in factorial answer\n#\nfactorial=1\n\n579\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  579\n21\ncounter=1\nnumber=$1\n#\nwhile [ $counter -le $number ]\ndo\n   factorial=$[ $factorial * $counter ]\n   counter=$[ $counter + 1 ]\ndone\n#\nresult=$(echo $factorial | sed '{\n:start\ns/\\(.*[0-9]\\)\\([0-9]\\{3\\}\\)/\\1,\\2/\nt start\n}')\n#\necho \"The result is $result\"\n#\n$\n$ ./fact.sh 20\nThe result is 2,432,902,008,176,640,000\n$\nAfter you use the normal factorial calculation script, the result of that script is used as the \ninput to the \nsed editor script, which adds commas. This value is then used in the echo \nstatement to produce the result.\nCreating sed Utilities\nAs you’ve seen in the short examples presented so far in this chapter, you can do lots of \ncool data-formatting things with the \nsed editor. This section shows a few handy well-\nknown \nsed editor scripts for performing common data-handling functions.\nSpacing with double lines\nTo start things off, look at a simple sed script to insert a blank line between lines in a \ntext fi le:\n$ sed 'G' data2.txt\nThis is the header line.\nThis is the first data line.\nThis is the second data line.\nThis is the last line.\n$\n\n580\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  580\nThat was pretty simple! The key to this trick is the default value of the hold space. \nRemember that the \nG command simply appends the contents of the hold space to the cur-\nrent pattern space contents. When you start the \nsed editor, the hold space contains an \nempty line. By appending that to an existing line, you create a blank line after the exist-\ning line.\nYou may have noticed that this script also adds a blank line to the last line in the data \nstream, producing a blank line at the end of the fi le. If you want to get rid of this, you can \nuse the negate symbol and the last line symbol to ensure that the script doesn’t add the \nblank line to the last line of the data stream:\n$ sed '$!G' data2.txt\nThis is the header line.\nThis is the first data line.\nThis is the second data line.\nThis is the last line.\n$\nNow that looks a little better. As long as the line isn’t the last line, the G command \nappends the contents of the hold space. When the \nsed editor gets to the last line, it skips \nthe \nG command.\nSpacing fi les that may have blanks\nTo take double spacing one step further, what if the text fi le already has a few blank lines, \nbut you want to double space all the lines? If you use the previous script, you’ll get some \nareas that have too many blank lines, because each existing blank line gets doubled:\n$ cat data6.txt\nThis is line one.\nThis is line two.\nThis is line three.\nThis is line four.\n$ \n$ sed '$!G' data6.txt\nThis is line one.\nThis is line two.\nThis is line three.\nThis is line four.\n$\n\n581\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  581\n21\nNow you have three blank lines where the original blank line was located. The solution to \nthis problem is to fi rst delete any blank lines from the data stream and then use the \nG com-\nmand to insert new blank lines after all the lines. To delete existing blank lines, you just \nneed to use the \nd command with a pattern that matches a blank line:\n/\n^\n$/d\nThis pattern uses the start line tag (the caret) and the end line tag (the dollar sign). \nAdding this pattern to the script produces the desired results:\n$ sed '/^$/d ; $!G' data6.txt\nThis is line one.\nThis is line two.\nThis is line three.\nThis is line four.\n$\nPerfect! It works just as expected.\nNumbering lines in a fi le\nChapter 19 showed you how to use the equal sign to display the line numbers of lines in the \ndata stream:\n$ sed '=' data2.txt\n1\nThis is the header line.\n2\nThis is the first data line.\n3\nThis is the second data line.\n4\nThis is the last line.\n$\nThis can be a little awkward to read, because the line number is on a line above the actual \nline in the data stream. A better solution is to place the line number on the same line as \nthe text.\nNow that you’ve seen how to combine lines using the \nN command, it shouldn’t be too hard \nto utilize that information in the \nsed editor script. The trick to this utility, however, is \nthat you can’t combine the two commands in the same script.\n\n582\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  582\nAfter you have the output for the equal sign command, you can pipe the output to another \nsed editor script that uses the N command to combine the two lines. You also need to use \nthe \nsubstitution command to replace the newline character with either a space or a tab \ncharacter. Here’s what the fi nal solution looks like:\n$ sed '=' data2.txt | sed 'N; s/\\n/ /'\n1 This is the header line.\n2 This is the first data line.\n3 This is the second data line.\n4 This is the last line.\n$\nNow that looks much better. This is a great little utility to have around when working on \nprograms where you need to see the line numbers used in error messages.\nThere are bash shell commands that can also add line numbers. However, they add some \nadditional (and potentially unwanted spacing):\n$ nl data2.txt\n     1  This is the header line.\n     2  This is the first data line.\n     3  This is the second data line.\n     4  This is the last line.\n$\n$ cat -n data2.txt\n     1  This is the header line.\n     2  This is the first data line.\n     3  This is the second data line.\n     4  This is the last line.\n$\nThe sed editor script handles the output without any additional spacing.\nPrinting last lines\nSo far, you’ve seen how to use the p command to print all the lines in a data stream or just \nlines that match a specifi c pattern. What if you just need to work with the last few lines of \na long listing, such as a log fi le?\nThe dollar sign represents the last line of a data stream, so it’s easy to display just the \nlast line:\n$ sed -n '$p' data2.txt\nThis is the last line.\n$\nNow how can you use the dollar sign symbol to display a set number of lines at the end of \nthe data stream? The answer is to create a rolling window.\n\n583\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  583\n21\nA rolling window is a common way to examine blocks of text lines in the pattern space by \ncombining them using the \nN command. The N command appends the next line of text to \nthe text already in the pattern space. After you have a block of 10 text lines in the pattern \nspace, you can check to see if you’re at the end of the data stream using the dollar sign. If \nyou’re not at the end, continue adding more lines to the pattern space, while removing the \noriginal lines (remember the \nD command, which deletes the fi rst line in the pattern space).\nBy looping through the \nN and D commands, you add new lines to the block of lines in the \npattern space while removing old lines. The \nbranch command is the perfect fi t for the \nloop. To end the loop, just identify the last line and use the \nq command to quit.\nHere’s what the fi nal \nsed editor script looks like:\n$ cat data7.txt\nThis is line 1.\nThis is line 2.\nThis is line 3.\nThis is line 4.\nThis is line 5.\nThis is line 6.\nThis is line 7.\nThis is line 8.\nThis is line 9.\nThis is line 10.\nThis is line 11.\nThis is line 12.\nThis is line 13.\nThis is line 14.\nThis is line 15.\n$\n$ sed '{\n> :start\n> $q ; N ; 11,$D\n> b start\n> }' data7.txt\nThis is line 6.\nThis is line 7.\nThis is line 8.\nThis is line 9.\nThis is line 10.\nThis is line 11.\nThis is line 12.\nThis is line 13.\nThis is line 14.\nThis is line 15.\n$\n\n584\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  584\nThe script fi rst checks whether the line is the last line in the data stream. If it is, the quit \ncommand stops the loop. The \nN command appends the next line to the current line in the \npattern space. The \n11,$D command deletes the fi rst line in the pattern space if the current \nline is after line 10. This creates the sliding window effect in the pattern space. Thus, the \nsed program script displays only the last 10 lines of the data7.txt fi le.\nDeleting lines\nAnother useful utility for the sed editor is to remove unwanted blank lines in a data \nstream. It’s easy to remove all the blank lines from a data stream, but it takes a little inge-\nnuity to selectively remove blank lines. This section shows you a couple of quick \nsed editor \nscripts that you can use to help remove unwanted blank lines from your data.\nDeleting consecutive blank lines\nIt can be a nuisance when extra blank lines crop up in data fi les. Often you have a data \nfi le that contains blank lines, but sometimes a data line is missing and produces too many \nblank lines (as you saw in the double-spacing example earlier).\nThe easiest way to remove consecutive blank lines is to check the data stream using a \nrange address. Chapter 19 showed you how to use ranges in addresses, including how to \nincorporate patterns in the address range. The \nsed editor executes the command for all \nlines that match within the specifi ed address range.\nThe key to removing consecutive blank lines is to create an address range that includes \na non-blank line and a blank line. If the \nsed editor comes across this range, it shouldn’t \ndelete the line. However, for lines that don’t match that range (two or more blank lines in a \nrow), it should delete the lines.\nHere’s the script to do this:\n/./,/\n^\n$/!d\nThe range is /./ to /\n^\n$/. The start address in the range matches any line that contains at \nleast one character. The end address in the range matches a blank line. Lines within this \nrange aren’t deleted.\nHere’s the script in action:\n$ cat data8.txt\nThis is line one.\nThis is line two.\nThis is line three.\nThis is line four.\n\n585\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  585\n21\n$\n$ sed '/./,/^$/!d' data8.txt\nThis is line one.\nThis is line two.\nThis is line three.\nThis is line four.\n$\nNo matter how many blank lines appear between lines of data in the fi le, the output places \nonly one blank line between the lines.\nDeleting leading blank lines\nIt is also a nuisance when data fi les contain multiple blank lines at the start of the fi le. \nOften when you are trying to import data from a text fi le into a database, the blank lines \ncreate null entries, throwing off any calculations using the data.\nRemoving blank lines from the top of a data stream is not a diffi cult task. Here’s the script \nthat accomplishes that function:\n/./,$!d\nThe script uses an address range to determine what lines are deleted. The range starts \nwith a line that contains a character and continues to the end of the data stream. Any line \nwithin this range is not deleted from the output. This means that any lines before the fi rst \nline that contain a character are deleted.\nLook at this simple script in action:\n$ cat data9.txt\nThis is line one.\nThis is line two.\n$\n$ sed '/./,$!d' data9.txt\nThis is line one.\nThis is line two.\n$\nThe test fi le contains two blank lines before the data lines. The script successfully removes \nboth of the leading blank lines, while keeping the blank line within the data intact.\n\n586\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  586\nDeleting trailing blank lines\nUnfortunately, deleting trailing blank lines is not as simple as deleting leading blank lines. \nJust like printing the end of a data stream, deleting blank lines at the end of a data stream \nrequires a little ingenuity and looping.\nBefore we start the discussion, let’s see what the script looks like:\nsed '{\n:start\n/\n^\n\\n*$/{$d; N; b start }\n}'\nThis may look a little odd to you at fi rst. Notice that there are braces within the normal \nscript braces. This allows you to group commands together within the overall command \nscript. The group of commands applies to the specifi ed address pattern. The address pattern \nmatches any line that contains only a newline character. When one is found, if it’s the last \nline, the \ndelete command deletes it. If it’s not the last line, the N command appends the \nnext line to it, and the \nbranch command loops to the beginning to start over.\nHere’s the script in action:\n$ cat data10.txt\nThis is the first line.\nThis is the second line.\n$ sed '{\n> :start\n> /^\\n*$/{$d ; N ; b start }\n> }' data10.txt\nThis is the first line.\nThis is the second line.\n$\nThe script successfully removed the blank lines from the end of the text fi le.\nRemoving HTML tags\nThese days, it’s not uncommon to download text from a website to save or use as data in an \napplication. Sometimes, however, when you download text from the website, you also get \nthe HTML tags used to format the data. This can be a problem when all you want to see is \nthe data.\n\n587\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  587\n21\nA standard HTML web page contains several different types of HTML tags, identifying for-\nmatting features required to properly display the page information. Here’s a sample of what \nan HTML fi le looks like:\n$ cat data11.txt\n<html>\n<head>\n<title>This is the page title</title>\n</head>\n<body>\n<p>\nThis is the <b>first</b> line in the Web page.\nThis should provide some <i>useful</i>\ninformation to use in our sed script.\n</body>\n</html>\n$\nHTML tags are identifi ed by the less-than and greater-than symbols. Most HTML tags come \nin pairs. One tag starts the formatting process (for example, \n<b> for bolding), and another \ntag stops the formatting process (for example, \n</b> to turn off bolding).\nRemoving HTML tags creates a problem, however, if you’re not careful. At fi rst glance, you’d \nthink that the way to remove HTML tags would be to just look for a text string that starts \nwith a less-than symbol (<), ends with a greater-than symbol (>), and has data in between \nthe symbols:\n s/<.*>//g\nUnfortunately, this command has some unintended consequences:\n$ sed 's/<.*>//g' data11.txt\nThis is the  line in the Web page.\nThis should provide some\ninformation to use in our sed script.\n$\n\n588\nPart III: Advanced Shell Scripting\nc21.indd  12/05/2014  Page  588\nNotice that the title text is missing, along with the text that was bolded and italicized. \nThe \nsed editor literally interpreted the script to mean any text between the less-than and \ngreater-than sign, including other less-than and greater-than signs! Each time the text was \nenclosed in HTML tags (such as \n<b>first</b>), the sed script removed the entire text.\nThe solution to this problem is to have the \nsed editor ignore any embedded greater-than \nsigns between the original tags. To do that, you can create a character class that negates \nthe greater-than sign. This changes the script to:\ns/<[\n^\n>]*>//g\nThis script now works properly, displaying the data you need to see from the web page \nHTML code:\n$ sed 's/<[^>]*>//g' data11.txt\nThis is the page title\nThis is the first line in the Web page.\nThis should provide some useful\ninformation to use in our sed script.\n$\nThat’s a little better. To clean things up some, you can add a delete command to get rid of \nthose pesky blank lines:\n$ sed 's/<[^>]*>//g ; /^$/d' data11.txt\nThis is the page title\nThis is the first line in the Web page.\nThis should provide some useful\ninformation to use in our sed script.\n$\nNow that’s much more compact; there’s only the data you need to see.\nSummary\nThe sed editor provides some advanced features that allow you to work with text patterns \nacross multiple lines. This chapter showed you how to use the \nnext command to retrieve \nthe next line in a data stream and place it in the pattern space. Once in the pattern space, \nyou can perform complex \nsubstitution commands to replace phrases that span more \nthan one line of text.\n\n589\nChapter 21: Advanced sed\nc21.indd  12/05/2014  Page  589\n21\nThe multiline delete command allows you to remove the fi rst line when the pattern space \ncontains two or more lines. This is a convenient way to iterate through multiple lines in \nthe data stream. Similarly, the multiline \nprint command allows you to print just the fi rst \nline when the pattern space contains two or more lines of text. The combination of the \nmultiline commands allows you to iterate through the data stream and create a multiline \nsubstitution system.\nNext, we covered the hold space. The hold space allows you to set aside a line of text while \nprocessing more lines of text. You can recall the contents of the hold space at any time and \neither replace the text in the pattern space or append the contents of the hold space to the \ntext in the pattern space. Using the hold space allows you to sort through data streams, \nreversing the order of text lines as they appear in the data.\nNext we reviewed the various \nsed editor fl ow control commands. The branch command \nprovides a way for you to alter the normal fl ow of \nsed editor commands in the script, \ncreating loops or skipping commands under certain conditions. The \ntest command pro-\nvides an \nif-then type of statement for your sed editor command scripts. The test \ncommand branches only if a prior \nsubstitution command succeeds in replacing text \nin a line.\nThe chapter concluded with a discussion of how to use \nsed scripts in your shell scripts. A \ncommon technique for large \nsed scripts is to place the script in a shell wrapper. You can \nuse command line parameter variables within the \nsed script to pass shell command line \nvalues. This creates an easy way to utilize your \nsed editor scripts directly from the com-\nmand line, or even from other shell scripts.\nThe next chapter digs deeper into the \ngawk world. The gawk program supports many \nfeatures of higher-level programming languages. You can create some pretty involved data \nmanipulation and reporting programs just by using \ngawk . The chapter describes the vari-\nous programming features and demonstrates how to use them to generate your own fancy \nreports from simple data. \n\n\n\n591\nc22.indd  12/16/2014  Page  591\nCHAPTER \n22\nAdvanced gawk\nIN THIS CHAPTER\nReexamining gawk\nUsing variables in gawk\nUsing structured commands\nFormatting your printing\nWorking with functions\nC\nhapter 19 introduced the gawk program and demonstrated the basics of using it to produce \nformatted reports from raw data fi les. This chapter dives more deeply into customizing \ngawk \nto produce reports. The \ngawk program is a full-fl edged programming language, providing \nfeatures that allow you to write advanced programs to manipulate data. If you are jumping into the \nshell script world from another programming language, you should feel right at home with \ngawk. \nIn this chapter, you’ll see how to use the \ngawk programming language to write programs to handle \njust about any data-formatting task you’ll run into.\nUsing Variables\nOne important feature of any programming language is the ability to store and recall values using \nvariables. The \ngawk programming language supports two different types of variables:\n ■\nBuilt-in variables\n ■\nUser-defi ned variables\nSeveral built-in variables are available for you to use in \ngawk. The built-in variables contain infor-\nmation used in handling the data fi elds and records in the data fi le. You can also create your own \nvariables in your \ngawk programs. The following sections walk you through how to use variables in \nyour \ngawk programs.\n\n592\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  592\nBuilt-in variables\nThe gawk program uses built-in variables to reference specifi c features within the program \ndata. This section describes the built-in variables available for you to use in your \ngawk \nprograms and demonstrates how to use them.\nThe field and record separator variables\nChapter 19 demonstrated one type of built-in variable available in gawk: the data fi eld \nvariables. The data fi eld variables allow you to reference individual data fi elds within a data \nrecord using a dollar sign and the numerical position of the data fi eld in the record. Thus, \nto reference the fi rst data fi eld in the record, you use the \n$1 variable. To reference the \nsecond data fi eld, you use the \n$2 variable, and so on.\nData fi elds are delineated by a fi eld separator character. By default, the fi eld separator \ncharacter is a whitespace character, such as a space or a tab. Chapter 19 showed how to \nchange the fi eld separator character either on the command line by using the \n-F command \nline parameter or within the \ngawk program using the special FS built-in variable.\nThe \nFS built-in variable belongs to a group of built-in variables that control how gawk \nhandles fi elds and records in both input data and output data. Table 22-1 lists the built-in \nvariables contained in this group.\nTABLE 22-1    The gawk Data Field and Record Variables\nVariableDescription\nFIELDWIDTHS\nA space-separated list of numbers defi ning the exact width (in spaces) of \neach data fi eld\nFS\nInput fi eld separator character\nRS\nInput record separator character\nOFS\nOutput fi eld separator character\nORS\nOutput record separator character\nThe FS and OFS variables defi ne how your gawk program handles data fi elds in the data \nstream. You’ve already seen how to use the \nFS variable to defi ne what character separates \ndata fi elds in a record. The \nOFS variable performs the same function but for the output by \nusing the \nprint command.\nBy default, \ngawk sets the OFS variable to a space, so when you use the command:\nprint $1,$2,$3\nyou see the output as:\nfield1 field2 field3\n\n593\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  593\n22\n22\nYou can see this in the following example:\n$ cat data1\ndata11,data12,data13,data14,data15\ndata21,data22,data23,data24,data25\ndata31,data32,data33,data34,data35\n$ gawk 'BEGIN{FS=\",\"} {print $1,$2,$3}' data1\ndata11 data12 data13\ndata21 data22 data23\ndata31 data32 data33\n$\nThe print command automatically places the value of the OFS variable between each data \nfi eld in the output. By setting the \nOFS variable, you can use any string to separate data \nfi elds in the output:\n$ gawk 'BEGIN{FS=\",\"; OFS=\"-\"} {print $1,$2,$3}' data1\ndata11-data12-data13\ndata21-data22-data23\ndata31-data32-data33\n$ gawk 'BEGIN{FS=\",\"; OFS=\"--\"} {print $1,$2,$3}' data1\ndata11--data12--data13\ndata21--data22--data23\ndata31--data32--data33\n$ gawk 'BEGIN{FS=\",\"; OFS=\"<-->\"} {print $1,$2,$3}' data1\ndata11<-->data12<-->data13\ndata21<-->data22<-->data23\ndata31<-->data32<-->data33\n$\nThe FIELDWIDTHS variable allows you to read records without using a fi eld separator char-\nacter. In some applications, instead of using a fi eld separator character, data is placed in \nspecifi c columns within the record. In these instances, you must set the \nFIELDWIDTHS \nvariable to match the layout of the data in the records.\nAfter you set the \nFIELDWIDTHS variable, gawk ignores the FS and calculates data fi elds \nbased on the provided fi eld width sizes. Here’s an example using fi eld widths instead of \nfi eld separator characters:\n$ cat data1b\n1005.3247596.37\n115-2.349194.00\n05810.1298100.1\n$ gawk 'BEGIN{FIELDWIDTHS=\"3 5 2 5\"}{print $1,$2,$3,$4}' data1b\n100 5.324 75 96.37\n115 -2.34 91 94.00\n058 10.12 98 100.1\n$\n\n594\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  594\nThe FIELDWIDTHS variable defi nes four data fi elds, and gawk parses the data record \naccordingly. The string of numbers in each record is split based on the defi ned fi eld width \nvalues.\nIt’s important to remember that after you set the FIELDWIDTHS variable, those values must remain constant. This \nmethod can’t accommodate variable-length data fi elds.\nThe RS and ORS variables defi ne how your gawk program handles records in the data \nstream. By default, \ngawk sets the RS and ORS variables to the newline character. The \ndefault \nRS variable value indicates that each new line of text in the input data stream is a \nnew record.\nSometimes, you run into situations where data fi elds are spread across multiple lines in the \ndata stream. A classic example of this is data that includes an address and phone number, \neach on a separate line:\nRiley Mullen\n123 Main Street\nChicago, IL 60601\n(312)555-1234\nIf you try to read this data using the default FS and RS variable values, gawk reads each \nline as a separate record and interprets each space in the record as a fi eld separator. This \nisn’t what you intended.\nTo solve this problem, you need to set the \nFS variable to the newline character. This indi-\ncates that each line in the data stream is a separate fi eld and all the data on a line belongs \nto the data fi eld. However, when you do that, you don’t know where a new record starts.\nTo solve this problem, set the \nRS variable to an empty string, and leave a blank line \nbetween data records in the data stream. The \ngawk program interprets each blank line as a \nrecord separator.\nThe following is an example of using this technique:\n$ cat data2\nRiley Mullen\n123 Main Street\nChicago, IL  60601\n(312)555-1234\nFrank Williams\n456 Oak Street\nIndianapolis, IN  46201\n\n595\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  595\n22\n(317)555-9876\nHaley Snell\n4231 Elm Street\nDetroit, MI 48201\n(313)555-4938\n$ gawk 'BEGIN{FS=\"\\n\"; RS=\"\"} {print $1,$4}' data2\nRiley Mullen (312)555-1234\nFrank Williams (317)555-9876\nHaley Snell (313)555-4938\n$\nPerfect! The gawk program interpreted each line in the fi le as a data fi eld and the blank \nlines as record separators.\nData variables\nBesides the fi eld and record separator variables, gawk provides some other built-in variables \nto help you know what’s going on with your data and extract information from the shell \nenvironment. Table 22-2 shows the other built-in variables in \ngawk.\nTABLE 22-2    More gawk Built-In Variables\nVariableDescription\nARGC\nThe number of command line parameters present\nARGIND\nThe index in ARGV of the current fi le being processed\nARGV\nAn array of command line parameters\nCONVFMT\nThe conversion format for numbers (see the printf statement), with a \ndefault value of \n%.6 g\nENVIRON\nAn associative array of the current shell environment variables and their \nvalues\nERRNO\nThe system error if an error occurs when reading or closing input fi les\nFILENAME\nThe fi lename of the data fi le used for input to the gawk program\nFNR\nThe current record number in the data fi le\nIGNORECASE\nIf set to a non-zero value, ignores the case of characters in strings used in \nthe \ngawk command\nNF\nThe total number of data fi elds in the data fi le\nNR\nThe number of input records processed\nOFMT\nThe output format for displaying numbers, with a default of %.6 g\nRLENGTH\nThe length of the substring matched in the match function\nRSTART\nThe start index of the substring matched in the match function\nYou should recognize a few of these variables from your shell script programming. The \nARGC and ARGV variables allow you to retrieve the number of command line parameters and \n\n596\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  596\ntheir values from the shell. This can be a little tricky, however, because gawk doesn’t count \nthe program script as part of the command line parameters:\n$ gawk 'BEGIN{print ARGC,ARGV[1]}' data1\n2 data1\n$\nThe ARGC variable indicates that two parameters are on the command line. This includes \nthe \ngawk command and the data1 parameter (remember that the program script doesn’t \ncount as a parameter). The \nARGV array starts with an index of 0, which represents the com-\nmand. The fi rst array value is the fi rst command line parameter after the \ngawk command.\nNote that unlike shell variables, when you reference a gawk variable in the script, you don’t add a dollar sign before \nthe variable name.\nThe ENVIRON variable may seem a little odd to you. It uses an associative array to retrieve \nshell environment variables. An associative array uses text for the array index values \ninstead of numeric values.\nThe text in the array index is the shell environment variable. The value of the array is the \nvalue of the shell environment variable. The following is an example of this:\n$ gawk '\n> BEGIN{\n> print ENVIRON[\"HOME\"]\n> print ENVIRON[\"PATH\"]\n> }'\n/home/rich\n/usr/local/bin:/bin:/usr/bin:/usr/X11R6/bin\n$\nThe ENVIRON[\"HOME\"] variable retrieves the HOME environment variable value from the \nshell. Likewise, the \nENVIRON[\"PATH\"] variable retrieves the PATH environment variable \nvalue. You can use this technique to retrieve any environment variable value from the shell \nto use in your \ngawk programs.\nThe \nFNR, NF, and NR variables come in handy when you’re trying to keep track of data \nfi elds and records in your \ngawk program. Sometimes, you’re in a situation where you don’t \nknow exactly how many data fi elds are in a record. The \nNF variable allows you to specify \nthe last data fi eld in the record without having to know its position:\n$ gawk 'BEGIN{FS=\":\"; OFS=\":\"} {print $1,$NF}' /etc/passwd\nrich:/bin/bash\ntesty:/bin/csh\nmark:/bin/bash\n\n597\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  597\n22\ndan:/bin/bash\nmike:/bin/bash\ntest:/bin/bash\n$\nThe NF variable contains the numerical value of the last data fi eld in the data fi le. You can \nthen use it as a data fi eld variable by placing a dollar sign in front of it.\nThe \nFNR and NR variables are similar to each other, but slightly different. The FNR variable \ncontains the number of records processed in the current data fi le. The \nNR variable con-\ntains the total number of records processed. Let’s look at a couple of examples to see this \ndifference:\n$ gawk 'BEGIN{FS=\",\"}{print $1,\"FNR=\"FNR}' data1 data1\ndata11 FNR=1\ndata21 FNR=2\ndata31 FNR=3\ndata11 FNR=1\ndata21 FNR=2\ndata31 FNR=3\n$\nIn this example, the gawk program command line defi nes two input fi les. (It specifi es the \nsame input fi le twice.) The script prints the fi rst data fi eld value and the current value of \nthe \nFNR variable. Notice that the FNR value was reset to 1 when the gawk program \nprocessed the second data fi le.\nNow, let’s add the \nNR variable and see what that produces:\n$ gawk '\n> BEGIN {FS=\",\"}\n> {print $1,\"FNR=\"FNR,\"NR=\"NR}\n> END{print \"There were\",NR,\"records processed\"}' data1 data1\ndata11 FNR=1 NR=1\ndata21 FNR=2 NR=2\ndata31 FNR=3 NR=3\ndata11 FNR=1 NR=4\ndata21 FNR=2 NR=5\ndata31 FNR=3 NR=6\nThere were 6 records processed\n$\nThe FNR variable value was reset when gawk processed the second data fi le, but the NR \nvariable maintained its count into the second data fi le. The bottom line is that if you’re \nusing only one data fi le for input, the \nFNR and NR values are the same. If you’re using mul-\ntiple data fi les for input, the \nFNR value is reset for each data fi le, and the NR value keeps \ncount throughout all the data fi les.\n\n598\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  598\nWhen using gawk, notice that the gawk script can often become larger than the rest of your shell script. In the \nexamples in this chapter, for simplicity we just run the \ngawk scripts directly from the command line, using the multi-\nline feature of the shell. When you use \ngawk in a shell script, you should place different gawk commands on sepa-\nrate lines. This makes it much easier for you to read and follow, rather than trying to cram it all onto one line in the \nshell script. Also, if you fi nd yourself using the same \ngawk scripts in different shell scripts, you can save the gawk \nscript in a separate fi le and reference it using the \n–f parameter (see Chapter 19).\nUser-defi ned variables\nJust like any other self-respecting programming language, gawk allows you to defi ne your \nown variables for use within the program code. A \ngawk user-defi ned variable name can \nbe any number of letters, digits, and underscores, but it can’t begin with a digit. It is also \nimportant to remember that \ngawk variable names are case sensitive.\nAssigning variables in scripts\nAssigning values to variables in gawk programs is similar to doing so in a shell script, using \nan assignment statement:\n$ gawk '\n> BEGIN{\n> testing=\"This is a test\"\n> print testing\n> }'\nThis is a test\n$\nThe output of the print statement is the current value of the testing variable. Like shell \nscript variables, \ngawk variables can hold either numeric or text values:\n$ gawk '\n> BEGIN{\n> testing=\"This is a test\"\n> print testing\n> testing=45\n> print testing\n> }'\nThis is a test\n45\n$\nIn this example, the value of the testing variable is changed from a text value to a \nnumeric value.\n\n599\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  599\n22\nAssignment statements can also include mathematical algorithms to handle numeric values:\n$ gawk 'BEGIN{x=4; x= x * 2 + 3; print x}'\n11\n$\nAs you can see from this example, the gawk programming language includes the standard \nmathematical operators for processing numerical values. These can include the remainder \nsymbol (%) and the exponentiation symbol (using either ^ or **).\nAssigning variables on the command line\nYou can also use the gawk command line to assign values to variables for the gawk \nprogram. This allows you to set values outside of the normal code, changing values on the \nfl y. Here’s an example of using a command line variable to display a specifi c data fi eld in \nthe fi le:\n$ cat script1\nBEGIN{FS=\",\"}\n{print $n}\n$ gawk -f script1 n=2 data1\ndata12\ndata22\ndata32\n$ gawk -f script1 n=3 data1\ndata13\ndata23\ndata33\n$\nThis feature allows you to change the behavior of the script without necessitating that you \nchange the actual script code. The fi rst example displays the second data fi eld in the fi le, \nwhile the second example displays the third data fi eld, just by setting the value of the \nn \nvariable in the command line.\nThere’s one problem with using command line parameters to defi ne variable values. When \nyou set the variable, the value isn’t available in the \nBEGIN section of the code:\n$ cat script2\nBEGIN{print \"The starting value is\",n; FS=\",\"}\n{print $n}\n$ gawk -f script2 n=3 data1\nThe starting value is\ndata13\ndata23\ndata33\n$\n\n600\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  600\nYou can solve this using the -v command line parameter. This allows you to specify vari-\nables that are set before the \nBEGIN section of code. The -v command line parameter must \nbe placed before the script code in the command line:\n$ gawk -v n=3 -f script2 data1\nThe starting value is 3\ndata13\ndata23\ndata33\n$\nNow the n variable contains the value set in the command line during the BEGIN section \nof code.\nWorking with Arrays\nMany programming languages provide arrays for storing multiple values in a single vari-\nable. The \ngawk programming language provides the array feature using associative arrays.\nAssociative arrays are different from numerical arrays in that the index value can be any \ntext string. You don’t have to use sequential numbers to identify data elements contained \nin the array. Instead, an associative array consists of a hodge-podge of strings referencing \nvalues. Each index string must be unique and uniquely identifi es the data element that’s \nassigned to it. If you’re familiar with other programming languages, this is the same con-\ncept as hash maps or dictionaries.\nThe following sections walk you through using associative array variables in your \ngawk \nprograms.\nDefi ning array variables\nYou can defi ne an array variable using a standard assignment statement. Here’s the format \nof the array variable assignment:\nvar[index] = element\nIn this example, var is the variable name, index is the associative array index value, and \nelement is the data element value. Here are some examples of array variables in gawk:\ncapital[\"Illinois\"] = \"Springfield\"\ncapital[\"Indiana\"] = \"Indianapolis\"\ncapital[\"Ohio\"] = \"Columbus\"\nWhen you reference an array variable, you must include the index value to retrieve the \nappropriate data element value:\n$ gawk 'BEGIN{\n> capital[\"Illinois\"] = \"Springfield\"\n\n601\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  601\n22\n> print capital[\"Illinois\"]\n> }'\nSpringfield\n$\nWhen you reference the array variable, the data element value appears. This also works \nwith numeric data element values:\n$ gawk 'BEGIN{\n> var[1] = 34\n> var[2] = 3\n> total = var[1] + var[2]\n> print total\n> }'\n37\n$\nAs you can see from this example, you can use array variables just as you would any other \nvariable in the \ngawk program.\nIterating through array variables\nThe problem with associative array variables is that you might not have any way of know-\ning what the index values are. Unlike numeric arrays, which use sequential numbers for \nindex values, an associative array index can be anything.\nIf you need to iterate through an associate array in \ngawk, you can use a special format of \nthe \nfor statement:\nfor (var in array)\n{\n  statements\n}\nThe for statement loops through the statements, each time assigning the variable var the \nnext index value from the array associative array. It’s important to remember that the vari-\nable is the value of the index and not the data element value. You can easily extract the \ndata element value by using the variable as the array index:\n$ gawk 'BEGIN{\n> var[\"a\"] = 1\n> var[\"g\"] = 2\n> var[\"m\"] = 3\n> var[\"u\"] = 4\n> for (test in var)\n> {\n>    print \"Index:\",test,\" - Value:\",var[test]\n> }\n> }'\n\n602\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  602\nIndex: u  - Value: 4\nIndex: m  - Value: 3\nIndex: a  - Value: 1\nIndex: g  - Value: 2\n$\nNotice that the index values aren’t returned in any particular order, but they each refer-\nence the appropriate data element value. This is somewhat important to know, because you \ncan’t count on the returned values being in the same order, just that the index and data \nvalues match.\nDeleting array variables\nRemoving an array index from an associative array requires a special command:\ndelete array[index]\nThe delete command removes the associative index value and the associated data element \nvalue from the array:\n$ gawk 'BEGIN{\n> var[\"a\"] = 1\n> var[\"g\"] = 2\n> for (test in var)\n> {\n>    print \"Index:\",test,\" - Value:\",var[test]\n> }\n> delete var[\"g\"]\n> print \"---\"\n> for (test in var)\n>    print \"Index:\",test,\" - Value:\",var[test]\n> }'\nIndex: a  - Value: 1\nIndex: g  - Value: 2\n---\nIndex: a  - Value: 1\n$\nAfter you delete an index value from the associative array, you can’t retrieve it.\nUsing Patterns\nThe gawk program supports several types of matching patterns to fi lter data records, in \nmuch the same way as the \nsed editor. Chapter 19 showed two special patterns in action. \nThe \nBEGIN and END keywords are special patterns that execute statements before or after \nthe data stream data has been read. Similarly, you can create other patterns to execute \nstatements when matching data appears in the data stream.\n\n603\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  603\n22\nThis section demonstrates how to use matching patterns in your gawk scripts to limit what \nrecords a program script applies to.\nRegular expressions\nChapter 20 showed how to use regular expressions as matching patterns. You can use either \na Basic Regular Expression (BRE) or an Extended Regular Expression (ERE) to fi lter which \nlines in the data stream the program script applies to.\nWhen using a regular expression, the regular expression must appear before the left brace \nof the program script that it controls:\n$ gawk 'BEGIN{FS=\",\"} /11/{print $1}' data1\ndata11\n$\nThe regular expression /11/ matches records that contain the string 11 anywhere in the \ndata fi elds. The \ngawk program matches the defi ned regular expression against all the data \nfi elds in the record, including the fi eld separator character:\n$ gawk 'BEGIN{FS=\",\"} /,d/{print $1}' data1\ndata11\ndata21\ndata31\n$\nThis example matches the comma used as the fi eld separator in the regular expression. This \nis not always a good thing. It can lead to problems trying to match data specifi c to one data \nfi eld that may also appear in another data fi eld. If you need to match a regular expression \nto a specifi c data instance, you should use the matching operator.\nThe matching operator\nThe matching operator allows you to restrict a regular expression to a specifi c data fi eld in \nthe records. The matching operator is the tilde symbol (~). You specify the matching opera-\ntor, along with the data fi eld variable, and the regular expression to match:\n$1 ~ /^data/\nThe $1 variable represents the fi rst data fi eld in the record. This expression fi lters records \nwhere the fi rst data fi eld starts with the text data. The following is an example of using \nthe matching operator in a \ngawk program script:\n$ gawk 'BEGIN{FS=\",\"} $2 ~ /^data2/{print $0}' data1\ndata21,data22,data23,data24,data25\n$\n\n604\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  604\nThe matching operator compares the second data fi eld with the regular expression \n/^data2/, which indicates the string starts with the text data2.\nThis is a powerful tool that is commonly used in \ngawk program scripts to search for specifi c \ndata elements in a data fi le:\n$ gawk -F: '$1 ~ /rich/{print $1,$NF}' /etc/passwd\nrich /bin/bash\n$\nThis example searches the fi rst data fi eld for the text rich. When it fi nds the pattern in a \nrecord, it prints the fi rst and last data fi eld values of the record.\nYou can also negate the regular expression match by using the \n! symbol:\n$1 !~ /expression/\nIf the regular expression isn’t found in the record, the program script is applied to the \nrecord data:\n$ gawk –F: '$1 !~ /rich/{print $1,$NF}' /etc/passwd\nroot /bin/bash\ndaemon /bin/sh\nbin /bin/sh\nsys /bin/sh\n--- output truncated ---\n$\nIn this example, the gawk program script prints the userid and shell for all the entries in \nthe \n/etc/passwd fi le that don’t match the userid rich!\nMathematical expressions\nIn addition to regular expressions, you can also use mathematical expressions in the \nmatching pattern. This feature comes in handy when matching numerical values in data \nfi elds. For example, if you want to display all the system users who belong to the root users \ngroup (group number 0), you could use this script:\n$ gawk -F: '$4 == 0{print $1}' /etc/passwd\nroot\nsync\nshutdown\nhalt\noperator\n$\nThe script checks for records where the fourth data fi eld contains the value 0. On this Linux \nsystem, fi ve user accounts belong to the root user group.\n\n605\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  605\n22\nYou can use any of the normal mathematical comparison expressions:\n ■\nx == y: Value x is equal to y.\n ■\nx <= y: Value x is less than or equal to y.\n ■\nx < y: Value x is less than y.\n ■\nx >= y: Value x is greater than or equal to y.\n ■\nx > y: Value x is greater than y.\nYou can also use expressions with text data, but you must be careful. Unlike regular expres-\nsions, expressions are an exact match. The data must match exactly with the pattern:\n$ gawk -F, '$1 == \"data\"{print $1}' data1\n$\n$ gawk -F, '$1 == \"data11\"{print $1}' data1\ndata11\n$\nThe fi rst test doesn’t match any records because the fi rst data fi eld value isn’t data in any \nof the records. The second test matches one record with the value \ndata11.\nStructured Commands\nThe gawk programming language supports the usual cast of structured programming com-\nmands. This section describes each of these commands and demonstrates how to use them \nwithin a \ngawk programming environment.\nThe if statement\nThe gawk programming language supports the standard if-then-else format of the \nif statement. You must defi ne a condition for the if statement to evaluate, enclosed in \nparentheses. If the condition evaluates to a \nTRUE condition, the statement immediately fol-\nlowing the \nif statement is executed. If the condition evaluates to a FALSE condition, the \nstatement is skipped. You can use this format:\nif (condition)\n   statement1\nOr you can place it on one line, like this:\nif (condition) statement1\nHere’s a simple example demonstrating this format:\n$ cat data4\n10\n\n606\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  606\n5\n13\n50\n34\n$ gawk '{if ($1 > 20) print $1}' data4\n50\n34\n$\nNot too complicated. If you need to execute multiple statements in the if statement, you \nmust enclose them with braces:\n$ gawk '{\n> if ($1 > 20)\n> {\n>   x = $1 * 2\n>   print x\n> }\n> }' data4\n100\n68\n$\nBe careful that you don’t confuse the if statement braces with the braces used to start and \nstop the program script. The \ngawk program can detect missing braces and produces an error \nmessage if you mess up:\n$ gawk '{\n> if ($1 > 20)\n> {\n>    x = $1 * 2\n>    print x\n> }' data4\ngawk: cmd. line:6: }\ngawk: cmd. line:6:  ^ unexpected newline or end of string\n$\nThe gawk if statement also supports the else clause, allowing you to execute one or more \nstatements if the \nif statement condition fails. Here’s an example of using the else clause:\n$ gawk '{\n> if ($1 > 20)\n> {\n>    x = $1 * 2\n>    print x\n> } else\n> {\n>    x = $1 / 2\n>    print x\n\n607\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  607\n22\n> }}' data4\n5\n2.5\n6.5\n100\n68\n$\nYou can use the else clause on a single line, but you must use a semicolon after the if \nstatement section:\nif (condition) statement1; else statement2\nHere’s the same example using the single line format:\n$ gawk '{if ($1 > 20) print $1 * 2; else print $1 / 2}' data4\n5\n2.5\n6.5\n100\n68\n$\nThis format is more compact but can be harder to follow.\nThe while statement\nThe while statement provides a basic looping feature for gawk programs. Here’s the format \nof the \nwhile statement:\nwhile (condition)\n{\n  statements\n}\nThe while loop allows you to iterate over a set of data, checking a condition that stops the \niteration. This is useful if you have multiple data values in each record that you must use \nin calculations:\n$ cat data5\n130 120 135\n160 113 140\n145 170 215\n$ gawk '{\n> total = 0\n> i = 1\n> while (i < 4)\n> {\n\n608\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  608\n>    total += $i\n>    i++\n> }\n> avg = total / 3\n> print \"Average:\",avg\n> }' data5\nAverage: 128.333\nAverage: 137.667\nAverage: 176.667\n$\nThe while statement iterates through the data fi elds in the record, adding each value \nto the total variable and incrementing the counter variable, \ni. When the counter value \nis equal to 4, the \nwhile condition becomes FALSE, and the loop terminates, dropping \nthrough to the next statement in the script. That statement calculates the average and \nprints the average. This process is repeated for each record in the data fi le.\nThe \ngawk programming language supports using the break and continue statements in \nwhile loops, allowing you to jump out of the middle of the loop:\n$ gawk '{\n> total = 0\n> i = 1\n> while (i < 4)\n> {\n>    total += $i\n>    if (i == 2)\n>       break\n>    i++\n> }\n> avg = total / 2\n> print \"The average of the first two data elements is:\",avg\n> }' data5\nThe average of the first two data elements is: 125\nThe average of the first two data elements is: 136.5\nThe average of the first two data elements is: 157.5\n$\nThe break statement is used to break out of the while loop if the value of the i \nvariable is 2.\nThe do-while statement\nThe do-while statement is similar to the while statement but performs the statements \nbefore checking the condition statement. Here’s the format for the \ndo-while statement:\ndo\n{\n\n609\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  609\n22\n  statements\n} while (condition)\nThis format guarantees that the statements are executed at least one time before the \ncondition is evaluated. This comes in handy when you need to perform statements before \nevaluating the condition:\n$ gawk '{\n> total = 0\n> i = 1\n> do\n> {\n>    total += $i\n>    i++\n> } while (total < 150)\n> print total }' data5\n250\n160\n315\n$\nThe script reads the data fi elds from each record and totals them until the cumulative value \nreaches 150. If the fi rst data fi eld is over 150 (as seen in the second record), the script is \nguaranteed to read at least the fi rst data fi eld before evaluating the condition.\nThe for statement\nThe for statement is a common method used in many programming languages for looping. \nThe \ngawk programming language supports the C-style of for loops:\nfor( variable assignment; condition; iteration process)\nThis helps simplify the loop by combining several functions in one statement:\n$ gawk '{\n> total = 0\n> for (i = 1; i < 4; i++)\n> {\n>    total += $i\n> }\n> avg = total / 3\n> print \"Average:\",avg\n> }' data5\nAverage: 128.333\nAverage: 137.667\nAverage: 176.667\n$\nBy defi ning the iteration counter in the for loop, you don’t have to worry about incre-\nmenting it yourself as you did when using the \nwhile statement.\n\n610\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  610\nFormatted Printing\nYou may have noticed that the print statement doesn’t exactly give you much control over \nhow \ngawk displays your data. About all you can do is control the output fi eld separator \ncharacter (\nOFS). If you’re creating detailed reports, often you need to place data in a spe-\ncifi c format and location.\nThe solution is to use the formatted printing command, called \nprintf. If you’re familiar \nwith C programming, the \nprintf command in gawk performs the same way, allowing you \nto specify detailed instructions on how to display data.\nHere’s the format of the \nprintf command:\nprintf \"format string\", var1, var2 . . .\nThe format string is the key to the formatted output. It specifi es exactly how the formatted \noutput should appear, using both text elements and format specifi ers. A format specifi er is \na special code that indicates what type of variable is displayed and how to display it. The \ngawk program uses each format specifi er as a placeholder for each variable listed in the \ncommand. The fi rst format specifi er matches the fi rst variable listed, the second matches \nthe second variable, and so on.\nThe format specifi ers use the following format:\n%[modifier]control-letter\nIn this example, control-letter is a one-character code that indicates what type of data \nvalue will be displayed, and \nmodifier defi nes an optional formatting feature.\nTable 22-3 lists the control letters that can be used in the format specifi er.\nTABLE 22-3  Format Specifi er Control Letters\nControl LetterDescription\nc\nDisplays a number as an ASCII character\nd\nDisplays an integer value\ni\nDisplays an integer value (same as d)\ne\nDisplays a number in scientifi c notation\nf\nDisplays a fl oating-point value\ng\nDisplays either scientifi c notation or fl oating point, whichever is shorter\no\nDisplays an octal value\ns\nDisplays a text string\n\n611\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  611\n22\nx\nDisplays a hexadecimal value\nX\nDisplays a hexadecimal value, but using capital letters for A through F\nThus, if you need to display a string variable, you use the format specifi er %s. If you need \nto display an integer variable, you use either \n%d or %i (%d is the C-style for decimals). If \nyou want to display a large value using scientifi c notation, you use the \n%e format specifi er:\n$ gawk 'BEGIN{\n> x = 10 * 100\n> printf \"The answer is: %e\\n\", x\n> }'\nThe answer is: 1.000000e+03\n$\nIn addition to the control letters, you can use three modifi ers for even more control over \nyour output:\n ■\nwidth: This is a numeric value that specifi es the minimum width of the output \nfi eld. If the output is shorter, \nprintf pads the space with spaces, using right \njustifi cation for the text. If the output is longer than the specifi ed width, it over-\nrides the \nwidth value.\n ■\nprec: This is a numeric value that specifi es the number of digits to the right of the \ndecimal place in fl oating-point numbers, or the maximum number of characters \ndisplayed in a text string.\n ■\n- (minus sign): The minus sign indicates that left justifi cation should be used \ninstead of right justifi cation when placing data in the formatted space.\nWhen using the \nprintf statement, you have complete control over how your output \nappears. For example, in the “Built-in variables” section, we used the \nprint command to \ndisplay data fi elds from our records:\n$ gawk 'BEGIN{FS=\"\\n\"; RS=\"\"} {print $1,$4}' data2\nRiley Mullen (312)555-1234\nFrank Williams (317)555-9876\nHaley Snell (313)555-4938\n$\nYou can use the printf command to help format the output so it looks better. First, let’s \njust convert the \nprint command to a printf command and see what that does:\n$ gawk 'BEGIN{FS=\"\\n\"; RS=\"\"} {printf \"%s %s\\n\", $1, $4}' data2\nRiley Mullen  (312)555-1234\nFrank Williams  (317)555-9876\nHaley Snell  (313)555-4938\n$\n\n612\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  612\nThat produces the same output as the print command. The printf command uses the %s \nformat specifi er as a placeholder for the two string values.\nNotice that you have to manually add the newline character at the end of the \nprintf com-\nmand to force a new line. Without it, the \nprintf command uses the same line on subse-\nquent prints.\nThis is useful if you need to print multiple things on the same line, but using separate \nprintf commands:\n$ gawk 'BEGIN{FS=\",\"} {printf \"%s \", $1} END{printf \"\\n\"}' data1\ndata11 data21 data31\n$\nBoth printf outputs appear on the same line. To be able to terminate the line, the END \nsection prints a single newline character.\nNext, let’s use a modifi er to format the fi rst string value:\n$ gawk 'BEGIN{FS=\"\\n\"; RS=\"\"} {printf \"%16s  %s\\n\", $1, $4}' data2\n   Riley Mullen  (312)555-1234\n Frank Williams  (317)555-9876\n    Haley Snell  (313)555-4938\n$\nBy adding the 16 modifi er value, we force the output for the fi rst string to use 16 spaces. \nBy default, the \nprintf command uses right justifi cation to place the data in the format \nspace. To make it left justifi ed, just add a minus sign to the modifi er:\n$ gawk 'BEGIN{FS=\"\\n\"; RS=\"\"} {printf \"%-16s  %s\\n\", $1, $4}' data2\nRiley Mullen      (312)555-1234\nFrank Williams    (317)555-9876\nHaley Snell       (313)555-4938\n$\nNow that looks pretty professional!\nThe \nprintf command also comes in handy when dealing with fl oating-point values. By \nspecifying a format for the variable, you can make the output look more uniform:\n$ gawk '{\n> total = 0\n> for (i = 1; i < 4; i++)\n> {\n>    total += $i\n> }\n> avg = total / 3\n> printf \"Average: %5.1f\\n\",avg\n> }' data5\n\n613\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  613\n22\nAverage: 128.3\nAverage: 137.7\nAverage: 176.7\n$\nBy using the %5.1f format specifi er, you can force the printf command to round the \nfl oating-point values to a single decimal place.\nBuilt-In Functions\nThe gawk programming language provides quite a few built-in functions that perform com-\nmon mathematical, string, and even time functions. You can utilize these functions in your \ngawk programs to help cut down on the coding requirements in your scripts. This section \nwalks you through the different built-in functions available in \ngawk.\nMathematical functions\nIf you’ve done programming in any type of language, you’re probably familiar with using \nbuilt-in functions in your code to perform common mathematical functions. The \ngawk pro-\ngramming language doesn’t disappoint those looking for advanced mathematical features.\nTable 22-4 shows the mathematical built-in functions available in \ngawk.\nTABLE 22- 4    The gawk Mathematical Functions\nFunctionDescription\natan2(x, y)\nThe arctangent of x / y, with x and y specifi ed in radians\ncos(x)\nThe cosine of x, with x specifi ed in radians\nexp(x)\nThe exponential of x\nint(x)\nThe integer part of x, truncated toward 0\nlog(x)\nThe natural logarithm of x\nrand()\nA random fl oating point value larger than 0 and less than 1\nsin(x)\nThe sine of x, with x specifi ed in radians\nsqrt(x)\nThe square root of x\nsrand(x)\nSpecifi es a seed value for calculating random numbers\nAlthough it does not have an extensive list of mathematical functions, gawk does provide \nsome of the basic elements you need for standard mathematical processing. The \nint() \nfunction produces the integer portion of a value, but it doesn’t round the value. It behaves \nmuch like a fl oor function found in other programming languages. It produces the nearest \ninteger to a value between the value and 0.\n\n614\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  614\nThis means that the int() function of the value 5.6 returns 5, while the int() function \nof the value -5.6 returns -5.\nThe \nrand() function is great for creating random numbers, but you need to use a trick to \nget meaningful values. The \nrand() function returns a random number, but only between \nthe values 0 and 1 (not including 0 or 1). To get a larger number, you need to scale the \nreturned value.\nA common method for producing larger integer random numbers is to create an algorithm \nthat uses the \nrand() function, along with the int() function:\nx = int(10 * rand())\nThis returns a random integer value between (and including) 0 and 9. Just substitute the 10 \nin the equation with the upper limit value for your application, and you’re ready to go.\nBe careful when using some of the mathematical functions, because the \ngawk programming \nlanguage does have a limited range of numeric values it can work with. If you go over that \nrange, you get an error message:\n$ gawk 'BEGIN{x=exp(100); print x}'\n26881171418161356094253400435962903554686976\n$ gawk 'BEGIN{x=exp(1000); print x}'\ngawk: warning: exp argument 1000 is out of range\ninf\n$\nThe fi rst example calculates the natural exponential function of 100, which is a very large \nnumber but within the range of the system. The second example attempts to calculate the \nnatural exponential function of 1,000, which goes over the numerical range limit of the \nsystem and produces an error message.\nBesides the standard mathematical functions, \ngawk also provides a few functions for bit-\nwise manipulating of data:\n ■\nand(v1, v2): Performs a bitwise AND of values v1 and v2\n ■\ncompl(val): Performs the bitwise complement of val\n ■\nlshift(val, count): Shifts the value val count number of bits left\n ■\nor(v1, v2): Performs a bitwise OR of values v1 and v2\n ■\nrshift(val, count): Shifts the value val count number of bits right\n ■\nxor(v1, v2): Performs a bitwise XOR of values v1 and v2\nThe bit manipulation functions are useful when working with binary values in your data.\n\n615\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  615\n22\nString functions\nThe gawk programming language also provides several functions you can use to manipulate \nstring values, shown in Table 22-5.\nTABLE 22-5    The gawk String Functions\nFunctionDescription\nasort(s [,d])\nThis function sorts an array s based on the data element values. \nThe index values are replaced with sequential numbers indicat-\ning the new sort order. Alternatively, the new sorted array is \nstored in array \nd if specifi ed.\nasorti(s [,d])\nThis function sorts an array s based on the index values. The \nresulting array contains the index values as the data element val-\nues, with sequential number indexes indicating the sort order. \nAlternatively, the new sorted array is stored in array \nd if specifi ed.\ngensub(r, s, h [, t])\nThis function searches either the variable $0, or the target string \nt if supplied, for matches of the regular expression r. If h is a \nstring beginning with either \ng or G, it replaces the matching text \nwith \ns. If h is a number, it represents which occurrence of r to \nreplace.\ngsub(r, s [,t])\nThis function searches either the variable $0, or the target string \nt if supplied, for matches of the regular expression r. If found, it \nsubstitutes the string \ns globally.\nindex(s, t)\nThis function returns the index of the string t in string s, or 0 if \nnot found.\nlength([s])\nThis function returns the length of string s, or if not specifi ed, the \nlength of \n$0.\nmatch(s, r [,a])\nThis function returns the index of the string s where the regular \nexpression \nr occurs. If array a is specifi ed, it contains the portion \nof \ns that matches the regular expression.\nsplit(s, a [,r])\nThis function splits s into array a using either the FS character, or \nthe regular expression \nr if supplied. It returns the number of \nfi elds.\nsprintf(format, \nvariables)\nThis function returns a string similar to the output of printf \nusing the \nformat and variables supplied.\nsub(r, s [,t])\nThis function searches either the variable $0, or the target string \nt, for matches of the regular expression r. If found, it substitutes \nthe string \ns for the fi rst occurrence.\nsubstr(s, i [,n])\nThis function returns the nth character substring of s, starting at \nindex \ni. If n is not supplied, the rest of s is used.\ntolower(s)\nThis function converts all characters in s to lowercase.\ntoupper(s)\nThis function converts all characters in s to uppercase.\n\n616\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  616\nSome string functions are relatively self-explanatory:\n$ gawk 'BEGIN{x = \"testing\"; print toupper(x); print length(x) }'\nTESTING\n7\n$\nHowever, some string functions can get pretty complicated. The asort and asorti func-\ntions are new \ngawk functions that allow you to sort an array variable based on either the \ndata element values (\nasort) or the index values (asorti). Here’s an example of using \nasort:\n$ gawk 'BEGIN{\n> var[\"a\"] = 1\n> var[\"g\"] = 2\n> var[\"m\"] = 3\n> var[\"u\"] = 4\n> asort(var, test)\n> for (i in test)\n>     print \"Index:\",i,\" - value:\",test[i]\n> }'\nIndex: 4  - value: 4\nIndex: 1  - value: 1\nIndex: 2  - value: 2\nIndex: 3  - value: 3\n$\nThe new array, test, contains the newly sorted data elements of the original array, but the \nindex values are now changed to numerical values, indicating the proper sort order.\nThe \nsplit function is a great way to push data fi elds into an array for further processing:\n$ gawk 'BEGIN{ FS=\",\"}{\n> split($0, var)\n> print var[1], var[5]\n> }' data1\ndata11 data15\ndata21 data25\ndata31 data35\n$\nThe new array uses sequential numbers for the array index, starting with index value 1 \ncontaining the fi rst data fi eld.\nTime functions\nThe gawk programming language contains a few functions to help you deal with time \nvalues, shown in Table 22-6.\n\n617\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  617\n22\nTABLE 22- 6    The gawk Time Functions\nFunctionDescription\nmktime(datespec)\nConverts a date specifi ed in the format YYYY MM DD HH MM SS \n[DST] into a timestamp value\nstrftime(format \n[,timestamp])\nFormats either the current time of day timestamp, or timestamp if \nprovided, into a formatted day and date, using the \ndate() shell \nfunction format\nsystime()\nReturns the timestamp for the current time of day\nThe time functions are often used when working with log fi les that contain dates that you \nneed to compare. By converting the text representation of a date to the epoch time (the \nnumber of seconds since midnight, January 1, 1970), you can easily compare dates.\nThe following is an example of using the time functions in a \ngawk program:\n$ gawk 'BEGIN{\n> date = systime()\n> day = strftime(\"%A, %B %d, %Y\", date)\n> print day\n> }'\nFriday, December 26, 2014\n$\nThis example uses the systime function to retrieve the current epoch timestamp from the \nsystem and then uses the \nstrftime function to convert it into a human-readable format \nusing the date shell command’s date format characters.\nUser-Defi ned Functions\nYou’re not limited to just using the built-in functions available in gawk. You can create \nyour own functions for use in \ngawk programs. This section shows you how to defi ne and \nuse your own functions in \ngawk programs.\nDefi ning a function\nTo defi ne you own function, you must use the function keyword:\nfunction name([variables])\n{\n   statements\n}\n\n618\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  618\nThe function name must uniquely identify your function. You can pass one or more vari-\nables into the function from the calling \ngawk program:\nfunction printthird()\n{\n   print $3\n}\nThis function prints the third data fi eld in the record.\nThe function can also return a value using the \nreturn statement:\nreturn value\nThe value can be a variable, or an equation that evaluates to a value:\nfunction myrand(limit)\n{\n   return int(limit * rand())\n}\nYou can assign the value returned from the function to a variable in the gawk program:\nx = myrand(100)\nThe variable contains the value returned from the function.\nUsing your functions\nWhen you defi ne a function, it must appear by itself before you defi ne any programming \nsections (including the \nBEGIN section). This may look a little odd at fi rst, but it helps keep \nthe function code separate from the rest of the \ngawk program:\n$ gawk '\n> function myprint()\n> {\n>     printf \"%-16s - %s\\n\", $1, $4\n> }\n> BEGIN{FS=\"\\n\"; RS=\"\"}\n> {\n>     myprint()\n> }' data2\nRiley Mullen     - (312)555-1234\nFrank Williams   - (317)555-9876\nHaley Snell      - (313)555-4938\n$\n\n619\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  619\n22\nThe function defi nes the myprint() function, which formats the fi rst and fourth data \nfi elds in the record for printing. The \ngawk program then uses the function to display the \ndata from the data fi le.\nAfter you defi ne a function, you can use it as often as necessary in the program section of \nthe code. This saves lots of work when using long algorithms.\nCreating a function library\nObviously, having to rewrite your gawk functions every time you need them is not a pleas-\nant experience. However, \ngawk provides a way for you to combine your functions into a \nsingle library fi le that you can use in all your \ngawk programming.\nFirst, you need to create a fi le that contains all your \ngawk functions:\n$ cat funclib\nfunction myprint()\n{\n  printf \"%-16s - %s\\n\", $1, $4\n}\nfunction myrand(limit)\n{\n  return int(limit * rand())\n}\nfunction printthird()\n{\n  print $3\n}\n$\nThe funclib fi le contains three function defi nitions. To use them, you need to use the -f \ncommand line parameter. Unfortunately, you can’t combine the \n-f command line parameter \nwith an inline \ngawk script, but you can use multiple -f parameters on the same command line.\nThus, to use your library, just create a fi le that contains your \ngawk program, and specify \nboth the library fi le and your program fi le on the command line:\n$ cat script4\nBEGIN{ FS=\"\\n\"; RS=\"\"}\n{\n    myprint()\n}\n$ gawk -f funclib -f script4 data2\nRiley Mullen     - (312)555-1234\nFrank Williams   - (317)555-9876\nHaley Snell      - (313)555-4938\n$\n\n620\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  620\nNow you just need to add the funclib fi le to your gawk command line whenever you \nneed to use a function defi ned in the library.\nWorking through a Practical Example\nThe advanced gawk features come in handy if you have to handle data values in a data fi le, \nsuch as tabulating sales fi gures or calculating bowling scores. When you work with data \nfi les, the key is to fi rst group related data records together and then perform any calcula-\ntions required on the related data.\nFor example, let’s work with a data fi le that contains the bowling scores from a game \nbetween two teams, each with two players:\n$ cat scores.txt\nRich Blum,team1,100,115,95\nBarbara Blum,team1,110,115,100\nChristine Bresnahan,team2,120,115,118\nTim Bresnahan,team2,125,112,116\n$\nEach player has scores from three separate games in the data fi le, and each player is identi-\nfi ed by a team name in the second column. Here’s the shell script to sort the data for each \nteam and calculate the totals and averages:\n$ cat bowling.sh\n#!/bin/bash\nfor team in $(gawk –F, '{print $2}' scores.txt | uniq)\ndo\n   gawk –v team=$team 'BEGIN{FS=\",\"; total=0}\n   {\n      if ($2==team)\n      {\n         total += $3 + $4 + $5;\n      }\n   }\n   END {\n      avg = total / 6;\n      print \"Total for\", team, \"is\", total, \",the average is\",avg\n   }' scores.txt\ndone\n$\nThe fi rst gawk statement inside the for loop fi lters out the team names in the data fi le \nand then uses the \nuniq function to return one value for each separate team name. The for \nloop then iterates for each separate team name.\n\n621\nChapter 22: Advanced gawk\nc22.indd  12/16/2014  Page  621\n22\nThe gawk statement inside the for loop is what’s doing the calculations. For each data \nrecord, it fi rst determines if the team name matches the loop team. That’s done by using the \n–v option in gawk, which allows us to pass a shell variable inside the gawk program. If the \nteam name matches, the code keeps a running sum of the three scores in the data record, \nadding each data record’s values, as long as that data record matches the team name.\nAt the end of each loop iteration, the \ngawk code displays the score totals, as well as the \naverage of the scores. The output should look like this:\n$ ./bowling.sh\nTotal for team1 is 635, the average is 105.833\nTotal for team2 is 706, the average is 117.667\n$\nNow you have a handy shell script to calculate the results of all your bowling tournaments; \nyou just need to plug the data from each player into the data text fi le and run the script!\nSummary\nThis chapter walked you through the more advanced features of the gawk programming \nlanguage. Every programming language requires using variables, and \ngawk is no different. \nThe \ngawk programming language includes some built-in variables that you can use to \nreference specifi c data fi eld values and retrieve information about the number of data fi e lds \nand records processed in the data fi le. You can also create your own variables for use in \nyour scripts.\nThe \ngawk programming language also provides many of the standard structured commands \nyou expect from a programming language. You can easily create fancy programs using \nif-then logic and while, do-while, and for loops. Each of these commands allows you \nto alter the fl ow of your \ngawk program script to iterate through data fi eld values to create \ndetailed data reports.\nThe \nprintf command is a great tool to have if you need to customize your report out-\nput. It allows you to specify the exact format for displaying data from the \ngawk program \nscript. You can easily create formatted reports, placing data elements in exactly the correct \nposition.\nFinally, this chapter discussed the many built-in functions available in the \ngawk program-\nming language and showed you how to create your own functions. The \ngawk program \ncontains many useful functions for handling mathematical features, such as standard \nsquare roots and logarithms, as well as trigonometric functions. There are also several \nstring-related functions that make extracting substrings from larger strings a breeze.\n\n622\nPart III: Advanced Shell Scripting\nc22.indd  12/16/2014  Page  622\nYou aren’t limited to the built-in functions in the gawk program. If you’re working on an \napplication that uses lots of specialized algorithms, you can create your own functions to \nprocess the algorithms and use those functions in your own code. You can also set up a \nlibrary fi le containing all the functions you use in your \ngawk programs, saving you time \nand effort in all your coding.\nThe next chapter switches gears a little. It examines a few other shell environments you \nmay run into in your Linux shell-scripting endeavors. Although the bash shell is the most \ncommon shell used in Linux, it’s not the only shell. It helps to know a little about some of \nthe other shells available and how they differ from the bash  shell. \n\n623\nc23.indd  12/09/2014  Page  623\nCHAPTER \n23\nWorking with Alternative Shells\nIN THIS CHAPTER\nUnderstanding the dash shell\nProgramming in the dash shell\nIntroducing the zsh shell\nWriting scripts for zsh\nA\nlthough the bash shell is the most widely used shell in Linux distributions, it isn’t the only \none. Now that you’ve seen the standard Linux bash shell and what you can do with it, it’s \ntime to examine a few other shells available in the Linux world. This chapter describes two \nother shells that you may run into in your Linux journey and how they differ from the bash shell.\nWhat Is the dash Shell?\nThe Debian dash shell has had an interesting past. It’s a direct descendant of the ash shell, a simple \ncopy of the original Bourne shell available on Unix systems (see Chapter 1). Kenneth Almquist \ncreated a small-scale version of the Bourne shell for Unix systems and called it the Almquist shell, \nwhich was then shortened to ash. This original version of the ash shell was extremely small and \nfast but without many advanced features, such as command line editing or history features, mak-\ning it diffi cult to use as an interactive shell.\nThe NetBSD Unix operating system adopted the ash shell and still uses it today as the default shell. \nThe NetBSD developers customized the ash shell by adding several new features, making it closer \nto the Bourne shell. The new features include command line editing using both emacs and vi editor \ncommands, as well as a history command to recall previously entered commands. This version of the \nash shell is also used by the FreeBSD operating system as the default login shell.\nThe Debian Linux distribution created its own version of the ash shell (called Debian ash, or dash) \nfor inclusion in its version of Linux. For the most part, dash copies the features of the NetBSD \nversion of the ash shell, providing the advanced command line editing capabilities.\nHowever, to add to the shell confusion, the dash shell is actually not the default shell in many \nDebian-based Linux distributions. Because of the popularity of the bash shell in Linux, most \n\n624\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  624\nDebian-based Linux distributions use the bash shell as the normal login shell and use the \ndash shell only as a quick-start shell for the installation script to install the distribution \nfi les.\nThe exception is the popular Ubuntu distribution. This often confuses shell script program-\nmers and causes a great number of problems with running shell scripts in a Linux environ-\nment. The Ubuntu Linux distribution uses the bash shell as the default interactive shell, \nbut uses the dash shell as the default \n/bin/sh shell. This “feature” really confuses shell \nscript programmers.\nAs you saw in Chapter 11, every shell script must start with a line that declares the shell \nused for the script. In our bash shell scripts, we’ve been using this:\n#!/bin/bash\nThis tells the shell to use the shell program located at /bin/bash to execute the script. \nIn the Unix world, the default shell was always \n/bin/sh. Many shell script programmers \nfamiliar with the Unix environment copy this into their Linux shell scripts:\n#!/bin/sh\nOn most Linux distributions, the /bin/sh fi le is a symbolic link (see Chapter 3) to the \n/bin/bash shell program. This allows you to easily port shell scripts designed for the Unix \nBourne shell to the Linux environment without having to modify them.\nUnfortunately, the Ubuntu Linux distribution links the \n/bin/sh fi le to the /bin/dash \nshell program. Because the dash shell contains only a subset of the commands available \nin the original Bourne shell, this can — and often does — cause some shell scripts to not \nwork properly.\nThe next section walks you through the basics of the dash shell and how it differs from \nthe bash shell. This is especially important to know if you write bash shell scripts that may \nneed to be run in an Ubuntu environment.\nThe dash Shell Features\nAlthough both the bash shell and the dash shell are modeled after the Bourne shell, they \nhave some differences. This section walks you through the features found in the Debian \ndash shell to acquaint you with how the dash shell works before we dive into the shell \nscripting features.\nThe dash command line parameters\nThe dash shell uses command line parameters to control its behavior. Table 23-1 lists the \ncommand line parameters and describes what each one does.\n\n625\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  625\n23\n23\nTABLE 23-1    The dash Command Line Parameters\nParameterDescription\n-a\nExports all variables assigned to the shell\n-c\nReads commands from a specifi ed command string\n-e\nIf not interactive, exits immediately if any untested command fails\n-f\nDisplays pathname wildcard characters\n-n\nIf not interactive, reads commands but doesn’t execute them\n-u\nWrites an error message to STDERR when attempting to expand a variable that \nis not set\n-v\nWrites input to STDERR as it is read\n-x\nWrites each command to STDERR as it is executed\n-I\nIgnores EOF characters from the input when in interactive mode\n-i\nForces the shell to operate in interactive mode\n-m\nTurns on job control (enabled by default in interactive mode)\n-s\nReads commands from STDIN (the default behavior if no fi le arguments are \npresent)\n-E\nEnables the emacs command line editor\n-V\nEnables the vi command line editor\nDebian added a few additional command line parameters to the original ash shell command \nline parameter list. The \n-E and -V command line parameters enable the special command \nline editing features of the dash shell.\nThe \n-E command line parameter allows you to use the emacs editor commands for editing \ncommand line text (see Chapter 10). You can use all the emacs commands for manipulating \ntext on a single line using the Ctrl and Meta key combinations.\nThe \n-V command line parameter allows you to use the vi editor commands for editing com-\nmand line text (again, see Chapter 10). This feature allows you to switch between normal \nmode and vi editor mode on the command line by using the \nEsc key. When you’re in vi \neditor mode, you can use all the standard vi editor commands (such as \nx to delete a char-\nacter, and \ni to insert text). After you fi nish editing the command line, you must press the \nEsc key again to exit vi editor mode.\nThe dash environment variables\nThe dash shell uses quite a few default environment variables uses to track information, \nand you can create your own environment variables as well. This section describes the \nenvironment variables and how dash handles them.\n\n626\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  626\nDefault environment variables\nThe dash environment variables are very similar to the environment variables used in bash \n(see Chapter 6). This is not by accident. Remember that both the dash and bash shells are \nextensions of the Bourne shell, so they both incorporate many of its features. However, \nbecause of its goal of simplicity, the dash shell contains signifi cantly fewer environment \nvariables than the bash shell. You need to take this into consideration when creating shell \nscripts in a dash shell environment.\nThe dash shell uses the \nset command to display environment variables:\n$set\nCOLORTERM=''\nDESKTOP_SESSION='default'\nDISPLAY=':0.0'\nDM_CONTROL='/var/run/xdmctl'\nGS_LIB='/home/atest/.fonts'\nHOME='/home/atest'\nIFS='  \n'\nKDEROOTHOME='/root/.kde'\nKDE_FULL_SESSION='true'\nKDE_MULTIHEAD='false'\nKONSOLE_DCOP='DCOPRef(konsole-5293,konsole)'\nKONSOLE_DCOP_SESSION='DCOPRef(konsole-5293,session-1)'\nLANG='en_US'\nLANGUAGE='en'\nLC_ALL='en_US'\nLOGNAME='atest'\nOPTIND='1'\nPATH='/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin'\nPPID='5293'\nPS1='$ '\nPS2='> '\nPS4='+ '\nPWD='/home/atest'\nSESSION_MANAGER='local/testbox:/tmp/.ICE-unix/5051'\nSHELL='/bin/dash'\nSHLVL='1'\nTERM='xterm'\nUSER='atest'\nXCURSOR_THEME='default'\n_='ash'\n$\nYour default dash shell environment will most likely differ, because different Linux distri-\nbutions assign different default environment variables at login.\n\n627\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  627\n23\nPositional parameters\nIn addition to the default environment variables, the dash shell also assigns special vari-\nables to any parameters defi ned in the command line. Here are the positional parameter \nvariables available for use in the dash shell:\n ■\n$0: The name of the shell\n ■\n$n: The nth position parameter\n ■\n$*: A single value with the contents of all the parameters, separated by the fi rst \ncharacter in the IFS environment variable, or a space if IFS isn’t defi ned\n ■\n$@: Expands to multiple arguments consisting of all the command line parameters\n ■\n$#: The number of positional parameters\n ■\n$?: The exit status of the most recent command\n ■\n$-: The current option fl ags\n ■\n$$: The process ID (PID) of the current shell\n ■\n$!: The process ID (PID) of the most recent background command\nAll the dash positional parameters mimic the same positional parameters available in the \nbash shell. You can use each of the positional parameters in your shell scripts just as you \nwould in the bash shell.\nUser-defined environment variables\nThe dash shell also allows you to set your own environment variables. As with bash, you \ncan defi ne a new environment variable on the command line by using the assignment \nstatement:\n$ testing=10 ; export testing\n$ echo $testing\n10\n$\nWithout the export command, user-defi ned environment variables are visible only in the \ncurrent shell or process.\nThere’s one huge difference between dash variables and bash variables. The dash shell doesn’t support variable \narrays. This small feature causes all sorts of problems for advanced shell script writers.\n\n628\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  628\nThe dash built-in commands\nJust as with the bash shell, the dash shell contains a set of built-in commands that it rec-\nognizes. You can use these commands directly from the command line interface, or you can \nincorporate them in your shell scripts. Table 23-2 lists the dash shell built-in commands.\nTABLE 23-2    The dash Shell Built-In Commands\nCommandDescription\nalias\nCreates an alias string to represent a text string\nbg\nContinues specifi ed job in background mode\ncd\nSwitches to the specifi ed directory\necho\nDisplays a text string and environment variables\neval\nConcatenates all arguments with a space\nexec\nReplaces the shell process with the specifi ed command\nexit\nTerminates the shell process\nexport\nExports the specifi ed environment variable for use in all child shells\nfg\nContinues specifi ed job in foreground mode\ngetopts\nObtains options and arguments from a list of parameters\nhash\nMaintains and retrieves a hash table of recent commands and their locations\npwd\nDisplays the value of the current working directory\nread\nReads a line from STDIN and assign the value to a variable\nreadonly\nReads a line from STDIN to a variable that can’t be changed\nprintf\nDisplays text and variables using a formatted string\nset\nLists or sets option fl ags and environment variables\nshift\nShifts the positional parameters a specifi ed number of times\ntest\nEvaluates an expression and returns 0 if true or 1 if false\ntimes\nDisplays the accumulated user and system times for the shell and all shell \nprocesses\ntrap\nParses and executes an action when the shell receives a specifi ed signal\ntype\nInterprets the specifi ed name and displays the resolution (alias, built-in, com-\nmand, keyword)\nulimit\nQueries or sets limits on processes\numask\nSets the value of the default fi le and directory permissions\nunalias\nRemoves the specifi ed alias\nunset\nRemoves the specifi ed variable or option fl ag from the exported variables\nwait\nWaits for the specifi ed job to complete and returns the exit status\n\n629\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  629\n23\nYou probably recognize all these built-in commands from the bash shell. The dash shell sup-\nports many of the same built-in commands as the bash shell. You’ll notice that there are no \ncommands for the command history fi le or for the directory stack. The dash shell doesn’t \nsupport these features.\nScripting in dash\nUnfortunately, the dash shell doesn’t recognize all the scripting features of the bash shell. \nShell scripts written for the bash environment often fail when run in the dash shell, \ncausing all sorts of grief for shell script programmers. This section describes the differ-\nences you’ll need to be aware of to get your shell scripts to run properly in a dash shell \nenvironment.\nCreating dash scripts\nYou probably guessed by now that creating shell scripts for the dash shell is pretty similar \nto creating shell scripts for the bash shell. You should always specify which shell you want \nto use in your script to ensure that the script runs with the proper shell.\nYou do this on the fi rst line of the shell:\n#!/bin/dash\nYou can also specify a shell command line parameter on this line, as was documented ear-\nlier in “The dash command line parameters” section.\nThings that don’t work\nUnfortunately, because the dash shell is only a subset of the Bourne shell features, some \nthings in bash shell scripts don’t work in the dash shell. These are often called bashisms. \nThis section is a quick summary of bash shell features you may be used to using in your \nbash shell scripts that don’t work if you’re in a dash shell environment.\nUsing arithmetic\nChapter 11 showed three ways to express a mathematical operation in the bash shell script:\n ■\nUsing the expr command: expr operation\n ■\nUsing square brackets: $[ operation ]\n ■\nUsing double parentheses: $(( operation ))\nThe dash shell supports the expr command and the double parentheses method but doesn’t \nsupport the square bracket method. This can be a problem if you have lots of mathematical \noperations that use the square brackets.\n\n630\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  630\nThe proper format for performing mathematical operations in dash shell scripts is to use \nthe double parentheses method:\n$ cat test5b\n#!/bin/dash\n# testing mathematical operations\nvalue1=10\nvalue2=15\nvalue3=$(( $value1 * $value2 ))\necho \"The answer is $value3\"\n$ ./test5b\nThe answer is 150\n$\nNow the shell can perform the calculation properly.\nThe test command\nAlthough the dash shell supports the test command, you must be careful how you use \nit. The bash shell version of the \ntest command is slightly different from the dash shell \nversion.\nThe bash shell \ntest command allows you to use the double equal sign (==) to test if two \nstrings are equal. This is an add-on to accommodate programmers familiar with using this \nformat in other programming languages.\nHowever, the \ntest command available in the dash shell doesn’t recognize the == symbol for \ntext comparisons. Instead, it only recognizes the \n= symbol. If you use the == symbol in your \nbash scripts, you need to change the text comparison symbol to just a single equal sign:\n$ cat test7\n#!/bin/dash\n# testing the = comparison\n \ntest1=abcdef\ntest2=abcdef\n \nif [ $test1 = $test2 ]\nthen\n   echo \"They're the same!\"\nelse\n   echo \"They're different\"\nfi\n$ ./test7\nThey're the same!\n$\n\n631\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  631\n23\nThis little bashism is responsible for many hours of frustration for shell programmers!\nThe function Command\nChapter 17 showed you how to defi ne your own functions in your shell scripts. The bash \nshell supports two methods for defi ning functions:\n ■\nUsing the function() statement\n ■\nUsing the function name only\nThe dash shell doesn’t support the \nfunction statement. Instead, in the dash shell you \nmust defi ne a function using the function name with parentheses.\nIf you’re writing shell scripts that may be used in the dash environment, always defi ne \nfunctions using the function name and not the function() statement:\n$ cat test10\n#!/bin/dash\n# testing functions\n \nfunc1() {\n   echo \"This is an example of a function\"\n}\n \ncount=1\nwhile [ $count -le 5 ]\ndo\n   func1\n   count=$(( $count + 1 ))\ndone\necho \"This is the end of the loop\"\nfunc1\necho \"This is the end of the script\"\n$ ./test10\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is an example of a function\nThis is the end of the loop\nThis is an example of a function\nThis is the end of the script\n$\nNow the dash shell recognizes the function defi ned in the script just fi ne and uses it within \nthe script.\n\n632\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  632\nThe zsh Shell\nAnother popular shell that you may run into is the Z shell (called zsh). The zsh shell is \nan open source Unix shell developed by Paul Falstad. It takes ideas from all the existing \nshells and adds many unique features to create a full-blown advanced shell designed for \nprogrammers.\nThe following are some of the features that make the zsh shell unique:\n ■\nImproved shell option handling\n ■\nShell compatibility modes\n ■\nLoadable modules\nOf all these features, a loadable module is the most advanced feature in shell design. As \nyou’ve seen in the bash and dash shells, each shell contains a set of built-in commands that \nare available without the need for external utility programs. The benefi t of built-in com-\nmands is execution speed. The shell doesn’t have to load a utility program into memory \nbefore running it; the built-in commands are already in the shell memory, ready to go.\nThe zsh shell provides a core set of built-in commands, plus the capability to add more \ncommand modules. Each command module provides a set of additional built-in commands for \nspecifi c circumstances, such as network support and advanced math functions. You can add \nonly the modules you think you need for your specifi c situation.\nThis feature provides a great way to either limit the size of the zsh shell for situations that \nrequire a small shell size and few commands or expand the number of available built-in \ncommands for situations that require faster execution speeds.\nParts of the zsh Shell\nThis section walks you through the basics of the zsh shell, showing the built-in commands \nthat are available (or can be added by installing modules), as well as the command line \nparameters and environment variables used by the zsh shell.\nShell options\nMost shells use command line parameters to defi ne the behavior of the shell. The zsh shell \nuses a few command line parameters to defi ne the operation of the shell, but mostly it uses \noptions to customize the behavior of the shell. You can set shell options either on the com-\nmand line or within the shell itself using the \nset command.\nTable 23-3 lists the command line parameters available for the zsh shell.\n\n633\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  633\n23\nTABLE 23-3    The zsh Shell Command Line Parameters\nParameterDescription\n-c\nExecutes only the specifi ed command and exits\n-i\nStarts as an interactive shell, providing a command line interface prompt\n-s\nForces the shell to read commands from STDIN\n-o\nSpecifi es command line options\nAlthough this may seem like a small set of command line parameters, the -o parameter is \nsomewhat misleading. It allows you to set shell options that defi ne features within the shell. \nBy far, the zsh shell is the most customizable shell available. You can alter lots of features \nfor your shell environment. The different options fi t into several general categories:\n ■\nChanging directories: Options that control how the cd and dirs commands han-\ndle directory changes\n ■\nCompletion: Options that control command completion features\n ■\nExpansion and globbing: Options that control fi le expansion in commands\n ■\nHistory: Options that control command history recall\n ■\nInitialization: Options that control how the shell handles variables and startup \nfi les when started\n ■\nInput/output: Options that control command handling\n ■\nJob control: Options that dictate how the shell handles and starts jobs\n ■\nPrompting: Options that defi ne how the shell works with command line prompts\n ■\nScripts and functions: Options that control how the shell processes shell scripts \nand defi nes shell functions\n ■\nShell emulation: Options that allow you to set the behavior of the zsh shell to \nmimic the behavior of other shell types\n ■\nShell state: Options that defi ne what type of shell to start\n ■\nzle: Options for controlling the zsh line editor (zle) feature\n ■\nOption aliases: Special options that can be used as aliases for other option names\nWith this many different categories of shell options, you can imagine just how many actual \noptions the zsh shell supports.\nBuilt-in commands\nThe zsh shell is unique in that it allows you to expand the built-in commands available in \nthe shell. This provides for a wealth of speedy utilities at your fi ngertips for a host of dif-\nferent applications.\n\n634\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  634\nThis section describes the core built-in commands, along with the various modules available \nat the time of this writing.\nCore built-in commands\nThe core of the zsh shell contains the basic built-in commands you’re used to seeing in \nother shells. Table 23-4 describes the built-in commands available for you.\nTABLE 23- 4    The zsh Core Built-In Commands\nCommandDescription\nalias\nDefi nes an alternate name for a command and arguments\nautoload\nPreloads a shell function into memory for quicker access\nbg\nExecutes a job in background mode\nbindkey\nBinds keyboard combinations to commands\nbuiltin\nExecutes the specifi ed built-in command instead of an executable fi le of \nthe same name\nbye\nThe same as exit\ncd\nChanges the current working directory\nchdir\nChanges the current working directory\ncommand\nExecutes the specifi ed command as an external fi le instead of a function or \nbuilt-in command\ndeclare\nSets the data type of a variable (same as typeset)\ndirs\nDisplays the contents of the directory stack\ndisable\nTemporarily disables the specifi ed hash table elements\ndisown\nRemoves the specifi ed job from the job table\necho\nDisplays variables and text\nemulate\nSets zsh to emulate another shell, such as the Bourne, Korn, or C shells\nenable\nEnables the specifi ed hash table elements\neval\nExecutes the specifi ed command and arguments in the current shell \nprocess\nexec\nExecutes the specifi ed command and arguments replacing the current \nshell process\nexit\nExits the shell with the specifi ed exit status. If none specifi ed, uses the exit \nstatus of the last command\nexport\nAllows the specifi ed environment variable names and values to be used in \nchild shell processes\nfalse\nReturns an exit status of 1\n\n635\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  635\n23\nfc\nSelects a range of commands from the history list\nfg\nExecutes the specifi ed job in foreground mode\nfloat\nSets the specifi ed variable for use as a fl oating point variable\nfunctions\nSets the specifi ed name as a function\ngetln\nReads the next value in the buffer stack and places it in the specifi ed \nvariable\ngetopts\nRetrieves the next valid option in the command line arguments and places \nit in the specifi ed variable\nhash\nDirectly modifi es the contents of the command hash table\nhistory\nLists the commands contained in the history fi le\ninteger\nSets the specifi ed variable for use as an integer value\njobs\nLists information about the specifi ed job or all jobs assigned to the shell \nprocess\nkill\nSends a signal (Default SIGTERM) to the specifi ed process or job\nlet\nEvaluates a mathematical operation and assigns the result to a variable\nlimit\nSets or displays resource limits\nlocal\nSets the data features for the specifi ed variable\nlog\nDisplays all users currently logged in who are affected by the watch \nparameter\nlogout\nSame as exit, but works only when the shell is a login shell\npopd\nRemoves the next entry from the directory stack\nprint\nDisplays variables and text\nprintf\nDisplays variables and text using C-style format strings\npushd\nChanges the current working directory and puts the previous directory in \nthe directory stack\npushln\nPlaces the specifi ed arguments into the editing buffer stack\npwd\nDisplays the full pathname of the current working directory\nread\nReads a line and assigns data fi elds to the specifi ed variables using the IFS \ncharacters\nreadonly\nAssigns a value to a variable that can’t be changed\nrehash\nRebuilds the command hash table\nset\nSets options or positional parameters for the shell\nsetopt\nSets the options for a shell\nshift\nReads and deletes the fi rst positional parameter and shifts the remaining \nones down one position\nContinues\n\n636\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  636\nCommandDescription\nsource\nFinds the specifi ed fi le and copies its contents into the current location\nsuspend\nSuspends the execution of the shell until it receives a SIGCONT signal\ntest\nReturns an exit status of 0 if the specifi ed condition is TRUE\ntimes\nDisplays the cumulative user and system times for the shell and processes \nthat run in the shell\ntrap\nBlocks the specifi ed signals from being processed by the shell and exe-\ncutes the specifi ed commands if the signals are received\ntrue\nReturns a zero exit status\nttyctl\nLocks and unlocks the display\ntype\nDisplays how the specifi ed command would be interpreted by the shell\ntypeset\nSets or displays attributes of variables\nulimit\nSets or displays resource limits of the shell or processes running in the shell\numask\nSets or displays the default permissions for creating fi les and directories\nunalias\nRemoves the specifi ed command alias\nunfunction\nRemoves the specifi ed defi ned function\nunhash\nRemoves the specifi ed command from the hash table\nunlimit\nRemoves the specifi ed resource limit\nunset\nRemoves the specifi ed variable attribute.\nunsetopt\nRemoves the specifi ed shell option\nwait\nWaits for the specifi ed job or process to complete\nwhence\nDisplays how the specifi ed command would be interpreted by the shell\nwhere\nDisplays the pathname of the specifi ed command if found by the shell\nWhich\nDisplays the pathname of the specifi ed command using csh-style output\nzcompile\nCompiles the specifi ed function or script for faster autoloading\nzmodload\nPerforms operations on loadable zsh modules\nThe zsh shell is no slouch when it comes to providing built-in commands! You should recog-\nnize most of these commands from their bash counterparts. The most important features of \nthe zsh shell built-in commands are modules.\nAdd-in modules\nThere’s a long list of modules that provide additional built-in commands for the zsh shell, \nand the list continues to grow as resourceful programmers create new modules. Table 23-5 \nshows some of the more popular modules available.\nTABLE 23- 4   (continued)\n\n637\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  637\n23\nTABLE 23-5    The  zsh  Modules\nModuleDescription\nzsh/datetime\nAdditional date and time commands and variables\nzsh/files\nCommands for basic fi le handling\nzsh/mapfile\nAccess to external fi les via associative arrays\nzsh/mathfunc\nAdditional scientifi c functions\nzsh/pcre\nThe extended regular expression library\nzsh/net/socket\nUnix domain socket support\nzsh/stat\nAccess to the stat system call to provide system statistics\nzsh/system\nInterface for various low-level system features\nzsh/net/tcp\nAccess to TCP sockets\nzsh/zftp\nA specialized FTP client command\nzsh/zselect\nBlocks and returns when fi le descriptors are ready\nzsh/zutil\nVarious shell utilities\nThe zsh shell modules cover a wide range of topics, from providing simple command line \nediting features to advanced networking functions. The idea behind the zsh shell is to pro-\nvide a basic minimum shell environment and let you add on the pieces you need to accom-\nplish your programming job.\nViewing, adding, and removing modules\nThe zmodload command is the interface to the zsh modules. You use this command to \nview, add, and remove modules from the zsh shell session.\nUsing the \nzmodload command without any command line parameters displays the cur-\nrently installed modules in your zsh shell: \n% zmodload\nzsh/zutil\nzsh/complete\nzsh/main\nzsh/terminfo\nzsh/zle\nzsh/parameter\n%\n\n638\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  638\nDifferent zsh shell implementations include different modules by default. To add a new \nmodule, just specify the module name on the \nzmodload command line:\n% zmodload zsh/zftp\n%\nNothing indicates that the module loaded. You can perform another zmodload command, \nand the new module should appear in the list of installed modules.\nAfter you load a module, the commands associated with the module are available as built-in \ncommands:\n% zftp open myhost.com rich testing1\nWelcome to the myhost FTP server.\n% zftp cd test\n% zftp dir\n01-21-11 11:21PM    120823 test1\n01-21-11 11:23PM    118432 test2\n% zftp get test1 > test1.txt\n% zftp close\n%\n \nThe zftp command allows you to conduct a complete FTP session directly from your zsh \nshell command line! You can incorporate these commands into your zsh shell scripts to per-\nform fi le transfers directly from your scripts.\nTo remove an installed module, use the \n-u parameter, along with the module name:\n% zmodload -u zsh/zftp\n% zftp\nzsh: command not found: zftp\n%\nIt’s a common practice to place zmodload commands in the $HOME/.zshrc startup fi le so your favorite func-\ntions load automatically when the zsh shell starts.\nScripting with zsh\nThe main purpose of the zsh shell was to provide an advanced programming environment \nfor shell programmers. With that in mind, it’s no surprise that the zsh shell offers many \nfeatures that make shell scripting easier.\n\n639\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  639\n23\nMathematical operations\nAs you would expect, the zsh shell allows you to perform mathematical functions with \nease. In the past, the Korn shell has led the way in supporting mathematical operations by \nproviding support for fl oating-point numbers. The zsh shell has full support for fl oating-\npoint numbers in all its mathematical operations!\nPerforming calculations\nThe zsh shell supports two methods for performing mathematical operations:\n ■\nThe let command\n ■\nDouble parentheses\nWhen you use the \nlet command, you should enclose the operation in double quotation \nmarks to allow for spaces:\n% let value1=\" 4 * 5.1 / 3.2 \"\n% echo $value1\n6.3750000000\n%\nBe careful, using fl oating point numbers may introduce a precision problem. To solve this, \nit’s always a good idea to use the \nprintf command and to specify the decimal precision \nneeded to correctly display the answer:\n% printf \"%6.3f\\n\" $value1\n6.375\n%\nNow that’s much better!\nThe second method is to use the double parentheses. This method incorporates two tech-\nniques for defi ning the mathematical operation:\n% value1=$(( 4 * 5.1 ))\n% (( value2 = 4 * 5.1 ))\n% printf \"%6.3f\\n\" $value1 $value2\n20.400\n20.400\n%\nNotice that you can place the double parentheses either around just the operation (pre-\nceded by a dollar sign) or around the entire assignment statement. Both methods produce \nthe same results.\nIf you don’t use the \ntypeset command to declare the data type of a variable beforehand, \nthe zsh shell attempts to automatically assign the data type. This can be dangerous when \nworking with both integer and fl oating-point numbers. Look at this example:\n\n640\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  640\n% value1=10\n% value2=$(( $value1 / 3 ))\n% echo $value2\n3\n%\nNow, that’s probably not the answer you want to come out from the calculation. When you \nspecify numbers without decimal places, the zsh shell interprets them as integer values \nand performs integer calculations. To ensure that the result is a fl oating-point number, you \nmust specify the numbers with decimal places:\n% value1=10.\n% value2=$(( $value1 / 3. ))\n% echo $value2\n3.3333333333333335\n%\nNow the result is in the fl oating-point format.\nMathematical functions\nWith the zsh shell, built-in mathematical functions are either feast or famine. The default \nzsh shell doesn’t include any special mathematical function. However, if you install the \nzsh/mathfunc module, you have more math functions than you’ll most likely ever need:\n% value1=$(( sqrt(9) ))\nzsh: unknown function: sqrt\n% zmodload zsh/mathfunc\n% value1=$(( sqrt(9) ))\n% echo $value1\n3.\n%\nThat was simple! Now you have an entire math library of functions at your fi ngertips.\nLots of mathematical functions are supported in zsh. For a complete listing of all the math functions that the zsh/\nmathfunc\n module provides, look at the manual page for zsh modules.\nStructured commands\nThe zsh shell provides the usual set of structured commands for your shell scripts:\n ■\nif-then-else statements\n ■\nfor loops (including the C-style)\n ■\nwhile loops\n\n641\nChapter 23: Working with Alternative Shells\nc23.indd  12/09/2014  Page  641\n23\n ■\nuntil loops\n ■\nselect statements\n ■\ncase statements\nThe zsh shell uses the same syntax for each of these structured commands that you’re used \nto from the bash shell. The zsh shell also includes a different structured command called \nrepeat. The repeat command uses this format:\nrepeat param\ndo\n   commands\ndone\nThe param parameter must be a number or a mathematical operation that evaluates to \na number. The \nrepeat command then performs the specifi ed commands that number of \ntimes:\n% cat test1\n#!/bin/zsh\n# using the repeat command\n \nvalue1=$(( 10 / 2 ))\nrepeat $value1\ndo\n   echo \"This is a test\"\ndone\n$ ./test1\nThis is a test\nThis is a test\nThis is a test\nThis is a test\nThis is a test\n%\nThis command allows you to repeat sections of code for a set number of times based on a \ncalculation.\nFunctions\nThe zsh shell supports the creation of your own functions either using the function com-\nmand or by defi ning the function name with parentheses:\n% function functest1 {\n> echo \"This is the test1 function\"\n}\n% functest2() {\n> echo \"This is the test2 function\"\n}\n\n642\nPart III: Advanced Shell Scripting\nc23.indd  12/09/2014  Page  642\n% functest1\nThis is the test1 function\n% functest2\nThis is the test2 function\n%\nAs with bash shell functions (see Chapter 17), you can defi ne functions within your shell \nscript and then either use global variables or pass parameters to your functions.\nSummary\n This chapter discussed two popular alternative Linux shells that you may run into. The \ndash shell was developed as part of the Debian Linux distribution and is mainly found in \nthe Ubuntu Linux distribution. It’s a smaller version of the Bourne shell, so it doesn’t sup-\nport as many features as the bash shell, which can cause problems for script writing.\nThe zsh shell is often found in programming environments, because it provides lots of cool \nfeatures for shell script programmers. It uses loadable modules to load separate code librar-\nies, which make using advanced functions as easy as running command line commands! \nThere are loadable modules for lots of different functions, from complex mathematical algo-\nrithms to network applications such as FTP and HTTP.\nThe next section of this book dives into some specifi c scripting applications you might run \ninto in the Linux environment. The next chapter shows how to write simple utilities to help \nwith your day-to-day Linux administration functions. Those can greatly help simplify com-\nmon tasks you perform on the system. \n\nc24.indd  12/05/2014  Page  643\nPart IV\nCreating Practical Scripts\nIN THIS PART\nChapter 24\nWriting Simple Script Utilities\nChapter 25\nProducing Scripts for Database, Web, and \nE-Mail\nChapter 26\nCreating Fun Little Shell Scripts\n\n\n\n645\nc24.indd  12/05/2014  Page  645\nCHAPTER \n24\nWriting Simple Script Utilities\nIN THIS CHAPTER\nAutomating backups\nManaging user accounts\nWatching disk space\nN\nowhere is shell script programming more useful than writing script utilities for the Linux \nsystem administrator. The typical Linux system administrator has many various jobs to do \ndaily, from monitoring disk space to backing up important fi les to managing user accounts. \nShell script utilities can make these tasks much easier! This chapter demonstrates some of the \ncapabilities you have writing script utilities in the bash shell.\nPerforming Archives\nWhether you’re responsible for a Linux system in a business environment or just using it at home, \nthe loss of data can be catastrophic. To help prevent bad things from happening, it’s always a good \nidea to perform regular backups (or archives).\nHowever, what’s a good idea and what’s practical are often two separate things. Trying to arrange \na backup schedule to store important fi les can be a challenge. This is another place where shell \nscripts often come to the rescue.\nThis section demonstrates two methods for using shell scripts to archive data on your Linux \nsystem.\nArchiving data fi les\nIf you’re using your Linux system to work on an important project, you can create a shell script \nthat automatically takes snapshots of specifi c directories. Designating these directories in a confi g-\nuration fi le allows you to change them when a particular project changes. This helps avoid a time-\nconsuming restore process from your main archive fi les.\nThis section shows you how to create an automated shell script that can take snapshots of specifi ed \ndirectories and keep an archive of your data’s past versions.\n\n646\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  646\nObtaining the required functions\nThe workhorse for archiving data in the Linux world is the tar command (see Chapter 4). \nThe \ntar command is used to archive entire directories into a single fi le. Here’s an example \nof creating an archive fi le of a working directory using the \ntar command:\n$ tar -cf archive.tar /home/Christine/Project/*.*\ntar: Removing leading '/' from member names\n$\n$ ls -l archive.tar\n-rw-rw-r--. 1 Christine Christine 51200 Aug 27 10:51 archive.tar\n$\nThe tar command responds with a warning message that it’s removing the leading forward \nslash from the pathname to convert it from an absolute pathname to a relative pathname \n(see Chapter 3). This allows you to extract the \ntar archived fi les anywhere you want in \nyour fi lesystem. You’ll probably want to get rid of that message in your script. You can \naccomplish this by redirecting \nSTDERR to the /dev/null fi le (see Chapter 15): \n$ tar -cf archive.tar /home/Christine/Project/*.* 2>/dev/null\n$\n$ ls -l archive.tar\n-rw-rw-r--. 1 Christine Christine 51200 Aug 27 10:53 archive.tar\n$\nBecause a tar archive fi le can consume lots of disk space, it’s a good idea to compress the \nfi le. You can do this by simply adding the \n-z option. This compresses the tar archive fi le \ninto a gzipped \ntar fi le, which is called a tarball. Be sure to use the proper fi le extensions to \ndenote that the fi le is a tarball. Either \n.tar.gz or .tgz is fi ne. Here’s an example of creat-\ning a tarball of the project directory:\n$ tar -zcf archive.tar.gz /home/Christine/Project/*.* 2>/dev/null\n$\n$ ls -l archive.tar.gz\n-rw-rw-r--. 1 Christine Christine 3331 Aug 27 10:53 archive.tar.gz\n$\nNow you have the main component for your archive script completed.\nInstead of modifying or creating a new archive script for each new directory or fi le you \nwant to back up, you can use a confi guration fi le. The confi guration fi le should contain \neach directory or fi le you want to be included in the archive.\n$ cat Files_To_Backup\n/home/Christine/Project\n/home/Christine/Downloads\n/home/Does_not_exist\n/home/Christine/Documents\n$\n\n647\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  647\n24\nIf you’re using a Linux distribution that includes a graphical desktop, be careful about archiving your entire $HOME \ndirectory. Although this may be tempting, the \n$HOME directory contains lots of confi guration and temporary fi les \nrelated to the graphical desktop. It creates a much larger archive fi le than you probably intended. Pick a subdirectory \nin which to store your working fi les, and use that subdirectory in your archive confi guration fi le.\nYou can have the script read through the confi guration fi le and add the names of each \ndirectory to the archive list. To do this, use the simple \nread command (see Chapter 14) to \nread each record from the fi le. But instead of using the \ncat command piped into a while \nloop (see Chapter 13), this script redirects standard input (\nSTDIN) using the exec com-\nmand (see Chapter 15). Here’s how it looks:\nexec < $CONFIG_FILE\nread FILE_NAME\nNotice that a variable is used for the archive confi guration fi le, CONFIG_FILE. Each record \nis read in from the confi guration fi le. As long as the \nread command fi nds a new confi gu-\nration fi le record to read, it returns an exit value of 0 for success in the \n? variable (see \nChapter 11). You can use this as a test in a \nwhile loop in order to read all the records from \nthe confi guration fi le:\nwhile [ $? -eq 0 ]\ndo\n[...]\nread FILE_NAME\ndone\nWhen the read command hits the end of the confi guration fi le, it returns a non-zero \nstatus. At that point, the \nwhile loop is exited.\nIn the \nwhile loop, two things need to happen. First, you must add the directory name \nto your archive list. Even more important is to check to see if that directory even exists! \nIt would be very easy to remove a directory from the fi lesystem and forget to update the \narchive confi guration fi le. You can check a directory’s existence using a simple \nif state-\nment (see Chapter 12). If the directory does exist, it is added to the list of directories to \narchive, \nFILE_LIST. Otherwise, a warning message is issued. Here is what this if state-\nment looks like:\nif [ -f $FILE_NAME -o -d $FILE_NAME ] \n        then \n                # If file exists, add its name to the list.\n                FILE_LIST=\"$FILE_LIST $FILE_NAME\"\n        else\n                # If file doesn't exist, issue warning\n\n648\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  648\n                echo\n                echo \"$FILE_NAME, does not exist.\" \n                echo \"Obviously, I will not include it in this archive.\"\n                echo \"It is listed on line $FILE_NO of the config file.\"\n                echo \"Continuing to build archive list...\" \n                echo \n        fi \n# \n        FILE_NO=$[$FILE_NO + 1]             # Increase Line/File number by one.\nBecause a record in our archive confi guration fi le can be a fi lename or a directory, the if \nstatement tests for the existence of both, using the \n-f and the -d options. The or option, \n-o, allows for either the fi le’s or the directory’s existence test to return a non-zero status \nfor the entire \nif statement to be treated as true.\nTo provide a little extra help in tracking down non-existent directories and fi les, the \nvariable \nFILE_NO is added. Thus, the script can tell you exactly what line number in the \narchive confi guration fi le contains the incorrect or missing fi le or directory.\nCreating a daily archive location\nIf you are just backing up a few fi les, it’s fi ne to keep the archive in your personal direc-\ntory. However, if several directories are being backed up, it is best to create a central repos-\nitory archive directory:\n$ sudo mkdir /archive\n[sudo] password for Christine:\n$\n$ ls -ld /archive\ndrwxr-xr-x. 2 root root 4096 Aug 27 14:10 /archive\n$\nAfter you have your central repository archive directory created, you need to grant access \nto it for certain users. If you do not do this, trying to create fi les in this directory fails, as \nshown here:\n$ mv Files_To_Backup /archive/\nmv: cannot move 'Files_To_Backup' to \n'/archive/Files_To_Backup': Permission denied\n$\nYou could grant the users needing to create fi les in this directory permission via sudo or \ncreate a user group. In this case, a special user group is created, \nArchivers:\n$ sudo groupadd Archivers\n$\n$ sudo chgrp Archivers /archive\n$\n$ ls -ld /archive\n\n649\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  649\n24\ndrwxr-xr-x. 2 root Archivers 4096 Aug 27 14:10 /archive\n$\n$ sudo usermod -aG Archivers Christine\n[sudo] password for Christine:\n$\n$ sudo chmod 775 /archive\n$\n$ ls -ld /archive\ndrwxrwxr-x. 2 root Archivers 4096 Aug 27 14:10 /archive\n$\nAfter a user has been added to the Archivers group, the user must log out and log back in \nfor the group membership to take effect. Now fi les can be created by this group’s members \nwithout the use of super-user privileges:\n$ mv Files_To_Backup /archive/\n$\n$ ls /archive\nFiles_To_Backup\n$\nKeep in mind that all Archivers group members can add and delete fi les from this direc-\ntory. It may be best to add the sticky bit (see Chapter 7) to the directory, in order to keep \ngroup members from deleting each other’s archive tarballs.\nYou should now have enough information to start building the script. The next section \nwalks you through creating the daily archive script.\nCreating a daily archive script\nThe Daily_Archive.sh script automatically creates an archive to a designated location, \nusing the current date to uniquely identify the fi le. Here’s the code for that portion of the \nscript:\nDATE=$(date +%y%m%d)\n#\n# Set Archive File Name\n#\nFILE=archive$DATE.tar.gz\n# \n# Set Configuration and Destination File\n#\nCONFIG_FILE=/archive/Files_To_Backup\nDESTINATION=/archive/$FILE\n#\nThe DESTINATION variable appends the full pathname for the archived fi le. The CONFIG_\nFILE\n variable points to the archive confi guration fi le containing the directories to be \narchived. These both can be easily changed to alternate directories and fi les if needed.\n\n650\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  650\nWhen you are new to script writing and are presented with an entire script (as shown nearby), make a habit of reading \nthrough the whole script. Try to follow the logic and script fl ow. Note any script syntax or sections you have trouble \nunderstanding, and reread the chapter that covers that topic. This reviewing habit helps you to build your script writ-\ning skills much more quickly.\nThe Daily_Archive.sh script, all put together, now looks like this:\n#!/bin/bash\n#\n# Daily_Archive - Archive designated files & directories\n########################################################\n#\n# Gather Current Date\n#\nDATE=$(date +%y%m%d)\n#\n# Set Archive File Name\n#\nFILE=archive$DATE.tar.gz\n#\n# Set Configuration and Destination File\n#\nCONFIG_FILE=/archive/Files_To_Backup\nDESTINATION=/archive/$FILE\n#\n######### Main Script #########################\n#\n# Check Backup Config file exists\n#\nif [ -f $CONFIG_FILE ]   # Make sure the config file still exists.\nthen                # If it exists, do nothing but continue on.\n     echo\nelse                # If it doesn't exist, issue error & exit script.\n     echo\n     echo \"$CONFIG_FILE does not exist.\"\n     echo \"Backup not completed due to missing Configuration File\"\n     echo\n     exit\nfi\n#\n# Build the names of all the files to backup\n#\nFILE_NO=1               # Start on Line 1 of Config File.\nexec < $CONFIG_FILE     # Redirect Std Input to name of Config File\n#\n\n651\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  651\n24\nread FILE_NAME          # Read 1st record\n#\nwhile [ $? -eq 0 ]      # Create list of files to backup.\ndo\n        # Make sure the file or directory exists.\n     if [ -f $FILE_NAME -o -d $FILE_NAME ]\n     then\n          # If file exists, add its name to the list.\n          FILE_LIST=\"$FILE_LIST $FILE_NAME\"\n     else\n          # If file doesn't exist, issue warning\n          echo\n          echo \"$FILE_NAME, does not exist.\"\n          echo \"Obviously, I will not include it in this archive.\"\n          echo \"It is listed on line $FILE_NO of the config file.\"\n          echo \"Continuing to build archive list...\"\n          echo\n     fi\n#\n     FILE_NO=$[$FILE_NO + 1]  # Increase Line/File number by one.\n     read FILE_NAME           # Read next record.\ndone\n#\n#######################################\n#\n# Backup the files and Compress Archive\n#\necho \"Starting archive...\"\necho\n#\ntar -czf $DESTINATION $FILE_LIST 2> /dev/null\n#\necho \"Archive completed\"\necho \"Resulting archive file is: $DESTINATION\"\necho\n#\nexit\nRunning the daily archive script\nBefore you attempt to test the script, remember that you need to change permissions on \nthe script fi le (see Chapter 11). The fi le’s owner must be given execute (\nx) privilege before \nthe script can be run:\n$ ls -l Daily_Archive.sh\n-rw-rw-r--. 1 Christine Christine 1994 Aug 28 15:58 Daily_Archive.sh\n$\n$ chmod u+x Daily_Archive.sh\n$\n\n652\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  652\n$ ls -l Daily_Archive.sh\n-rwxrw-r--. 1 Christine Christine 1994 Aug 28 15:58 Daily_Archive.sh\n$\nTesting the Daily_Archive.sh script is straightforward:\n$ ./Daily_Archive.sh\n/home/Does_not_exist, does not exist.\nObviously, I will not include it in this archive.\nIt is listed on line 3 of the config file.\nContinuing to build archive list...\nStarting archive...\nArchive completed\nResulting archive file is: /archive/archive140828.tar.gz\n$ ls /archive\narchive140828.tar.gz  Files_To_Backup\n$\nYou can see that the script caught one directory that does not exist, /home/Does_not_\nexist\n. It lets you know what line number in the confi guration fi le this erroneous directory \nis on and continues making a list and archiving the data. Your data is now safely archived \nin a tarball fi le.\nCreating an hourly archive script\nIf you are in a high-volume production environment where fi les are changing rapidly, a \ndaily archive might not be good enough. If you want to increase the archiving frequency to \nhourly, you need to take another item into consideration.\nWhen backing up fi les hourly and trying to use the \ndate command to timestamp each \ntarball, things can get pretty ugly pretty quickly. Sifting through a directory of tarballs \nwith fi lenames looking like this is tedious:\narchive010211110233.tar.gz\nInstead of placing all the archive fi les in the same folder, you can create a directory hierar-\nchy for your archived fi les. Figure 24-1 demonstrates this principle.\n\n653\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  653\n24\nFIGURE 24-1\nCreating an archive directory hierarchy\n/archive/hourly\nBase\nMonth\nDay\n02\n01\n01\n02\n01\nThe archive directory contains directories for each month of the year, using the month \nnumber as the directory name. Each month’s directory in turn contains folders for each day \nof the month (using the day’s numerical value as the directory name). This allows you to \njust timestamp the individual tarballs and place them in the appropriate directory for the \nday and month.\nFirst, the new directory \n/archive/hourly must be created, along with the appropri-\nate permissions set upon it. Remember from early in this chapter that members of the \nArchivers group are granted permission to create archives in this directory area. Thus, \nthe newly created directory must have its primary group and group permissions changed:\n$ sudo mkdir /archive/hourly\n[sudo] password for Christine:\n$\n\n654\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  654\n$ sudo chgrp Archivers /archive/hourly\n$\n$ ls -ld /archive/hourly/\ndrwxr-xr-x. 2 root Archivers 4096 Sep  2 09:24 /archive/hourly/\n$\n$ sudo chmod 775 /archive/hourly\n$\n$ ls -ld /archive/hourly\ndrwxrwxr-x. 2 root Archivers 4096 Sep  2 09:24 /archive/hourly\n$\nAfter the new directory is set up, the Files_To_Backup confi guration fi le for the hourly \narchives can be moved to the new directory:\n$ cat Files_To_Backup\n/usr/local/Production/Machine_Errors\n/home/Development/Simulation_Logs\n$\n$ mv Files_To_Backup /archive/hourly/\n$\nNow, there is a new challenge to solve. The script must create the individual month and day \ndirectories automatically. If these directories already exist, and the script tries to create \nthem, an error is generated. This is not a desirable outcome!\nIf you peruse the command line options for the \nmkdir command (see Chapter 3), you’ll fi nd \nthe \n-p command line option. This option allows you to create directories and subdirectories \nin a single command; plus, the added benefi t is that it doesn’t produce an error message if \nthe directory already exists. Perfect fi t for what is needed in the script!\nWe’re now ready to create the \nHourly_Archive.sh script. Here is the top half of the \nscript:\n#!/bin/bash\n#\n# Hourly_Archive - Every hour create an archive\n#########################################################\n#\n# Set Configuration File\n#\nCONFIG_FILE=/archive/hourly/Files_To_Backup\n#\n# Set Base Archive Destination Location\n#\nBASEDEST=/archive/hourly\n#\n# Gather Current Day, Month & Time\n#\n\n655\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  655\n24\nDAY=$(date +%d)\nMONTH=$(date +%m)\nTIME=$(date +%k%M)\n#\n# Create Archive Destination Directory\n#\nmkdir -p $BASEDEST/$MONTH/$DAY\n#\n# Build Archive Destination File Name\n#\nDESTINATION=$BASEDEST/$MONTH/$DAY/archive$TIME.tar.gz\n#\n########## Main Script ####################\n[...]\nAfter the script reaches the “Main Script” portion of Hourly_Archive.sh, the script is an \nexact duplicate of the \nDaily_Archive.sh script. Lots of the work has already been done!\nHourly_Archive.sh retrieves the day and month values from the date command, \nalong with the timestamp used to uniquely identify the archive fi le. It then uses that \ninformation to create the archive directory for the day (or to silently exit if it already \nexists). Finally, the script uses the \ntar command to create the archive and compress it \ninto a tarball.\nRunning the hourly archive script\nAs with the Daily_Archive.sh script, it’s a good idea to test the Hourly_Archive.sh \nscript before putting it in the \ncron table. Before the script is run, the permissions must be \nmodifi ed. Also, the hour and minute is checked via the \ndate command. Having the current \nhour and minute allows the fi nal archive fi lename to be verifi ed for correctness: \n$ chmod u+x Hourly_Archive.sh\n$\n$ date +%k%M\n1011\n$\n$ ./Hourly_Archive.sh\nStarting archive...\nArchive completed\nResulting archive file is: /archive/hourly/09/02/archive1011.tar.gz\n$\n$ ls /archive/hourly/09/02/\narchive1011.tar.gz\n$\n\n656\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  656\nThe script worked fi ne the fi rst time, creating the appropriate month and day directories, \nand then creating the properly named archive fi le. Notice that the archive fi le has the \nappropriate hour (\n10) and minute (11) in its name, archive1011.tar.gz.\nIf you run the Hourly_Archive.sh script during the day, when the hour is in single digits, your archive \nfi le’s name will only have three digits. For example, if you run the script at 1:15am, the archive fi le’s name is \narchive115.tar.gz. If you prefer to always have four digits in the archive fi le name, modify the script line, \nTIME=$(date +%k%M), to TIME=$(date +%k0%M). By adding a zero (0) after the %k, any single digit hours \nare padded to two digits with a leading zero. Thus, \narchive115.tar.gz is instead named archive0115\n.tar.gz\n.\nJust to test things out, the script was run a second time to see if it would have a problem \nwith the existing directory, \n/archive,hourly/09/02:\n$ date +%k%M\n1017\n$\n$ ./Hourly_Archive.sh\nStarting archive...\nArchive completed\nResulting archive file is: /archive/hourly/09/02/archive1017.tar.gz\n$ ls /archive/hourly/09/02/\narchive1011.tar.gz  archive1017.tar.gz\n$\nNo problems with the existing directory! The script again ran fi ne and created a second \narchive fi le. It’s now ready for the \ncron table.\nManaging User Accounts\nManaging user accounts is much more than just adding, modifying, and deleting accounts. \nYou must also consider security issues, the need to preserve work, and the accurate man-\nagement of the accounts. This can be a time-consuming task. Here is another instance \nwhen writing script utilities is a real timesaver!\n\n657\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  657\n24\nObtaining the required functions\nDeleting an account is the more complicated accounts management task. When deleting an \naccount, at least four separate actions are required:\n \n1.  Obtain the correct user account name to delete.\n \n2.  Kill any processes currently running on the system that belongs to that account.\n \n3.  Determine all fi les on the system belonging to the account.\n \n4.  Remove the user account.\nIt’s easy to miss a step. The shell script utility in this section helps you avoid making such \nmistakes.\nGetting the correct account name\nThe fi rst step in the account deletion process is the most important: obtaining the correct \nuser account name to delete. Because this is an interactive script, you can use the \nread \ncommand (see Chapter 14) to obtain the account name. If the script user walks away and \nleaves the question hanging, you can use the \n-t option on the read command and timeout \nafter giving the script user 60 seconds to answer the question:\necho \"Please enter the username of the user \" \necho -e \"account you wish to delete from system: \\c\"\nread -t 60 ANSWER\nBecause interruptions are part of life, it’s best to give users three chances to answer the \nquestion. This is accomplished by using a \nwhile loop (Chapter 13) with the -z option, to \ntest whether the \nANSWER variable is empty. The ANSWER variable is empty when the script \nfi rst enters the \nwhile loop on purpose. The question to fi ll the ANSWER variable is at the \nend of the loop:\nwhile [ -z \"$ANSWER\" ]\ndo\n[...]\necho \"Please enter the username of the user \" \necho -e \"account you wish to delete from system: \\c\"\nread -t 60 ANSWER\ndone\nA way to communicate with the script user is needed when the fi rst question timeout \noccurs, when there is one more chance to answer the question, and so on. The \ncase state-\nment (see Chapter 12) is the structured command that works perfectly here. Using the \n\n658\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  658\nincremented ASK_COUNT variable, different messages can be set up to communicate to the \nscript user. The code for this section looks like this:\ncase $ASK_COUNT in\n2)\n     echo\n     echo \"Please answer the question.\"\n     echo\n;;\n3)\n     echo\n     echo \"One last try...please answer the question.\"\n     echo\n;;\n4)\n     echo\n     echo \"Since you refuse to answer the question...\"\n     echo \"exiting program.\"\n     echo\n     #\n     exit \n;;\nesac\n#\nNow the script has all the structure it needs to ask the user what account to delete. There \nare several more questions in this script to ask the user and asking just that one question \nwas lots of code! Therefore, let’s turn this piece of code into a function (see Chapter 17) in \norder to use it in multiple locations in your \nDelete_User.sh script.\nCreating a function to get the correct account name\nThe fi rst thing you need to do is declare the function’s name, get_answer. Next, clear out \nany previous answers to questions your script user gave using the \nunset command (see \nChapter 6). The code to do these two items looks like this:\nfunction get_answer {\n#\nunset ANSWER\nThe other original code item you need to change is the question to the script user. The \nscript doesn’t ask the same question each time, so two new variables are created, \nLINE1 \nand \nLINE2, to handle question lines:\necho $LINE1\necho -e $LINE2\" \\c\"\n\n659\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  659\n24\nHowever, not every question has two lines to display. Some have only one line. An if \nstatement (see Chapter 12) assists with this problem. The function tests if \nLINE2 is empty \nand only uses \nLINE1 if it is:\nif [ -n \"$LINE2\" ]\nthen\n     echo $LINE1\n     echo -e $LINE2\" \\c\"\nelse\n     echo -e $LINE1\" \\c\"\nfi\nFinally, the function needs to clean up after itself by clearing out the LINE1 and LINE2 \nvariables. Thus, the function now looks like this:\nfunction get_answer {\n#\nunset ANSWER\nASK_COUNT=0\n#\nwhile [ -z \"$ANSWER\" ]\ndo\n     ASK_COUNT=$[ $ASK_COUNT + 1 ]\n#\n     case $ASK_COUNT in\n     2)\n           echo\n[...]\n     esac\n#\n     echo\n     if [ -n \"$LINE2\" ]\n     then                 #Print 2 lines\n           echo $LINE1\n           echo -e $LINE2\" \\c\"\n     else                    #Print 1 line\n           echo -e $LINE1\" \\c\"\n     fi\n#\n     read -t 60 ANSWER\ndone\n# \nunset LINE1\nunset LINE2\n#\n}  #End of get_answer function\n\n660\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  660\nTo ask the script user what account to delete, a few variables must be set and the get_\nanswer\n function should be called. Using the new function makes the script code much \nsimpler:\nLINE1=\"Please enter the username of the user \" \nLINE2=\"account you wish to delete from system:\"\nget_answer\nUSER_ACCOUNT=$ANSWER\nVerifying the entered account name\nBecause of potential typographical errors, the user account name that was entered should \nbe verifi ed. This is easy because the code is already in place to handle asking a question:\nLINE1=\"Is $USER_ACCOUNT the user account \" \nLINE2=\"you wish to delete from the system? [y/n]\"\nget_answer\nAfter the question is asked, the script must process the answer. The variable ANSWER again \ncarries the script user’s answer to the question. If the user answered “yes,” the correct user \naccount to delete has been entered and the script can continue. A \ncase statement (see \nChapter 12) processes the answer. The \ncase statement must be coded so it checks for the \nmultiple ways the answer “yes” can be entered.\ncase $ANSWER in\ny|Y|YES|yes|Yes|yEs|yeS|YEs|yES ) \n#\n;;\n*)                   \n       echo \n       echo \"Because the account, $USER_ACCOUNT, is not \"\n       echo \"the one you wish to delete, we are leaving the script...\"\n       echo \n       exit\n;;\nesac\nSometimes, this script needs to handle a yes/no answer from the user. Thus, again, it \nmakes sense to create a function to handle this task. Only a few changes need to be made \nto the preceding code. The function’s name must be declared and the variables \nEXIT_\nLINE1\n and EXIT_LINE2 added to the case statement. These changes, along with some \nvariable cleanup at the end, result in the \nprocess_answer function:\nfunction process_answer {\n#\ncase $ANSWER in\ny|Y|YES|yes|Yes|yEs|yeS|YEs|yES ) \n;;\n*)                   \n\n661\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  661\n24\n        echo \n        echo $EXIT_LINE1 \n        echo $EXIT_LINE2\n        echo \n        exit\n;;\nesac\n#\nunset EXIT_LINE1\nunset EXIT_LINE2\n#\n} #End of process_answer function\nA simple function call now processes the answer:\nEXIT_LINE1=\"Because the account, $USER_ACCOUNT, is not \" \nEXIT_LINE2=\"the one you wish to delete, we are leaving the script...\"\nprocess_answer\nDetermining whether the account exists\nThe user has given us the name of the account to delete and has verifi ed it. Now is a good \ntime to double-check that the user account really exists on the system. Also, it is a good \nidea to show the full account record to the script user to check one more time that this is \nthe account to delete. To accomplish these items, a variable, \nUSER_ACCOUNT_RECORD, is \nset to the outcome of a \ngrep (see Chapter 4) search for the account through the /etc/\npasswd\n fi le. The -w option allows an exact word match for this particular user account:\nUSER_ACCOUNT_RECORD=$(cat /etc/passwd | grep -w $USER_ACCOUNT)\nIf no user account record is found in /etc/passwd, the account has already been deleted \nor never existed in the fi rst place. In either case, the script user must be notifi ed of this \nsituation and the script exited. The exit status of the \ngrep command helps here. If the \naccount record is not found, the \n? variable is set to 1:\nif [ $? -eq 1 ] \nthen \n     echo \n     echo \"Account, $USER_ACCOUNT, not found. \" \n     echo \"Leaving the script...\" \n     echo \n     exit \nfi \nIf the record was found, you still need to verify with the script user that this is the correct \naccount. Here is where all the work to set up the functions really pays off! You just need to \nset the proper variables and call the functions:\necho \"I found this record:\" \necho $USER_ACCOUNT_RECORD \n\n662\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  662\necho \n# \nLINE1=\"Is this the correct User Account? [y/n]\" \nget_answer\n# \nEXIT_LINE1=\"Because the account, $USER_ACCOUNT, is not\" \nEXIT_LINE2=\"the one you wish to delete, we are leaving the script...\" \nprocess_answer\nRemoving any account processes\nSo far, the script has obtained and verifi ed the correct name of the user account to be \ndeleted. In order to remove the user account from the system, the account cannot own any \nprocesses currently running. Thus, the next step is to fi nd and kill off those processes. This \nis going to get a little complicated!\nFinding the user processes is the easy part. Here the script can use the \nps command (see \nChapter 4) and the \n-u option to locate any running processes owned by the account. By \nredirecting the output to \n/dev/null, the user doesn’t see any display. This is handy, \nbecause if there are no processes, the \nps command only shows a header, which can be con-\nfusing to the script user:\nps -u $USER_ACCOUNT >/dev/null #Are user processes running?\nThe ps command’s exit status and a case structure are used to determine the next step \nto take:\ncase $? in\n1)   # No processes running for this User Account\n     #\n     echo \"There are no processes for this account currently running.\"\n     echo\n;;\n0)   # Processes running for this User Account.\n     # Ask Script User if wants us to kill the processes.\n     #\n     echo \"$USER_ACCOUNT has the following processes running: \"\n     echo\n     ps -u $USER_ACCOUNT\n     #\n     LINE1=\"Would you like me to kill the process(es)? [y/n]\"\n     get_answer\n     #\n[...]\nesac\nIf the ps command’s exit status returns a 1, there are no processes running on the system \nthat belong to the user account. However, if the exit status returns a \n0, processes owned by \n\n663\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  663\n24\nthis account are running on the system. In this case, the script needs to ask the script user \nif he would like to have these processes killed. This task can be accomplished by using the \nget_answer function.\nYou might think that the next action the script does is to call the \nprocess_answer func-\ntion. Unfortunately, the next item is too complicated for \nprocess_answer. Another case \nstatement must be embedded to process the script user’s answer. The fi rst part of the \ncase \nstatement looks very similar to the \nprocess_answer function:\ncase $ANSWER in                  \n     y|Y|YES|yes|Yes|yEs|yeS|YEs|yES ) # If user answers \"yes\", \n                                        #kill User Account processes.\n[...]\n;; \n*)   # If user answers anything but \"yes\", do not kill. \n     echo \n     echo \"Will not kill the process(es)\" \n     echo \n;; \nesac\nAs you can see, there is nothing interesting in the case statement itself. Where things get \ninteresting is within the “yes” section of the \ncase statement. Here, the user account pro-\ncesses need to be killed. To build the command necessary to kill off one or more processes, \nthree commands are needed. The fi rst command is the \nps command again. It is needed to \ngather up the process IDs (PIDs) of the currently running user account processes. The nec-\nessary \nps command is assigned to the variable, COMMAND_1:\nCOMMAND_1=\"ps -u $USER_ACCOUNT --no-heading\"\nThe second command strips off just the PIDs. This simple gawk command (see Chapter 19) \nstrips off the fi rst fi eld from the \nps command’s output, which happens to be the PIDs:\ngawk '{print $1}' \nThe third command, xargs, has not yet been introduced in this book. The xargs command \nbuilds and executes commands from standard input, \nSTDIN (see Chapter 15). It is a great \ncommand to use at the end of a pipe, building and executing commands from each \nSTDIN \nitem produced. The \nxargs command is actually killing off each process via its PID:\nCOMMAND_3=\"xargs -d \\\\n /usr/bin/sudo /bin/kill -9\"\nThe xargs command is assigned to variable COMMAND_3. It uses the -d option to denote \nwhat is considered a delimiter. In other words, because the \nxargs command can accept \nmultiple items as input, what separates one item from another item? In this case, \n\\n \n(newline) is used to set the delimiter. Thus, when each PID is sent to \nxargs, it treats the \nPID as a separate item to be processed. Because the \nxargs command is being assigned to a \nvariable, the backslash (\n\\) in the \\n must be escaped with an additional backslash (\\). \n\n664\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  664\nNotice that xargs needs the full pathname of the commands it is using on each PID. Both \nthe \nsudo and kill (see Chapter 4) commands are used to kill any of the user account’s \nrunning processes. Notice also that the kill signal \n-9 is used.\nAll three commands are hooked together via a pipe. The \nps command produces a list of the \nuser’s running processes, which include the PID of each process. The \nps command passes \nits standard output (\nSTDOUT) as STDIN to the gawk command. The gawk command, in \nturn, strips off only the PIDs from the \nps command’s STDOUT (see Chapter 15). The xargs \ncommand takes each PID the \ngawk command produces as STDIN. It creates and executes \na \nkill command for each PID to kill all the user’s running processes. The command pipe \nlooks like this:\n$COMMAND_1 | gawk '{print $1}' | $COMMAND_3\nThus, the complete case statement for killing off any of the user account’s running pro-\ncesses is as follows:\ncase $ANSWER in                  \n     y|Y|YES|yes|Yes|yEs|yeS|YEs|yES ) # If user answers \"yes\", \n                                        #kill User Account processes.\n       echo\n       echo \"Killing off process(es)...\"\n       #\n       # List user processes running code in variable, COMMAND_1\n       COMMAND_1=\"ps -u $USER_ACCOUNT --no-heading\"\n       #\n       # Create command to kill proccess in variable, COMMAND_3\n       COMMAND_3=\"xargs -d \\\\n /usr/bin/sudo /bin/kill -9\"\n       #\n       # Kill processes via piping commands together\n       $COMMAND_1 | gawk '{print $1}' | $COMMAND_3\n       #\n       echo\n       echo \"Process(es) killed.\"\n     ;;\nBy far, this is the most complicated piece of the script! However, now with any user \naccount–owned processes killed, the script can move on to the next step: fi nding all the \nuser account’s fi les.\nFinding account files\nWhen a user account is deleted from the system, it is a good practice to archive all the fi les \nthat belonged to that account. Along with that practice, it is also important to remove the \nfi les or assign their ownership to another account. If the account you delete has a User ID \n\n665\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  665\n24\nof 1003, and you don’t remove or reassign those fi les, then the next account that is created \nwith a User ID of 1003 owns those fi les! You can see the security disasters that can occur in \nthis scenario.\nThe \nDelete_User.sh script doesn’t do all that for you, but it creates a report that can be \nused in the \nDaily_Archive.sh script as an archive confi guration fi le. And you can use \nthe report to help you remove or reassign the fi les.\nTo fi nd the user’s fi les, you can use the \nfind command. In this case, the find command \nsearches the entire fi lesystem with the \n-u option, which pinpoints any user account–owned \nfi les. The command looks like the following: \nfind / -user $USER_ACCOUNT > $REPORT_FILE\nThat was pretty simple compared to dealing with the user account processes! It gets even \neasier in the next step of the \nDelete_User.sh script: actually removing the user account.\nRemoving the account\nIt’s always a good idea to be a little paranoid about removing a user account from the sys-\ntem. Therefore, you should ask one more time if the script user really wants to remove the \naccount. \nLINE1=\"Remove $User_Account's account from system? [y/n]\"\nget_answer\n# \nEXIT_LINE1=\"Since you do not wish to remove the user account,\"\nEXIT_LINE2=\"$USER_ACCOUNT at this time, exiting the script...\"\nprocess_answer\nFinally, we get to the main purpose of our script, actually removing the user account from \nthe system. Here the \nuserdel command (see Chapter 7) is used:\nuserdel $USER_ACCOUNT\nNow that we have all the pieces, we are ready to put them together into a whole, useful \nscript utility.\nCreating the script\nRecall that the Delete_User.sh script is highly interactive with the script’s user. \nTherefore, it is important to include lots of verbiage to keep the script user informed about \nwhat is going on during the script’s execution. \nAt the top of the script, the two functions \nget_answer and process_answer are \ndeclared. The script then goes to the four steps of removing the user: obtaining and \n\n666\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  666\nconfi rming the user account name, fi nding and killing the user’s processes, creating a \nreport of all fi les owned by the user account, and actually removing the user account.\nWhen you are new to script writing and are presented with an entire script (as shown in the following code), you \nshould make a habit of reading through the whole script to improve your script-writing skills!\nHere’s the entire Delete_User.sh script:\n#!/bin/bash\n#\n#Delete_User - Automates the 4 steps to remove an account\n#\n###############################################################\n# Define Functions\n#\n#####################################################\nfunction get_answer {\n#\nunset ANSWER\nASK_COUNT=0\n#\nwhile [ -z \"$ANSWER\" ]    #While no answer is given, keep asking.\ndo\n     ASK_COUNT=$[ $ASK_COUNT + 1 ]\n#\n     case $ASK_COUNT in   #If user gives no answer in time allotted\n     2)\n          echo\n          echo \"Please answer the question.\"\n          echo\n     ;;\n     3)\n          echo\n          echo \"One last try...please answer the question.\"\n          echo\n     ;;\n     4)\n          echo\n          echo \"Since you refuse to answer the question...\"\n          echo \"exiting program.\"\n          echo\n          #\n          exit\n     ;;\n     esac\n#\n\n667\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  667\n24\n     echo\n#\n     if [ -n \"$LINE2\" ]\n     then               #Print 2 lines\n          echo $LINE1\n          echo -e $LINE2\" \\c\"\n     else                    #Print 1 line\n          echo -e $LINE1\" \\c\"\n     fi\n#\n#     Allow 60 seconds to answer before time-out\n     read -t 60 ANSWER\ndone\n# Do a little variable clean-up\nunset LINE1\nunset LINE2\n#\n}  #End of get_answer function\n#\n#####################################################\nfunction process_answer {\n#\ncase $ANSWER in\ny|Y|YES|yes|Yes|yEs|yeS|YEs|yES )\n# If user answers \"yes\", do nothing.\n;;\n*)\n# If user answers anything but \"yes\", exit script\n        echo\n        echo $EXIT_LINE1\n        echo $EXIT_LINE2\n        echo\n        exit\n;;\nesac\n#\n# Do a little variable clean-up\n#\nunset EXIT_LINE1\nunset EXIT_LINE2\n#\n} #End of process_answer function\n#\n##############################################\n# End of Function Definitions\n#\n############# Main Script ####################\n# Get name of User Account to check\n#\n\n668\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  668\necho \"Step #1 - Determine User Account name to Delete \"\necho\nLINE1=\"Please enter the username of the user \"\nLINE2=\"account you wish to delete from system:\"\nget_answer\nUSER_ACCOUNT=$ANSWER\n#\n# Double check with script user that this is the correct User Account\n#\nLINE1=\"Is $USER_ACCOUNT the user account \"\nLINE2=\"you wish to delete from the system? [y/n]\"\nget_answer\n#\n# Call process_answer funtion:\n#     if user answers anything but \"yes\", exit script\n#\nEXIT_LINE1=\"Because the account, $USER_ACCOUNT, is not \"\nEXIT_LINE2=\"the one you wish to delete, we are leaving the script...\"\nprocess_answer\n#\n################################################################\n# Check that USER_ACCOUNT is really an account on the system\n#\nUSER_ACCOUNT_RECORD=$(cat /etc/passwd | grep -w $USER_ACCOUNT)\n#\nif [ $? -eq 1 ]  # If the account is not found, exit script\nthen\n     echo\n     echo \"Account, $USER_ACCOUNT, not found. \"\n     echo \"Leaving the script...\"\n     echo\n     exit\nfi\n#\necho\necho \"I found this record:\"\necho $USER_ACCOUNT_RECORD\n#\nLINE1=\"Is this the correct User Account? [y/n]\"\nget_answer\n#\n#\n# Call process_answer function:\n#  if user answers anything but \"yes\", exit script\n#\nEXIT_LINE1=\"Because the account, $USER_ACCOUNT, is not \"\nEXIT_LINE2=\"the one you wish to delete, we are leaving the script...\"\nprocess_answer\n#\n\n669\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  669\n24\n##################################################################\n# Search for any running processes that belong to the User Account\n#\necho\necho \"Step #2 - Find process on system belonging to user account\"\necho\n#\nps -u $USER_ACCOUNT >/dev/null #Are user processes running?\n#\ncase $? in\n1)   # No processes running for this User Account\n     #\n     echo \"There are no processes for this account currently running.\"\n     echo\n;;\n0)   # Processes running for this User Account.\n     # Ask Script User if wants us to kill the processes.\n     #\n     echo \"$USER_ACCOUNT has the following processes running: \"\n     echo\n     ps -u $USER_ACCOUNT\n     #\n     LINE1=\"Would you like me to kill the process(es)? [y/n]\"\n     get_answer\n     #\n     case $ANSWER in\n     y|Y|YES|yes|Yes|yEs|yeS|YEs|yES )  # If user answers \"yes\",\n                                        # kill User Account processes.\n       #\n       echo\n       echo \"Killing off process(es)...\"\n       #\n       # List user processes running code in variable, COMMAND_1\n       COMMAND_1=\"ps -u $USER_ACCOUNT --no-heading\"\n       #\n       # Create command to kill proccess in variable, COMMAND_3\n       COMMAND_3=\"xargs -d \\\\n /usr/bin/sudo /bin/kill -9\"\n       #\n       # Kill processes via piping commands together\n       $COMMAND_1 | gawk '{print $1}' | $COMMAND_3\n       #\n       echo\n       echo \"Process(es) killed.\"\n     ;;\n     *)   # If user answers anything but \"yes\", do not kill.\n          echo\n          echo \"Will not kill the process(es)\"\n          echo\n     ;;\n\n670\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  670\n     esac\n;;\nesac\n#################################################################\n# Create a report of all files owned by User Account\n#\necho\necho \"Step #3 - Find files on system belonging to user account\"\necho\necho \"Creating a report of all files owned by $USER_ACCOUNT.\"\necho\necho \"It is recommended that you backup/archive these files,\"\necho \"and then do one of two things:\"\necho \"  1) Delete the files\"\necho \"  2) Change the files' ownership to a current user account.\"\necho\necho \"Please wait. This may take a while...\"\n#\nREPORT_DATE=$(date +%y%m%d)\nREPORT_FILE=$USER_ACCOUNT\"_Files_\"$REPORT_DATE\".rpt\"\n#\nfind / -user $USER_ACCOUNT > $REPORT_FILE 2>/dev/null\n#\necho\necho \"Report is complete.\"\necho \"Name of report:      $REPORT_FILE\"\necho \"Location of report:  $(pwd)\"\necho\n####################################\n#  Remove User Account\necho\necho \"Step #4 - Remove user account\"\necho\n#\nLINE1=\"Remove $USER_ACCOUNT's account from system? [y/n]\"\nget_answer\n#\n# Call process_answer function:\n#       if user answers anything but \"yes\", exit script\n#\nEXIT_LINE1=\"Since you do not wish to remove the user account,\"\nEXIT_LINE2=\"$USER_ACCOUNT at this time, exiting the script...\"\nprocess_answer\n#\nuserdel $USER_ACCOUNT          #delete user account\necho\necho \"User account, $USER_ACCOUNT, has been removed\"\necho\n#\nexit\n\n671\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  671\n24\nThat was lots of work! However, the Delete_User.sh script is a great timesaver and helps \nyou avoid lots of nasty problems when deleting user accounts.\nRunning the script\nBecause it is intended to be an interactive script, the Delete_User.sh script should \nnot be placed in the \ncron table. However, it is still important to ensure that it works as \nexpected.\nTo run this type of script, you must either be logged in as the root user account or use the sudo command to run the \nscript as the root user account.\nBefore the script is tested, the appropriate permissions are set on the script’s fi le:\n$ chmod u+x Delete_User.sh\n$\n$ ls -l Delete_User.sh\n-rwxr--r--. 1 Christine Christine 6413 Sep  2 14:20 Delete_User.sh\n$\nThe script is tested by removing an account, Consultant, that was set up for a temporary \nconsultant on this system:\n$ sudo ./Delete_User.sh\n[sudo] password for Christine:\nStep #1 - Determine User Account name to Delete\nPlease enter the username of the user\naccount you wish to delete from system: Consultant\nIs Consultant the user account\nyou wish to delete from the system? [y/n]\nPlease answer the question.\nIs Consultant the user account\nyou wish to delete from the system? [y/n] y\nI found this record:\nConsultant:x:504:506::/home/Consultant:/bin/bash\nIs this the correct User Account? [y/n] yes\nStep #2 - Find process on system belonging to user account\nConsultant has the following processes running:\n\n672\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  672\n  PID TTY          TIME CMD\n 5443 pts/0    00:00:00 bash\n 5444 pts/0    00:00:00 sleep\nWould you like me to kill the process(es)? [y/n] Yes\nKilling off process(es)...\nProcess(es) killed.\nStep #3 - Find files on system belonging to user account\nCreating a report of all files owned by Consultant.\nIt is recommended that you backup/archive these files,\nand then do one of two things:\n  1) Delete the files\n  2) Change the files' ownership to a current user account.\nPlease wait. This may take a while...\nReport is complete.\nName of report:      Consultant_Files_140902.rpt\nLocation of report:  /home/Christine\nStep #4 - Remove user account\nRemove Consultant's account from system? [y/n] y\nUser account, Consultant, has been removed\n$\n$ ls Consultant*.rpt\nConsultant_Files_140902.rpt\n$\n$ cat Consultant_Files_140902.rpt\n/home/Consultant\n/home/Consultant/Project_393\n/home/Consultant/Project_393/393_revisionQ.py\n/home/Consultant/Project_393/393_Final.py\n[...]\n/home/Consultant/.bashrc\n/var/spool/mail/Consultant\n$\n$ grep Consultant /etc/passwd\n$\n\n673\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  673\n24\nThat worked great! Notice the script was run using sudo, because super-user privileges are \nneeded for deleting accounts. Also notice that the \nread timeout was tested, by delaying \nanswering the following question:\nIs Consultant the user account\nyou wish to delete from the system? [y/n]\nPlease answer the question.\nNote that several different versions of “yes” answers were used for the various questions \nto ensure that the \ncase statement test was working correctly. And fi nally, notice that the \nConsultant user’s fi les were found and put into a report fi le, and the account was deleted.\nNow you have a script utility that assists you when you need to delete user accounts. Even \nbetter, you can modify it to meet your organization’s needs!\nMonitoring Disk Space\nOne of the biggest problems with multi-user Linux systems is the amount of available disk \nspace. In some situations, such as in a fi le-sharing server, disk space can fi ll up almost \nimmediately just because of one careless user.\nIf you have a production Linux system, you should not depend upon disk space reports to protect your server from its \ndisk space fi lling up. Instead, consider setting disk quotas. If the \nquota package is installed, you can fi nd out more \ninformation about managing disk quotas by typing \nman -k quota at the shell prompt. If the quota package is \nnot currently installed on your system, use your favorite search engine instead to locate further information.\nThis shell script utility helps you determine the top ten disk space consumers for desig-\nnated directories. It produces a date-stamped report that allows disk space consumption \ntrends to be monitored.\nObtaining the required functions\nThe fi rst tool you need to use is the du command (see Chapter 4). This command displays \nthe disk usage for individual fi les and directories. The \n-s option lets you summarize totals \nat the directory level. This comes in handy when calculating the total disk space used by \nan individual user. Here’s what it looks like to use the \ndu command to summarize each \nuser’s \n$HOME directory for the /home directory contents:\n$ sudo du -s /home/*\n[sudo] password for Christine:\n\n674\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  674\n4204    /home/Christine\n56      /home/Consultant\n52      /home/Development\n4       /home/NoSuchUser\n96      /home/Samantha\n36      /home/Timothy\n1024    /home/user1\n$\nThe -s option works well for users’ $HOME directories, but what if we wanted to view disk \nconsumption in a system directory such as \n/var/log?\n$ sudo du -s /var/log/*\n4       /var/log/anaconda.ifcfg.log\n20      /var/log/anaconda.log\n32      /var/log/anaconda.program.log\n108     /var/log/anaconda.storage.log\n40      /var/log/anaconda.syslog\n56      /var/log/anaconda.xlog\n116     /var/log/anaconda.yum.log\n4392    /var/log/audit\n4       /var/log/boot.log\n[...]\n$\nThe listing quickly becomes too detailed. The -S (capital S) option works better for our pur-\nposes here, providing a total for each directory and subdirectory individually. This allows \nyou to pinpoint problem areas quickly:\n$ sudo du -S /var/log/\n4       /var/log/ppp\n4       /var/log/sssd\n3020    /var/log/sa\n80      /var/log/prelink\n4       /var/log/samba/old\n4       /var/log/samba\n4       /var/log/ntpstats\n4       /var/log/cups\n4392    /var/log/audit\n420     /var/log/gdm\n4       /var/log/httpd\n152     /var/log/ConsoleKit\n2976    /var/log/\n$\nBecause we are interested in the directories consuming the biggest chunks of disk space, \nthe \nsort command (see Chapter 4) is used on the listing produced by du:\n$ sudo du -S /var/log/ | sort -rn\n4392    /var/log/audit\n\n675\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  675\n24\n3020    /var/log/sa\n2976    /var/log/\n420     /var/log/gdm\n152     /var/log/ConsoleKit\n80      /var/log/prelink\n4       /var/log/sssd\n4       /var/log/samba/old\n4       /var/log/samba\n4       /var/log/ppp\n4       /var/log/ntpstats\n4       /var/log/httpd\n4       /var/log/cups\n$\nThe -n option allows you to sort numerically. The -r option lists the largest numbers fi rst \n(reverse order). This is perfect for fi nding the largest disk consumers.\nThe \nsed editor brings more clarity to this listing. To focus on the top ten disk space con-\nsumers, when line 11 is reached, \nsed is set to delete the rest of the listing. The next step \nis to add a line number for each line in the listing. Chapter 19 shows you how to accom-\nplish this by adding an equal sign (=) to the \nsed command. To get those line numbers on \nthe same line as the disk space text, combine the text lines using the \nN command, as was \nshown in Chapter 21. The \nsed commands needed look like this:\nsed '{11,$D; =}' | \nsed 'N; s/\\n/ /' |\nNow the output can be cleaned up using the gawk command (see Chapter 22). The out-\nput from the \nsed editor is piped into the gawk command and printed using the printf \nfunction. \ngawk '{printf $1 \":\" \"\\t\" $2  \"\\t\" $3 \"\\n\"}'\nAfter the line number, a colon ( :) is added, and tab (\\t) characters are put between the \nindividual fi elds for each text line’s output row. This produces a nicely formatted listing of \nthe top ten disk space consumers.\n$ sudo du -S /var/log/ |\n> sort -rn |\n> sed '{11,$D; =}' |\n> sed 'N; s/\\n/ /' |\n> gawk '{printf $1 \":\" \"\\t\" $2 \"\\t\" $3 \"\\n\"}'\n[sudo] password for Christine:\n1:      4396    /var/log/audit\n2:      3024    /var/log/sa\n3:      2976    /var/log/\n4:      420     /var/log/gdm\n5:      152     /var/log/ConsoleKit\n6:      80      /var/log/prelink\n7:      4       /var/log/sssd\n\n676\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  676\n8:      4       /var/log/samba/old\n9:      4       /var/log/samba\n10:     4       /var/log/ppp\n$\nNow you’re in business! The next step is to use this information to create the script.\nCreating the script\nTo save time and effort, the script creates a report for multiple designated directories. A \nvariable to accomplish this called \nCHECK_DIRECTORIES is used. For our purposes here, the \nvariable is set to just two directories:\nCHECK_DIRECTORIES=\" /var/log /home\"\nThe script contains a for loop to perform the du command on each directory listed in the \nvariable. This technique is used (see Chapter 13) to read and process values in a list. Each \ntime the \nfor loop iterates through the list of values in the variable CHECK_DIRECTORIES, \nit assigns to the \nDIR_CHECK variable the next value in the list:\nfor DIR_CHECK in $CHECK_DIRECTORIES \ndo \n[...]\n  du -S $DIR_CHECK\n[...]\ndone\nTo allow quick identifi cation, a date stamp is added to the report’s fi lename, using the date \ncommand. Using the \nexec command (see Chapter 15) the script redirects its output to the \ndate stamped report fi le:\nDATE=$(date '+%m%d%y')\nexec > disk_space_$DATE.rpt\nNow to produce a nicely formatted report, the script uses the echo command to put in a \nfew report titles:\necho \"Top Ten Disk Space Usage\"\necho \"for $CHECK_DIRECTORIES Directories\"\nSo let’s see what this script looks like all put together:\n#!/bin/bash\n#\n# Big_Users - Find big disk space users in various directories\n###############################################################\n# Parameters for Script\n#\nCHECK_DIRECTORIES=\" /var/log /home\"  #Directories to check\n#\n\n677\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  677\n24\n############## Main Script #################################\n#\nDATE=$(date '+%m%d%y')               #Date for report file\n#\nexec > disk_space_$DATE.rpt          #Make report file STDOUT\n#\necho \"Top Ten Disk Space Usage\"      #Report header\necho \"for $CHECK_DIRECTORIES Directories\"\n#\nfor DIR_CHECK in $CHECK_DIRECTORIES  #Loop to du directories\ndo\n  echo \"\"\n  echo \"The $DIR_CHECK Directory:\"   #Directory header\n#\n# Create a listing of top ten disk space users in this dir\n  du -S $DIR_CHECK 2>/dev/null |\n  sort -rn |\n  sed '{11,$D; =}' |\n  sed 'N; s/\\n/ /' |\n  gawk '{printf $1 \":\" \"\\t\" $2  \"\\t\" $3 \"\\n\"}'\n#\ndone                                 #End of loop\n#\nexit\nAnd there you have it. This simple shell script creates a date stamped report of the top ten \ndisk space consumers for each directory you choose.\nRunning the script\nBefore having the Big_Users script run automatically, you want to test it a few times \nmanually to ensure that it does what you think it should do. And as you know by now, \nbefore you test it, you must set the proper permissions. However, in this case, the \nbash \ncommand was used, so the \nchmod u+x command was not needed prior to running the \nscript:\n$ ls -l Big_Users.sh\n-rw-r--r--. 1 Christine Christine 910 Sep  3 08:43 Big_Users.sh\n$\n$ sudo bash Big_Users.sh\n [sudo] password for Christine:\n$\n$ ls disk_space*.rpt\ndisk_space_090314.rpt\n$\n$ cat disk_space_090314.rpt\nTop Ten Disk Space Usage\n\n678\nPart IV: Creating Practical Scripts\nc24.indd  12/05/2014  Page  678\nfor  /var/log /home Directories\nThe /var/log Directory:\n1:      4496    /var/log/audit\n2:      3056    /var/log\n3:      3032    /var/log/sa\n4:      480     /var/log/gdm\n5:      152     /var/log/ConsoleKit\n6:      80      /var/log/prelink\n7:      4       /var/log/sssd\n8:      4       /var/log/samba/old\n9:      4       /var/log/samba\n10:     4       /var/log/ppp\nThe /home Directory:\n1:      34084   /home/Christine/Documents/temp/reports/archive\n2:      14372   /home/Christine/Documents/temp/reports\n3:      4440    /home/Timothy/Project__42/log/universe\n4:      4440    /home/Timothy/Project_254/Old_Data/revision.56\n5:      4440    /home/Christine/Documents/temp/reports/report.txt\n6:      3012    /home/Timothy/Project__42/log\n7:      3012    /home/Timothy/Project_254/Old_Data/data2039432\n8:      2968    /home/Timothy/Project__42/log/answer\n9:      2968    /home/Timothy/Project_254/Old_Data/data2039432/answer\n10:     2968    /home/Christine/Documents/temp/reports/answer\n$ \nIt worked! Now you can set up the shell script to execute automatically as needed. You do \nthis using the \ncron table (see Chapter 16). It’s a good idea to have it run early Monday \nmorning. If you do this, you can have your coffee and review your weekly disk consump-\ntion report fi rst thing Monday morning!\nSummary\nThis chapter put some of the shell-scripting information presented in the book to good use \nfor creating Linux utilities. When you’re responsible for a Linux system, whether it’s a large \nmulti-user system or your own system, you need to watch lots of things. Instead of manu-\nally running commands, you can create shell script utilities to do the work for you.\nThe fi rst section walked you through using shell scripts for archiving and backing up data \nfi les on the Linux system. The \ntar command is a popular command for archiving data. The \nchapter showed you how to use it in shell scripts to create archive fi les and how to manage \nthe archive fi les in an archive directory. \nThe next section covered using a shell script for the four steps needed to delete user \naccounts. Creating functions for shell code that is repeated within a script makes the code \neasier to read and modify. This script combined many of the different structured commands, \n\n679\nChapter 24: Writing Simple Script Utilities\nc24.indd  12/05/2014  Page  679\n24\nsuch as the case and while commands. The chapter demonstrated the difference in script \nstructure for a script destined for the \ncron tables versus an interactive script.\nThe chapter ended with how to use the \ndu command to determine disk space consumption. \nThe \nsed and gawk commands were then used to retrieve specifi c information from the \ndata. Passing the output from a command to \nsed and gawk  to parse data is a common \nfunction in shell scripts, so it’s a good idea to know how to do it.\nNext, more advanced shell scripts are covered. These scripts cover database, web, and \ne-mail topics. \n\n\n\n681\nc25.indd  12/12/2014  Page  681\nCHAPTER \n25\nProducing Scripts for Database, \nWeb, and E-Mail\nIN THIS CHAPTER\nWriting database shell scripts\nUsing the Internet from your scripts\nE-mailing reports from scripts\nS\no far we’ve covered many different features of shell scripts. However, there’s still more! You \ncan also utilize advanced applications outside your shell scripts to provide advanced features, \nsuch as accessing databases, retrieving data from the Internet, and e-mailing reports. This \nchapter shows how to use these three common features found in Linux systems all from within your \nshell scripts.\nUsing a MySQL Database\nOne of the problems with shell scripts is persistent data. You can store all the information you want \nin your shell script variables, but at the end of the script, the variables just go away. Sometimes, \nyou’d like for your scripts to be able to store data that you can use later. \nIn the old days, to store and retrieve data from a shell script required creating a fi le, reading data \nfrom the fi le, parsing the data, and then saving the data back into the fi le. Searching for data \nin the fi le meant reading every record in the fi le to look for your data. Nowadays with databases \nbeing all the rage, it’s a snap to interface your shell scripts with professional-quality open source \ndatabases. Currently, the most popular open source database used in the Linux world is MySQL. Its \npopularity has grown as a part of the Linux-Apache-MySQL-PHP (LAMP) server environment, which \nmany Internet web servers use for hosting online stores, blogs, and applications.\nThis section describes how to use a MySQL database in your Linux environment to create database \nobjects and how to use those objects in your shell scripts.\n\n682\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  682\nUsing MySQL\nMost Linux distributions include the MySQL server and client packages in their software \nrepositories, making it a snap to install a full MySQL environment on your Linux system. \nFigure 25-1 demonstrates the Add Software feature in the Ubuntu Linux distribution.\nFIGURE 25-1\nInstalling MySQL server on an Ubuntu Linux system\nAfter searching for the mysql-server package, just select the mysql-server entry that \nappears, and the Package Manager downloads and installs the complete MySQL server (and \nclient) software. It doesn’t get any easier than that!\nOnce installed, the portal to the MySQL database is the \nmysql command line interface \nprogram. This section describes how to use the \nmysql client program to interact with your \ndatabase.\nConnecting to the server\nThe mysql client program allows you to connect to any MySQL database server anywhere \non the network, using any user account and password. By default, if you enter the \nmysql \nprogram on a command line without any parameters, it attempts to connect to a MySQL \nserver running on the same Linux system, using the Linux login username.\n\n683\nChapter 25: Producing Scripts for Database, Web, and E-Mail\nc25.indd  12/12/2014  Page  683\n25\nMost likely, this isn’t how you want to connect to the database though. It’s usually safer \nto create a special user account for the application to use, rather than using your standard \nuser account in the MySQL server. That way, you can limit access to the application user, \nand if the application is compromised, you can easily delete and recreate it if necessary.\nYou use the \n-u command line parameter to specify the user name to log in as:\n$ mysql -u root –p\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 42\nServer version: 5.5.38-0ubuntu0.14.04.1 (Ubuntu)\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\nmysql>\nThe -p parameter tells the mysql program to prompt for a password to use with the user \naccount to log in. Enter the password that you assigned to the root user account, either \nduring the installation process, or using the \nmysqladmin utility. After you’re logged in to \nthe server, you can start entering commands.\nThe mysql commands\nThe mysql program uses two different types of commands:\n ■\nSpecial mysql commands\n ■\nStandard SQL statements\nThe \nmysql program uses its own set of commands that let you easily control the environ-\nment and retrieve information about the MySQL server. The mysql commands use either a \nfull name (such as \nstatus) or a shortcut (such as \\s).\nYou can use either the full command or the shortcut command directly from the \nmysql \ncommand prompt:\nmysql> \\s\n--------------\nmysql  Ver 14.14 Distrib 5.5.38, for debian-linux-gnu (i686) using readline 6.3\nConnection id:            43\n\n684\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  684\nCurrent database:\nCurrent user:             root@localhost\nSSL:                      Not in use\nCurrent pager:            stdout\nUsing outfile:            ''\nUsing delimiter:         ;\nServer version:          5.5.38-0ubuntu0.14.04.1 (Ubuntu)\nProtocol version:        10\nConnection:              Localhost via UNIX socket\nServer characterset:     latin1\nDb     characterset:     latin1\nClient characterset:     utf8\nConn.  characterset:     utf8\nUNIX socket:             /var/run/mysqld/mysqld.sock\nUptime:                  2 min 24 sec\nThreads: 1 Questions: 575 Slow queries: 0 Opens: 421 Flush tables: 1\n     Open tables: 41  Queries per second avg: 3.993\n--------------\nmysql>\nThe mysql program implements all the standard Structured Query Language (SQL) com-\nmands supported by the MySQL server. One uncommon SQL command that the \nmysql pro-\ngram implements is the \nSHOW command. Using this command, you can extract information \nabout the MySQL server, such as the databases and tables created:\nmysql> SHOW DATABASES;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n+--------------------+\n2 rows in set (0.04 sec)\nmysql> USE mysql;\nDatabase changed\nmysql> SHOW TABLES;\n+---------------------------+\n| Tables_in_mysql           |\n+---------------------------+\n| columns_priv              |\n| db                        |\n| func                      |\n| help_category             |\n| help_keyword              |\n| help_relation             |\n| help_topic                |\n\n685\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  685\n| host                      |\n| proc                      |\n| procs_priv                |\n| tables_priv               |\n| time_zone                 |\n| time_zone_leap_second     |\n| time_zone_name            |\n| time_zone_transition      |\n| time_zone_transition_type |\n| user                      |\n+---------------------------+\n17 rows in set (0.00 sec)\nmysql>\nIn this example, we used the SHOW SQL command to display the databases currently confi g-\nured on the MySQL server and the \nUSE SQL command to connect to a single database. Your \nmysql session can be connected to only one database at a time.\nYou’ll notice that we added a semicolon after each command. The semicolon indicates the \nend of a command to the \nmysql program. If you don’t use a semicolon, it prompts for more \ndata:\nmysql> SHOW\n  -> DATABASES;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n+--------------------+\n2 rows in set (0.00 sec)\nmysql>\nThis feature can come in handy when you’re working with long commands. You can enter \npart of the command on a line, press the Enter key, and continue on the next line. This can \ncontinue for as many lines as you like until you use the semicolon to indicate the end of \nthe command.\nThroughout this chapter, we use uppercase letters for SQL commands. This has become a common way to write SQL \ncommands, but the \nmysql program allows you to specify SQL commands using either uppercase or lowercase.\nCreating a database\nThe MySQL server organizes data into databases. A database usually holds the data for \na single application, separating it from other applications that use the database server. \n\n686\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  686\nCreating a separate database for each shell script application helps eliminate confusion and \ndata mix-ups.\nHere’s the SQL statement required to create a new database:\nCREATE DATABASE name;\nThat’s pretty simple. Of course, you must have the proper privileges to create new databases \non the MySQL server. The easiest way to do that is to log in as the root user account:\n$ mysql -u root –p\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 42\nServer version: 5.5.38-0ubuntu0.14.04.1 (Ubuntu)\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\nmysql> CREATE DATABASE mytest;\nQuery OK, 1 row affected (0.02 sec)\nmysql>\nYou can see whether the new database was created by using the SHOW command:\nmysql> SHOW DATABASES;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| mytest             |\n+--------------------+\n3 rows in set (0.01 sec)\nmysql>\nYes, it was successfully created. Now you can create a user account to access the new \ndatabase.\n\n687\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  687\nCreating a user account\nSo far, you’ve seen how to connect to the MySQL server using the root administrator \naccount. This account has total control over all the MySQL server objects (much like how \nthe \nroot Linux account has complete control over the Linux system).\nIt’s extremely dangerous to use the \nroot MySQL account for normal applications. If there \nwere a breach of security and someone fi gured out the password for the root user account, \nall sorts of bad things could happen to your system (and data).\nTo prevent that, it’s wise to create a separate user account in MySQL that has privileges \nonly for the database used in the application. You do this with the \nGRANT SQL statement:\nmysql> GRANT SELECT,INSERT,DELETE,UPDATE ON test.* TO test IDENTIFIED\nby 'test';\nQuery OK, 0 rows affected (0.35 sec)\nmysql>\nThat’s quite a long command. Let’s walk through the pieces and see what it’s doing.\nThe fi rst section defi nes the privileges the user account has on the database(s). This state-\nment allows the user account to query the database data (the select privilege), insert new \ndata records, delete existing data records, and update existing data records.\nThe \ntest.* entry defi nes the database and tables to which the privileges apply. This is \nspecifi ed in the following format:\ndatabase.table\nAs you can see from this example, you’re allowed to use wildcard characters when specify-\ning the database and tables. This format applies the specifi ed privileges to all the tables \ncontained in the database named test.\nFinally, you specify the user account(s) to which the privileges apply. The neat thing about \nthe \ngrant command is that if the user account doesn’t exist, it creates it. The \nidentified by portion allows you to set a default password for the new user account.\nYou can test the new user account directly from the \nmysql program:\n$ mysql mytest -u test –p\nEnter password: \nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 42\n\n688\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  688\nServer version: 5.5.38-0ubuntu0.14.04.1 (Ubuntu)\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\nmysql>\nThe fi rst parameter specifi es the default database to use (mytest), and as you’ve already \nseen, the \n-u parameter defi nes the user account to log in as, along with the -p to prompt \nfor the password. After entering the password assigned to the test user account, you’re con-\nnected to the server.\nNow that you have a database and a user account, you’re ready to create some tables for the \ndata.\nCreating a table\nThe MySQL server is considered a relational database. In a relational database, data is orga-\nnized by data fi elds, records, and tables. A data fi eld is a single piece of information, such as \nan employee’s last name or a salary. A record is a collection of related data fi elds, such as \nthe employee ID number, last name, fi rst name, address, and salary. Each record indicates \none set of the data fi elds.\nThe table contains all the records that hold the related data. Thus, you’ll have a table called \nEmployees that holds the records for each employee.\nTo create a new table in the database, you need to use the \nCREATE TABLE SQL command:\n$ mysql mytest -u root -p\nEnter password:\nmysql> CREATE TABLE employees (\n  -> empid int not null,\n  -> lastname varchar(30),\n  -> firstname varchar(30),\n  -> salary float,\n  -> primary key (empid));\nQuery OK, 0 rows affected (0.14 sec)\nmysql>\nFirst, notice that to create the new table, we needed to log in to MySQL using the root user \naccount because the \ntest user doesn’t have privileges to create a new table. Next, notice \nthat we specifi ed the \nmytest database on the mysql program command line. If we hadn’t \ndone that, we would need to use the \nUSE SQL command to connect to the test database.\n\n689\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  689\nIt’s extremely important to make sure you’re in the right database before creating the new table. Also, make sure \nyou’re logged in using the administrative user account (\nroot for MySQL) to create the tables.\nEach data fi eld in the table is defi ned using a data type. The MySQL database supports lots \nof different data types. Table 25-1 shows some of the more popular data types you may need.\nTABLE 25 -1    MySQL  Data  Types\nData TypeDescription\ncharA fi xed-length string value\nvarcharA variable-length string value\nintAn integer value\nfl oatA fl oating-point value\nbooleanA Boolean true/false value\ndateA date value in YYYY-MM-DD format\ntimeA time value in HH:mm:ss format\ntimestampA date and time value together\ntextA long string value\nBLOBA large binary value, such as an image or video clip\nThe empid data fi eld also specifi es a data constraint. A data constraint restricts what type \nof data you can enter to create a valid record. The \nnot null data constraint indicates that \nevery record must have an \nempid value specifi ed.\nFinally, the \nprimary key defi nes a data fi eld that uniquely identifi es each individual \nrecord. This means that each data record must have a unique \nempid value in the table.\nAfter creating the new table, you can use the appropriate command to ensure that it’s cre-\nated. In \nmysql, it’s the show tables command:\nmysql> show tables;\n+----------------+\n| Tables_in_test |\n+----------------+\n| employees      |\n+----------------+\n1 row in set (0.00 sec)\nmysql>\n\n690\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  690\nWith the table created, you’re now ready to start saving some data. The next section covers \nhow to do that.\nInserting and deleting data\nNot surprisingly, you use the INSERT SQL command to insert new data records into the \ntable. Each \nINSERT command must specify the data fi eld values for the MySQL server to \naccept the record.\nHere’s the format of the \nINSERT SQL command:\nINSERT INTO table VALUES (...)\nThe values are in a comma-separated list of the data values for each data fi eld:\n$ mysql mytest -u test -p\nEnter password:\nmysql> INSERT INTO employees VALUES (1, 'Blum', 'Rich', 25000.00);\nQuery OK, 1 row affected (0.35 sec)\nThe example uses the –u command line prompt to log in as the test user account that was \ncreated in MySQL.\nThe \nINSERT command pushes the data values you specify into the data fi elds in the table. \nIf you attempt to add another record that duplicates the \nempid data fi eld value, you get an \nerror message:\nmysql> INSERT INTO employees VALUES (1, 'Blum', 'Barbara', 45000.00);\nERROR 1062 (23000): Duplicate entry '1' for key 1\nHowever, if you change the empid value to a unique value, everything should be okay:\nmysql> INSERT INTO employees VALUES (2, 'Blum', 'Barbara', 45000.00);\nQuery OK, 1 row affected (0.00 sec)\nYou should now have two data records in your table.\nIf you need to remove data from your table, you use the \nDELETE SQL command. However, \nyou need to be very careful with it.\nHere’s the basic \nDELETE command format:\nDELETE FROM table;\nwhere table specifi es the table to delete records from. There’s just one small problem with \nthis command: It removes all the records in the table.\n\n691\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  691\nTo just specify a single record or a group of records to delete, you must use the WHERE \nclause. The \nWHERE clause allows you to create a fi lter that identifi es which records to \nremove. You use the \nWHERE clause like this:\nDELETE FROM employees WHERE empid = 2;\nThis restricts the deletion process to all the records that have an empid value of 2. When \nyou execute this command, the \nmysql program returns a message indicating how many \nrecords matched the fi lter:\nmysql> DELETE FROM employees WHERE empid = 2;\nQuery OK, 1 row affected (0.29 sec)\nAs expected, only one record matched the fi lter and was removed.\nQuerying data\nAfter you have all your data in your database, it’s time to start running reports to extract \ninformation.\nThe workhorse for all your querying is the SQL \nSELECT command. The SELECT command is \nextremely versatile, but with versatility comes complexity.\nHere’s the basic format of a \nSELECT statement:\nSELECT datafields FROM table\nThe datafields parameter is a comma-separated list of the data fi eld names you want the \nquery to return. If you want to receive all the data fi eld values, you can use an asterisk as a \nwildcard character.\nYou must also specify the specifi c table you want the query to search. To get meaningful \nresults, you must match your query data fi elds with the proper table.\nBy default, the \nSELECT command returns all the data records in the specifi ed table:\nmysql> SELECT * FROM employees;\n+-------+----------+------------+--------+\n| empid | lastname | firstname  | salary |\n+-------+----------+------------+--------+\n|     1 | Blum     | Rich       |  25000 |\n|     2 | Blum     | Barbara    |  45000 |\n|     3 | Blum     | Katie Jane |  34500 |\n|     4 | Blum     | Jessica    |  52340 |\n+-------+----------+------------+--------+\n4 rows in set (0.00 sec)\nmysql>\n\n692\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  692\nYou can use one or more modifi ers to defi ne how the database server returns the data \nrequested by the query. Here’s a list of commonly used modifi ers:\n ■\nWHERE: Displays a subset of records that meet a specifi c condition\n ■\nORDER BY: Displays records in a specifi ed order\n ■\nLIMIT: Displays only a subset of records\nThe \nWHERE clause is the most common SELECT command modifi er. It allows you to specify \nconditions to fi lter data from the result set. Here’s an example of using the \nWHERE clause:\nmysql> SELECT * FROM employees WHERE salary > 40000;\n+-------+----------+-----------+--------+\n| empid | lastname | firstname | salary |\n+-------+----------+-----------+--------+\n|     2 | Blum     | Barbara   |  45000 |\n|     4 | Blum     | Jessica   |  52340 |\n+-------+----------+-----------+--------+\n2 rows in set (0.01 sec)\nmysql>\nNow you can see the power of adding database access to your shell scripts! You can easily \ncontrol your data management needs just with a few SQL commands and the \nmysql pro-\ngram. The next section describes how you can incorporate these features into your shell \nscripts.\nUsing the database in your scripts\nNow that you have a working database going, it’s fi nally time to turn our attention back to \nthe shell scripting world. This section describes what you need to do to interact with your \ndatabases using shell scripts.\nLogging into the server\nIf you’ve created a special user account in MySQL for your shell scripts, you need to use it \nto log in with the \nmysql command. There are a couple ways to do that. One method is to \ninclude the password on the command line using the \n-p parameter:\nmysql mytest -u test –p test\nThis, however, is not a good idea. Anyone who has access to your script will know the user \naccount and password for your database.\nTo solve this problem, you can use a special confi guration fi le used by the \nmysql program. \nThe \nmysql program uses the $HOME/.my.cnf fi le to read special startup commands and \nsettings. One of those settings is the default password for \nmysql sessions started by the \nuser account.\n\n693\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  693\nTo set the default password in this fi le, just create the following:\n$ cat .my.cnf\n[client]\npassword = test\n$ chmod 400 .my.cnf\n$\nThe chmod command is used to restrict the .my.cnf fi le so only you can view it. You can \ntest this now from the command line:\n$ mysql mytest -u test\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 44\nServer version: 5.5.38-0ubuntu0.14.04.1 (Ubuntu)\nCopyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved.\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\nmysql>\nPerfect! Now you don’t have to include the password on the command line in your shell \nscripts.\nSending commands to the server\nAfter establishing the connection to the server, you’ll want to send commands to interact \nwith your database. There are two methods to do this:\n ■\nSend a single command and exit.\n ■\nSend multiple commands.\nTo send a single command, you must include the command as part of the \nmysql command \nline. For the \nmysql command, you do this using the -e parameter:\n$ cat mtest1\n#!/bin/bash\n# send a command to the MySQL server\nMYSQL=$(which mysql)\n$MYSQL mytest -u test -e 'select * from employees'\n\n694\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  694\n$ ./mtest1\n+-------+----------+------------+---------+\n| empid | lastname | firstname  | salary  |\n+-------+----------+------------+---------+\n|     1 | Blum     | Rich       | 25000   |\n|     2 | Blum     | Barbara    | 45000   |\n|     3 | Blum     | Katie Jane | 34500   |\n|     4 | Blum     | Jessica    | 52340   |\n+-------+----------+------------+---------+\n$\nThe database servers return the results from the SQL commands to the shell scripts, which \ndisplay them in \nSTDOUT.\nIf you need to send more than one SQL command, you can use fi le redirection (see Chapter \n15). To redirect lines in the shell script, you must defi ne an end of file string. The end of \nfi le string indicates the beginning and end of the redirected data.\nThis is an example of defi ning an end of fi le string, with data in it:\n$ cat mtest2\n#!/bin/bash\n# sending multiple commands to MySQL\nMYSQL=$(which mysql)\n$MYSQL mytest -u test <<EOF\nshow tables;\nselect * from employees where salary > 40000;\nEOF\n$ ./mtest2\nTables_in_test\nemployees\nempid    lastname    firstname    salary\n2        Blum        Barbara      45000\n4        Blum        Jessica      52340\n$\nThe shell redirects everything with the EOF delimiters to the mysql command, which \nexecutes the lines as if you typed them yourself at the prompt. Using this method, you \ncan send as many commands to the MySQL server as you need. You’ll notice, however, \nthat there’s no separation between the output from each command. In the next section, \n“Formatting data,” you’ll see how to fi x this problem.\n\n695\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  695\nYou should also notice that the mysql program changed the default output style when you used the redirected input \nmethod. Instead of creating the ASCII symbol boxes around the data, the \nmysql program detected that the input \nwas redirected, so it returned just the raw data. This comes in handy when you need to extract the individual data \nelements.\nOf course, you’re not limited to just retrieving data from the tables. You can use any type \nof SQL command in your script, such as an \nINSERT statement:\n$ cat mtest3\n#!/bin/bash\n# send data to the table in the MySQL database\nMYSQL=$(which mysql)\nif [ $# -ne 4 ]\nthen\n echo \"Usage: mtest3 empid lastname firstname salary\"\nelse\n statement=\"INSERT INTO employees VALUES ($1, '$2', '$3', $4)\"\n $MYSQL mytest -u test << EOF\n $statement\nEOF\n if [ $? -eq 0 ]\n then\n    echo Data successfully added\n else\n    echo Problem adding data\n fi\nfi\n$ ./mtest3\nUsage: mtest3 empid lastname firstname salary\n$ ./mtest3 5 Blum Jasper 100000\nData added successfully\n$\n$ ./mtest3 5 Blum Jasper 100000\nERROR 1062 (23000) at line 1: Duplicate entry '5' for key 1\nProblem adding data\n$\nThis example demonstrates a few things about using this technique. When you specify the \nend of fi le string, it must be the only thing on the line, and the line must start with the \n\n696\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  696\nstring. If we had indented the EOF text to match the rest of the if-then indentation, it \nwouldn’t work.\nInside the \nINSERT statement, notice that there are single quotes around the text values \nand double quotes around the entire \nINSERT statement. It’s important not to mix up the \nquotes used for the string values with the quotes used to defi ne the script variable text.\nAlso, notice how we used the special \n$? variable to test the exit status of the mysql pro-\ngram. This helps you determine whether the command failed.\nJust sending output from the commands to \nSTDOUT is not the easiest way to manage and \nmanipulate the data. The next section shows you some tricks you can use to help your \nscripts capture data retrieved from the database.\nFormatting data\nThe standard output from the mysql command doesn’t lend itself to data retrieval. If you \nneed to actually do something with the data you retrieve, you need to do some fancy data \nmanipulation. This section describes some of the tricks you can use to help extract data \nfrom your database reports.\nThe fi rst step in trying to capture database data is to redirect the output from the \nmysql \nand \npsql commands in an environment variable. This allows you to use the output infor-\nmation in other commands. Here’s an example:\n$ cat mtest4\n#!/bin/bash\n# redirecting SQL output to a variable\nMYSQL=$(which mysql)\ndbs=$($MYSQL mytest -u test -Bse 'show databases')\nfor db in $dbs\ndo\n echo $db\ndone\n$ ./mtest4\ninformation_schema\ntest\n$\n\n697\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  697\nThis example uses two additional parameters on the mysql program command line. The -B \nparameter specifi es for the \nmysql program to work in batch mode, and in combination with \nthe \n-s (silent) parameter, the column headings and formatting symbols are suppressed.\nBy redirecting the output of the \nmysql command to a variable, this example is able to step \nthrough the individual values of each returned record.\nThe \nmysql program also supports an additional popular format, called Extensible Markup \nLanguage (XML). This language uses HTML-like tags to identify data names and values.\nFor the \nmysql program, you do this using the -X command line parameter:\n$ mysql mytest -u test -X -e 'select * from employees where empid = 1'\n<?xml version=\"1.0\"?>\n<resultset statement=\"select * from employees\">\n<row>\n    <field name=\"empid\">1</field>\n    <field name=\"lastname\">Blum</field>\n    <field name=\"firstname\">Rich</field>\n    <field name=\"salary\">25000</field>\n</row>\n</resultset>\n$\nUsing XML, you can easily identify individual rows of data, along with the individual \ndata values in each record. You can then use standard Linux string handling functions to \nextract the data you need!\nUsing the Web\nOften when you think of shell script programming, the last thing you think of is the \nInternet. The command line world often seems foreign to the fancy, graphical world of the \nInternet. There are, however, several different utilities you can easily use in your shell \nscripts to gain access to data content on the web, as well as on other network devices.\nAlmost as old as the Internet itself, the Lynx program was created in 1992 by students at \nthe University of Kansas as a text-based browser. Because it’s text-based, the Lynx program \nallows you to browse websites directly from a terminal session, replacing the fancy graphics \non web pages with HTML text tags. This allows you to surf the Internet from just about any \ntype of Linux terminal. A sample Lynx screen is shown in Figure 25-2.\n\n698\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  698\nFIGURE 25-2\nViewing a web page using Lynx\nLynx uses the standard keyboard keys to navigate around the web page. Links appear as \nhighlighted text within the web page. Using the right-arrow key allows you to follow a link \nto the next web page.\nYou may be wondering how you can use a graphical text program in your shell scripts. The \nLynx program also provides a feature that allows you to dump the text contents of a web \npage to \nSTDOUT. This feature is great for mining for data contained within a web page. This \nsection describes how to use the Lynx program within your shell scripts to extract data \nfrom websites.\nInstalling Lynx\nEven though the Lynx program is somewhat old, it’s still in active development. At the \ntime of this writing, the latest version of Lynx is version 2.8.8, released in June 2010, with \na new release in development. Because of its popularity among shell script programmers, \nmany Linux distributions install the Lynx program in their default installations.\nIf you’re using an installation that doesn’t provide the Lynx program, check your distribu-\ntion’s installation packages. Most likely you’ll fi nd it there for easy installation.\nIf your distribution doesn’t include the Lynx package, or if you just want the latest version, \nyou can download the source code from the \nlynx.isc.org website and compile it yourself \n\n699\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  699\n(assuming that you’ve got the C development libraries installed on your Linux system). See \nChapter 9 for information on how to compile and install source code distribution packages.\nThe Lynx program uses the curses text-graphics library in Linux. Most distributions have this installed by default. If \nyour distribution doesn’t, consult your particular distribution’s instructions on installing the curses library before try-\ning to compile Lynx.\nThe next section describes how to use the lynx command from the command line.\nThe lynx command line\nThe lynx command line command is extremely versatile in what information it can retrieve \nfrom the remote website. When you view a web page in your browser, you’re only seeing \npart of the information that’s transferred to your browser. Web pages consist of three types \nof data elements:\n ■\nHTTP headers\n ■\nCookies\n ■\nHTML content\nHTTP headers provide information about the type of data sent in the connection, the server \nsending the data, and the type of security used in the connection. If you’re sending special \ntypes of data, such as video or audio clips, the server identifi es that in the HTTP headers. \nThe Lynx program allows you to view all the HTTP headers sent within a web page session.\nIf you’ve done any type of web browsing, no doubt you’re familiar with web page cookies. \nWebsites use cookies to store data about your website visit for future use. Each individual \nsite can store information, but it can only access the information it sets. The \nlynx com-\nmand provides options for you to view cookies sent by web servers, as well as reject or \naccept specifi c cookies sent from servers.\nThe Lynx program allows you to view the actual HTML content of the web page in three dif-\nferent formats:\n ■\nIn a text-graphics display on the terminal session using the curses graphical library\n ■\nAs a text fi le, dumping the raw data from the web page\n ■\nAs a text fi le, dumping the raw HTML source code from the web page\nFor shell scripts, viewing the raw data or HTML source code is a gold mine. After you \ncapture the data retrieved from a website, you can easily extract individual pieces of \ninformation.\n\n700\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  700\nAs you can see, the Lynx program is extremely versatile in what it can do. However, with \nversatility comes complexity, especially when it comes to command line parameters. The \nLynx program is one of the more complex programs you’ll run into in the Linux world.\nHere’s the basic format of the \nlynx command:\nlynx options URL\nwhere URL is the HTTP or HTTPS destination you want to connect to, and options are one \nor more options that modify the behavior of Lynx as it interacts with the remote website. \nThere are options for just about any type of web interaction required by Lynx. Use the \nman \ncommand to view all the options available for Lynx.\nMany of the command line parameters defi ne behaviors that control Lynx when you’re using \nit in full-screen mode, allowing you to customize the behavior of Lynx as you’re traversing \nweb pages.\nThere are often groups of command line parameters that you fi nd useful in your normal \nbrowsing environment. Instead of having to enter these parameters on the command line \nevery time you use Lynx, Lynx provides a general confi guration fi le that defi nes the base \nbehavior when you use Lynx. This confi guration fi le is discussed in the next section.\nThe Lynx confi guration fi le\nThe lynx command reads a confi guration fi le for many of its parameter settings. By \ndefault, this fi le is located at \n/usr/local/lib/lynx.cfg, although you’ll fi nd that many \nLinux distributions change this to the \n/etc directory (/etc/lynx.cfg) (the Ubuntu dis-\ntribution places the \nlynx.cfg file in the /etc/lynx-cur folder).\nThe \nlynx.cfg confi guration fi le groups related parameters into sections to make fi nding \nparameters easier. Here’s the format of an entry in the confi guration fi le:\nPARAMETER:value\nwhere PARAMETER is the full name of the parameter (often, but not always in uppercase \nletters) and \nvalue is the value associated with the parameter.\nPerusing this fi le, you’ll fi nd many parameters that are similar to the command line param-\neters, such as the \nACCEPT_ALL_COOKIES parameter, which is equivalent to setting the \n-accept_all_cookies command line parameter.\nThere are also a few confi guration parameters that are similar in function but different in \nname. The \nFORCE_SSL_COOKIES_SECURE confi guration fi le parameter setting can be over-\nridden by the \n-force_secure command line parameter.\nHowever, you’ll also fi nd quite a few confi guration parameters that don’t match with com-\nmand line parameters. These values can be set only from the confi guration fi le.\n\n701\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  701\nThe most common confi guration parameters that you can’t set on the command line are \nfor the proxy servers. Some networks (especially corporate networks) use a proxy server as \na middleman between the client’s browser and the destination website server. Instead of \nsending HTTP requests directly to the remote web server, client browsers must send their \nrequests to the proxy server. The proxy server in turn sends the requests to the remote web \nserver, retrieves the results, and forwards them back to the client browser.\nThis may seem like a waste of time, but it’s a vital function in protecting clients from dan-\ngers on the Internet. A proxy server can fi lter inappropriate content and malicious coding, \nor even detect sites used for Internet data phishing schemes (rogue servers pretending \nto be someone else in order to capture customer data). Proxy servers can also help reduce \nInternet bandwidth usage, because they cache commonly viewed web pages and return \nthem to clients instead of having to download the original page again.\nThese are the confi guration parameters used to defi ne proxy servers:\nhttp_proxy:http://some.server.dom:port/\nhttps_proxy:http://some.server.dom:port/\nftp_proxy:http://some.server.dom:port/\ngopher_proxy:http://some.server.dom:port/\nnews_proxy:http://some.server.dom:port/\nnewspost_proxy:http://some.server.dom:port/\nnewsreply_proxy:http://some.server.dom:port/\nsnews_proxy:http://some.server.dom:port/\nsnewspost_proxy:http://some.server.dom:port/\nsnewsreply_proxy:http://some.server.dom:port/\nnntp_proxy:http://some.server.dom:port/\nwais_proxy:http://some.server.dom:port/\nfinger_proxy:http://some.server.dom:port/\ncso_proxy:http://some.server.dom:port/\nno_proxy:host.domain.dom\nYou can defi ne a different proxy server for any network protocol supported by Lynx. The \nNO_PROXY parameter is a comma-separated list of websites that you prefer to have direct \naccess to without using the proxy server. These are often internal websites that don’t \nrequire fi ltering.\nCapturing data from Lynx\nWhen you use Lynx in a shell script, most likely you’re trying to obtain a specifi c piece (or \npieces) of information from a web page. The technique to accomplish this is called screen \nscraping. In screen scraping, you’re trying to programmatically fi nd data in a specifi c loca-\ntion on a graphical screen so you can capture it and use it in your shell script.\nThe easiest way to perform screen scraping with \nlynx is to use the -dump option. This \noption doesn’t bother trying to display the web page on the terminal screen. Instead, it \ndisplays the web page text data directly to \nSTDOUT:\n\n702\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  702\n$ lynx -dump http://localhost/RecipeCenter/\nThe Recipe Center\n          \"Just like mom used to make\"\nWelcome\n   [1]Home\n   [2]Login to post\n   [3]Register for free login\n     _____________________________________________________________\n   [4]Post a new recipe\nEach link is identifi ed by a tag number, and Lynx displays a listing of all the tag references \nafter the web page data.\nAfter you have all the text data from the web page, you probably know what tools we’re \ngoing to get out of the toolbox to start work on extracting data. That’s right, our old \nfriends the \nsed and gawk programs (see Chapter 19).\nFirst, let’s fi nd some interesting data to collect. The Yahoo! weather web page is a great \nsource for fi nding the current weather conditions anywhere in the world. Each location \nuses a separate URL to display weather information for that city (you can fi nd the specifi c \nURL for your city by going to the site in a normal browser and entering your city’s informa-\ntion). Here’s the \nlynx command for fi nding the weather in Chicago, Illinois:\n lynx -dump http://weather.yahoo.com/united-states/illinois/chicago-2379574/\nThis command dumps lots and lots of data from the web page. The fi rst step is to fi nd the \nprecise information you want. To do that, redirect the output from the \nlynx command to a \nfi le, and then search the fi le for your data. After doing that with the preceding command, \nwe found this text in the output fi le:\nCurrent conditions as of 1:54 pm EDT\nMostly Cloudy\n   Feels Like:\n          32 °F\n   Barometer:\n          30.13 in and rising\n   Humidity:\n          50%\n   Visibility:\n          10 mi\n   Dewpoint:\n          15 °F\n   Wind:\n          W 10 mph\n\n703\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  703\nThat’s all the information about the current weather you really need. There’s just one small \nproblem with this output. You’ll notice that the numbers are on a line below the heading. \nTrying to just extract individual numbers will be diffi cult. Chapter 19 discusses how to deal \nwith a problem just like this.\nThe key to solving this is to write a \nsed script that can search for the data heading fi rst. \nWhen you fi nd it, you can then go to the correct line to extract the data. We’re fortunate in \nthis example in that all the data we need are on lines by themselves. We should be able to \nsolve this with just the \nsed script. If there had also been other text on the same line, we’d \nneed to get out the \ngawk tool to fi lter out just the data we needed.\nFirst, you need to create a \nsed script that looks for the location text and then skips to the \nnext line to get the text that describes the current weather condition and prints it. Here’s \nwhat that looks like for the Chicago weather page:\n$ cat sedcond\n/IL, United States/{\nn\np\n}\n$\nThe address specifi es to look for the line with the desired text. If the sed command fi nds \nit, the \nn command skips to the next line, and the p command prints the contents of the \nline, which is the text describing the current weather conditions of the city.\nNext, you’ll need a \nsed script that can search for the Feels Like text and then go to the \nnext line to print the temperature:\n$ cat sedtemp\n/Feels Like/{\np\n}\n$\nPerfect. Now, you can use these two sed scripts in a shell script that fi rst captures the \nlynx output of the web page to a temporary fi le, and then applies the two sed scripts to \nthe web page data to extract only the data you’re looking for. Here’s an example of how to \ndo that:\n$ cat weather\n#!/bin/bash\n# extract the current weather for Chicago, IL\nURL=\"http://weather.yahoo.com/united-states/illinois/chicago-2379574/\"\nLYNX=$(which lynx)\nTMPFILE=$(mktemp tmpXXXXXX)\n$LYNX -dump $URL > $TMPFILE\n\n704\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  704\nconditions=$(cat $TMPFILE | sed -n -f sedcond)\ntemp=$(cat $TMPFILE | sed -n -f sedtemp | awk '{print $4}')\nrm -f $TMPFILE\necho \"Current conditions: $conditions\"\necho The current temp outside is: $temp\n$ ./weather\nCurrent conditions: Mostly Cloudy\nThe current temp outside is: 32 °F\n$\nThe weather script connects to the Yahoo! weather web page for the desired city, saves the \nweb page to a temporary fi le, extracts the appropriate text, removes the temporary fi le, and \nthen displays the weather information. The beauty of this is that after you’ve extracted \nthe data from a website, you can do whatever you want with it, such as create a table of \ntemperatures. You can then create a \ncron job (see Chapter 16) that runs every day to track \ndaily temperatures.\nThe Internet is a dynamic place. Don’t be surprised if you spend hours working out the precise location of data on a \nweb page, only to fi nd that it’s moved a couple of weeks later, breaking your scripts. In fact, it’s quite possible that \nthis example won’t work by the time you read this book. The important thing is to know the process for extracting \ndata from web pages. You can then apply that principle to any situation.\nUsing E-Mail\nWith the popularity of e-mail, these days just about everyone has an e-mail address. \nBecause of that, people often expect to receive data via e-mail instead of seeing fi les or \nprintouts. That’s no different in the shell scripting world. If you generate any type of \nreport from your shell script, most likely at some point you’ll be asked to e-mail the results \nto someone.\nThe main tool you have available for sending e-mail messages from your shell scripts is the \nMailx program. Not only can you use it interactively to read and send messages, but you can \nalso use the command line parameters to specify how to send a message.\nSome Linux distributions require that you also install a mail server package (such as sendmail or Postfi x) before you \ncan install the mailutils package that includes the Mailx program.\nHere’s the format for the Mailx program’s command line for sending messages:\nmail [-eIinv] [-a header] [-b addr] [-c addr] [-s subj] to-addr\n\n705\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  705\nThe mail command uses the command line parameters shown in Table 25-2.\nTABLE 25 -2    The Mailx Command Line Parameters\nParameterDescription\n-a\nSpecifi es additional SMTP header lines\n-b\nAdds a BCC: recipient to the message\n-c\nAdds a CC: recipient to the message\n-e\nDoesn’t send the message if it’s empty\n-i\nIgnores TTY interrupt signals\n-I\nForces Mailx to run in interactive mode\n-n\nDoesn’t read the /etc/mail.rc startup fi le\n-s\nSpecifi es a Subject line\n-v\nDisplays details of the delivery on the terminal\nAs you can see from Table 25-2, you can pretty much create an entire e-mail message just \nfrom the command line parameters. You just need to add the message body.\nTo do that, you need to redirect text to the \nmail command. Here’s a simple example of how \nto create and send an e-mail message directly from the command line: \n$ echo \"This is a test message\" | mailx -s \"Test message\" rich\nThe Mailx program sends the text from the echo command as the message body. This \nprovides an easy way for you to send messages from your shell scripts. Here’s a quick \nexample: \n$ cat factmail\n#!/bin/bash\n# mailing the answer to a factorial\n \nMAIL=$(which mailx)\n \nfactorial=1\ncounter=1\n \nread -p \"Enter the number: \" value\nwhile [ $counter -le $value ]\ndo\n   factorial=$[$factorial * $counter]\n   counter=$[$counter + 1]\n\n706\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  706\ndone\n \necho The factorial of $value is $factorial | $MAIL -s \"Factorial\nanswer\" $USER\necho \"The result has been mailed to you.\"\nThis script does not assume that the Mailx program is located in the standard location. It \nuses the \nwhich command to determine just where the mail program is.\nAfter calculating the result of the factorial function, the shell script uses the \nmail com-\nmand to send the message to the user-defi ned \n$USER environment variable, which should \nbe the person executing the script.\n$ ./factmail\nEnter the number: 5\nThe result has been mailed to you.\n$\nYou just need to check your mail to see if the answer arrived:\n$ mail\n\"/var/mail/rich\": 1 message 1 new\n>N   1 Rich Blum          Mon Sep  1 10:32  13/586   Factorial answer\n? \nReturn-Path: <rich@rich-Parallels-Virtual-Platform>\nX-Original-To: rich@rich-Parallels-Virtual-Platform\nDelivered-To: rich@rich-Parallels-Virtual-Platform\nReceived: by rich-Parallels-Virtual-Platform (Postfix, from userid 1000)\n        id B4A2A260081; Mon,  1 Sep 2014 10:32:24 -0500 (EST)\nSubject: Factorial answer\nTo: <rich@rich-Parallels-Virtual-Platform>\nX-Mailer: mail (GNU Mailutils 2.1)\nMessage-Id: <20101209153224.B4A2A260081@rich-Parallels-Virtual-Platform>\nDate: Mon,  1 Sep 2014 10:32:24 -0500 (EST)\nFrom: rich@rich-Parallels-Virtual-Platform (Rich Blum)\nThe factorial of 5 is 120\n?\nIt’s not always convenient to send just one line of text in the message body. Often, you’ll \nneed to send an entire output as the e-mail message. In those situations, you can always \nredirect text to a temporary fi le and use the \ncat command to redirect the output to the \nmail program.\nHere’s an example of sending a larger amount of data in an e-mail message:\n$ cat diskmail\n#!/bin/bash\n\n707\nChapter 25: Producing Scripts for Database, Web, and E-Mail\n25\nc25.indd  12/12/2014  Page  707\n# sending the current disk statistics in an e-mail message\n \ndate=$(date +%m/%d/%Y)\nMAIL=$(which mailx)\nTEMP=$(mktemp tmp.XXXXXX)\n \ndf -k > $TEMP\ncat $TEMP | $MAIL -s \"Disk stats for $date\" $1\nrm -f $TEMP\nThe diskmail program gets the current date using the date command (along with some \nspecial formatting), fi nds the location of the Mailx program, and creates a temporary fi le. \nAfter all that, it uses the \ndf command to display the current disk space statistics (see \nChapter 4), redirecting the output to the temporary fi le.\nIt then redirects the temporary fi le to the \nmail command, using the fi rst command line \nparameter for the destination address and the current date in the Subject header. When \nyou run the script, you don’t see anything appear on the command line output:\n$ ./diskmail rich\nBut if you check your mail, you should see the sent message:\n$ mail\n\"/var/mail/rich\": 1 message 1 new\n>N   1 Rich Blum          Mon Sep  1 10:35  19/1020  Disk stats for 09/01/2014\n? \nReturn-Path: <rich@rich-Parallels-Virtual-Platform>\nX-Original-To: rich@rich-Parallels-Virtual-Platform\nDelivered-To: rich@rich-Parallels-Virtual-Platform\nReceived: by rich-Parallels-Virtual-Platform (Postfix, from userid 1000)\n        id 3671B260081; Mon,  1 Sep 2014 10:35:39 -0500 (EST)\nSubject: Disk stats for 09/01/2014\nTo: <rich@rich-Parallels-Virtual-Platform>\nX-Mailer: mail (GNU Mailutils 2.1)\nMessage-Id: <20101209153539.3671B260081@rich-Parallels-Virtual-Platform>\nDate: Mon,  1 Sep 2014 10:35:39 -0500 (EST)\nFrom: rich@rich-Parallels-Virtual-Platform (Rich Blum)\nFilesystem           1K-blocks      Used  Available Use% Mounted on\n/dev/sda1             63315876   2595552   57504044   5% /\nnone                    507052       228     506824   1% /dev\nnone                    512648       192     512456   1% /dev/shm\nnone                    512648       100     512548   1% /var/run\nnone                    512648         0     512648   0% /var/lock\nnone                4294967296         0 4294967296   0% /media/psf\n?\nNow you just need to schedule the script to run every day using the cron feature, and you \ncan get disk space reports automatically e-mailed to your inbox! System administration \ndoesn’t get much easier than that!\n\n708\nPart IV: Creating Practical Scripts\nc25.indd  12/12/2014  Page  708\nSummary\nThis chapter walked through how to use some advanced features within your shell scripts. \nFirst, we discussed how to use the MySQL server to store persistent data for your applica-\ntions. Just create a database and unique user account in MySQL for your application, and \ngrant the user account privileges to only that database. You can then create tables to store \nthe data that your application uses. The shell script uses the \nmysql command line tool to \ninterface with the MySQL server, submit SELECT queries, and retrieve the results to display. \nNext we discussed how to use the \nlynx text-based browser to extract data from websites on \nthe Internet. The \nlynx  tool can dump all the text from a web page, and you can use stan-\ndard shell programming skills to store that data and search it for the content you’re look-\ning for. Finally, we walked through how to use the standard Mailx program to send reports \nusing the Linux e-mail server installed on your Linux system. The Mailx program allows \nyou to easily send output from commands to any e-mail address.\nIn the next chapter we fi nish up by looking at some more shell script examples that show \nyou just what you can do with your shell scripting knowledge. \n\n709\nc26.indd  12/08/2014  Page  709\nCHAPTER \n26\nCreating Fun Little Shell Scripts\nIN THIS CHAPTER\nSending a message\nGetting inspiration\nSending a text\nT\nhe primary reason for learning to write bash shell scripts is to be able to create your own Linux \nsystem utilities. Understanding how to write useful and practical script utilities is important. \nHowever, sometimes it helps to do something fun to learn a concept or skill. The scripts in this \nchapter are not necessarily practical, but they can be lots of fun! And they help solidify script-\nwriting concepts.\nSending a Message\nMessages can be sent in many ways in an offi ce or a home environment — text message, e-mail, \nand even making a phone call. One method, not commonly used any more, is sending a message \ndirectly to a fellow system user’s terminal. Because this technique is largely unknown, it can be \nfun to communicate with someone with this method.\nThis shell script utility helps you to quickly and easily send a message to someone who is logged \nonto your Linux system. It is a rather simple script, but it can be loads of fun!\nUnderstanding the required functions\nFor this simple script, only a few functions are required. Several of the commands are common and \nhave been covered in the book. However, a few of the commands have only been touched on, and \nyou may not be familiar with the primary command needed. This section looks at the commands \nneeded to put together this simple, but interesting script.\n\n710\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  710\nDetermining who is on the system\nThe fi rst utility needed is the who command. The who utility allows you to see all the users \ncurrently logged into the system:\n$ who\nchristine tty2         2015-09-10 11:43\ntimothy   tty3         2015-09-10 11:46\n[...]\n$\nIn this partial listing, all the information needed for sending messages is shown. By \ndefault, the \nwho command gives you the short version of information available. The same \ninformation is provided, when \nwho -s is issued:\n ■\nUser name\n ■\nUser’s terminal\n ■\nTime the user logged into the system\nTo send a message, you only need the fi rst two items. Both the user name and the user’s \ncurrent terminal are necessary.\nAllowing messages\nUsers can disallow anyone to send them messages via the mesg utility. Therefore, before \nyou start attempting to send messages, it’s a good idea to check whether messages are \nallowed. For yourself, you can simply enter the \nmesg command as follows:\n$ mesg\nis n\n$\nThe is n result shows that messaging is turned off. If the result showed is y, messages \nwould be allowed. \nSome distributions, such as Ubuntu, come with messaging turned off by default. Other distributions, such as CentOS, \ncome with messaging turned on by default. Thus, you need to check your status and other user’s message status \nbefore attempting to send a message.\nTo check everyone else’s message status, you can use the who command again. Keep in \nmind that this checks the message status only for those who are currently logged into the \nsystem. You use the \n-T option to check their message status:\n$ who -T\nchristine - tty2         2015-09-10 12:56\n\n711\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  711\n26\ntimothy   - tty3         2015-09-10 11:46\n[...]\n$\nThe dash (-) after each user name indicates that messaging is turned off for those users. If \nit is turned on, you see a plus (\n+) sign. \nTo allow messages to be sent to you, if it is turned off, you need to use the message com-\nmand with the \ny option:\n$ whoami\nchristine\n$\n$ mesg y\n$\n$ mesg\nis y\n$\nMessaging is turned on by the user christine, when the command mesg y is issued. The \nuser’s individual message status is checked, by issuing the \nmesg command. Sure enough, \nthe command shows \nis y, which indicates messages are allowed to this user.\nUsing the \nwho command, other users can see how the user christine has changed her \nmessage status. The message status is now set to a plus sign, which indicates the user is \nallowing messages to be sent to her.\n$ who -T\nchristine + tty2         2015-09-10 12:56\ntimothy   - tty3         2015-09-10 11:46\n[...]\n$\nFor two-way communication, you need to allow messaging and one or more users also need \nto allow messaging. In this example, the user \ntimothy has also turned on his messaging:\n$ who -T\nchristine + tty2         2015-09-10 12:56\ntimothy   + tty3         2015-09-10 11:46\n[...]\n$\nNow that messaging is allowed between you and at least one other user, you can try out the \ncommand to send messages. However, the \nwho command is also still needed, because it pro-\nvides the necessary information in order to send a message.\nSending a message to another user\nThe primary tool for this script is the write command. As long as messaging is allowed, \nthe \nwrite command allows you to send a message to another logged-in user using his user-\nname and current terminal.\n\n712\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  712\nThe write command only allows you to successfully send messages to users logged onto a virtual console terminal \n(see Chapter 2). A user logged into the graphical environment will not be able to receive messages.\nIn this example, a message is sent from user christine to user timothy logged on the \ntty3 terminal. From \nchristine’s terminal, the session looks as follows:\n$ who\nchristine tty2         2015-09-10 13:54\ntimothy   tty3         2015-09-10 11:46\n[...]\n$\n$ write timothy tty3\nHello Tim!\n$\nAfter the message is initiated by the write command, a blank line is shown for you to \nbegin inputting the message text. It may be as many lines as you desire. When the Enter \nkey is pressed, a new line is available for more message text. After you are fi nished enter-\ning message text, the whole message is sent by pressing the Ctrl+D key combination.\nThe receiver of the message sees something like the following:\nMessage from christine@server01 on tty2 at 14:11 ...\nHello Tim!\nEOF\nThe receiver can see which user on which terminal sent the message. A time stamp is also \nincluded. Notice the \nEOF shown at the message’s bottom. It indicates End Of File, which lets \nthe message recipient know that the entire message is being displayed.\nOften, a message recipient needs to press the Enter key in order to get a prompt to show again, after a message is \nreceived.\nNow you can send messages! The next step is to use these commands to create the script.\nCreating the script\nUsing a script to send messages helps overcome a few potential problems. First, if you have \nlots of users on the system, trying to fi nd the one user you want to send a message to can \nbe a pain! You must also determine whether that particular user has messaging turned on. \n\n713\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  713\n26\nIn addition, a script speeds things up allowing you to quickly send a message to a particu-\nlar user in one easy step.\nChecking if user is logged on\nThe fi rst issue is to let the script know to which user you want to send a message. This is \neasily done by sending a parameter (Chapter 14) along with the script’s execution. For the \nscript to determine whether that particular user is logged on the system, the \nwho command \nis employed as shown in this bit of script code:\n# Determine if user is logged on:\n#\nlogged_on=$(who | grep -i -m 1 $1 | gawk '{print $1}')\n#\nIn the preceding code, the results of the who command are piped into the grep command \n(Chapter 4). The \ngrep command uses the -i option to ignore case, which allows the user-\nname to be entered using uppercase or lowercase letters. The \n-m 1 option is included on \nthe \ngrep command, in case the user is logged into the system multiple times. The grep \ncommand produces either nothing, if the user is not logged on, or the username’s fi rst login \ninformation. This output is passed to the \ngawk command (Chapter 19). The gawk command \nreturns only the fi rst item, either nothing or the username. This fi nal output from the \ngawk command is stored in the variable logged_on.\nSome Linux distributions, such as Ubuntu, may not have the gawk command installed by default. To install it, type \nsudo apt-get install gawk. Also, you can fi nd more information about installing software packages in \nChapter 9.\nWhen the variable, logged_on, contains either nothing (if the user is not logged on) or \nthe username, it can be tested and acted upon:\n#\nif [ -z $logged_on ]\nthen\n   echo \"$1 is not logged on.\"\n   echo \"Exiting script...\"\n   exit\nfi\n#\nEmploying the use of an if statement and a test command (Chapter 12), the logged_on \nvariable is tested to determine if it is a zero-length variable. If it is a zero-length variable, \nthe script user is informed via \necho commands that the user is not currently logged onto \n\n714\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  714\nthe system, and the script is exited via the exit command. If the user is logged onto the \nsystem, the \nlogged_on variable contains the user’s username, and the script continues.\nIn the following example, a username, \nCharlie, is passed as a parameter to the shell \nscript. This user is not currently logged onto the system:\n$ ./mu.sh Charlie\nCharlie is not logged on.\nExiting script...\n$\nThe code worked perfectly! Now instead of you digging through the who command results \nto determine whether a user is logged onto the system, the message script does that for \nyou.\nChecking if user accepts messages\nThe next important item is to determine whether a logged on user accepts messages. This \nscript portion operates very closely to the script section for determining whether a user is \nlogged on:\n# Determine if user allows messaging:\n#\nallowed=$(who -T | grep -i -m 1 $1 | gawk '{print $2}')\n#\nif [ $allowed != \"+\" ]\nthen\n   echo \"$1 does not allowing messaging.\"\n   echo \"Exiting script...\"\n   exit\nfi\n#\nNotice that this time, the who -T command and option are used. This displays a + next \nto the username, if messaging is allowed. Otherwise, it displays a \n- next to the username, \nif messaging is not allowed. The results from the \nwho command are then piped into grep \nand \ngawk to pull out only the messaging indicator. The messaging indicator is stored in \nthe \nallowed variable. Finally, an if statement is employed to test for a messaging indica-\ntor not set to \n+. If the indicator is not set to +, the script user is informed and the script \nis exited. However, if the messaging indicator shows messaging is allowed, the script \ncontinues.\nTo test out this script’s section, a user who is logged into the system with messaging dis-\nabled is tested. The user \nSamantha currently has messaging disabled:\n$ ./mu.sh Samantha\nSamantha does not allowing messaging.\nExiting script...\n$\n\n715\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  715\n26\nThe test worked as expected. This script portion eliminates any need to manually check for \nmessaging being enabled or disabled.\nChecking if message was included\nThe message to be sent is also included as a script parameter. Therefore, another needed \ncheck is whether a message was included as a parameter to the \nmu.sh shell script. To test \nfor the message parameter, an \nif statement, similar to those used earlier, must be included \nin the script’s code:\n# Determine if a message was included:\n#\nif [ -z $2 ]\nthen\n    echo \"No message parameter included.\"\n    echo \"Exiting script...\"\n    exit\nfi\n#\nTo test out this script portion, a message was not included for a user who is both logged \ninto the system and allows messaging:\n$ ./mu.sh Timothy\nNo message parameter included.\nExiting script...\n$\nThis is exactly what is needed! Now that the script has performed these preliminary checks, \nthe primary task of sending a message can be undertaken.\nTransmitting a simple message\nBefore a message is sent, the user’s current terminal must be identifi ed and stored in a \nvariable. The \nwho, grep, and gawk commands are employed again:\n# Send message to user:\n#\nuterminal=$(who | grep -i -m 1 $1 | gawk '{print $2}')\n#\nTo transmit the message, both the echo and the write commands are used:\n#\necho $2 | write $logged_on $uterminal\n#\nBecause write is an interactive utility, it must have the message piped into it for the \nscript to work properly. The \necho command is used to send the message, $2, to STDOUT, \n\n716\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  716\nwhich in turn is piped into the write command. The logged_on variable holds the user-\nname, and the \nuterminal variable holds the user’s current terminal. \nNow, you can test sending a simple message to a designated user via the script:\n$ ./mu.sh Timothy test\n$\nThe user Timothy receives the following message on his terminal:\nMessage from christine@server01 on tty2 at 10:23 ...\ntest\nEOF\nSuccess! You can now send simple one word messages to other users on your system via \nyour script.\nTransmitting a long message\nOften, you want to send more than just a single word to another system user. Let’s try a \nlonger message using the current script:\n$ ./mu.sh Timothy Boss is coming. Look busy.\n$\nThe user Timothy receives the following message on his terminal:\nMessage from christine@server01 on tty2 at 10:24 ...\nBoss\nEOF\nIt didn’t work. Only the fi rst word of the message, Boss, was sent. This is due to the script \nusing parameters (Chapter 14). Recall that the bash shell considers a space to differentiate \nbetween parameters. Thus, because there are spaces in the message, each word is treated as \na different parameter. The script must be modifi ed to fi x this problem.\nThe \nshift command (Chapter 14) and a while loop (Chapter 13) help with this long mes-\nsage issue:\n# Determine if there is more to the message:\n#\nshift\n#\nwhile [ -n \"$1\" ]\ndo\n   whole_message=$whole_message' '$1\n   shift\ndone\n#\n\n717\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  717\n26\nRecall that the shift command allows you to process the various provided script param-\neters without knowing the total number of parameters. The \nshift command simply moves \nthe next parameter in line down to parameter \n$1. First, a primary shift must be issued \nbefore the \nwhile loop, because the message starts in parameter $2, instead of parameter \n$1.\nAfter the \nwhile loop is initiated, it continues grabbing each message word, tacking the \nword onto the \nwhole_message variable. The loop then shifts to the next parameter. After \nthe fi nal parameter is processed, the \nwhile loop exits and the whole_message variable \ncontains the entire message to send.\nOne additional script modifi cation is needed to fi x this problem. Instead of just send-\ning parameter \n$2 to the write utility, the script is modifi ed to send the variable, \nwhole_message: \n# Send message to user:\n#\nuterminal=$(who | grep -i -m 1 $1 | gawk '{print $2}')\n#\necho $whole_message | write $logged_on $uterminal\n#\nNow, again try to send that warning message about the boss coming his way to Timothy:\n$ ./mu.sh Timothy Boss is coming\nUsage: grep [OPTION]... PATTERN [FILE]...\nTry 'grep --help' for more information.\n$\nOops! That didn’t work either. This is because when shift was used in the script, the $1 \nparameter contents were removed. Thus, when the script attempts to use \n$1 in the grep \ncommand, it generates an error. To fi x this problem a variable, \nmuser, is used to capture \nthe \n$1 parameter’s value:\n# Save the username parameter\n#\nmuser=$1\n#\nNow muser stores the username. The $1 parameter in the script’s various grep and echo \ncommands can be replaced by the \nmuser variable:\n# Determine if user is logged on:\n#\nlogged_on=$(who | grep -i -m 1 $muser | gawk '{print $1}')\n[...]\n   echo \"$muser is not logged on.\"\n[...]\n# Determine if user allows messaging:\n#\n\n718\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  718\nallowed=$(who -T | grep -i -m 1 $muser | gawk '{print $2}')\n[...]\n   echo \"$muser does not allowing messaging.\"\n[...]\n# Send message to user:\n#\nuterminal=$(who | grep -i -m 1 $muser | gawk '{print $2}')\n[...]\nTo test out the script changes, a multi-word message is sent again. In addition, some \nemphasis is added to the message by tacking on exclamation points:\n$ ./mu.sh Timothy The boss is coming! Look busy!\n$\nThe user Timothy receives the following message on his terminal:\nMessage from christine@server01 on tty2 at 10:30 ...\nThe boss is coming! Look busy!\nEOF\nIt worked! You can now employ the script to quickly send messages to other users on the \nsystem. Here’s the fi nal message script with all the needed checks and changes:\n#!/bin/bash\n#\n#mu.sh - Send a Message to a particular user\n#############################################\n#\n# Save the username parameter\n#\nmuser=$1\n#\n# Determine if user is logged on:\n#\nlogged_on=$(who | grep -i -m 1 $muser | gawk '{print $1}')\n#\nif [ -z $logged_on ]\nthen\n   echo \"$muser is not logged on.\"\n   echo \"Exiting script...\"\n   exit\nfi\n#\n# Determine if user allows messaging:\n#\nallowed=$(who -T | grep -i -m 1 $muser | gawk '{print $2}')\n#\n\n719\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  719\n26\nif [ $allowed != \"+\" ]\nthen\n   echo \"$muser does not allowing messaging.\"\n   echo \"Exiting script...\"\n   exit\nfi\n#\n# Determine if a message was included:\n#\nif [ -z $2 ]\nthen\n    echo \"No message parameter included.\"\n    echo \"Exiting script...\"\n    exit\nfi\n#\n# Determine if there is more to the message:\n#\nshift\n#\nwhile [ -n \"$1\" ]\ndo\n   whole_message=$whole_message' '$1\n   shift\ndone\n#\n# Send message to user:\n#\nuterminal=$(who | grep -i -m 1 $muser | gawk '{print $2}')\n#\necho $whole_message | write $logged_on $uterminal\n#\nexit\nBecause you have made it to the last chapter in this book, you should be ready for a script-\nwriting challenge. Here are some suggested improvements for the message script that you \ncan attempt on your own:\n ■\nInstead of passing the username and message as parameters, use options (see \nChapter 14).\n ■\nIf a user is logged into multiple terminals, allow a message to be sent to those mul-\ntiple terminals. (Hint: Use multiple \nwrite commands.)\n ■\nIf the message to be sent is for a user who is currently only logged into the GUI, \nproduce a message for the script user and exit the script. (Remember the \nwrite \ncommand can only write to virtual console terminals.)\n ■\nAllow a long message stored in a fi le to be sent to a terminal. (Hint: Use the cat \ncommand output piped into the \nwrite utility, instead of the echo command.)\n\n720\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  720\nNot only does reading through the script help solidify the script-writing concepts you are \nlearning, but so does modifying the script. Come up with your own creative modifi cation. \nHave a little fun! It helps you learn.\nObtaining a Quote\nInspirational quotes have long been used in the business environment. You may have a few \non your offi ce wall right now. This fun little and interesting script helps you obtain a daily \ninspirational quote to use as you please.\nThis section takes you through how to create this script. Included is a new rich utility that \nhas not been covered in the book yet. The script also uses some utilities that have been \ncovered, such as \nsed and gawk.\nUnderstanding the required functions\nSeveral great websites allow you to obtain daily inspiration quotes. Just open your favor-\nite search engine, and you can fi nd many sites. After you fi nd a site for your daily quote, \nyou need a utility to download that quote. For this script, the \nwget utility is just what’s \nneeded.\nLearning about the wget utility\nThe wget utility is a fl exible tool that allows web pages to be downloaded to your local \nLinux system. From these pages, you can glean your daily inspirational quote.\nThe wget command is an extremely rich utility. In this chapter, only a small portion of its power is used. Find out \nmore about \nwget via the man pages.\nTo download a web page via wget, you just need the wget command and the website’s \naddress:\n$ wget www.quotationspage.com/qotd.html\n--2015-09-23 09:14:28--  http://www.quotationspage.com/qotd.html\nResolving www.quotationspage.com... 67.228.101.64\nConnecting to www.quotationspage.com|67.228.101.64|:80. connected\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\n\n721\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  721\n26\nSaving to: \"qotd.html\"\n    [ <=>                              ] 13,806 --.-K/s   in 0.1s\n2015-09-23 09:14:28 (118 KB/s) - \"qotd.html\" saved [13806]\n$\nThe website’s information is stored in a fi le named after the web page. In this case, it’s \nqotd.html. And as you might have guessed by now, the fi le is full of HTML code:\n$ cat qotd.html\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n<html xmlns:fb=\"http://ogp.me/ns/fb#\">\n<head>\n        <title>Quotes of the Day - The Quotations Page</title>\n[...]\nOnly a partial HTML code listing is shown here. For the script, the sed and gawk utilities \nhelp strip out the desired inspirational quote. But before tackling the script, you need a \nlittle more control over the \nwget utility’s input and output.\nYou can use a variable to hold the web address (URL). Simply pass the variable to \nwget as a \nparameter. Just don’t forget to use the \n$ along with the variable name:\n$ url=www.quotationspage.com/qotd.html\n$\n$ wget $url\n--2015-09-23 09:24:21--  http://www.quotationspage.com/qotd.html\nResolving www.quotationspage.com... 67.228.101.64\nConnecting to www.quotationspage.com|67.228.101.64|:80 connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: \"qotd.html.3\"\n    [ <=>                        ] 13,806      --.-K/s   in 0.1s\n2015-09-23 09:24:21 (98.6 KB/s) - \"qotd.html.3\" saved [13806]\n$\nThe daily inspiration quote script is eventually to be run daily via cron (Chapter 16) or \nsome other script automation utility. Thus, having the \nwget command’s session output dis-\nplay to \nSTDOUT is undesirable. To store the session output to a log fi le, use the option -o. \nThis allows session output to be viewed at a later time:\n$ url=www.quotationspage.com/qotd.html\n$\n\n722\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  722\n$ wget -o quote.log $url\n$\n$ cat quote.log\n--2015-09-23 09:41:46--  http://www.quotationspage.com/qotd.html\nResolving www.quotationspage.com... 67.228.101.64\nConnecting to www.quotationspage.com|67.228.101.64|:80 connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: \"qotd.html.1\"\n     0K .......... ...                                81.7K=0.2s\n2015-09-23 09:41:46 (81.7 KB/s) - \"qotd.html.1\" saved [13806]\n$\nThe wget utility now stores its session output into the log fi le as it retrieves web page \ninformation. If desired, you can view the logged session output by using the \ncat command, \nas shown in the preceding code.\nFor various reasons, you may decide that you do not want wget to produce a log fi le or display session output. In \nthis case, just use the \n-q option, and the wget command quietly performs its directed duties.\nTo control where the web page information is stored, use the -O option on the wget com-\nmand. Thus, instead of having the web address as the storage fi le name, you can use the \nfi lename of your choice:\n$ url=www.quotationspage.com/qotd.html\n$\n$ wget -o quote.log -O Daily_Quote.html $url\n$ \n$ cat Daily_Quote.html\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n<html xmlns:fb=\"http://ogp.me/ns/fb#\">\n<head>\n[...]\n$\nUsing the -O option allows the web page data to be stored in the designated fi le, Daily_\nQuote.html\n. Now that the wget utility’s output is controlled, the next required function, \nchecking the web address’s validity, can be explored.\n\n723\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  723\n26\nTesting a web address\nWeb addresses change. Sometimes, it seems this happens daily. Therefore, it is important \nto test the address validity within the script. The \nwget utility gives the ability to conduct \nsuch a test with the \n--spider option:\n$ url=www.quotationspage.com/qotd.html\n$\n$ wget --spider $url\nSpider mode enabled. Check if remote file exists.\n--2015-09-23 12:45:41--  http://www.quotationspage.com/qotd.html\nResolving www.quotationspage.com... 67.228.101.64\nConnecting to www.quotationspage.com|67.228.101.64|:80 connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nRemote file exists and could contain further links,\nbut recursion is disabled -- not retrieving.\n$\nThis output indicates that the URL is valid, but it’s too much to read through. You can cut \ndown on the output by adding the \n-nv option, which stands for non-verbose:\n$ wget -nv --spider $url\n2015-09-23 12:49:13 \nURL: http://www.quotationspage.com/qotd.html 200 OK\n$\nThe -nv option allows just the web address’s status to be displayed, making the output \nmuch easier to read. Contrary to what you may think, the \nOK at the non-verbose line’s end \ndoes not indicate that the web address is valid. The indication is that the web address came \nback as it was sent. This concept is a little unclear, until you see an invalid web address.\nTo see an invalid web address indicator, the \nURL variable is changed to an incorrect web \naddress. The \nwget command is reissued using this bad address:\n$ url=www.quotationspage.com/BAD_URL.html\n$\n$ wget -nv --spider $url\n2015-09-23 12:54:33 \nURL: http://www.quotationspage.com/error404.html 200 OK\n$\nNotice that the output still has an OK at its end. However, the web address ends in \nerror404.html. This indicates the web address is invalid. \n\n724\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  724\nWith the necessary wget command to grab the inspirational quote’s web page information, \nand the ability to test the web page’s address, it is time to start building the script. Your \ndaily inspirational quote awaits retrieval.\nCreating the script\nTo test the script as it is built, a parameter containing the website’s URL is passed to the \nscript. Within the script, the variable \nquote_url contains the passed parameter’s value:\n#\nquote_url=$1\n#\nChecking the passed URL\nIt is always a good idea to have checks in place within your script. The fi rst check is to \nensure that the daily inspirational quote script website’s URL is still valid.\nAs you would expect, the script checks the web address validity with \nwget and the \n--spider option. However, the resulting indicator must be saved so the indicator can be \nchecked later with an \nif statement test. Thus, the resulting indicator must be saved to a \nvariable. This is a little tricky with the \nwget command.\nTo save the indicator output, the standard \n$() syntax is used around the command. But \nin addition, \nSTDERR and STDOUT redirection is needed. This is accomplished by tacking on \n2>&1 to the end of the wget command:\n#\ncheck_url=$(wget -nv --spider $quote_url 2>&1)\n#\nNow the indicator status message is saved within the check_url variable. To carve out the \nerror indicator, \nerror404, from the check_url string, parameter expansion and the echo \ncommand can be used:\n#\nbad_url=$(echo ${check_url/*error404*/error404})\n#\nIn this example, string parameter expansion allows the string stored in check_url to be \nsearched. Think of string parameter expansion as a quick and easy \nsed alternative. Using \nwildcards around the search word, \n*error404* allows the entire string to be searched. If \nthe search is successful, the \necho command sends the string error404 to be stored into \nthe \nbad_url variable. If the search is not successful, the bad_url variable contains the \ncheck_url variable’s contents.\n\n725\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  725\n26\nNow an if statement (Chapter 12) is employed to check the bad_url variable’s string. If \nthe string \nerror404 is found, a message is displayed and the script exits: \n#\nif [ \"$bad_url\" = \"error404\" ]\nthen\n    echo \"Bad web address\"\n    echo \"$quote_url invalid\"\n    echo \"Exiting script...\"\n    exit\nfi\n#\nAn easier and shorter method can be used. This method removes the need for string param-\neter expansion and the \nbad_url variable altogether. A double bracket if statement allows \na search to be conducted of the \ncheck_url variable:\nif [[ $check_url == *error404* ]]\nthen\n    echo \"Bad web address\"\n    echo \"$quote_url invalid\"\n    echo \"Exiting script...\"\n    exit\nfi\nThe test statement within the if structure searches the check_url variable’s string. If \nthe string \nerror404 is found anywhere within the variable string, a message is displayed \nand the script exits. If the indicator string does not contain the error message, the script \ncontinues. This statement saves time and effort. No need for any string parameter expan-\nsion or even the \nbad_url variable.\nNow that the check is in place, the script can be tested with an invalid web address. The \nurl variable is set to an incorrect URL and passed to the get_quote.sh script:\n$ url=www.quotationspage.com/BAD_URL.html\n$\n$ ./get_quote.sh $url\nBad web address\nwww.quotationspage.com/BAD_URL.html invalid\nExiting script...\n$\nThat works great. Just to make sure that all is well, now a valid web address is tested:\n$ url=www.quotationspage.com/qotd.html\n$\n\n726\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  726\n$ ./get_quote.sh $url\n$\nNo error message received. The script works perfectly so far! This is the only check needed, \nso the next item to be added to the script is obtaining the web page’s data.\nObtaining web page information\nGrabbing the inspiration daily quote’s web page data is simple. The wget command shown \nearlier in the chapter is used in the script. The only needed change is to store the log fi le \nand the HTML fi le, which contains the web page information, in the \n/tmp directory:\n#\nwget -o /tmp/quote.log -O /tmp/quote.html $quote_url\n#\nBefore moving on to the rest of the script, this code section should be tested using a valid \nweb address:\n$ url=www.quotationspage.com/qotd.html\n$\n$ ./get_quote.sh $url\n$\n$ ls /tmp/quote.*\n/tmp/quote.log  /tmp/quote.html\n$\n$ cat /tmp/quote.html\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n<html xmlns:fb=\"http://ogp.me/ns/fb#\">\n<head>\n[...]\n</body>\n</html>\n$\nThe script still works well! The log fi le, /tmp/quote.log, and the html fi le, /tmp/quote\n.html\n, were properly created.\nIf you do not want cookies to be involved when obtaining website information, you can add the --no-cookies \noption to the \nwget command. By default, storing cookies is turned off. \nThe next task is to dig the daily inspirational quote out of the HTML code within the down-\nloaded web page HTML fi le. This task requires both the \nsed and the gawk utilities.\n\n727\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  727\n26\nParsing out the desired information\nIn order to pull out the actual inspirational quote, some processing must take place. This \npart of the script uses \nsed and gawk to parse out the desired information.\nThis section is where the most variety is introduced when you modify this script for your own use. The sed and gawk \nutilities are used to search for keywords that are specifi c for this particular quote website’s data. You may need to \nuse different keywords as well as different \nsed and gawk commands to extract the data you desire.\nThe script fi rst needs to remove all the HTML tags from the downloaded web page’s informa-\ntion stored in the \n/tmp/quote.html fi le. The sed utility can provide this capability:\n#\nsed 's/<[^>]*//g' /tmp/quote.html \n#\nThe preceding code should look very familiar. It was covered in Chapter 21 in the \n“Removing HTML tags” section.\nAfter the HTML tags are removed, the output looks like the following:\n$ url=www.quotationspage.com/qotd.html\n$\n$ ./get_quote.sh $url\n[...]\n        >Quotes of the Day - The Quotations Page>\n>\n[...]\n>>Selected from Michael Moncur's Collection of Quotations\n - September 23, 2015>>\n>>>Horse sense is the thing a horse has which keeps \n[...]\n>\n$\nThis snipped listing shows that there is still too much unnecessary data in this fi le. \nTherefore, some additional parsing must be done. Fortunately, the quote text needed is \nsituated right next to the current date. Therefore, the script can use the current date as a \nsearch term!\nThe \ngrep command, the $() format, and the date command can help here. The output \nfrom the \nsed command is piped into the grep command. The grep command uses the \n\n728\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  728\ncurrent date formatted to match the date used on the quotation’s web page. After the date \ntext line is found, two additional text lines are pulled with the \n-A2 parameter:\n#\nsed 's/<[^>]*//g' /tmp/quote.html |\ngrep \"$(date +%B' '%-d,' '%Y)\" -A2\n#\nNow the script’s output looks similar to the following:\n$ ./get_quote.sh $url\n>>Selected from Michael Moncur's Collection of Quotations\n - September 23, 2015>>\n>>>Horse sense is the thing a horse has which keeps it from\n betting on people.> >>>>>>>>>>>>>>>>>>W. C. Fields> (1880 -\n 1946)> &nbsp; >>>\n>>Newspapermen learn to call a murderer 'an alleged murderer'\n and the King of England 'the alleged King of England' to \navoid libel suits.> >>>>>>>>>>>>>>>>>>Stephen Leacock> (1869\n - 1944)> &nbsp; >>> - More quotations on: [>Journalism>] >\n$\nIf your Linux system’s date is set differently than the quote of the day page’s date, you get a blank line instead of a \nquote. The preceding \ngrep command assumes your system date is the same as the web page’s date.\nAlthough the output is greatly reduced, there is still too much clutter in the text. The extra \n> symbols can easily be removed with the sed utility. In the script, the output from the \ngrep command is piped into the sed utility, which strips off the > symbols:\n#\nsed 's/<[^>]*//g' /tmp/quote.html |\ngrep \"$(date +%B' '%-d,' '%Y)\" -A2 |\nsed 's/>//g' \n#\nWith the new script line, the output is now a little clearer:\n$ ./get_quote.sh $url\nSelected from Michael Moncur's Collection of Quotations\n - September 23, 2015\nHorse sense is the thing a horse has which keeps it from\n betting on people. W. C. Fields (1880 - 1946) &nbsp;\nNewspapermen learn to call a murderer 'an alleged murderer'\n and the King of England 'the alleged King of England' to \navoid libel suits. Stephen Leacock (1869 - 1944) &nbsp;  - \nMore quotations on: [Journalism]\n$\n\n729\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  729\n26\nNow we’re getting somewhere! However, we can still remove a little more clutter from the \nquotation.\nYou may have noticed that two quotations are listed in the output instead of one. This hap-\npens occasionally with this particular website. Some days, it may be one quote, and other \ndays, it may be two. Therefore, the script needs a way to pull out only the fi rst quote.\nThe \nsed utility can help again with this problem. Using the sed utility’s next and delete \ncommands (Chapter 21), the string \n&nbsp; is located. After it’s found, sed moves to the \nnext line of the data and deletes it:\n#\nsed 's/<[^>]*//g' /tmp/quote.html |\ngrep \"$(date +%B' '%-d,' '%Y)\" -A2 |\nsed 's/>//g' |\nsed '/&nbsp;/{n ; d}'\n#\nNow the script can be tested to see if the new sed addition fi xes the multiple quotation \nproblem:\n$ ./get_quote.sh $url\nSelected from Michael Moncur's Collection of Quotations\n - September 23, 2015\nHorse sense is the thing a horse has which keeps it from\nbetting on people. W. C. Fields (1880 - 1946) &nbsp;\n$\nThe extra quotation is removed! One item remains for the quotation cleanup. At the quota-\ntion’s end, the string \n&nbsp; is still hanging around. The script could use another sed \ncommand to remove this pesky item, but just for variety, the \ngawk command is used:\n#\nsed 's/<[^>]*//g' /tmp/quote.html |\ngrep \"$(date +%B' '%-d,' '%Y)\" -A2 |\nsed 's/>//g' |\nsed '/&nbsp;/{n ; d}' |\ngawk 'BEGIN{FS=\"&nbsp;\"} {print $1}'\n#\nIn the preceding code, the input fi eld separator variable, FS, is used with the gawk com-\nmand (Chapter 22). The string \n&nbsp; is set as a fi eld separator, which causes gawk to drop \nit from the output:\n$ ./get_quote.sh $url\nSelected from Michael Moncur's Collection of Quotations\n - September 23, 2015\nHorse sense is the thing a horse has which keeps it from\nbetting on people. W. C. Fields (1880 - 1946)\n$\n\n730\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  730\nOne last needed script action is to save this quotation text to a fi le. Here the tee command \n(Chapter 15) helps. Now the entire quote extraction process looks as follows:\n#\nsed 's/<[^>]*//g' /tmp/quote.html |\ngrep \"$(date +%B' '%-d,' '%Y)\" -A2 |\nsed 's/>//g' |\nsed '/&nbsp;/{n ; d}' |\ngawk 'BEGIN{FS=\"&nbsp;\"} {print $1}' |\ntee /tmp/daily_quote.txt  > /dev/null\n#\nThe extracted quote is saved to /tmp/daily_quote.txt, and any output produced by \nthe \ngawk command is redirected to /dev/null (see Chapter 15). To make the script a little \nmore self-directed, the URL is hard-coded into the script:\n#\nquote_url=www.quotationspage.com/qotd.html\n#\nNow these two new changes to the daily inspirational quote script can be tested:\n$ ./get_quote.sh\n$\n$ cat /tmp/daily_quote.txt\nSelected from Michael Moncur's Collection of Quotations\n - September 23, 2015\nHorse sense is the thing a horse has which keeps it from\nbetting on people. W. C. Fields (1880 - 1946)\n$\nThat works perfectly! The daily inspiration quote was extracted from the website’s data and \nstored in a text fi le. You may have noticed by now that this quotation is less a traditional \ninspirational quote and more a humorous quote. Just know that some people fi nd humor \ninspirational!\nFor your review, here’s the fi nal daily inspirational quote script with all the needed checks \nand changes:\n#!/bin/bash\n#\n# Get a Daily Inspirational Quote\n#####################################\n#\n# Script Variables ####\n#\nquote_url=www.quotationspage.com/qotd.html\n#\n# Check url validity ###\n#\n\n731\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  731\n26\ncheck_url=$(wget -nv --spider $quote_url 2>&1)\n#\nif [[ $check_url == *error404* ]]\nthen\n    echo \"Bad web address\"\n    echo \"$quote_url invalid\"\n    echo \"Exiting script...\"\n    exit\nfi\n#\n# Download Web Site's Information\n#\nwget -o /tmp/quote.log -O /tmp/quote.html $quote_url\n#\n# Extract the Desired Data\n#\nsed 's/<[^>]*//g' /tmp/quote.html |\ngrep \"$(date +%B' '%-d,' '%Y)\" -A2 |\nsed 's/>//g' |\nsed '/&nbsp;/{n ; d}' |\ngawk 'BEGIN{FS=\"&nbsp;\"} {print $1}' |\ntee /tmp/daily_quote.txt  > /dev/null\n#\nexit\nThis script is an excellent opportunity to try out some of your newly learned script writing \nand command line skills. The following are a few suggested changes for the daily inspira-\ntional quote script that you can attempt on your own:\n ■\nChange the website to your favorite quotation or sayings website, and make the \nnecessary changes to the quote extraction commands.\n ■\nTry different sed and gawk commands for extracting the daily quotation. \n ■\nSet up the script to run daily on an automated basis via cron (see Chapter 16).\n ■\nAdd a command to display the quote text fi le at certain times, such as when you \nfi rst log in for the day.\nReading your daily quotes can inspire you. They may just inspire you to get out of that \nnext business meeting. The next chapter section helps you write a script that does just \nthat. \nGenerating an Excuse\nYou’ve been there. That endless staff meeting that is full of unimportant information. You \nwould really rather be working on that fascinating bash shell script project back at your \ndesk. Here’s a little fun script you can use to get out of the next staff meeting.\n\n732\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  732\nShort Message Service (SMS) allows text messages to be sent between cell phones. However, \nyou can also use SMS to send text messages directly from e-mail or the command line. The \nscript in this section allows you to construct a text message to be sent at a specifi ed time \ndirectly to your phone. Receiving a “critical” message from your Linux system is the perfect \nexcuse for leaving a staff meeting early.\nUnderstanding the required functions\nYou can send an SMS message from the command line in several ways. One way is via your \nsystem’s e-mail using your phone carrier’s SMS service. Another way is using the \ncurl \nutility.\nLearning about curl\nSimilar to wget, the curl utility allows you to transfer data from a particular web server. \nUnlike \nwget, it also allows you to transfer data to a web server. Transferring data to a par-\nticular web server is exactly what is needed here.\nSome Linux distributions, such as Ubuntu, may not have the curl command installed by default. To install it, type \nsudo apt-get install curl. Also, you can fi nd more information about installing software packages in \nChapter 9.\nBesides the curl utility, you need a website that provides free SMS message transfer. The \none used here for this script is \nhttp://textbelt.com/text. This website allows you to \nsend up to 75 text messages per day for free. You need it only for one text message, so it \nshould be no problem.\nIf your company already uses an SMS provider, such as http://sendhub.com or http://eztexting.com, \nyou can use those sites in your script instead. Be aware that the syntax needs to change depending upon those SMS \nprovider’s requirements. \nTo use curl and http://textbelt.com/text to send yourself a text message, you need \nto use the following syntax:\n$ curl http://textbelt.com/text \\\n-d number=YourPhoneNumber \\\n-d \"message=Your Text Message\"\nThe -d option tells curl to send specifi ed data to the website. In this case, the web-\nsite needs particular data sent in order to send a text message. This data includes \n\n733\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  733\n26\nYourPhoneNumber, which is your cell phone number starting with the area code. And it \nalso includes \nYour Text Message, which is the text message you desire to send.\nThe curl utility can handle much more than simply transferring data to and from a web server. It can handle many \nother network protocols, such as FTP, without any human intervention as well. Look at the man pages for \ncurl to \ndiscover its rich power.\nWhen the message is sent, the website provides a success message, \"success\": true, if \nno problems occurred:\n$ curl http://textbelt.com/text \\\n> -d number=3173334444 \\\n> -d \"message=Test from curl\"\n{\n  \"success\": true\n}$\n$\nOr it provides a fail message, \"success\": false, if data, such as the phone number, is \nincorrect:\n$ curl http://textbelt.com/text \\\n-d number=317AAABBBB \\\n-d \"message=Test from curl\"\n{\n  \"success\": false,\n  \"message\": \"Invalid phone number.\"\n}$\n$\nIf your cell phone carrier is not in the United States of America, it is likely that http://textbelt.com/text \nwill not work for you. You can try \nhttp://textbelt.com/Canada if your cell phone carrier is in Canada. If \nyour cell phone carrier is located elsewhere, try \nhttp://textbell.com/intl instead. For additional help, see \nhttp://textbelt.com.\nThe success/fail messages are very helpful, but they are unwanted for the script. To remove \nthese messages, simply redirect \nSTDOUT to /dev/null (see Chapter 15). Unfortunately, \nnow \ncurl supplies undesired output:\n$ curl http://textbelt.com/text \\\n> -d number=3173334444 \\\n> -d \"message=Test from curl\" > /dev/null\n\n734\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  734\n  % Total    % Received % Xferd  Average Speed...\n                                 Dload  Upload...\n  0    21    0    21    0    45     27     58 ...\n$\nThe preceding snipped listing shows various statistics, which may be helpful when debug-\nging your \ncurl command. However, for the script, this information must be suppressed. \nFortunately, the \ncurl command has a -s option, which makes it silent:\n$ curl -s http://textbelt.com/text \\\n> -d number=3173334444 \\\n> -d \"message=Test from curl\" > /dev/null\nThat is much better. The curl command is ready to be put into a script. However, before \nlooking at the script, one more topic needs to be addressed: sending text messages via \ne-mail.\nChoosing to use e-mail\nIf you choose not to use the text message relay service provided by http://textbelt\n.com/text\n or if for some reason it doesn’t work for you, you can always substitute sending \na text message via e-mail. This section briefl y covers how to accomplish this substitution. \nIf your cell phone carrier is not in the United States of America, it is likely that this web service will not work for you. \nAlso, your cell phone carrier may block SMS messages from this site. In this case, you must attempt to use e-mail \ninstead.\nWhether or not e-mail works as a substitute depends upon your cell phone carrier. If your \ncell phone carrier has an SMS gateway, you are in luck. Contact your cell phone carrier \nand fi nd out the name of the gateway. Often, it is something similar to \ntxt.att.net or \nvtext.com.\nYou can often fi nd out your cell phone carrier’s SMS gateway on your own via the Internet. One \ngreat site listing various SMS gateways, along with usage tips, is \nhttp://martinfitzpatrick.name/\nlist-of-email-to-sms-gateways/\n. If you cannot fi nd your carrier there, use your favorite search engine to \nlocate it.\nThe basic syntax for sending a text message via e-mail is as follows:\nmail -s \"your text message\" your_phone_number@your_sms_gateway\n\n735\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  735\n26\nIf the mail command does not work on your Linux system, you need to install the mailutils package. See \nChapter 9 for a review of installing software packages.\nUnfortunately, after you enter the syntax, you must type your message and press Ctrl+D to \nsend the text message. This is similar to sending a regular e-mail (see Chapter 24). Using \nthis method doesn’t work well in a script. Instead, you can store your e-mail message in a \nfi le and use it to send a text message. The basic idea for this method is as follows:\n$ echo \"This is a test\" > message.txt\n$ mail -s \"Test from email\" \\\n3173334444@vtext.com < message.txt\nNow the e-mail syntax is more compatible with a script. However, be aware that many \nproblems may exist with this approach. First, you must have a mail server running on your \nsystem (see Chapter 24). Secondly, your phone service provider may block SMS messages \ncoming from your system via e-mail. This is often true, if you are attempting this method \nfrom your home.\nIf your phone service provider blocks SMS messages coming from your system, you can use a cloud-based e-mail \nprovider as an SMS relay. Use your favorite Internet browser and search for the words \nSMS relay \nyour_favorite_cloud_email and see what sites come up.\nAlthough sending a text message via e-mail is a potential alternative, it can be fraught \nwith problems. If you can, it is much easier to use a free SMS relay website and the \ncurl \nutility. The script in the next section uses \ncurl to send a text message to the phone of \nyour choice.\nCreating the script\nAfter you have the required functions, creating the script to send a text message is fairly \nsimple. You just need a few variables and the \ncurl command.\nYou need three variables for the script. Setting up these particular data items as variables \nmakes it easier if any of this information changes. The variables are shown here:\n#\nphone=\"3173334444\"\nSMSrelay_url=http://textbelt.com/text\n\n736\nPart IV: Creating Practical Scripts\nc26.indd  12/08/2014  Page  736\ntext_message=\"System Code Red\"\n#\nThe only other needed item is the curl utility. Thus, here is the entire send a text message \nscript:\n#!/bin/bash\n#\n# Send a Text Message\n################################\n#\n# Script Variables ####\n#\nphone=\"3173334444\"\nSMSrelay_url=http://textbelt.com/text\ntext_message=\"System Code Red\"\n#\n# Send text ###########\n#\ncurl -s $SMSrelay_url -d \\\nnumber=$phone \\\n-d \"message=$text_message\" > /dev/null\n#\nexit\nIf you see this script as simple and easy, you are right! Even more important, that means \nyou have learned a great deal about shell script writing. Even easy scripts need to be \ntested, so be sure to test this script using your cell phone number in the \nphone variable \nbefore continuing. \nWhile you are testing your script, be aware that this website, http://textbelt.com/text, does not allow you \nto send more than three text messages to the same phone number in less than three minutes.\nTo have a text message sent to you at a desired time, you must employ the at command. If \nyou need a reminder, the \nat command was covered in Chapter 16.\nFirst, you can test the use of the \nat command with your new script. Have the at utility \nexecute the script by using the \n-f option along with the script’s fi le name, send_text\n.sh\n, in this case. Have the script run immediately using the Now option:\n$ at -f send_text.sh Now\njob 22 at 2015-09-24 10:22\n$\nThe script runs instantly. However, it may be a minute or two before you receive the text \nmessage on your phone.\n\n737\nChapter 26: Creating Fun Little Shell Scripts\nc26.indd  12/08/2014  Page  737\n26\nTo have the script run at another time, you simply use other at command options (see \nChapter 16). In the following example, the script is run 25 minutes from the current time.\n$ at -f send_text.sh Now + 25 minutes\njob 23 at 2015-09-24 10:48\n$\nNote in the example, the at command provides an informational message when the script is \nsubmitted. The date and time listed in that message is when the script will execute.\nWhat fun! Now you have a script utility that will be of assistance when you need an excuse \nto get out of that staff meeting. Better yet, you could modify the script to send yourself \ntruly serious system messages that need to be addressed.\nSummary\nThis chapter showed how to put some of the shell-scripting information presented in the \nbook to use for fun little shell scripts. Each script reinforced material covered in the chap-\nters along with a few new commands and ideas.\nThe chapter demonstrated how to send a message to another user on the Linux system. The \nscript checked to see whether the user was logged on to the system and whether the user \nallowed messaging. After those checks were made, the passed message was sent using the \nwrite command. Included were some suggestions for modifying this script, which improve \nyour shell-scripting abilities. \nThe next section walked you through obtaining website information using the \nwget utility. \nThe created script pulled a quote from the web. After retrieval, the script used several utili-\nties to pull out the actual quote text. These now familiar commands included \nsed, grep, \ngawk, and the tee command. For this script, suggestions were made for how the script \ncould be modifi ed. These are well worth pursuing to solidify and improve your new skills.\nThe chapter ended with a very fun and simple script for sending yourself a text message. \nWe explored the \ncurl  utility, along with SMS concepts. Although this is a fun script, it can \nbe modifi ed and used for more serious purposes. \nThanks for joining us on this journey through the Linux command line and shell scripting. \nWe hope you’ve enjoyed the journey and have learned how to get around on the command \nline and how to create shell scripts to save time. But don’t stop your command line educa-\ntion here. There’s always something new being developed in the open source world, whether \nit’s a new command line utility or a full-blown shell. Stay in touch with the Linux commu-\nnity and follow along with the new advances and features. \n\n\n\n739\nbapp01.indd  12/08/2014  Page  739\nQuick Guide to bash Commands\nIN THIS APPENDIX\nViewing the bash built-in commands\nReviewing GNU additional shell commands\nLooking at bash environment variables\nA\ns you’ve seen throughout this book, the bash shell contains lots of features and thus has \nlots of commands available. This appendix provides a concise guide to allow you to quickly \nlook up a feature or command that you can use from the bash command line or from a bash \nshell script.\nReviewing Built-In Commands\nThe bash shell includes many popular commands built into the shell. This provides for faster \nprocessing times when using these commands. Table A-1 shows the built-in commands available \ndirectly from the bash shell.\nTABLE A-1  bash Built-In Commands\nCommandDescription\n:\nExpands listed arguments and redirects as specifi ed\n.\nReads and executes commands from a designated fi le in the current shell\nalias\nDefi nes an alias for the specifi ed command\nbg\nResumes a job in background mode\nbind\nBinds a keyboard sequence to a readline function or macro\nbreak\nExits from a for, while, select, or until loop\nbuiltin\nExecutes the specifi ed shell built-in command\ncaller\nReturns the context of any active subroutine call\ncd\nChanges the current directory to the specifi ed directory\nAPPENDIX \nA\nContinues\n\n74 0\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  740\nCommandDescription\ncommand\nExecutes the specifi ed command without the normal shell lookup\ncompgen\nGenerates possible completion matches for the specifi ed word\ncomplete\nDisplays how the specifi ed words would be completed\ncompopt\nChanges options for how the specifi ed words would be completed\ncontinue\nResumes the next iteration of a for, while, select, or until loop\ndeclare\nDeclares a variable or variable type\ndirs\nDisplays a list of currently remembered directories\ndisown\nRemoves the specifi ed jobs from the jobs table for the process\necho\nDisplays the specifi ed string to STDOUT\nenable\nEnables or disables the specifi ed built-in shell command\neval\nConcatenates the specifi ed arguments into a single command, and executes \nthe command\nexec\nReplaces the shell process with the specifi ed command\nexit\nForces the shell to exit with the specifi ed exit status\nexport\nSets the specifi ed variables to be available for child shell processes\nfc\nSelects a list of commands from the history list\nfg\nResumes a job in foreground mode\ngetopts\nParses the specifi ed positional parameters\nhash\nFinds and remembers the full pathname of the specifi ed command\nhelp\nDisplays a help fi le\nhistory\nDisplays the command history\njobs\nLists the active jobs\nkill\nSends a system signal to the specifi ed process ID (PID)\nlet\nEvaluates each argument in a mathematical expression\nlocal\nCreates a limited-scope variable in a function\nlogout\nExits a login shell\nmapfile\nReads STDIN lines and puts them into an indexed array\npopd\nRemoves entries from the directory stack\nprintf\nDisplays text using formatted strings\npushd\nAdds a directory to the directory stack\nTABLE A-1   (continued)\n\n741\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  741\nA\nThe built-in commands provide higher performance than external commands, but the more \nbuilt-in commands that are added to a shell, the more memory it consumes with commands \nthat you may never use. The bash shell also contains external commands that provide \nextended functionality for the shell. These are discussed in the following section.\nLooking at Common bash Commands\nIn addition to the built-in commands, the bash shell utilizes external commands to allow \nyou to maneuver around the fi lesystem and manipulate fi les and directories. Table A-2 \nshows the common external commands you’ll want to use when working in the bash shell.\npwd\nDisplays the pathname of the current working directory\nread\nReads one line of data from STDIN, and assigns it to a variable\nreadarray\nReads STDIN lines, and puts them into an indexed array\nreadonly\nReads one line of data from STDIN, and assigns it to a variable that can’t be \nchanged\nreturn\nForces a function to exit with a value that can be retrieved by the calling \nscript\nset\nSets and displays environment variable values and shell attributes\nshift\nRotates positional parameters down one position\nshopt\nToggles the values of variables controlling optional shell behavior\nsource\nReads and executes commands from a designated fi le in the current shell\nsuspend\nSuspends the execution of the shell until a SIGCONT signal is received\ntest\nReturns an exit status of 0 or 1 based on the specifi ed condition\ntimes\nDisplays the accumulated user and system shell times.\ntrap\nExecutes the specifi ed command if the specifi ed system signal is received\ntype\nDisplays how the specifi ed word would be interpreted if used as a command\ntypeset\nDeclares a variable or variable type\nulimit\nSets a limit on the specifi ed resource for system users\numask\nSets default permissions for newly created fi les and directories\nunalias\nRemoves the specifi ed alias\nunset\nRemoves the specifi ed environment variable or shell attribute\nwait\nWaits for the specifi ed process to complete, and returns the exit status\n\n742\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  742\nTABLE A-2    The bash Shell External Commands\nCommandDescription\nbzip2\nCompresses using the Burrows-Wheeler block sorting text compression \nalgorithm and Huffman coding\ncat\nLists the contents of the specifi ed fi le\nchage\nChanges the password expiration date for the specifi ed system user \naccount\nchfn\nChanges the specifi ed user account’s comment information\nchgrp\nChanges the default group of the specifi ed fi le or directory\nchmod\nChanges system security permissions for the specifi ed fi le or directory\nchown\nChanges the default owner of the specifi ed fi le or directory\nchpasswd\nReads a fi le of login name and password pairs and updates the passwords\nchsh\nChanges the specifi ed user account’s default shell\nclear\nRemoves text from a terminal emulator or virtual console terminal\ncompress\nOriginal Unix fi le compression utility\ncoproc\nSpawns a subshell in background mode and executes the designated \ncommand\ncp\nCopies the specifi ed fi les to an alternate location\ncrontab\nInitiates the editor for the user’s crontable fi le, if allowed\ncut\nRemoves a designated portion of each specifi ed fi le’s lines\ndate\nDisplays the date in various formats\ndf\nDisplays current disk space statistics for all mounted devices\ndu\nDisplays disk usage statistics for the specifi ed fi le path\nemacs\nInvokes the emacs text editor\nfile\nViews the fi le type of the specifi ed fi le\nfind\nPerforms a recursive search for fi les\nfree\nChecks available and used memory on the system\ngawk\nStreams editing using programming language commands\ngrep\nSearches a fi le for the specifi ed text string\ngedit\nInvokes the GNOME Desktop editor\ngetopt\nParses command options including long options\ngroups\nDisplays group membership of the designated user\ngroupadd\nCreates a new system group\ngroupmod\nModifi es an existing system group\ngzip\nThe GNU Project’s compression using Lempel-Ziv compression\n\n74 3\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  743\nA\nhead\nDisplays the fi rst portion of the specifi ed fi le’s contents\nhelp\nDisplays the help pages for bash built-in commands\nkillall\nSends a system signal to a running process based on process name\nkwrite\nInvokes the KWrite text editor\nless\nAdvanced viewing of fi le contents\nlink\nCreates a link to a fi le using an alias name\nln\nCreates a symbolic or hard link to a designated fi le\nls\nLists directory contents\nmakewhatis\nCreates the whatis database allowing man page keyword searches\nman\nDisplays the man pages for the designated command or topic\nmkdir\nCreates the specifi ed directory under the current directory\nmore\nLists the contents of the specifi ed fi le, pausing after each screen of data\nmount\nDisplays or mounts disk devices into the virtual fi lesystem\nmv\nRenames a fi le\nnano\nInvokes the nano text editor\nnice\nRuns a command using a different priority level on the system\npasswd\nChanges the password for a system user account\nps\nDisplays information about the running processes on the system\npwd\nDisplays the current directory\nrenice\nChanges the priority of a running application on the system\nrm\nDeletes the specifi ed fi le\nrmdir\nDeletes the specifi ed directory\nsed\nStreams line editing using editor commands\nsleep\nPauses bash shell operation for a specifi ed amount of time\nsort\nOrganizes data in a data fi le based on the specifi ed order\nstat\nViews the fi le statistics of the specifi ed fi le\nsudo\nRuns an application as the root user account\ntail\nDisplays the last portion of the specifi ed fi le’s contents\ntar\nArchives data and directories into a single fi le\ntop\nDisplays the active processes, showing vital system statistics\ntouch\nCreates a new empty fi le or updates the timestamp on an existing fi le\numount\nRemoves a mounted disk device from the virtual fi lesystem\nuptime\nDisplays information on how long the system has been running\nuseradd\nCreates a new system user account\nContinues\n\n74 4\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  744\nCommandDescription\nuserdel\nRemoves an existing system user account.\nusermod\nModifi es an existing system user account\nvi\nInvokes the vim text editor\nvmstat\nProduces a detailed report on memory and CPU usage on the system\nwhereis\nDisplays a designated command’s fi les, including binary, source code, and \nman pages\nwhich\nFinds the location of an executable fi le\nwho\nDisplays users currently logged into system\nwhoami\nDisplays the current user’s username \nxargs\nTakes items from STDIN, builds commands, and executes the commands\nzip\nUnix version of the Windows PKZIP program\nYou can accomplish just about any task you need to on the command line using these \ncommands.\nAssessing Environment Variables\nThe bash shell also utilizes many environment variables. Although environment variables \naren’t commands, they often affect how shell commands operate, so it’s important to know \nthe shell environment variables. Table A-3 shows the default environment variables avail-\nable in the bash shell.\nTABLE A-3    bash Shell Default Environment Variables\nVariableDescription\n*\nContains all the command line parameters as a single text value\n@\nContains all the command line parameters as separate text values\n#\nThe number of command line parameters\n?\nThe exit status of the most recently used foreground process\n-\nThe current command line option fl ags\n$\nThe process ID (PID) of the current shell\nTABLE A-2   (continued)\n\n74 5\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  745\nA\n!\nThe PID of the most recently executed background process\n0\nThe name of the command from the command line\n_\nThe absolute pathname of the shell\nBASH\nThe full fi lename used to invoke the shell\nBASHOPTS\nEnabled shell options in a colon-separated list\nBASHPID\nThe current bash shell’s process ID\nBASH_ALIASES\nAn array containing the currently used aliases.\nBASH_ARGC\nThe number of parameters in the current subroutine\nBASH_ARGV\nAn array containing all the command line parameters specifi ed\nBASH_CMDS\nAn array containing the internal hash table of commands\nBASH_COMMAND\nThe name of the command currently being executed\nBASH_ENV\nWhen set, each bash script attempts to execute a startup fi le defi ned by \nthis variable before running.\nBASH_EXECUTION_\nSTRING\nThe command used in the -c command line option\nBASH_LINENO\nAn array containing the line numbers of each command in the script\nBASH_REMATCH\nAn array containing text elements that match a specifi ed regular \nexpression\nBASH_SOURCE\nAn array containing source fi le names for the declared functions in the shell\nBASH_SUBSHELL\nThe number of subshells spawned by the current shell\nBASH_VERSINFO\nA variable array that contains the individual major and minor version num-\nbers of the current instance of the bash shell\nBASH_VERSION\nThe version number of the current instance of the bash shell\nBASH_XTRACEFD\nWhen set to a valid fi le descriptor integer, trace output is generated and \nseparated from diagnostic and error messages. The fi le descriptor must \nhave \nset -x enabled. \nCOLUMNS\nContains the terminal width of the terminal used for the current instance \nof the bash shell\nCOMP_CWORD\nAn index into the variable COMP_WORDS, which contains the current cursor \nposition\nContinues\n\n74 6\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  746\nCommandDescription\nCOMP_KEY\nThe completion invocation character keyboard key\nCOMP_LINE\nThe current command line\nCOMP_POINT\nThe index of the current cursor position relative to the beginning of the \ncurrent command\nCOMP_TYPE\nThe completion type integer value\nCOM_WORDBREAKS\nA set of characters used as word separators when performing word \ncompletion\nCOMP_WORDS\nA variable array that contains the individual words on the current com-\nmand line\nCOMPREPLY\nA variable array that contains the possible completion codes generated \nby a shell function\nCOPROC\nA variable array that holds fi le descriptors for an unnamed coprocess’ I/O\nDIRSTACK\nA variable array that contains the current contents of the directory stack\nEMACS\nWhen set, the shell assumes it’s running in an emacs shell buffer and dis-\nables line editing.\nENV\nWhen the shell is invoked in POSIX mode, each bash script attempts to \nexecute a startup fi le defi ned by this variable before running.\nEUID\nThe numeric effective user ID of the current user\nFCEDIT\nThe default editor used by the fc command\nFIGNORE\nA colon-separated list of suffi xes to ignore when performing fi le name \ncompletion\nFUNCNAME\nThe name of the currently executing shell function\nFUNCNEST\nThe maximum level for nesting functions\nGLOBIGNORE\nA colon-separated list of patterns defi ning the set of fi lenames to be \nignored by fi le name expansion\nGROUPS\nA variable array containing the list of groups of which the current user is a \nmember\nhistchars\nUp to three characters that control history expansion\nTABLE A-3   (continued)\n\n747\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  747\nA\nHISTCMD\nThe history number of the current command\nHISTCONTROL\nControls what commands are entered in the shell history list\nHISTFILE\nThe name of the fi le to save the shell history list (.bash_history by \ndefault)\nHISTFILESIZE\nThe maximum number of lines to save in the history fi le\nHISTIGNORE\nA colon-separated list of patterns used to decide which commands are \nignored for the history fi le\nHISTSIZE\nThe maximum number of commands stored in the history fi le\nHISTTIMEFORMAT\nWhen set, determines the format string for the history fi le entries’ time \nstamps\nHOSTFILE\nContains the name of the fi le that should be read when the shell needs to \ncomplete a hostname\nHOSTNAME\nThe name of the current host\nHOSTTYPE\nA string describing the machine the bash shell is running on\nIGNOREEOF\nThe number of consecutive EOF characters the shell must receive before \nexiting. If this value doesn’t exist, the default is 1.\nINPUTRC\nThe name of the readline initialization fi le (The default is .inputrc.)\nLANG\nThe locale category for the shell\nLC_ALL\nOverrides the LANG variable, defi ning a locale category\nLC_COLLATE\nSets the collation order used when sorting string values\nLC_CTYPE\nDetermines the interpretation of characters used in fi le name expansion \nand pattern matching\nLC_MESSAGES\nDetermines the locale setting used when interpreting double-quoted \nstrings preceded by a dollar sign\nLC_NUMERIC\nDetermines the locale setting used when formatting numbers\nLINENO\nThe line number in a script currently executing\nLINES\nDefi nes the number of lines available on the terminal\nMACHTYPE\nA string defi ning the system type in cpu-company-system format\nContinues\n\n74 8\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  748\nCommandDescription\nMAILCHECK\nHow often (in seconds) the shell should check for new mail (default is 60)\nMAPFILE\nArray variable containing the mapfile command’s read text; used only \nwhen no variable name is given\nOLDPWD\nThe previous working directory used in the shell\nOPTERR\nIf set to 1, the bash shell displays errors generated by the getopts \ncommand.\nOSTYPE\nA string defi ning the operating system the shell is running on\nPIPESTATUS\nA variable array containing a list of exit status values from the processes in \nthe foreground process\nPOSIXLY_\nCORRECT\nIf set, bash starts in POSIX mode.\nPPID\nThe process ID (PID) of the bash shell’s parent process\nPROMPT_COMMAND\nIf set, the command to execute before displaying the primary prompt\nPS1\nThe primary command line prompt string\nPS2\nThe secondary command line prompt string\nPS3\nThe prompt to use for the select command\nPS4\nThe prompt displayed before the command line is echoed if the bash -x \nparameter is used.\nPWD\nThe current working directory\nRANDOM\nReturns a random number between 0 and 32767. Assigning a value to this \nvariable seeds the random number generator.\nREADLINE_LINE\nThe readline line buffer contents\nREADLINE_POINT\nThe current readline line buffer’s insertion point position\nREPLY\nThe default variable for the read command\nSECONDS\nThe number of seconds since the shell was started. Assigning a value \nresets the timer to the value.\nSHELL\nThe shell’s full pathname\nSHELLOPTS\nA colon-separated list of enabled bash shell options\nSHLVL\nIndicates the shell level, incremented by 1 each time a new bash shell is \nstarted\nTABLE A-3   (continued)\n\n74 9\nAppendix A: Quick Guide to bash Commands\nbapp01.indd  12/08/2014  Page  749\nA\nTIMEFORMAT\nA format specifying how the shell displays time values\nTMOUT\nThe value of how long (in seconds) the select and read commands \nshould wait for input. The default of 0 indicates to wait indefi nitely.\nTMPDIR\nWhen set to a directory name, the shell uses the directory as a location for \ntemporary shell fi les.\nUID\nThe numeric real user ID of the current user\nYou display the environment variables using the set  built-in command. The default shell \nenvironment variables set at boot time can and often do vary between different Linux \ndistributions. \n\n\n\n751\nbapp02.indd  12/08/2014  Page  751\nQuick Guide to sed and gawk\nIN THIS APPENDIX\nThe basics for using sed\nWhat you need to know about gawk\nI\nf you do any type of data handling in your shell scripts, most likely you’ll need to use either the \nsed program or the gawk program (and sometimes both). This appendix provides a quick refer-\nence for \nsed and gawk commands that come in handy when working with data in your \nshell scripts.\nThe sed Editor\nThe sed editor can manipulate data in a data stream based on commands you either enter into the \ncommand line or store in a command text fi le. It reads one line of data at a time from the input and \nmatches that data with the supplied editor commands, changes data in the stream as specifi ed in \nthe commands, and then outputs the new data to \nSTDOUT.\nStarting the sed editor\nHere’s the format for using the sed command:\nsed options script file \nThe options parameters allow you to customize the behavior of the sed command and include the \noptions shown in Table B-1.\nTABLE B -1    The sed Command Options\nOptionDescription\n-e script\nAdds commands specifi ed in script to the commands run while processing the \ninput\n-f file\nAdds the commands specifi ed in the fi le file to the commands run while processing \nthe input\n-n\nDoesn’t produce output for each command, but waits for the print command\nAPPENDIX \nB\n\n752\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  752\nThe script parameter specifi es a single command to apply against the stream data. If \nmore than one command is required, you must use either the \n-e option to specify them in \nthe command line or the \n-f option to specify them in a separate fi le.\nsed commands\nThe sed editor script contains commands that sed processes for each line of data in \nthe input stream. This section describes some of the more common \nsed commands you’ll \nwant to use.\nSubstitution\nThe s command substitutes text in the input stream. Here’s the format of the s command: \ns/pattern/replacement/flags \npattern is the text to replace, and replacement is the new text that sed inserts in its \nplace.\nThe \nflags parameter controls how the substitution takes place. Four types of substitution \nfl ags are available: \n ■\nA number indicates the pattern occurrence that should be replaced.\n ■\ng indicates that all occurrences of the text should be replaced.\n ■\np indicates that the contents of the original line should be printed.\n ■\nw file indicates that the results of the substitution should be written to a fi le.\nIn the fi rst type of substitution, you can specify which occurrence of the matching pattern \nthe \nsed editor should replace. For example, you use the number 2 to replace only the sec-\nond occurrence of the pattern.\nAddressing\nBy default, the commands you use in the sed editor apply to all lines of the text data. If \nyou want to apply a command to only a specifi c line, or a group of lines, you must use line \naddressing.\nThere are two forms of line addressing in the \nsed editor:\n ■\nA numeric range of lines\n ■\nA text pattern that fi lters out a line\nBoth forms use the same format for specifying the address:\n[address]command \n\n753\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  753\nB\nWhen using numeric line addressing, you reference lines by their line position in the text \nstream. The \nsed editor assigns the fi rst line in the text stream as line number 1 and con-\ntinues sequentially for each new line.\n$ sed '2,3s/dog/cat/' data1 \nThe other method of restricting which lines a command applies to is a bit more compli-\ncated. The \nsed editor allows you to specify a text pattern that it uses to fi lter lines for the \ncommand. Here’s the format for this:\n/pattern/command \nYou must encapsulate the pattern you specify in forward slashes. The sed editor applies \nthe command only to lines that contain the text pattern that you specify.\n$ sed '/rich/s/bash/csh/' /etc/passwd \nThis fi lter fi nds the line that contains the text rich and replaces the text bash with csh.\nYou can also group more than one command together for a specifi c address:\naddress {\n     command1\n     command2\n     command3 } \nThe sed editor applies each of the commands you specify only to lines that match the \naddress specifi ed. The \nsed editor processes each command listed on the address line(s): \n$ sed '2{\n> s/fox/elephant/ \n> s/dog/cat/ \n> }' data1 \nThe sed editor applies each of the substitutions to the second line in the data fi le.\nDeleting lines\nThe delete command, d, pretty much does what it says. It deletes any text lines that match \nthe addressing scheme supplied. Be careful with the delete command, because if you forget \nto include an addressing scheme, all the lines are deleted from the stream:\n$ sed 'd' data1 \nThe delete command is obviously most useful when used in conjunction with a specifi ed \naddress. This allows you to delete specifi c lines of text from the data stream, either by line \nnumber:\n$ sed '3d' data6 \n\n754\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  754\nor by a specifi c range of lines:\n$ sed '2,3d' data6 \nThe pattern-matching feature of the sed editor also applies to the delete command:\n$ sed '/number 1/d' data6 \nOnly lines matching the specifi ed text are deleted from the stream.\nInserting and appending text\nAs you would expect, like any other editor, the sed editor allows you to insert and append \ntext lines to the data stream. The difference between the two actions can be confusing: \n ■\nThe insert command (i) adds a new line before the specifi ed line.\n ■\nThe append command (a) adds a new line after the specifi ed line.\nThe format of these two commands can be confusing: You can’t use these commands on a \nsingle command line. You must specify the line to insert or append on a separate line by \nitself. Here’s the format for doing this:\nsed '[address]command\\\nnew line' \nThe text in new line appears in the sed editor output in the place you specify. Remember \nthat when you use the insert command, the text appears before the data stream text:\n$ echo \"testing\" | sed 'i\\\n> This is a test'\nThis is a test\ntesting \n$ \nAnd when you use the append command, the text appears after the data stream text:\n$ echo \"testing\" | sed 'a\\ \n> This is a test'\ntesting \nThis is a test \n$ \nThis allows you to insert text at the end of the normal text.\nChanging lines\nThe change command allows you to change the contents of an entire line of text in the \ndata stream. It works the same as the insert and append commands, in that you must \nspecify the new line separately from the rest of the \nsed command:\n$ sed '3c\\ \n> This is a changed line of text.' data6 \n\n755\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  755\nB\nThe backslash character is used to indicate the new line of data in the script.\nTransform command\nThe transform command (y) is the only sed editor command that operates on a single char-\nacter. The transform command uses this format:\n[address]y/inchars/outchars/ \nThe transform command performs a one-to-one mapping of the inchars and the \noutchars values. The fi rst character in inchars is converted to the fi rst character \nin \noutchars. The second character in inchars is converted to the second character in \noutchars. This mapping continues throughout the length of the specifi ed characters. If \nthe \ninchars and outchars are not the same length, the sed editor produces an error \nmessage.\nPrinting lines\nSimilar to the p fl ag in the substitution command, the p command prints a line in the sed \neditor output. The most common use for the print command is for printing lines that con-\ntain matching text from a text pattern:\n$ sed -n '/number 3/p' data6 \nThis is line number 3. \n$ \nThe print command allows you to fi lter only specifi c lines of data from the input stream.\nWriting to a file\nThe w command is used to write lines to a fi le. Here’s the format for the w command:\n[address]w filename \nThe filename can be specifi ed as either a relative or absolute pathname, but in either \ncase, the person running the \nsed editor must have write permissions for the fi le. The \naddress can be any type of addressing method used in sed, such as a single line number, \na text pattern, or a range of line numbers or text patterns.\nHere’s an example that prints only the fi rst two lines of a data stream to a text fi le:\n$ sed '1,2w test' data6 \nThe output fi le test contains only the fi rst two lines from the input stream.\nReading from a file\nYou’ve already seen how to insert and append text into a data stream from the sed com-\nmand line. The read command (\nr) allows you to insert data contained in a separate fi le.\n\n756\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  756\nHere’s the format of the read command:\n[address]r filename \nThe filename parameter specifi es either an absolute or relative pathname for the fi le that \ncontains the data. You can’t use a range of addresses for the read command. You can specify \nonly a single line number or text pattern address. The \nsed editor inserts the text from the \nfi le after the address.\n$ sed '3r data' data2 \nThe sed editor inserts the complete text from the data fi le into the data2 fi le, starting at \nline 3 of the data2 fi le.\nThe gawk Program\nThe gawk program is the GNU version of the original awk program in Unix. The awk pro-\ngram takes stream editing one step further than the \nsed editor by providing a program-\nming language instead of just editor commands. This section describes the basics of the \ngawk program as a quick reference to its abilities.\nThe gawk command format\nThe basic format of the gawk program is as follows:\ngawk options program file \nTable B-2 shows the options available with the gawk program.\nTABLE B -2    The  gawk  Options\nOptionDescription\n-F fs\nSpecifi es a fi le separator for delineating data fi elds in a line\n-f file\nSpecifi es a fi le name to read the program from\n-v var=value\nDefi nes a variable and default value used in the gawk program\n-mf N\nSpecifi es the maximum number of fi elds to process in the data fi le\n-mr N\nSpecifi es the maximum record size in the data fi le\n-W keyword\nSpecifi es the compatibility mode or warning level for gawk. Use the help \noption to list all the available keywords.\nThe command line options provide an easy way to customize features in the gawk program.\n\n757\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  757\nB\nUsing gawk\nYou can use gawk either directly from the command line or from within your shell scripts. \nThis section demonstrates how to use the \ngawk program and how to enter scripts for gawk \nto process.\nReading the program script from the command line\nA gawk program script is defi ned by an opening and closing brace. You must place script \ncommands between the two braces. Because the \ngawk command line assumes that the \nscript is a single text string, you must also enclose your script in single quotation marks. \nHere’s an example of a simple \ngawk program script specifi ed on the command line:\n$ gawk '{print $1}' \nThis script displays the fi rst data fi eld in every line of the input stream.\nUsing multiple commands in the program script\nA programming language wouldn’t be very useful if you could execute only one command. \nThe \ngawk programming language allows you to combine commands into a normal program. \nTo use multiple commands in the program script specifi ed on the command line, just place a \nsemicolon between commands:\n$ echo \"My name is Rich\" | gawk '{$4=\"Dave\"; print $0}' \nMy name is Dave \n$ \nThe script performs two commands: It replaces the fourth data fi eld with a different value, \nand then it displays the entire data line in the stream.\nReading the program from a file\nAs with the sed editor, the gawk editor allows you to store your programs in a fi le and \nrefer to them in the command line:\n$ cat script2 \n{ print $5 \"'s userid is \" $1 } \n$ gawk -F: -f script2 /etc/passwd \nThe gawk program processes all the commands specifi ed in the fi le on the input \nstream data.\nRunning scripts before processing data\nThe gawk program also allows you to specify when the program script is run. By default, \ngawk reads a line of text from the input and then executes the program script on the data \nin the line of text. Sometimes, you may need to run a script before processing data, such as \nto create a header section for a report. To do that, you use the \nBEGIN keyword. This forces \n\n758\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  758\ngawk to execute the program script specifi ed after the BEGIN keyword before reading \nthe data:\n$ gawk 'BEGIN {print \"This is a test report\"}'\nThis is a test report \n$ \nYou can place any type of gawk command in the BEGIN section, such as commands that \nassign default values to variables.\nRunning scripts after processing data\nSimilar to the BEGIN keyword, the END keyword allows you to specify a program script that \ngawk executes after reading the data:\n$ gawk 'BEGIN {print \"Hello World!\"} {print $0} END {print\n     \"byebye\"}' data1\nHello World! \nThis is a test \nThis is a test \nThis is another test. \nThis is another test. \nbyebye \n$ \nThe gawk program executes the code in the BEGIN section fi rst, then processes any data in \nthe input stream, and then executes the code in the \nEND section.\nThe gawk variables\nThe gawk program is more than just an editor; it’s a complete programming environment. \nAs such, lots of commands and features are associated with \ngawk. This section shows the \nmain features you need to know for programming with \ngawk.\nBuilt-in variables\nThe gawk program uses built-in variables to reference specifi c features within the program \ndata. This section describes the \ngawk built-in variables available for you to use in your \ngawk programs and demonstrates how to use them.\nThe \ngawk program defi nes data as records and data fi elds. A record is a line of data (delin-\neated by the newline characters by default), and a data fi eld is a separate data element \nwithin the line (delineated by a white space character, such as a space or tab, by default).\nThe \ngawk program uses data fi eld variables to reference data elements within each record. \nTable B-3 describes these variables.\n\n759\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  759\nB\nTABLE B -3    The gawk Data Field and Record Variables\nVariableDescription\n$0\nThe entire data record\n$1\nThe fi rst data fi eld in the record\n$2\nThe second data fi eld in the record\n$n\nThe nth data fi eld in the record\nFIELDWIDTHS\nA space-separated list of numbers defi ning the exact width (in spaces) of \neach data fi eld\nFS\nInput fi eld separator character\nRS\nInput record separator character\nOFS\nOutput fi eld separator character\nORS\nOutput record separator character\nIn addition to the fi eld and record separator variables, gawk provides some other built-in \nvariables to help you know what’s going on with your data and extract information from \nthe shell environment. Table B-4 shows the other built-in variables in \ngawk.\nTABLE B - 4    More gawk Built-In Variables\nVariableDescription\nARGC\nThe number of command line parameters present\nARGIND\nThe index in ARGV of the current fi le being processed\nARGV\nAn array of command line parameters\nCONVFMT\nThe conversion format for numbers (see the printf statement), with a \ndefault value of %.6 g\nENVIRON\nAn associative array of the current shell environment variables and their \nvalues\nERRNO\nThe system error if an error occurs reading or closing input fi les\nFILENAME\nThe fi lename of the data fi le used for input to the gawk program\nFNR\nThe current record number in the data fi le\nIGNORECASE\nIf set to a non-zero value, gawk all string functions (including regular \nexpressions); ignore the case of characters.\nNF\nThe total number of data fi elds in the data fi le\nNR\nThe number of input records processed\nContinues\n\n760\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  760\nVariableDescription\nOFMT\nThe output format for displaying numbers, with a default of %.6 g\nRLENGTH\nThe length of the substring matched in the match function\nRSTART\nThe start index of the substring matched in the match function\nYou can use the built-in variables anywhere in the gawk program script, including the \nBEGIN and END sections.\nAssigning variables in scripts\nAssigning values to variables in gawk programs is similar to how you assign values to vari-\nables in a shell script — using an assignment statement:\n$ gawk ' \n> BEGIN{ \n> testing=\"This is a test\" \n> print testing \n> }' \nThis is a test \n$ \nAfter you assign a value to a variable, you can use that variable anywhere in your \ngawk script.\nAssigning variables in the command line\nYou can also use the gawk command line to assign values to variables for the gawk \nprogram. This allows you to set values outside of the normal code, changing values on the \nfl y. Here’s an example of using a command line variable to display a specifi c data fi eld in \nthe fi le:\n$ cat script1 \nBEGIN{FS=\",\"} \n{print $n} \n$ gawk -f script1 n=2 data1 \n$ gawk -f script1 n=3 data1 \nThis feature is a great way to process data from your shell scripts in the gawk script.\nThe gawk program features\nSome features of the gawk program make it handy for manipulating data, allowing you to \ncreate \ngawk scripts that can parse just about any type of text fi le, including log fi les.\nTABLE B - 4   (continued)\n\n761\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  761\nB\nRegular expressions\nYou can use either a Basic Regular Expression (BRE) or an Extended Regular Expression \n(ERE) to fi lter the lines in the data stream to which the program script applies.\nWhen using a regular expression, the regular expression must appear before the left brace \nof the program script that it controls:\n$ gawk 'BEGIN{FS=\",\"} /test/{print $1}' data1\nThis is a test \n$ \nThe matching operator\nThe matching operator allows you to restrict a regular expression to a specifi c data fi eld in \nthe records. The matching operator is the tilde character (~). You specify the matching \noperator, along with the data fi eld variable, and the regular expression to match:\n$1 ~ /^data/ \nThis expression fi lters records where the fi rst data fi eld starts with the text data.\nMathematical expressions\nIn addition to regular expressions, you can also use mathematical expressions in the \nmatching pattern. This feature comes in handy when matching numerical values in data \nfi elds. For example, if you want to display all the system users who belong to the root users \ngroup (group number 0), you could use this script: \n$ gawk -F: '$4 == 0{print $1}' /etc/passwd \nThis script displays the fi rst data fi eld value for all lines that contain the value 0 in the \nfourth data fi eld.\nStructured commands\nThe gawk program supports the structured commands discussed in this section.\nThe \nif-then-else statement: \n if (condition) statement1; else statement2 \nThe while statement: \n while (condition) \n{\n    statements \n} \n\n762\nAppendix B: Quick Guide to sed and gawk\nbapp02.indd  12/08/2014  Page  762\nThe do-while statement: \n do {\n    statements \n} while (condition) \nThe for statement: \n for(variable assignment; condition; iteration process) \n This provides a wealth of programming opportunities for the gawk script programmer. You \ncan write \ngawk programs that rival the functions of just about any higher-level language \nprogram. \n\n763\nbindex.indd  12/16/2014  Page  763\n Index\nSymbols\n/ (forward slash), 540\n-- (double dash), 378–379, 382\n; (semicolon), 119, 269, 270, 512\n! (exclamation mark), 569–571\n!! (double exclamation mark), \n129\n? (question mark), 550\n. (dot), 66, 271\nPATH environm.ent variable, \n149\nregular expressions, 542–543\nrelative fi lepaths, 58\n' (closing quotation mark), 508\n(( expression )) (double \nparentheses command), \n325–326\n[] (square brackets), 543–545\n{} (braces), 510, 551–553\n* (asterisk), 536, 548–549\ndefault \ncase, 480\n\\ (backslash character), 755\n& (ampersand), 427–428, 433, 576\n` (backtick character), 277\n^ (caret character), 540–541\n+ (plus sign), 551\n< (less than), 280\n<< (double less-than symbol), 281\n= (equal sign)\ngawk, 598–599\nsed, 529\n| (pipe character), 106, 553\n| (vertical line), 282\n~ (tilde), 56, 603–604\n$(), 578–579, 727\n$ (dollar sign)\nanchor character, 541–542\nenvironment variables, 137\nsed, 582\nspecial address, 570\n$($#), 372–373\n$?, 453, 454–455, 696\n$@, 373–375\n$*, 373–375\n$#, 371–373\n.. (double dot), relative fi lepaths, \n58\n# (pound sign), in shell scripts, \n270–271\n$() format, 277, 367\n$? special variable, 292\n$$ variable, 432\n[[ expression ]] (double bracket \ncommand), 326–327\n> (greater than), 279\n&>, 399\n>> (double greater than), 280, 397\n$0, 368, 456, 511\n$1, 366, 511–512\n1>, 399\n$2, 511\n2>, 399\nA\nabsolute directory references, \n56–57, 66\naccess permission triplets, \n176–177\naccounts\ncreating multiple, 361–362\nmanaging, 656–673\ncreating script for, \n665–671\ndetermining existence, \n661–662\nfi nding fi les, 664–665\ngetting name, 657–660\nremoving account, 665\nremoving processes, \n662–664\nrunning script for, 671–673\nverifying name, 660–661\nMySQL, 687–688\nroot, 163\nchanging fi le ownership, \n182\n/etc/shadow fi le, 164\nmounting media, 97\nsystem, 163\ngroups, 173\nuser, 161\naddress pattern, 574\naddresses\ne-mail, 558–560\nrange, 584\nsed, 518–521, 752–753\ngrouping, 520–521\nnumeric, 519–520\ntext pattern fi lters, 520\nweb, 721, 723–726\naging passwords, 171–172\nAIX Unix, 191\nalias, 131–132, 157, 234\nAlmquist, Kenneth, 623\nAlmquist shell, 623\nanacron, 443–445\nanchor characters\n^ (caret character), 540–541\n$ (dollar sign), 541–542, 582\ncombining, 542\nappend command, 754\nappend command, 523–525\napt-cache, 212\napt-cache show, 243\napt-get, 212\napt-get install, 244\naptitude, 212\ninstalling software packages, \n215–217\nmain window, 213\nmanaging packages, 212–215\nrepositories, 219–220\nuninstalling software, 218–\n219\nupdating software, 217–218\naptitude dist-upgrade, 218\naptitude full-upgrade, 218\n\n764\nIndex\nbindex.indd  12/16/2014  Page  764\naptitude install, 216\naptitude purge, 218\naptitude safe-update, 217\naptitude safe-upgrade, \n217–218\naptitude search, 215\naptitude show, 213–214\narchiving data, 110–111\ndaily archive script\ncreating, 649–651\nrunning, 651–652\nhourly archive script\ncreating, 652–655\nrunning, 655–656\nscripting, 645–656\nconfi guration fi le, 646–648\ncreating daily archive \nlocation, 648–649\nARGC variable, 595–596\nARGV variable, 595–596\narrays\nassociative, 596, 600\nfunctions, 461–464\npassing, 461–463\nreturning, 463–464\ngawk, 600–602\ndeleting, 601–602\niteration through, 601–602\nvariable assignment, \n600–601\nvariable, 158–159\nASCII character ordering, 312\nash shell, 10, 114, 623\nasort, 616\nasorti, 616\nassignment statement, 598\nassociative array, 596, 600\nasterisk, 548–549\nat, 438–441\natd, 438\nAthena(Xaw), 25\natq, 440–441\natrm, 441\nAT&T Unix, 86, 89\nawk, 509. See also gawk\nB\nbackground jobs, 429–430\nbackground mode\ncommands, 121–123\nprocess lists, 123–124\nscripts, 427–429\nbackslash character, 755\nbacktick character, 277\nbackups, 202\nbasename, 368\nbash, 116, 677\nbash calculator (bc), 288–292\nbash —help, 118\nbash shell, 10, 624\nBourne variables, 144\ncommand line parameters, \n118\ncommands\nbuilt-in, 127, 739–741\ncommon, 741–744\nenvironment variables, \n744–749\nas default shell, 113–114\nenvironment variables, \n144–148\nconventions, 139\ndefault, 744–749\ntypes, 135\nEOF key combination, 511\nexternal commands, \n742–744\nhidden fi les, 60\njob control, 434\nline numbers, 582\nmanual, 49–52\nmathematical operations, \n287–288\nreferencing current directory, \n271\nsignaling, 419–420\nstarting, 47–48\nusing prompt, 48–49\nBASH_ENV environment variable, \n156, 157\n.bash_history, 129–130\n.bash_login fi le, 445\n.bash_profile fi le, 445\n.bashrc fi le, 156, 445\nfunction defi nition in, \n468–470\ndirectly defi ning, 469\nsourcing function fi les, \n469–470\nlocations, 153, 157\nbatch, 439\nbc, 288–292\nBEGIN key word, 514–516, 602\nBell Labs, 86\nBerkeley Software Distribution \n(BSD), 89\nbg, 435\n/bin/bash, 624\n/bin/sh, 114, 115, 624\nbitwise manipulation, 614\nblock device fi les, 7\nblock preallocation, 190\nBoolean operators, 324–325\nBourne shell, 10, 623\nmathematical operations, 285, \n287\nbrackets\ndouble, 326–327\nmath, 287–288\npattern matching, 326–327\nsquare, 543–545\nbranch command, 572–574\nBRE. See POSIX Basic Regular \nExpression engine\nbreak command, 352–354, 378\ntext menus, 482\nbreak statement, 608\nbroken dependencies, \n225–227\nBSD. See Berkeley Software \nDistribution\nB-tree fi lesystem, 192\nBtrfs fi lesystem, 192\nbuffers\nemacs, 248–249\nscrollback, 39\nbuilt-in commands\nbash, 127, 739–741\ndash shell, 628–629\nreviewing, 739–741\nshell, 125–132\nzsh, 633–638\nadd-in modules, 636–637\ncore, 634–636\nviewing, adding, removing \nmodules, 637–638\nbuilt-in variables, \ngawk, 592–600, \n759–760\nBurrows-Wheeler block \nsorting text \ncompression, 109\nbzip2, 109\n\n765\nIndex\nbindex.indd  12/16/2014  Page  765\nC\nC shell, 114\nCanonical, 13\ncase command, 327–329\ncase sensitivity\nenvironment variables, 139\nregular expressions, 538\ncase statement\ndefault case, 480\nmenu functions, 479\nmenu logic, 480–481\nprocessing options, 377, 380\nremoving account processes, \n662–663\nshell script menus, 477\nzsh, 641\ncat, 78–79, 391, 396, 397\ncd, 55–58, 127\nCDs, ejecting, 99\nCentOS, 13, 14, 19, 29, 114, 710\n/etc/profile, 152–153\nKonsole terminal, 36\nchage, 170–172\nchange command, 525–526\nchanging ownership, 181–182\ncharacter classes, 543–545\nnegating, 546\nspecial, 547–548\ncharacter device fi les, 7\ncharacter mode, 409\ncharacters\nanchor\n^ (caret character), 540–\n541\n$ (dollar sign), 541–542, \n582\ncombining, 542\nASCII ordering, 312\ndot, 542–543\nEOF, 291–292, 511\nescape, 286, 334, 539–540\nfi eld separation, 511, 592, 729\nranges, 546–547\nregular expressions, 537–549\nanchor, 540–542\nasterisk, 548–549\nbraces, 551–553\ncharacter classes, 543–545\ndot character, 542–543\nescape, 539–540\nnegating character classes, \n546\npipe symbol, 553\nplain text, 537–539\nplus sign, 551\nquestion mark, 550\nranges, 546–547\nspecial, 539–540\nspecial character classes, \n547–548\nreplacing with \nsed, 518\nshell escape, 286\nspecial, 539–540\ndot character, 542–543\ntransforming with \nsed, 527\nwildcard, 63, 535–536\naptitude search, 215\ncron tables, 442\nkillall command, 96\nreading directory using, \n339–340\nreplacement strings, \n575–576\nstring parameter \nexpansion, 724–725\nchecking fi lesystems, 198–200\nchecklist widget, 497\nchfn, 170–172\nchgrp, 181–182, 184\nchild process\nforking, 126, 127\nuser-defi ned local variables, \n140\nchild shell\nparent relationships, 115–125\nuser-defi ned variables\nglobal, 141–142\nlocal, 139–140\nchmod, 179–181, 183, 320, 677, \n693\nchown, 181–182\nchpasswd, 169–170\nchsh, 170–172\nclear command, 478, 480\nCLI. See command line interface\nclient, 25\nclone, 192\nclosing fi le descriptors, 407–408\ncombining anchors, 542\ncommand aliases, 131–132, 157, 234\ncommand grouping, 120\ncommand line interface (CLI), 23, \n48, 269\naccessing via graphical \nterminal emulation, \n28–29\naccessing via Linux console \nterminal, 25–28\nfunctions, 467–470\ncreating on, 468\ngawk, 510–511\nvariable assignment, \n599–600, 760\nLynx, 699–700\nMailx, 705\nprompt, 48–49\nsed, 506–508\nusing web from, 697–704\ncommand line mode, 237\ncommand line options, 376–386\ngetopt command, 380–384\ngetopts command, 381, \n384–386\nprocessing, 377\nwith values, 379–380\nseparating from parameters, \n378–379\nstandardizing, 387\nzsh, 632–633\ncommand line parameters, \n365–370\nbash shell, 118\ncounting, 371–373\ndash shell, 624–625\ngawk, 598–599\ngetopt command, 380–384\ngetopts command, 381, \n384–386\niterating with \nfor statement, \n373–375\nreading, 366–368\nscript name, 368–370\nshifting, 375–376\nspecial variables, 371–375\ntesting, 370\nuseradd, 166–167\nxterm, 43–44\nzsh, 632–633\ncommand modules, 632\nadd-in, 636–637\nviewing, adding, removing, \n637–638\n\n766\nIndex\nbindex.indd  12/16/2014  Page  766\ncommand substitution\nbacktick, 282\nsubshells, 279\nvariables, 277–279\ncommands. See also specific \ncommands\nbackground mode, 121–123\nexternal, 125–127\nbash shell, 742–744\nhistory, 128–131\nlists of, 119–121\nreading values from, 336–337\nshell built-in, 125–132\nbash shell, 127, 739–741\ndash shell, 628–629\nzsh, 633–638\nstructured\ngawk, 605–609\nzsh, 640–641\ncomma-separated value fi les, 361\ncommon bash commands, 741–744\nCompiz, 25\ncompound testing, 324–325\ncompress, 109\ncompressing data, 108–110, 646\nconditions, 305\nconfigure, 230–231\nconsecutive blank lines, 584–585\nconsole terminals, 24\ncontinue command, 355–357\ncontinue statement, 608\ncookies, 699\ncoproc, 124–125\nco-processes, 121, 124–125\ncopying fi les, 65–67\ncopy-on-write (COW), 192\ncopy-on-write fi lesystems, 192\ncoreutils, 9\nCOW. See copy-on-write\ncp, 65–67, 70\nCPU utilization, 94\nCREATE DATABASE name;, 686\ncreating directories, 73–74\ncreating fi les, 64–65\ncreating fi lesystems, 196–198\ncron, 438, 441–443, 721\ncron directories, 443\ncron tables, 655, 671\nbuilding, 442–443\nlooking at, 441–442\ncrontab, 442–443\ncsh, 114\n.csv fi les, 361\ncurl, 732–734\ncurly braces, 551–553\ncurses, 699\nD\ndash, 114\ndash shell, 115, 623–624\nbuilt-in commands, 628–629\ncommand line parameters, \n624–625\nenvironment variables, \n625–628\nuser-defi ned, 627\nfeatures, 624–629\npositional parameters, 627\nscripting, 629–631\narithmetic, 629–630\nfunction command, 631\ntest command, 630–631\ndata blocks, 188–189\ndata constraints, 689\ndata fi eld variables, 511–512, 592, \n603\ndata fi elds, 688\ndata fi les\narchiving, 110–111, 645–656\nconfi guration fi le, 646–648\ncreating location for, \n648–649\ndaily script, 649–652\nhourly script, 652–656\nscripting, 645–656\ncompressing, 108–110, 646\nlooping on, 350–351\nsearching, 107–108\nsorting, 102–106\nworking with, 102–111\ndata mode journaling, 189, 190\ndata types, MySQL, 689\ndatabases\ncreating, 685–686\nrelational, 688\nusing in scripts, 692–697\ndate, 269–270, 652, 655, 727\ndeb, 220\nDebian, 18\ndash shell, 623–624\npackage management, 212–220\ndeb-src, 220\nDEC. See Digital Equipment \nCorporation\ndefault exit status, 453–454\ndefault fi le permissions, 177–179\ndefault group, 322\ndefault interactive shell, 114–115\ndefault shell program, 113\ndefault system shell, 114\nDELETE, 690–691\ndelete command, sed, 521–523, \n729\ndeleting directories, 74–77\ndeleting fi les, 72–73\ndeleting lines, 584–586\ndependencies, 211\nbroken, 225–227\ndesktop environment, 11–17, \n25\nGNOME desktop, 13\nKDE, 12–13\nUnity desktop, 13–14\n/dev/hdx, 193\ndevice drivers, 7\ndevice fi les, 7\ndevice names, hard drives, 193\n/dev/sdx, 193\ndf, 100, 495\ndialog command, 484–485\nscripting, 493–495\nspecifying widget, 485\ndialog package, 484–491\noptions, 491–493\noutput, 485\nusing in script, 493–495\nwidgets, 484–485\nfselect, 490, 491\ninputbox, 487–488\nmenu, 489–490\nmsgbox, 486\ntextbox, 488–489\nyesno, 487\nDickey, Thomas, 484\nDigital Equipment Corporation \n(DEC), 41\n\n767\nIndex\nbindex.indd  12/16/2014  Page  767\ndirectories\nabsolute references, 56–57, 66\ncommon names, 54–55\ncounting fi les, 554–555\ncreating, 73–74\ncron, 443\ndeleting, 74–77\nfi le comparisons using, \n314–315\nHOME, 164–167, 271\n$HOME/bin, 271\nlisting, 59–64\nmanaging, 73–77\nparent, 74\nreading using wildcards, \n339–340\nreferencing current, 271\nrelative references, 57–59, 66\nroot, 53\ntemporar y, 413–414\n/tmp, 411\ntraversing, 55–59\n/usr/sbin, 165\nvirtual, 53, 57\nmounting media, 98\ndisk blocks, 189\ndisk space\nchecking usage with \ndu, \n101–102\nchecking with \ndf, 100\nmonitoring, 96–102\nscripting, 673–678\ndiskmail, 707\ndisplay server, 25\ndisplaying messages, scripting, \n272–273\ndistribution, 17\ndo statement, 331–332\ndocking, 21\ndone command, 358\ndone statement, 331–332\ndot operator, 271, 467, 469\ndot special character, 542–543\ndouble bracket command, 326–\n327\ndouble less-than symbol, 281\ndouble line spacing, 579–580\ndouble parentheses command, \n325–326\ndo-while statement, 608–609\ndpkg, 212, 220\npackage information, 214–215\ndu, 101–102, 673–675\ndumb terminal, 23\ndyne:bolic, 19, 21\nE\necho $BASH_SUBSHELL, 120\necho command, 127, 272–273, 406\nbackground process list, \n123–124\n-en option, 479\nenvironment variables, 137\nmenu layouts, 478–479\n-n parameter, 273\necho $my_variable, 139–140\neditors. See also \ngawk; sed editor\nemacs, 242–251\nbasics, 245–247\nbuffers, 248–249\non console, 245–250\ncopying and pasting, 247\nediting, 247\nin GUI, 250–251\ninstalling, 244\nsearching and replacing, \n248\nwindows, 249–250\nKate, 256–260\nsessions, 257\nterminal window, 257–258\nnano, 240–242\nstream, 505–506\nvim, 233–240\nbasics, 235–237\ncopying and pasting, \n238–239\nediting, 238\ninstalling, 235\nmodes, 236–237\nsearching and substituting, \n239–240\nvisual mode, 239\nelif statement, 302–304\nelse clause, 303–304, 606\ntest command, 305\nemacs, 242–251\nbasics, 245–247\nbuffers, 248–249\ncopying and pasting, 247\nediting, 247\ninstalling, 244\nsearching and replacing, 248\nusing\non console, 245–250\nin GUI, 250–251\nwindows, 249–250\ne-mail, 704–707\nSMS gateways, 734–735\ne-mail addresses, parsing, \n558–560\nempty fi les, 318–319\nEND keyword, 515, 602\nenv, 136–137, 138\nENVIRON variable, 596\nenvironment fi les, 150\nenvironment variable\nOPTARG, 384\nOPTIND, 384, 386\nPATH, 165, 271, \n359\nenvironment variables, 135–138\n$ (dollar sign), 137\narrays, 158–159\nbash shell, 144–148\ncommands, 744–749\nconventions, 139\ndefault, 744–749\ntypes, 135\nBASH_ENV, 156, 157\ncapturing database data, \n696–697\ncase sensitivity, 139\ndash shell, 625–628\nuser-defi ned, 627\necho command, 137\nin \ngawk, 596\nglobal, 136–137\nsetting, 140–142\nIFS, 350–351\nlocal, 138\nsetting, 139–140\nLS_COLORS, 59\nPAM, 151\nPATH, 148–150\npersisting, 157\n\n768\nIndex\nbindex.indd  12/16/2014  Page  768\nremoving, 142–143\nscripting, 274–275\nsubshell, 142\nglobal, 137\nsystem, 136, 150–157\ninteractive shell, 156\nlocating, 150–157\nlogin shell, 150–155\nnon-interactive shell, \n156–157\ntypes, 135\nuser-defi ned, 138–142\nEOF text string, 291–292\nbash key combination, 511\nmysql scripting, 694\nepoch time, 617\nequal sign command, 529\nERE. See POSIX Extended Regular \nExpression engine\nerror messages, 398\nescape character, 286, 334\nregular expressions, 539–540\n/etc/apt/sources.list, 219\n/etc/bash.bashrc, 153\n/etc/cron.hourly, 444\n/etc/cron.monthly, 444\n/etc/fstab, 199\n/etc/group fi le, 173\n/etc/init.d folder, 6\n/etc/inittabs fi le, 6\n/etc/login.defs, umask \nvalues, 179\n/etc/lynx.cfg, 700\n/etc/passwd, 113, 162–164, 168, \n301–302, 661\ninformation in comments \nfi eld, 170–171\nprocessing data in, 350–351\n/etc/profile, 150–154, 157\nCentOS, 152–153\nUbuntu LInux, 151\numask values, 179\n/etc/profile.d, 153–154, 157\n/etc/rcX.d folders, 6\n/etc/shadow, 164\n/etc/skel directory, 164\n/etc/yum.conf, 222\n/etc/yum.repos.d, 228\nEterm, 29\nexcuse generator, 731–737\ncreating script, 735–737\ne-mail to SMS gateways, \n734–735\nsending SMS message, 732–734\nexec, 402, 404, 406, 647, 676\nexecutable fi les, 321\nfi nding, 359–360\nexecute privilege, 651\nexit, 119, 127, 421–422\nexit command, 293–295\nEXIT signal, 423–424\nexit status\ncodes\nchecking, 292–293\ndialog widget output, 485\ntest command, 305\ndefault, 453–454\nps, 662\nexiting scripts, 292–295\nexport, 141–142, 143\nexpr command, 285–286, 629\next fi lesystem, 8, 188\next2 fi lesystem, 8, 188–189\next3 fi lesystem, 8, 190\next4 fi lesystem, 8, 190, 201\nextended fi lesystem. See ext \nfi lesystem\nextended partition, 195\nextended regular expressions, \n537, 549–554\nExtensible Markup Language \n(XML), 697\nextents, 190\nexternal commands, 125–127\nbash shell, 742–744\neztexting.com, 732\nF\nFalstad, Paul, 632\nfdisk, 193–196, 203\ncommands, 194\nt command, 203–204\nFedora Linux, 17, 29, 212\npackage management, 221\nWayland display server, 11\nfg, 435\nFHS. See Filesystem Hierarchy \nStandard\nfi eld separation character, 511, \n592, 729\nfi eld separators, internal, 337–338\nFIELDWIDTHS variable, 593–594\nfile, 77–78\nfi le descriptor, 395\nclosing, 407–408\ncreating input, 405–406\ncreating output, 403–404\nlisting open, 408–410\nread/write, 406–407\nredirecting, 404–405\nfi le descriptors, standard, 395–\n398\nfi le globbing, 63, 339\nfi le paths, 53\nfi le permissions, 161, 175–179\nchanging, 179–181\ncodes, 177–179\ndefault, 177–179\nOctal mode, 178\nsymbols, 176–177\nfi le redirection, 416, 694\nfi les\naccount, 664–665\ncomparisons\nchecking directories, \n314–315\nchecking for fi le, 316–317\nchecking for object \nexistence, 315–316\ndate, 322–324\ndefault group, 322\nexecute permission, 321\nownership, 321–322\nread access, 317–318\nwrite permission, 319–320\ncopying, 65–67\ncreating, 64–65\ndata, working with, 102–111\ndate, 322–324\ndefault group, 322\ndeleting, 72–73\nexecutable, fi nding, 359–360\nhandling, 64–73\nhidden, 60\nlinking, 68–70\nlisting, 59–64\nbasic, 59–61\nfi ltering output, 62–64\nenvironment variables (cont inued)\n\n769\nIndex\nbindex.indd  12/16/2014  Page  769\nlong, 61–62\nlooping on data, 350–351\nnumbering lines in, 581–582\nownership, 321–322\nchanging, 181–182\nprinting last lines, 582–584\nreading editor commands \nfrom, 508–509, 755–756\nreading \ngawk scripts from, \n513–514, 757\nredirecting loop output to, 358\nremoving, 72–73\nrenaming, 70–72\nscript, creating, 270–272\nsed using, 530–533\nreading, 531–533\nwriting, 530–531\nsharing, 182–184\nspacing with double lines, \n579–581\nstartup, 445\ntemporar y, 411–414\nviewing contents, 77–83\nfi le type, 77–78\nparts of fi les, 81–83\nwhole fi le, 78–81\nFilesystem Hierarchy Standard \n(FHS), 55\nfi lesystems, 8, 187\nbasic, 188–189\nchecking and repairing, \n198–200\nchecking available types, \n196–197\nchecking disk space usage, \n100\ncopy-on-write, 192\ncreating, 196–198, 208–209\njournaling, 189–192, 197\nmethods, 189–190\nmanagement, 8–9\nmounting media, 97\nmounting new, 197–198\nnavigating, 52–59\nabsolute directory \nreferences, 56–57\nrelative directory \nreferences, 57–59\ntraversing directories, \n55–59\npartitions\ncreating, 193–196\nextended, 195\nprimary, 195\nworking with, 192–200\nFinal Term, 29\nfind, 665\nfi nding executable fi les, 359–360\nfinger, 170, 171\nfl ags, 752\nfl oating-point math, 288–292\nFluxbox, 16\nFNR variable, 596–597\nfolding marker, 252\nfor command, 331–340\nchanging fi eld separator, \n337–338\nC-style, 341–343\nmultiple variables, 342–343\npiping output, 358–359\nreading complex values in list, \n333–335\nreading directory using \nwildcards, 339–340\nreading list from variable, \n335–336\nreading values from command, \n336–337\nreading values in list, 332–333\nredirecting output to fi le, 358\nfor loops, 640–641\nnested, 347–348\nfor statement, 609, 620\n/etc/profile, 153\niterating parameters, 373–375\nforking, 126, 127\nformat specifi ers, 610\ncontrol letters, 610–611\nmodifi ers, 611\nformat string, 610\nformatted numbers, 544–545\nformatted printing, 610–613\nfragmentation, 189\nFreeBSD, 623\nFS variable, 592, 729\nfsck, 199–200\nfselect widget, 490, 491\nfunction command, 631\nfunction key word, 450, 617–618\nfunction() statement, 631\nfunctions\narray variables, 461–464\npassing, 463–464\nreturning, 463–464\ncommand line usage, 467–470\ncreating, 450\ncreating on command line, 468\ndash shell, 631\ndefault exit status, 453–454\ndefi ning in \n.bashrc fi le, \n468–470\ngawk\nbuilt-in, 613–617\ncreating library, 619–620\ndefi ning, 617–618\nmathematical, 613–614\nstr ing, 615–616\ntime, 616–617\nuser-defi ned, 619–620\nusing, 618–619\nget account name, 658–660\nlibraries, 465–467\nparameter passing to, 456–459\nrecursion, 464–465\nreturn command, 454–455\nreturning values, 453–456\nscope, 466\nstub, 479\nusing, 450–452\nusing output, 455–456\nvariables in, 456–461\nglobal, 459–460\nlocal, 460–461\nparameter passing, \n456–459\nzsh, 641–642\nmathematical, 640\nFvwm, 16\nfvwm95, 16\nG\ngawk, 509–516, 536, 727, 730, \n756–762\narrays, 600–602\ndeleting, 601–602\niteration through, 601–602\nvariable assignment, \n600–601\nbitwise manipulation, 614\n\n770\nIndex\nbindex.indd  12/16/2014  Page  770\ncommand format, 510, 756\ndata fi eld var iables, 511–512, \n759\nenvironment variables, 596\nfeatures, 760–762\nformatted printing, 610–613\nfunctions\nbuilt-in, 613–617\ncreating library, 619–620\ndefi ning, 617–618\nmathematical, 613–614\nstr ing, 615–616\ntime, 616–617\nuser-defi ned, 617–620\nusing, 618–619\ninstalling, 713\nintervals, 552\nmatching operator, 761\nmathematical expressions, 761\nmultiple commands in program \nscr ipt, 512–513, 757\noptions, 756\npattern matching, 602–605\nmatching operator, 603–\n604\nmathematical expressions, \n604–605\nregular expressions, 603, \n761\nprint command, 593\nreading program fi le, 513–514, \n757\nreading script from command \nl ine, 510–511, 757\nrecord variables, 759\nrunning scripts\nafter processing data, \n514–515, 758\nbefore processing data, \n514, 757–758\nstructured commands, 605–\n609, 761–762\ndo-while statement, \n608–609\nif statement, 605–607\nfor statement, 609\nwhile statement, 607–608\nusing, 757\nvar iables, 511–512, 591–600\nARGC, 595–596\nARGV, 595–596\nassigning in scripts, \n598–599\nassigning on command \nline, 599–600, \n760\nbuilt-in, 592–598, 758\ndata, 595–597\ndata fi eld, 592–595, 603\nENVIRON, 596\nfi eld and record separator, \n592–595\nFIELDWIDTHS, 593–594\nFNR, 596–597\nFS, 592\nNF, 596–597\nNR, 596–597\nOFS, 592–593\nORS, 594\nRS, 594\nuser-defi ned, 598–600\ngdialog, 500\ngedit, 260–265\nbasic features, 262\nplug-ins, 264–265\npreferences, 262–265\nGentoo, 18\ngetopt command, 380–384\ncommand format, 381\nusing in scripts, 382–384\ngetopts command, 381, 384–386\nGID. See group ID\nglobal environment variables, \n136–137\nsetting, 140–142\nsubshell, 137\nglobal variables, 135\nin functions, 459–460\nGNOME desktop, 13, 29\nGNOME editor. See gedit\nGNOME graphical environment, \n500–504\nGNOME Terminal, 29–35\naccessing, 30–31\nmenu bar, 31–35\ngnome-terminal, 31\nGNU, 3, 9–11\ngzip, 109\nvim, 233\nGNU bash shell, 113–114\nGNU Network Object Model \nEnvironment. See \nGNOME desktop\nGNU utilities, 9–11\ngrant command, 687\ngraphical interfaces, elements, 25\ngraphical terminals, 24–25, 28–29\ngrep, 107–108, 299, 727–728\ngroup ID (GID), 172\nchanging, 175\nsystem accounts, 173\ngroupadd, 174\ngrouping commands, 520–521\ngrouping expressions, 553–554\ngroupmod, 175\ngroups, 172–175, 648–649\nchanging fi le ownership, \n181–182\ncreating, 174\ndefault, 322\n/etc/group fi le, 173\nmodifying, 175\npasswords, 173\nsystem accounts, 173\nGuake, 29\ngunzip, 109\ngzcat, 109\ngzip, 109–110\nH\nhard drives, device names, 193\nhard links, 68–70\nhardware management, 7–8\nhdparm, 196\nhead, 82–83\nhidden fi les, 60\nhistory command, 128–131\nhold space, 567–569\n$HOME, 275, 316–317\nHOME directory, 164–167, 271\n$HOME startup fi les, 154–155\n$HOME/.bash_login, 154–155\n$HOME/.bash_profile, 154–\n155\n$HOME/.bashrc, 157\n$HOME/bin directory, 271\n$HOME/.my.cnf fi le, 692–693\n$HOME/.profile, 154–155\nhpfs fi lesystem, 8\nHTML content, 699\nHTML tags, 727\nremoving, 586–588\nHTTP headers, 699\nHuffman coding, 109\ngawk (cont inued)\n\n771\nIndex\nbindex.indd  12/16/2014  Page  771\nI\nIBM, 191\nIDE drives, 193\nif statement, 605–607\narchiving data fi les script, \n647–648\ngawk, 605–607\nnested, 301–304\nIFS, 337–338\nIFS environment variable, \n350–351\nif-then statements, 574\ncompound testing, 324–325\ncontinue command, \n355–357\nas \ntest command alternative, \n306\nworking with, 297–300\nif-then-else statements, \n300–301, 640–641\nincremental search, 248\ninit process, 6\ninit run levels, 6\ninline input redirection, 280–281, \n291\ninner loop, 348\nbreaking out, 353–354\ninode numbers, 69–70, 188\ninode table, 188, 189\ninodes, 188\njournaling, 189\ninput redirection, 291\ninline, 280–281, 291\nscripting, 280–281, 402–403\ninputbox widget, 487–488\nINSERT, 690, 695–696\ninsert command, 754\ninsert command, 523–525\ninsert mode, 236–237\ninserting text, 523–525, 577\ninstalling software\naptitude, 215–217\nlocal, 224\nint(), 613–614\ninteractive shell, 156\ninternal fi eld separator, 337–338\ninterprocess communication, 95\ninterrupting processes, 420–421\nintervals, 551–553\nIRIX Unix, 191\niso9660 fi lesystem, 8, 98\nJ\nJFS. See Journaled File System\njfs fi lesystem, 8\njob control, 432\njob number, 421\njob queue, 439\njobs\nbackground, 429–430\ncontrolling, 432–435\nrestarting, 434–435\nscheduling, 438–441\nlisting pending, 440–441\noutput, 439–440\nremoving, 441\nstarted, 421\nstopped, 421–422\nviewing, 432–434\njobs, 122, 432–434\nJournaled File System (JFS), 191\njournaling fi lesystems, 189–192, \n197\nmethods, 189–190\nJWM, 16\nK\nK Desktop Environment (KDE). \nSee KDE\nKate editor, 256–260\nsessions, 257\nterminal window, 257–258\nKDE, 12–13, 35. See also Konsole \nTerminal\neditors, 251–260\nKate, 256–260\nKWrite, 251–256\nkdialog, 496–499\nusing, 498–499\nwidgets, 496–497\nkernel, 4–9\nfi lesystem management, 8–9\nhardware management, 7–8\nsoftware program \nmanagement, 6–7\nsystem memory management, \n5–6\nKickoff Application \nLauncher\n, 35\nkill, 95–96, 432, 434, 664\nkillall, 96\nKnoppix, 20\nKonsole Terminal, 29, 30\naccessing, 35–36\nmenu bar, 37–41\nkorn shell, 10\nKwin, 25\nKWrite, 251–256\nedit menu, 253–254\nFind feature, 254–255\ntools, 255\nkwrite command, 252–253\nL\nlabel parameter, 572–573\nLam, Savio, 484\nLAMP. See Linux-Apache-MySQL-\nPHP\nleading blank lines, 585\nLempel-Ziv coding, 109\nless, 80–81\nlet command, 639\nlibraries, 465–467\nfunction scope, 466\nsourcing, 467, 469\nLillyTerm, 29\nline addressing, 518–521, 752\ngrouping, 520–521\nnumeric, 519–520\ntext pattern fi lters, 520\nline numbers, 529, 581–582\nlines\nlisting, 529–530\nprinting, 528–529\nprinting numbers, 529\nlinked fi les, 234\nlinking fi les, 68–70\nlinks, 68–70\nsymbolic, 77\nLinux, 3\ncommon directory names, \n54–55\ndesktop environment, 11–17\ndistributions, 17–21\nfi le structure, 54\nGNU utilities, 9–11\nkernel, 4–9\nfi lesystem management, \n8–9\nhardware management, \n7–8\nsoftware program \nmanagement, 6–7\n\n772\nIndex\nbindex.indd  12/16/2014  Page  772\nsystem memory \nmanagement, 5–6\nprocess signals, 95\nsignals, 420\nLinux console, 24\nCLI access, 25–28\nLinux LiveCD, 19–21\nLinux Mint, 212, 220\nLinux-Apache-MySQL-PHP \n(LAMP), 681\nLISP, 249\nlist parameter, 482\nlisting lines, 529–530\nlists\nreading complex values in, \n333–335\nreading from variable, 335–336\nreading values in, 332–333\nLiveCD, 3, 19–21\nln, 69\nload average, 93, 94\nlocal environment variables, 138\nsetting, 139–140\nlocal installation, 224\nlocal keyword, 460–461\nlocal variables, 135\nin functions, 460–461\nlog fi les, timestamps, 104\nlogging, scripting, 414–415\nlogging in, 26–27\nlogical partitions, 200\nlogical volume (LV), 200–201\nchanging size, 209\ncreating, 206–207\nLVM2 features for, 202–203\nmanaging, 200–209\nlayout, 200–201\nwith LVM, 203–209\nLogical Volume Manager (LVM), \n200, 201–209\ncommands, 209\nmirroring, 202–203\nmodifying, 209\nsnapshots, 202\nstriping, 202\nusing, 203–209\ncreating fi lesystem, \n208–209\ncreating logical volumes, \n206–207\ncreating volume groups, \n205–206\ndefi ning physical volumes, \n203–205\nversions, 202\nlogin name, 161\nlogin program, 163\nlogin shell, 150–155\nloops\nfor, 640–641\nnested, 347–348\ncontrolling, 351–357\nbreak command, 352–354\ncontinue command, \n355–357\non fi le data, 350–351\ninner, 348\nbreaking out, 353–354\nnested, 347–350\nouter, breaking out, 354\nprocessing output, 358–359\nuntil, 641\nnested, 349–350\nwhile, 378, 640–641\narchiving data fi les script, \n647\nlong messages, 717\nmenu dialog, 495\nnested, 348–350\nreading fi les, 391–392\ntext menus, 482\nls, 59–64\nbasic listing, 59–61\n-F parameter, 59–60\nfi ltering output, 62–64\n-i parameter, 69\ninode numbers, 69\n-l parameter, 61–62\n-li parameter, 70\nlong listing, 61–62\n-R parameter, 60–61\nls -l command, 285\nLS_COLORS environment \nvariable, 59\nlsof, 408–410\nLV. See logical volume\nlvcreate, 206–207, 208\nlvdisplay, 207\nlvextend, 209\nLVM. See Logical Volume Manager\nlvreduce, 209\nLXTerminal, 29\nLynx, 697–704\ncapturing data from, 701–704\ncommand line, 699–700\nconfi guration fi le, 700–701\n-dump option, 701–702\ninstalling, 698–699\nscripting, 699–700\nlynx command, 699–700\nlynx.cfg, 700–701\nM\nmail, 706\nmailutils, 704, 735\nMailx, 704–707\ncommand line parameters, 705\nmake, 231–232\nman, 49–52, 197\nman bash, 118\nman -k keyword, 51\nman pages, 49–52\naccessing, 49–50\nfi lesystem commands, 197\nkeywords, 51\nsection areas, 51\nsection names, 50\nman xterm, 49, 52\nmanaging directories, 73–77\nmanaging packages, \naptitude, \n212–215\nMandriva, 212\npackage management, 221\nmatching operator, 603–604, 761\nmath\nbrackets, 287–288\nexpr command, 285–286\nfl oating-point, 288–292\ngawk\nbitwise manipulation, 614\nbuilt-in functions, 613–614\npattern matching, 604–605\nscripting, 285–292\nmathematical comparisons, \n325–326\nmathematical functions, 640\nMauelshagen, Heinz, 201\nmedia\nmounting, 97–99\nremovable, 97, 98\nunmounting, 99–100\nLinux (cont inued)\n\n773\nIndex\nbindex.indd  12/16/2014  Page  773\nurpm repositories, 228\nuser access, 98\nmeminfo, 495\nmemory management, 5–6\npages, 6\nswap space, 5–6\nmenu scripts, 477\nmenu widget, 489–490\nmesg, 710–711\nmessages. See also Short Message \nService\nallowing, 710–711\nchecking if included, 715\nchecking if users accepting, \n714–715\ndisplaying from scripts, \n272–273\nerror, 398\nscript for, 712–720\nsending, 709–720\nSMS, 732\ntransmitting\nlong, 716–720\nsimple, 715–716\nmetacharacter wildcards, 63, 67\nMetacity, 25\nMicrosoft Windows, 8, 11\nminix fi lesystem, 8\nMint, 19, 212, 220\nMir display server, 11, 25\nmirroring, 202–203\nmkdir, 73–74, 198, 654\nmke2fs, 196\nmkefs, 196\nmkfs.btrfs, 196, 197\nmkfs.ext3, 196\nmkfs.ext4, 196, 197, 208\nmkfs.xfs, 196\nmkfs.zfs, 196\nmkreiserfs, 196\nmktemp, 411–413, 495\nmktemp -t, 413\nmonitoring\ndisk space, 96–102\nscripting, 673–678\nprograms, 85–96\nreal-time process monitoring, \n92–95\nmore, 79–80\nmount, 97–99, 198, 208\nmount points, 53\nmounting, 97–100\nautomatic, 97\nmanual, 97–99\nnew fi lesystems, 197–198\nunmounting, 99–100\nvirtual directories, 98\nmoving fi les, 70–72\nmrxvt, 29\nmsdos fi lesystem, 8\nmsgbox widget, 486\nmultiline commands, 561–567\ndelete, 566–567\nnext, 562–565\nprint, 567\nmultiple background jobs, \n429–430\nmultiple commands, scripting, \n269–270\nmulti-word values, 334–335\nmv, 70–72\nmysql, 692\ndefault output style, 695\n-e parameter, 693–694\nEOF text in scripting, 694\nredirecting output, 696\nMySQL, 681–697\nconnecting to server, \n682–683\ncreating database, 685–686\ncreating table, 688–690\ncreating user account, \n687–688\ndata types, 689\ndefault password, 692–693\nformatting data, 696–697\ninserting and deleting data, \n690–691\ninstalling in Ubuntu, 682\nquerying data, 691–692\nstartup commands, 692–693\nusing in scripts, 692–697\nmysql client, 682–683\ncommands, 683–685\nmysql-client package, 214\nmysql-server package, 682\nN\n$n, 511\nn command, 562–563\nnano, 240–242\nncp fi lesystem, 8\nnegating character classes, 546\nnested loops, 347–350\nnetwork browser, 25\nnetwork device fi les, 7\nnew line, 754\nnext command, 562–565, 729\nmulti-line, 563–565\nsingle-line, 562–563\nNF variable, 596–597\nnfs fi lesystem, 8\nnice, 436–437\nnodes, 8\nnohup, 430–431\nnohup.out, 431\nnon-incremental search, 248\nnon-interactive shell, 156–157\nnormal mode, 236–237\nNR variable, 596–597\nntfs fi lesystem, 8, 98\nnumber manipulation. See math\nnumbering lines, 581–582\nnumeric comparisons, 307–308\nnumeric line addressing, \n519–520\nO\nOctal mode, 178\nOFS variable, 592–593\nopen source software (OSS), 9\nOpenSolaris, 192\nopenSUSE, 12, 18, 192, 212\npackage management, 221\nOPTARG environment variable, \n384\nOPTIND environment variable, \n384, 386\noptions. See command line \noptions\nordered mode journaling, \n189, 190\nORS variable, 594\nOSS. See open source software\nouter loop, breaking out, 354\noutput redirection, 397–398, \n399–400\npermanent, 401–402\nscripting, 279–280, 400–402\ntemporary, 400–401\noutput suppression, 410–411\n\n774\nIndex\nbindex.indd  12/16/2014  Page  774\nP\nPackage Management System \n(PMS), 211–212\naptitude, 212–220\ninstalling software \npackages, 215–217\nmain window, 213\nmanaging packages, \n212–215\nrepositories, 219–220\nuninstalling software, \n218–219\nupdating software, 217–218\nurpm, 221\nbroken dependencies, 227\ninstalling software, 224\nlisting installed software, \n221\npackage details, 222\nrepositories, 228\nuninstalling software, 225\nupdating software, 225\nyum, 221–228\nbroken dependencies, \n225–227\ninstalling software, \n223–224\nlisting installed packages, \n221–223\nrepositories, 227–228\nuninstalling software, 225\nupdating software, 224–225\nzypper, 221\nbroken dependencies, 227\ninstalling software, 224\nlisting installed software, \n221\npackage details, 222\nrepositories, 228\nuninstalling software, 225\nupdating software, 225\npager, 50\npages, 6\nPAM. See Pluggable \nAuthentication Modules\nparameters. See also command \nline parameters\ndash shell\ncommand line, 624–625\ncommand-line, 624–625\npositional, 627\nlist, 482\nls, 59–62, 69, 70\nmysql, 693–694\npassing arrays as, 462\npassing to function, 456–459\npositional, 366, 627\nps, 86–92, 116, 117\nBSD-style, 89–91\nGNU long, 91–92\nUnix-style, 86–89\nrm, 73, 76\ntesting, 370\nuseradd\nchange default values, 167\ncommand line, 166–167\n-D, 165, 167\nparent directories, 74\nparent process ID (PPID), 116\nparent shell, child relationships, \n115–125\nparity entry, 202\npartitions\nchanging type, 203–204\ncreating, 193–196\nextended, 195\nprimary, 195\npartprobe, 196\npasswd, 169–170\npasswords\naging, 171–172\nchanging, 169–170\ngroup, 173\nMySQL, 692–693\nreading, 391\nstorage of, 163\nPATH environment variable, 148–\n150, 165, 271, 359\npattern matching\ndouble bracket command, \n326–327\nfi ltering fi le listings, 63–64\ngawk, 602–605\nmatching operator, 603–\n604\nmathematical expressions, \n604–605\nregular expressions, 603, \n761\ngrep, 107–108\nsed, 520, 526\npattern space, 563, 567, 570–571\n/pattern/command, 753\npausing processes, 421–422\nPCLinuxOS, 19\nLiveCD, 20\npermanent redirection, 401–402\npermissions, 651\naccess triplets, 176–177\nfi le, 175–179\nchanging, 179–181\ncodes, 177–179\ncomparisons by, 319–321\ndefault, 177–179\nOctal mode, 178\nsymbols, 176–177\ngroup, 172\nphone numbers, validating, \n556–558\nphysical volumes (PV), 200\ndefi ning, 203–205\nPID. See process ID\npipe character, 106, 553\npipes, 121\nloop output, 358–359\nreading from fi les, 391\nscripting, 281–284\ntee command, 414–415\nPKZIP, 109\nPluggable Authentication Modules \n(PAM), 151\nplus sign, 551\nPMS. See Package Management \nSystem\npositional parameters, 366, 627\nPOSIX Basic Regular Expression \nengine (BRE), 537\nPOSIX Extended Regular \nExpression engine \n(ERE), 537\nPostfi x, 704\nPPID. See parent process ID\npresent working directory, 56–57\nprimary key, 689\nprimary partition, 195\nprint command, 593\nprintenv, 136, 138, 156\nprintf command, 610–613, \n639, 675\n\n775\nIndex\nbindex.indd  12/16/2014  Page  775\nprinting\nformatted, 610–613\nlast lines, 582–584\nlines, 755\npriority, 436\nproc fi lesystem, 8\nprocess, 116\nbackground, 123–124\nco-processes, 121, 124–125\nexamining, 85–92\nforking, 126, 127\ninit, 6\ninterrupting, 420–421\npausing, 421–422\nreal-time monitoring, 92–95\nremoving for account, 662–664\nsignals, 95\nstate code, 91\nstopping, 95–96\nuser-defi ned local variables, \n140\nprocess ID (PID), 86\nbackground mode scripts, 428\njobs, 122\nkill command, 95\nopen fi le descriptors, 409\nprocess lists, 119–121\nbackground, 123–124\nco-processing, 124–125\n.profile fi le, 445\nprofile.d, 153–154\nprograms\ndefault shell, 113\ngawk scripts\nmultiple commands, 512–\n513, 757\nreading fi le, 513–514, 757\nlogin, 163\nmanagement, 6–7\nmonitoring, 85–96\nproxy servers, 701\nps, 85–92, 126\nbackground mode commands, \n122\nBSD-style parameters, 89–91\n-ef parameter combination, \n88\nexit status, 662\n-f parameter, 116, 117\n—forest parameter, 117\nGNU long parameters, 91–92\nUnix-style parameters, 86–89\nps —forest, 129\npsql, 696\nPuppy Linux, 19\nLiveCD, 20\nPuppy Linux antiX, 16\nPV. See physical volume\npvcreate, 204\npvdisplay, 204–205\npwd, 56–57, 127, 298\nQ\n-qa parameters, 282\nquestion mark, 550\nR\nradiolist widget, 497\nRAID striping, 202\nrand(), 614\nrange address, 584\nranges, 546–547\nread access, 317–318\nread command, 388–389, 403, \n406, 647, 657\nfrom fi le, 391–392\nmenu functions, 479, 480\nwith no display, 391\nsed, 531–533\ntiming out, 389–391\nreading from fi le\ngawk, 513–514, 757\npipes, 391\nsed, 508–509, 755–756\nwhile loop, 391–392\nread-write snapshots, 202\nreal-time process monitoring, \n92–95\nrecords, 688–690\nrecovery commands, 198\nrecursion, 464–465\nRed Hat Linux, 13, 17. See also \nRHEL\npackage management, \n221–228\nRed Hat Package Management \nsystem (RPM), 282\nredirection\ncreating, 403–408\nerrors, 398–400\nfi le, 416, 694\nloop output, 358\nfi le descriptors, 404–405\ninput, 291, 402–403\ninline, 280–281, 291\nscripting, 280–281\nmysql output, 696\noutput, 279–280, 397–398\npermanent, 401–402\nin scripts, 400–402\nsed scripts, 578–579\ntemporary, 400–401\noutput and errors, 399–400\nregular expression engine, 537\nregular expressions, 108, 520\nBRE patterns, 537–549\nbuilding, 556\ncase sensitivity, 538\ncharacters\nanchor, 540–542\nasterisk, 548–549\nbraces, 551–553\ncharacter classes, \n543–545\ndot character, 542–543\nescape, 539–540\nnegating character classes, \n546\npipe symbol, 553\nplain text, 537–539\nplus sign, 551\nquestion mark, 550\nranges, 546–547\nspecial, 539–540\nspecial character classes, \n547–548\nspecial characters, \n539–540\ncombining anchors, 542\ncounting directory fi les, \n554–555\ndefi ning, 535–536\ngawk, 603, 761\ngrouping expressions, \n553–554\n\n776\nIndex\nbindex.indd  12/16/2014  Page  776\nintervals, 551–553\nparsing e-mail addresses, \n558–560\ntypes, 536–537\nvalidating phone numbers, \n556–558\nwhitespace, 539\nReiser, Hans, 190\nReiser4 fi lesystem, 191, 192\nReiserFS fi lesystem, 8, 190–191\nrelational database, 688\nrelative directory references, \n57–59, 66\nremovable media, 97, 98\nremoving fi les, 72–73\nremoving HTML tags, 586–588\nrenaming fi les, 70–72\nrenice, 437\nrepairing fi lesystems, 198–200\nrepeat command, 641\nreplacement strings, 575–576\nreplace-string, 248\nrepositories, 211\naptitude, 219–220\nurpm, 228\nyum, 227–228\nzypper, 228\nrestarting jobs, 434–435\nreturn command, 454–455\nRHEL, 29, 191, 192\nrjs_mkfs, 196\nrm, 72–73\n-f option, 76\n-i parameter, 73\n-r option, 75–76\n-R parameter, 76\nrmdir, 74–75\nrolling window, 582–583\nroot directory, 53\nroot drive, 53\nroot MySQL account, 687\nroot user account, 163\nchanging fi le ownership, 182\n/etc/shadow fi le, 164\nmounting media, 97\nROXTerm, 29\nrpm, 212, 281–283\nRPM. See Red Hat Package \nManagement system\nrpmfusion.org, 228\nrpm.list, 282\nRS variable, 594\nrun level, 6–7\nrun-parts, 444\nrxvt, 29\nrxvt-unicode, 29\nS\ns command, 563–564\nSakura, 29\nSATA drives, 193\nscale, 289\nscheduling\njobs, 438–441\nregular scripts, 441–445\nscheduling priority, 436\nscope, function, 466\nscreen scraping, 701–704\nscript exits, trapping, 423–424\nscript name, reading, 368–370\nscripting\narchiving data fi les, 645–656\nconfi guration fi le, 646–648\ncreating daily archive \nlocation, 648–649\ndaily archive script, \n649–652\nhourly archive script, \n652–656\nbackground mode, 427–429\nbc, 289–292\ncomment line, 270\ncreating fi le, 270–272\ncreating multiple user \naccounts, 361–362\ndash shell, 629–631\narithmetic, 629–630\nfunction command, 631\ntest command, 630–631\ndialog command in, 493–495\ndisplaying messages, 272–273\nexcuse generator, 731–737\ncreating script, 735–737\ne-mail to SMS gateways, \n734–735\nsending SMS message, \n732–734\nexiting, 292–295\nfi le descriptors\nclosing, 407–408\nlisting open, 408–410\nredirection, 403–408\nfi nding executable fi les, \n359–360\nfl oating-point math, 288–292\nfunctions, 449–452\narray variables, 461–464\ncommand line usage, \n467–470\ncreating, 450\ncreating on command line, \n468\ndefault exit status, 453–\n454\ndefi ning in \n.bashrc fi le, \n468–470\nglobal variables, 459–460\nlibraries, 465–467\nlocal variables, 460–461\nparameter passing to, \n456–459\npassing arrays, 461–463\nreturn command, 454–455\nreturning arrays, 463–464\nreturning values, 453–456\nscope, 466\nusing, 450–452\nusing output, 455–456\nvariables in, 456–461\ngawk, assigning in scripts, \n598–599\ngetopt command in, 382–384\ngetting quotes, 720–731\nchecking web addresses, \n724–726\nparsing out information, \n727–731\ntesting web addresses, \n723–724\nweb page information, 726\ninput redirection, 280–281, \n402–403\nlogging, 414–415\nLynx, 699–700\nmanaging user accounts, \n656–673\ncreating script for, 665–671\ndetermining existence, \n661–662\nfi nding fi les, 664–665\nregular expressions (cont inued)\n\n777\nIndex\nbindex.indd  12/16/2014  Page  777\ngetting name, 657–660\nremoving account, 665\nremoving processes, \n662–664\nrunning script for, 671–673\nverifying name, 660–661\nmonitoring disk space, \n673–678\nmultiple commands, 269–270\noptions, 376–386\nprocessing, 377\nprocessing with values, \n379–380\nseparating from \nparameters, 378–379\noutput redirection, 279–280, \n400–402\nparameters, 365–370\ncommand line, 365–370\ncounting, 371–373\nreading, 366–368\nscript name, 368–370\nshifting, 375–376\nspecial variables, 371–375\ntesting, 370\nperforming math, 285–292\npipes, 281–284\nredirecting input and output, \n279–281\nrunning without hang-up, \n430–431\nscheduling, 441–445\nsed commands in, 577–579\nredirecting output, \n578–579\nwrappers, 578\nstarting with new shell, 445\nsuppressing output, 410–411\ntemporary fi les, 411–414\ntext strings, 272\nuser input, 388–392\nusing database in, 692–697\nformatting data, 696–697\nlogging in, 692–693\nsending commands, \n693–696\nvariables, 274–278\nenvironment, 274–275\nuser, 275–277\nzenity in, 501–503\nzsh, 638–642\nscrollback buffer, 39\nSCSI drives, 193\nsearching data, 107–108\nsecurity settings\nchanging, 179–182\nOctal mode, 178\nsed editor, 505–509, 536, 675, \n727, 751–756\n&, 576\naddress pattern, 574\naddresses, 518–521\nbranch command, 572–574\nbranching, 572–574\nchanging lines, 525–526\ncharacters\nreplacing, 518\ntransforming, 527\ncommand options, 506, 751\ncommands\naddressing, 752–753\nchanging lines, 754–755\ndefi ning, 506–507\ndeleting lines, 753–754\ninserting and appending \ntext, 523–525, 754\nprinting lines, 755\nreading from fi le, 508–509, \n755–756\nsubstitution, 752\ntransform command, 755\nwriting to fi le, 755\nd command, 566–567\ndeleting lines, 521–523\nfi les, 530–533\nreading, 531–533\nwriting, 530–531\nG command, 579–581\nhold space commands, \n567–569\ninserting and appending text, \n523–525, 754\ninserting text, 577\nlabel parameter, 572–573\nline addressing, 752\nmultiline commands, 561–567\ndelete, 566–567, 729\nnext, 562–565, 729\nprint, 567\nmultiple commands, 507–508\nn command, 562–563\nN command, 563–565\nnegating commands, 569–572\noptions script fi le, 751\nP command, 566–567\nparsing web data, 703\nprinting, 517, 527–530\nline numbers, 529\nlines, 528–529\nlisting lines, 529–530\nreplacement via pattern, \n575–577\nreplacing individual words, \n576–577\ns command, 507, 563–565\nscript fl ow, 572–575\nin scripts, 577–579\nredirecting output, \n578–579\nshell script wrappers, 578\nstarting, 751–756\nsubstitution fl ags, 516–518\ntesting, 574–575\ntext strings, 537–538\nutilities, 579–588\ndeleting consecutive blank \nlines, 584–585\ndeleting leading blank \nlines, 585\ndeleting lines, 584–586\ndeleting trailing blank \nlines, 586\nnumbering lines, 581–582\nprinting last lines, 582–\n584\nremoving HTML tags, \n586–588\nspacing fi les with blanks, \n580–581\nspacing with double lines, \n579–580\nselect command, 482–483, 641\nSELECT command, 691–692\nself-containment, 464\nsemicolon, 269, 270\nsendhub.com, 732\nsendmail, 704\nserial cable, 23\nset, 138, 382, 626\nset group ID (SGID), 183–184\nset user ID (SUID), 183\nsetterm, 27–28\noptions, 28\n\n778\nIndex\nbindex.indd  12/16/2014  Page  778\nsetterm -background white, \n27\nsetterm -foreground black, \n27\nsetterm -inversescreen on, \n27\nSGI. See Silicon Graphics \nIncorporated\nSGID. See set group ID\nshadow fi le, 163\nsharing fi les, 182–184\nshell, 10–11\nbuilt-in commands, 125–132\nfunction scope, 466\ninteractive, 156\nnon-interactive, 156–157\nparent and child relationships, \n115–125\nstarting scripts with new, 445\ntypes, 113–115\nshell escape character, 286\nshell prompt, using, 48–49\nshell script, 269–270\ninteractive, 477\nshell script wrappers, 578\nshell scripts, 10, 156\nmulti-processing, 121\nshift, 375–376, 378, 716–717\nShort Message Service (SMS), 732\nSHOW command, 684–685\nSIGCONT, 432\nSIGHUP, 420, 431, 434\nSIGINT, 420–421, 422–423, 426\nSIGKILL, 422\nsignaling, 127\nbash shell, 419–420\nsignals, 127\nEXIT, 423–424\ngenerating, 420–422\nLinux, 420\nprocess, 95\nTERM, 95\ntrapping, 422–423\nSIGQUIT, 420\nSIGTERM, 420\nSIGTSTP, 421\nSilicon Graphics Incorporated \n(SGI), 191\nsingle quotation marks, 333–334\nsingle-user mode, 6\nSlackware, 17, 20\nSlax, 20\nsleep, 121–122, 421\nbackground process list, \n123–124\nsmb fi lesystem, 8\nSMS. See Short Message Service\nSMS gateways, 734\nsnapshots, 202, 203\nsoftware, installing\npackage management, 211–228\nfrom source code, 228–232\nsoftware program management, \n6–7\nsort, 102–106, 283, 674–675\nsorting data, 102–106\nsource command, 467, 469\nsourcing function fi les, 469–470\nspacing fi les with blanks, 580–\n581\ns/pattern/replacement/\nflags\n, \n752\nspecial character classes, 547–548\nspecial characters, 539–540\ndot character, 542–543\nsplit, 616\nSQL. See Structured Query \nLanguage\nsquare brackets, 543–545\nst, 29\nstandard fi le descriptors, 395–398\nstandard input, 388–389\nstartup fi les, 150, 154, 445\n$HOME, 154–155\nSTDERR, 398\ndialog widget output, 485\nredirecting, 398–400, 485\nredirection\npermanent, 401–402\ntemporary, 400–401\nSTDIN, 396–397\ngawk input, 510\nredirecting, 402–403\nsed input, 506\nSTDOUT, 397–398\ngawk output, 510\nredirection\nalternative fi le descriptor, \n404–405\npermanent, 401–402\ntemporary, 400–401\nsed output, 506, 507\nsticky bit, 178, 183\nstorage devices, 53\nstream editor, 505\nstrftime, 617\nstring comparisons, 308–313\nequality, 309–310\norder, 310–312\nsize, 312–313\nstring delimiters, 518\nstring parameter expansion, \n724–725\nstrings\nEOF text, 291–292\nbash key combination, 511\nmysql scripting, 694\nformat, 610\ngawk f unctions, 615–616\nreplacement, 575–576\nscripting, 272\nsed, 537–538\nstriping, 202\nstructured commands, 297\ngawk, 605–609\ndo-while statement, \n608–609\nif statement, 605–607\nfor statement, 609\nwhile statement, 607–608\ngawk program, 761–762\nzsh, 640–641\nStructured Query Language (SQL), \n684\nstub functions, 479\nsu, 224\nsubshell, 117, 121\nbackground mode commands, \n121–123\nbackground process lists, \n123–124\ncommand substitution, 279\nenvironment variables, 142\nglobal, 137\nexamining co-processing, \n124–125\nforking, 126, 127\nglobal environment variables, \n137\nuses of, 121–125\nsubstitution command, \n563–565, 752\n\n779\nIndex\nbindex.indd  12/16/2014  Page  779\nindividual word replacement, \n576–577\nparentheses in, 576\nsubstitution fl ags, 516–518, 752\nsudo, 217, 664\nmounting media, 97\nSUID. See set user ID\nSun Microsystems, 192\nsuppressing command output, \n410–411\nswap space, 5–6\nswapping out, 6\nsymbolic links, 68–70, 77, 624\nsysstat, 228–229\nsysstat, 231–232\nsystem accounts, 163\ngroups, 173\nsystem environment variables, \n136\ninteractive shell, 156\nlocating, 150–157\nlogin shell, 150–155\nnon-interactive shell, 156–157\nsystem information, 93\nsystem memory, status, 94\nsystem memory management, 5–6\nsystime, 617\nsysv fi lesystem, 8\nT\ntab activity, 39\ntab auto-complete, 68\ntab silence, 39\ntables, creating in MySQL, \n688–690\ntac command, 572\ntail, 81–82\ntailpacking, 191\ntar, 110–111, 646\ntarball, 110–111, 228, 646\ntasks, 94\ntcsh, 114\ntcsh shell, 10\ntee, 414–415\nTektronix 4014, 41\nteletypewriter, 26\ntemporary directory, 413–414\ntemporary fi les, 411–414\ntemporary redirection, 400–401\nTERM signal, 95\nterminal (TTY), 86\nterminal emulation, 24–25, 28–29\ncolor, 59\nTerminator, 29\nTerminology, 29\ntest command, 304–324, 340, \n574–575, 630–631\nfi le comparisons, 313–324\nchecking directories, \n314–315\nchecking for fi le, 316–317\nchecking for object \nexistence, 315–316\ndate, 322–324\ndefault group, 322\nempty fi les, 318–319\nexecute permission, 321\nownership, 321–322\nread access, 317–318\nwrite permission, 319–320\nnumeric comparisons, \n307–308\nstring comparisons, 308–313\nequality, 309–310\norder, 310–312\nsize, 312–313\ntest commands, 343–344\nmultiple, 344–346\ntesting parameters, 370\ntext\ninserting, 577\ninserting and appending with \nsed, 523–525\nmanipulating, 505–516\nremoving HTML tags from, \n586–588\ntext menus\ncreating, 477–483\nmenu functions, 479–480\nmenu layout, 478–479\nmenu logic, 480–481\ntext mode virtual consoles, 26\ntext pattern fi lters, 520, 526\ntext strings\nEOF, 291–292\nbash key combination, 511\nmysql scripting, 694\nscripting, 272\nsed, 537–538\ntextbelt.com/text, 732–734\ntextbox widget, 488–489\ntilda, 29\ntime\nepoch, 617\ngawk, f unctions, 616–617\ntimestamps\nanacron, 444\nhourly backups, 652–653, 655\nlog fi les, 104\nrenaming fi les, 71\n/tmp directory, 411\ntop, 93–95\nTorvalds, Linus, 4–5\ntouch, 64, 177\ntrailing blank lines, 586\ntransform command, 755\ntransform command, 527\ntrap, 422–423, 425\ntrapping script exit, 423–424\ntrapping signals, 422–423\ntraps\nmodifying or removing, \n424–427\nscript exits, 423–424\ntty, 26\nTTY. See terminal\ntype, 126\nfi lesystems, 196–197\ntypeset command, 639\nU\nUbuntu Linux, 19, 212, 220\ndefault shell, 624\n/etc/profile, 151\nGNOME terminal, 30\ninstalling MySQL, 682\nLiveCD, 20\nls alias, 131\nLynx confi guration fi le \nlocation, 700\nMir display server, 11\nPATH environment variable, \n148\nUnity desktop, 13–14\nufs fi lesystem, 8\nUID. See user ID\n$UID, 275\numask, 177–179\numount, 99–100\numsdos fi lesystem, 8\n\n780\nIndex\nbindex.indd  12/16/2014  Page  780\nuninstalling software, aptitude, \n218–219\nUnity desktop, 13–14, 30\nUniversity of California, Berkeley, \n89\nunmounting media, 99–100\nunset, 143\nuntil command, 346–347\nuntil loops, 641\nnested, 349–350\nupdating software, \naptitude, \n217–218\nURLs, 721\nchecking, 724–726\ntesting, 723–724\nurpm, 221\nbroken dependencies, 227\ninstalling software, 224\nlisting installed software, 221\npackage details, 222\nrepositories, 228\nuninstalling software, 225\nupdating software, 225\nUSB memory sticks, 21\n$USER, 275\nuser accounts, 161\ncreating multiple, 361–362\nmanaging, 656–673\ncreating script for, 665–671\ndetermining existence, \n661–662\nfi nding fi les, 664–665\ngetting name, 657–660\nremoving account, 665\nremoving processes, \n662–664\nrunning script for, 671–673\nverifying name, 660–661\nMySQL, 687–688\nroot, 163\nchanging fi le ownership, \n182\n/etc/shadow fi le, 164\nmounting media, 97\nuser ID (UID), 161, 274\nreserved, 163\nuser input, 388–392\nuser variables, 275–277\nuseradd, 164–168\nparameters\nchange default values, 167\ncommand line, 166–167\n-D, 165, 167\nuser-defi ned variables, 138–142\ndash shell, 627\ngawk, 598–600\nglobal, 140–142\nlocal, 139–140\nuserdel, 168\nusermod, 169\nadding users to groups, 173\nusers\nadding new, 164–168\nchecking if accepting \nmessages, 714–715\nchecking if logged in, 713–714\ncreating multiple accounts, \n361–362\ncurrently logged in, 710\nfi le ownership, 181–182\nmedia access, 98\nmodifying, 168–172\nprocesses, 662–664\nremoving, 168\n/usr/bin/batch, 439\n/usr/sbin directory, 165\nUXterm, 29\nV\nvariable arrays, 158–159\n${variable} format, 275\nvariables. See also environment \nvariables\ncommand substitution, \n277–279\nfunctions\noutput assignment to, \n455–456\nusing, 456–461\ngawk, 511–512, 591–600\nARGC, 595–596\nARGV, 595–596\narray variable assignment, \n600–601\nassigning in scripts, \n598–599\nassigning on command \nline, 599–600\nbuilt-in, 592–600, 759–760\ndata, 595–597\ndata fi eld, 592\nENVIRON, 596\nfi eld and record separator, \n592–595\nFIELDWIDTHS, 593–594\nFNR, 596–597\nFS, 592\nNF, 596–597\nNR, 596–597\nOFS, 592–593\nORS, 594\nRS, 594\nmultiple, 342–343\nreading list from, 335–336\nscripting, 274–278\nspecial parameter \nvariables, 371–375\nuser, 275–277\nuser-defi ned, 138–142\ndash shell, 627\ngawk, 598–600\nglobal, 140–142\nlocal, 139–140\n/var/spool/anacron, 444\n/var/spool/at, 438\nvfat fi lesystem, 8, 97, 98\nVFS. See Virtual Files System\nVG. See volume group\nvgchange, 209\nvgcreate, 205\nvgdisplay, 205\nvgextend, 209\nvgreduce, 209\nvgremove, 209\nvi, 234–235\nvi editor, 233\nviewing fi le contents, 77–83\nvim, 233–240\nbasics, 235–237\ncopying and pasting, 238–239\nediting, 238\ninstalling, 235\nmodes, 236–237\nsearching and substituting, \n239–240\nvisual mode, 239\nvim, 505\nvirtual consoles, 24, 25–28\nappearance, 27\ntext mode, 26\nvirtual directories, 53, 57\nmounting media, 98\n\n781\nIndex\nbindex.indd  12/16/2014  Page  781\nVirtual Files System (VFS), 9\nvirtual memory, 5, 6\nvisual mode, 239\nvolume group (VG), 200\ncreating, 205–206\nVT102, 41, 42\nVT220, 41, 42\nW\nw command, 530–531\nWayland, 11\nWayland Compositor, 25\nwc command, 280\nweb addresses, 721\nchecking, 724–726\ntesting, 723–724\nweb page information, 726\nwget, 720–722\n--spider option, 724\nWHERE clause, 691, 692\nwhich, 126, 243–244\nwhile command, 343–346, 361\nformat, 343–344\nmultiple test commands, \n344–346\nwhile loop, 378, 640–641\narchiving data fi les script, 647\nlong messages, 717\nmenu dialog, 495\nnested, 348–350\nreading fi les, 391–392\ntext menus, 482\nwhile statement, 607–608\nwho, 269–270, 397, 710\n-s option, 710\n-T option, 710–711, 714\nwhoeson, 495\nwidgets libraries, 25\nkdialog, 496–499\nzenity, 500–504\nwildcard characters, 63, 535–536\naptitude search, 215\ncron tables, 442\nkillall command, 96\nreading directory using, \n339–340\nreplacement strings, 575–576\nstring parameter expansion, \n724–725\nwildcard metacharacters, 63, 67\nwindow manager, 25\nwritable-snapshot, 192\nwrite, 711–712, 715–716\nwrite access, 319–320\nwriteback mode journaling, 189, \n190, 192\nwriting to fi le, 755\nWterm, 29\nX\nX Intrinsics, 25\nX Window system, 7, 11–12, 496\nxargs command, 663–664\nXfce, 16\nXfce4 Terminal, 29\nXFS fi lesystem, 8, 191–192\nXML. See Extensible Markup \nLanguage\nXserver, 25\nxterm, 29, 41–44\naccessing, 42–43\ncommand line parameters, \n43–44\nman page, 49\nY\nYahoo! weather, 702–704\nYakuake, 29\nyesno widget, 487\nyum, 221–228\nbroken dependencies, \n225–227\ninstalling software, \n223–224\nlisting installed packages, \n221–223\nrepositories, 227–228\nuninstalling software, 225\nupdating software, \n224–225\nyum clean all, 226\nyum deplist, 226\nyum erase, 225\nyum install, 223, 244\nyum list, 224, 243\nyum list installed, 221\nyum localinstall, 224\nyum remove, 225\nyum update, 224, 227\nZ\nz shell. See zsh\nzenity, 500–504\nusing in scripts, 501–503\nwidgets, 500–501\nzero, 292\nZFS fi lesystem, 192, 199\nzftp, 638\nzip, 109, 110\nzip fi les, 108–109\nzmodload command, 637–638\nzsh (zshell), 10, 288\nbuilt-in commands, 633–638\nadd-in modules, 636–637\ncore, 634–636\nviewing, adding, removing \nmodules, 637–638\noptions, 632–633\nscripting, 638–642\ncalculations, 639–640\nfunctions, 641–642\nmathematical functions, \n640\nmathematical operations, \n639–640\nstructured commands, \n640–641\nzsh/mathfunc module, 640\nzypper , 221\nbroken dependencies, 227\ninstalling software, 224\nlisting installed software, \n221\npackage details, 222\nrepositories, 228\nuninstalling software, 225\nupdating software, 225 \n\nWILEY END USER LICENSE AGREEMENT\nGo to www.wiley.com/go/eula to access Wiley’s ebook EULA.",
  "version": "1.10.100"
}
